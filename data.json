{"key":"Dog","link":"https:\/\/en.wikipedia.org\/wiki\/Dog","headline":"Dog - Wikipedia","content":"\n The dog (Canis familiaris[4][5] or Canis lupus familiaris[5]) is a domesticated descendant of the wolf. Also called the domestic dog, it is derived from extinct gray wolves,[6][7] and the gray wolf is the dog's closest living relative.[8] The dog was the first species to be domesticated[9][8] by humans. Experts estimate that hunter-gatherers domesticated dogs more than 15,000 years ago in Oberkassel, Bonn,[7] which was before the development of agriculture.[1] Due to their long association with humans, dogs have expanded to a large number of domestic individuals[10] and gained the ability to thrive on a starch-rich diet that would be inadequate for other canids.[11]\n The dog has been selectively bred over millennia for various behaviors, sensory capabilities, and physical attributes.[12] Dog breeds vary widely in shape, size, and color. They perform many roles for humans, such as hunting, herding, pulling loads, protection, assisting police and the military, companionship, therapy, and aiding disabled people. Over the millennia, dogs became uniquely adapted to human behavior, and the human–canine bond has been a topic of frequent study.[13] This influence on human society has given them the sobriquet of \"man's best friend\".[14]\n In 1758, the Swedish botanist and zoologist Carl Linnaeus published in his Systema Naturae, the two-word naming of species (binomial nomenclature). Canis is the Latin word meaning \"dog\",[15] and under this genus, he listed the domestic dog, the wolf, and the golden jackal. He classified the domestic dog as Canis familiaris and, on the next page, classified the grey wolf as Canis lupus.[2] Linnaeus considered the dog to be a separate species from the wolf because of its upturning tail (cauda recurvata), which is not found in any other canid.[16]\n In 1999, a study of mitochondrial DNA (mtDNA) indicated that the domestic dog may have originated from the grey wolf, with the dingo and New Guinea singing dog breeds having developed at a time when human communities were more isolated from each other.[17] In the third edition of Mammal Species of the World published in 2005, the mammalogist W. Christopher Wozencraft listed under the wolf Canis lupus its wild subspecies and proposed two additional subspecies, which formed the domestic dog clade: familiaris, as named by Linnaeus in 1758 and, dingo named by Meyer in 1793. Wozencraft included hallstromi (the New Guinea singing dog) as another name (junior synonym) for the dingo. Wozencraft referred to the mtDNA study as one of the guides informing his decision.[3] Mammalogists have noted the inclusion of familiaris and dingo together under the \"domestic dog\" clade[18] with some debating it.[19]\n In 2019, a workshop hosted by the IUCN\/Species Survival Commission's Canid Specialist Group considered the dingo and the New Guinea singing dog to be feral Canis familiaris and therefore did not assess them for the IUCN Red List of Threatened Species.[4]\n The earliest remains generally accepted to be those of a domesticated dog were discovered in Bonn-Oberkassel, Germany. Contextual, isotopic, genetic, and morphological evidence shows that this dog was not a local wolf.[20] The dog was dated to 14,223 years ago and was found buried along with a man and a woman, all three having been sprayed with red hematite powder and buried under large, thick basalt blocks. The dog had died of canine distemper.[21] Earlier remains dating back to 30,000 years ago have been described as Paleolithic dogs, but their status as dogs or wolves remains debated[22] because considerable morphological diversity existed among wolves during the Late Pleistocene.[1]\n This timing indicates that the dog was the first species to be domesticated[9][8] in the time of hunter-gatherers,[7] which predates agriculture.[1] DNA sequences show that all ancient and modern dogs share a common ancestry and descended from an ancient, extinct wolf population which was distinct from the modern wolf lineage.[6][7]\n The dog is a classic example of a domestic animal that likely travelled a commensal pathway into domestication.[22][23] The questions of when and where dogs were first domesticated have taxed geneticists and archaeologists for decades.[9] Genetic studies suggest a domestication process commencing over 25,000 years ago, in one or several wolf populations in either Europe, the high Arctic, or eastern Asia.[10] In 2021, a literature review of the current evidence infers that the dog was domesticated in Siberia 23,000 years ago by ancient North Siberians, then later dispersed eastward into the Americas and westward across Eurasia,[20] with dogs likely accompanying the first humans to inhabit the Americas.[24]\n Dogs are the most variable mammal on earth with around 450 globally recognized dog breeds.[10][25] In the Victorian era, directed human selection developed the modern dog breeds, which resulted in a vast range of phenotypes.[8] Most breeds were derived from small numbers of founders within the last 200 years,[8][10] and since then dogs have undergone rapid phenotypic change and were formed into today's modern breeds due to artificial selection imposed by humans. The skull, body, and limb proportions vary significantly between breeds, with dogs displaying more phenotypic diversity than can be found within the entire order of carnivores. These breeds possess distinct traits related to morphology, which include body size, skull shape, tail phenotype, fur type and colour.[8] Their behavioural traits include guarding, herding, and hunting,[8] retrieving, and scent detection. Their personality traits include hypersocial behavior, boldness, and aggression.[8] Present day dogs are dispersed around the world.[10] An example of this dispersal is the numerous modern breeds of European lineage during the Victorian era.[7]\n All healthy dogs, regardless of their size and type, have an identical skeletal structure with the exception of the number of bones in the tail, although there is significant skeletal variation between dogs of different types.[26][27] The dog's skeleton is well adapted for running; the vertebrae on the neck and back have extensions for powerful back muscles to connect to, the long ribs provide plenty of room for the heart and lungs, and the shoulders are unattached to the skeleton allowing great flexibility.[26][27]\n Compared to the dog's wolf-like ancestors, selective breeding since domestication has seen the dog's skeleton greatly enhanced in size for larger types as mastiffs and miniaturised for smaller types such as terriers; dwarfism has been selectively utilised for some types where short legs are advantageous such as dachshunds and corgis.[27] Most dogs naturally have 26 vertebrae in their tails, but some with naturally short tails have as few as three.[26]\n The dog's skull has identical components regardless of breed type, but there is significant divergence in terms of skull shape between types.[27][28] The three basic skull shapes are the elongated dolichocephalic type as seen in sighthounds, the intermediate mesocephalic or mesaticephalic type, and the very short and broad brachycephalic type exemplified by mastiff type skulls.[27][28]\n A dog's senses include vision, hearing, smell, taste, touch. One study suggests that dogs can feel Earth's magnetic field.[29]\n The coats of domestic dogs are of two varieties: \"double\" being familiar with dogs (as well as wolves) originating from colder climates, made up of a coarse guard hair and a soft down hair, or \"single\", with the topcoat only. Breeds may have an occasional \"blaze\", stripe, or \"star\" of white fur on their chest or underside.[30] Premature graying can occur in dogs from as early as one year of age; this is associated with impulsive behaviors, anxiety behaviors, fear of noise, and fear of unfamiliar people or animals.[31]\n As with many canids, one of the primary functions of a dog's tail is to communicate their emotional state, which can be crucial in getting along with others.[32] In some hunting dogs, the tail is traditionally docked to avoid injuries.\n Some breeds of dogs are prone to specific genetic ailments such as elbow and hip dysplasia, blindness, deafness, pulmonic stenosis, a cleft palate, and trick knees. Two severe medical conditions significantly affecting dogs are pyometra, affecting unspayed females of all breeds and ages, and gastric dilatation volvulus (bloat), which affects larger breeds or deep-chested dogs. Both of these are acute conditions and can kill rapidly. Dogs are also susceptible to parasites such as fleas, ticks, mites, hookworms, tapeworms, roundworms, and heartworms, which is a roundworm species that lives in the hearts of dogs.\n Several human foods and household ingestibles are toxic to dogs, including chocolate solids, causing theobromine poisoning, onions and garlic, causing thiosulphate, sulfoxide or disulfide poisoning, grapes and raisins, macadamia nuts, and xylitol.[33] The nicotine in tobacco can also be dangerous to dogs. Signs of ingestion can include copious vomiting (e.g., from eating cigar butts) or diarrhea. Some other symptoms are abdominal pain, loss of coordination, collapse, or death.[34][page needed]\n Dogs are also vulnerable to some of the same health conditions as humans, including diabetes, dental and heart disease, epilepsy, cancer, hypothyroidism, and arthritis.\n The typical lifespan of dogs varies widely among breeds, but the median longevity (the age at which half the dogs in a population have died and half are still alive) ranges from 10 to 13 years.[35][36] The median longevity of mixed-breed dogs, taken as an average of all sizes, is one or more years longer than that of purebred dogs when all breeds are averaged.[35][36][37] For dogs in England, increased body weight has been found to be negatively correlated with longevity (i.e., the heavier the dog, the shorter its lifespan), and mixed-breed dogs live on average 1.2 years longer than purebred dogs.[38]\n In domestic dogs, sexual maturity happens around six months to one year for both males and females, although this can be delayed until up to two years of age for some large breeds, and is the time at which female dogs will have their first estrous cycle. They will experience subsequent estrous cycles semiannually, during which the body prepares for pregnancy. At the peak of the cycle, females will become estrous, mentally and physically receptive to copulation. Because the ova survive and can be fertilized for a week after ovulation, more than one male can sire the same litter.[12] Fertilization typically occurs two to five days after ovulation; 14–16 days after ovulation, the embryo attaches to the uterus, and after seven to eight more days, a heartbeat is detectable.[39][40]\n Dogs bear their litters roughly 58 to 68 days after fertilization,[12][41] with an average of 63 days, although the length of gestation can vary. An average litter consists of about six puppies.[42]\n Neutering is the sterilization of animals, usually by removing the male's testicles or the female's ovaries and uterus, to eliminate the ability to procreate and reduce sex drive. Because of dogs' overpopulation in some countries, many animal control agencies, such as the American Society for the Prevention of Cruelty to Animals (ASPCA), advise that dogs not intended for further breeding should be neutered, so that they do not have undesired puppies that may later be euthanized.[43] According to the Humane Society of the United States, three to four million dogs and cats are euthanized each year.[44] Many more are confined to cages in shelters. Spaying or castrating dogs is considered a major factor in keeping overpopulation down.[45]\n Neutering reduces problems caused by hypersexuality, especially in male dogs.[46] Spayed female dogs are less likely to develop cancers affecting the mammary glands, ovaries, and other reproductive organs.[47][page needed] However, neutering increases the risk of urinary incontinence in female dogs,[48] prostate cancer in males,[49] and osteosarcoma, hemangiosarcoma, cruciate ligament rupture, obesity, and diabetes mellitus in either sex.[50]\n A common breeding practice for pet dogs is mating between close relatives (e.g., between half and full siblings).[51] Inbreeding depression is considered to be due mainly to the expression of homozygous deleterious recessive mutations.[52] Outcrossing between unrelated individuals, including dogs of different breeds, results in the beneficial masking of deleterious recessive mutations in progeny.[53]\n In a study of seven dog breeds (the Bernese Mountain Dog, Basset Hound, Cairn Terrier, Brittany, German Shepherd Dog, Leonberger, and West Highland White Terrier), it was found that inbreeding decreases litter size and survival.[54] Another analysis of data on 42,855 Dachshund litters found that as the inbreeding coefficient increased, litter size decreased and the percentage of stillborn puppies increased, thus indicating inbreeding depression.[55] In a study of Boxer litters, 22% of puppies died before reaching 7 weeks of age. Stillbirth was the most frequent cause of death, followed by infection. Mortality due to infection increased significantly with increases in inbreeding.[56]\n Dog behavior is the internally coordinated responses (actions or inactions) of the domestic dog (individuals or groups) to internal and external stimuli.[57] Dogs' minds have been shaped by millennia of contact with humans; dogs have acquired the ability to understand and communicate with humans; they are uniquely attuned to human behaviors.[13] Behavioral scientists thought that a set of social-cognitive abilities in domestic dogs that are not possessed by the dog's canine relatives or other highly intelligent mammals, such as great apes, are parallel to children's social-cognitive skills.[58]\n Unlike other domestic species selected for production-related traits, dogs were initially selected for their behaviors.[59][60] In 2016, a study found that only 11 fixed genes showed variation between wolves and dogs.[61] These gene variations were unlikely to have been the result of natural evolution and indicate selection on both morphology and behavior during dog domestication. These genes have been shown to affect the catecholamine synthesis pathway, with the majority of the genes affecting the fight-or-flight response[60][62] (i.e., selection for tameness) and emotional processing.[60] Dogs generally show reduced fear and aggression compared with wolves.[60][63] Some of these genes have been associated with aggression in some dog breeds.[60] Traits of high sociability and lack of fear in dogs may include genetic modifications related to Williams-Beuren syndrome in humans, which cause hypersociability at the expense of problem-solving ability.[64]\n Researchers have tested dogs' ability to perceive information, retain it as knowledge, and apply it to solve problems. Studies of two dogs suggest that dogs can learn by inference. A study with Rico, a Border Collie, showed that he knew the labels of over 200 different items. He inferred the names of novel things by exclusion learning and correctly retrieved those new items immediately and after four weeks of the initial exposure. A study of another Border Collie, Chaser, documented that he had learned the names and could associate them by verbal command with over 1,000 words.[65]\n One study of canine cognitive abilities found that dogs' capabilities are no more exceptional than those of other animals, such as horses, chimpanzees, or cats.[66] One limited study of 18 household dogs found that they lacked spatial memory and were more focused on the \"what\" of a task than the \"where\".[67]\n Dogs demonstrate a theory of mind by engaging in deception.[68] An experimental study showed evidence that Australian dingos can outperform domestic dogs in non-social problem-solving, indicating that domestic dogs may have lost much of their original problem-solving abilities once they joined humans.[69] Another study showed that after undergoing training to solve a simple manipulation task, dogs faced with an unsolvable version of the same problem look at humans, while socialized wolves do not.[70]\n Dog communication is how dogs convey information to other dogs, understand messages from humans and translate the information that dogs are transmitting.[71]: xii  Communication behaviors of dogs include eye gaze, facial expression,[72][73] vocalization, body posture (including movements of bodies and limbs), and gustatory communication (scents, pheromones, and taste). Humans communicate to dogs by using vocalization, hand signals, and body posture. As dogs, with their acute sense of hearing, they rely on the auditory aspect of communication for understanding and responding to various cues, including the distinctive barking patterns that convey different messages.\n Dogs commonly mark their territories by scent-marking them by urinating on them. Dogs may also do it to communicate anxiety or frustration. Dogs are more likely to leave urine messages when entering a new environment.[74][75]\n The dog is probably the most widely abundant large carnivoran living in the human environment.[76][77]\nIn 2013, the estimated global dog population was between 700 million[78] and 987 million.[79] About 20% of dogs live as pets in developed countries.[80] In the developing world, dogs are typically feral or communally owned, with pet dogs being uncommon. Most of these dogs live their lives as scavengers and have never been owned by humans, with one study showing their most common response when approached by strangers is to run away (52%) or respond aggressively (11%).[81] The majority of research on dog cognition has focused on pet dogs living in human homes.[82]\n Although dogs are the most abundant and widely distributed terrestrial carnivores, feral and free-ranging dogs' potential to compete with other large carnivores is limited by their strong association with humans.[76] For example, a review of the studies on dogs' competitive effects on sympatric carnivores did not mention any research on competition between dogs and wolves.[83][84] Although wolves are known to kill dogs, they tend to live in pairs or in small packs in areas where they are highly persecuted, giving them a disadvantage when facing large dog groups.[83][85] In some instances, wolves have displayed an uncharacteristic fearlessness of humans and buildings when attacking dogs, to the extent that they have to be beaten off or killed.[86] Although the numbers of dogs killed each year are relatively low, it induces a fear of wolves entering villages and farmyards to take dogs, and losses of dogs to wolves have led to demands for more liberal wolf hunting regulations.[83]\n Coyotes and big cats have also been known to attack dogs. In particular, leopards are known to have a preference for dogs and have been recorded to kill and consume them, no matter their size.[87] Siberian tigers in the Amur River region have killed dogs in the middle of villages. This indicates that the dogs were targeted. Amur tigers will not tolerate wolves as competitors within their territories, and the tigers could be considering dogs in the same way.[88] Striped hyenas are known to kill dogs in their range.[89]\n Dogs have been described as omnivores.[12][90][91] Compared to wolves, dogs from agricultural societies have extra copies of amylase and other genes involved in starch digestion that contribute to an increased ability to thrive on a starch-rich diet.[11] Similar to humans, some dog breeds produce amylase in their saliva and are classified as having a high starch diet.[92] However, more like cats and less like other omnivores, dogs can only produce bile acid with taurine and they cannot produce vitamin D, which they obtain from animal flesh.\nOf the twenty-one amino acids common to all life forms (including selenocysteine), dogs cannot synthesize ten: arginine, histidine, isoleucine, leucine, lysine, methionine, phenylalanine, threonine, tryptophan, and valine.[93][94][95]\nAlso more like cats, dogs require arginine to maintain nitrogen balance. These nutritional requirements place dogs halfway between carnivores and omnivores.[96]\n As a domesticated or semi-domesticated animal, the dog is nearly universal among human societies. Notable exceptions once included:\n Dogs were introduced to Antarctica as sled dogs, but were later outlawed by international agreement due to the possible risk of spreading infections.[105]\n Domestic dogs inherited complex behaviors, such as bite inhibition, from their wolf ancestors, who would have been pack hunters with complex body language. These sophisticated forms of social cognition and communication may account for their trainability, playfulness and ability to fit into human households and social situations,[106] probably also for early human hunter-gatherers.\n Dogs perform many roles for people, such as hunting, herding, pulling loads, protection, assisting police and the military, companionship and aiding disabled individuals. This influence on human society has given them the nickname \"man's best friend\" in the Western world. In some cultures, however, dogs are also a source of meat.[107][108]\n It is estimated that three-quarters of the world's dog population lives in the developing world as feral, village, or community dogs, with pet dogs uncommon.[109][page needed]\n \"The most widespread form of interspecies bonding occurs between humans and dogs\"[110] and the keeping of dogs as companions, particularly by elites, has a long history.[14] Pet dog populations grew significantly after World War II as suburbanization increased.[14] In the 1950s and 1960s, dogs were kept outside more often than they tend to be today[111] (the expression \"in the doghouse\" – recorded since 1932[112] – to describe exclusion from the group implies a distance between the doghouse and the home) and were still primarily functional, acting as a guard, children's playmate, or walking companion. From the 1980s, there have been changes in the pet dog's role, such as the increased role of dogs in the emotional support of their human guardians.[113][page needed] People and their dogs have become increasingly integrated and implicated in each other's lives.[114][page needed][115]\n Within the second half of the 20th century, the first dogs' social status major shift has been \"commodification\", shaping it to conform to social expectations of personality and behavior.[115] The second has been the broadening of the family's concept and the home to include dogs-as-dogs within everyday routines and practices.[115]\n A vast range of commodity forms aim to transform a pet dog into an ideal companion.[116] The list of goods, services, and places available for dogs is enormous.[116] Dog training books, classes, and television programs proliferated as the process of commodifying the pet dog continued.[117] The majority of contemporary dog owners describe their pet as part of the family, although some ambivalence about the relationship is evident in the popular reconceptualization of the dog-human family as a pack.[115] Some dog trainers, such as on the television program Dog Whisperer, have promoted a dominance model of dog-human relationships. However, it has been disputed that \"trying to achieve status\" is characteristic of dog-human interactions.[118] The idea of the \"alpha dog\" trying to be dominant is based on a controversial theory about wolf packs.[119][120] Increasingly, human family-members engage in activities centered on the dog's perceived needs and interests, or in which the dog is an integral partner, such as dog dancing and dog yoga.[116]\n According to statistics published by the American Pet Products Manufacturers Association in the National Pet Owner Survey in 2009–2010, an estimated 77.5 million people in the United States have pet dogs.[121] The same source shows that nearly 40% of American households own at least one dog, of which 67% own just one dog, 25% two dogs and nearly 9% more than two dogs. There does not seem to be any sex preference among dogs as pets, as the statistical data reveal an equal number of male and female pet dogs. Although several programs promote pet adoption, less than one-fifth of the owned dogs come from shelters.[121]\n A study using magnetic resonance imaging (MRI) to compare humans and dogs showed that dogs have the same response to voices and use the same parts of the brain as humans do. This gives dogs the ability to recognize human emotional sounds.[122][123][124]\n In addition to dogs' role as companion animals, dogs have been bred for herding livestock (collies, sheepdogs),[125][page needed][12] hunting (hounds, pointers)[126][page needed] and rodent control (terriers).[12] Other types of working dogs include search and rescue dogs,[127] detection dogs trained to detect illicit drugs[128] or chemical weapons;[129] guard dogs; dogs who assist fishermen with the use of nets; and dogs that pull loads.[12] In 1957, the dog Laika became the first animal to be launched into Earth orbit, aboard the Soviets' Sputnik 2; she died during the flight.[130][131]\n Various kinds of service dogs and assistance dogs, including guide dogs, hearing dogs, mobility assistance dogs and psychiatric service dogs, assist individuals with disabilities.[132][133] Some dogs owned by people with epilepsy have been shown to alert their handler when the handler shows signs of an impending seizure, sometimes well in advance of onset.[134]\n People often enter their dogs in competitions, such as breed-conformation shows or sports, including racing, sledding and agility competitions. In conformation shows, also referred to as breed shows, a judge familiar with the specific dog breed evaluates individual purebred dogs for conformity with their established breed type as described in the breed standard. As the breed standard only deals with the dog's externally observable qualities (such as appearance, movement and temperament), separately tested qualities (such as ability or health) are not part of the judging in conformation shows.\n Dog meat is consumed in some East Asian countries, including Korea,[135][page needed] China,[107] Vietnam[108] and the Philippines,[136] which dates back to antiquity.[137] Based on limited data, it is estimated that 13–16 million dogs are killed and consumed in Asia every year.[138] In China, debates have ensued over banning the consumption of dog meat.[139]  Following the Sui and Tang dynasties of the first millennium, however, people living on northern China's plains began to eschew eating dogs, which is likely due to Buddhism and Islam's spread, two religions that forbade the consumption of certain animals, including the dog. As members of the upper classes shunned dog meat, it gradually became a social taboo to eat it, even though the general population continued to consume it for centuries afterward.[citation needed] Dog meat is also consumed in some parts of Switzerland.[140] Other cultures, such as Polynesia and pre-Columbian Mexico, also consumed dog meat in their history. In some parts of Poland[141][142] and Central Asia,[143][144] dog fat is reportedly believed to be beneficial for the lungs. Proponents of eating dog meat have argued that placing a distinction between livestock and dogs is Western hypocrisy and that there is no difference in eating different animals' meat.[145][146][147][148]\n In Korea, the primary dog breed raised for meat, the Nureongi, differs from those breeds raised for pets that Koreans may keep in their homes.[149]\n The most popular Korean dog dish is called bosintang, a spicy stew meant to balance the body's heat during the summer months. Followers of the custom claim this is done to ensure good health by balancing one's gi, or the body's vital energy. A 19th-century version of bosintang explains that the dish is prepared by boiling dog meat with scallions and chili powder. Dog is not as widely consumed as beef, pork and chicken.[149]\n In 2018, the WHO reported that 59,000 people died globally from rabies, with 59.6% in Asia and 36.4% in Africa. Rabies is a disease for which dogs are the most important vector.[150] Dog bites affect tens of millions of people globally each year. Children in mid-to-late childhood are the largest percentage bitten by dogs, with a greater risk of injury to the head and neck. They are more likely to need medical treatment and have the highest death rate.[151] Sharp claws can lacerate flesh that can lead to serious infections.[152] In the U.S., cats and dogs are a factor in more than 86,000 falls each year.[153] It has been estimated that around 2% of dog-related injuries treated in U.K. hospitals are domestic accidents. The same study found that while dog involvement in road traffic accidents was difficult to quantify, dog-associated road accidents involving injury more commonly involved two-wheeled vehicles.[154]\n Toxocara canis (dog roundworm) eggs in dog feces can cause toxocariasis. In the United States, about 10,000 cases of Toxocara infection are reported in humans each year, and almost 14% of the U.S. population is infected.[155] Untreated toxocariasis can cause retinal damage and decreased vision.[156] Dog feces can also contain hookworms that cause cutaneous larva migrans in humans.[157][158]\n Dogs suffer from the same common disorders as humans; these include cancer, diabetes, heart disease and neurologic disorders. Their pathology is similar to humans, as is their response to treatment and their outcomes. Researchers are identifying the genes associated with dog diseases similar to human disorders, but lack mouse models to find cures for both dogs and humans. The genes involved in canine obsessive-compulsive disorders led to the detection of four genes in humans' related pathways.[10]\n The scientific evidence is mixed as to whether a dog's companionship can enhance human physical health and psychological well-being.[159] Studies suggest that there are benefits to physical health and psychological well-being[160] have been criticized for being poorly controlled.[161] It states that \"the health of elderly people is related to their health habits and social supports but not to their ownership of, or attachment to, a companion animal.\" Earlier studies have shown that people who keep pet dogs or cats exhibit better mental and physical health than those who do not, making fewer visits to the doctor and being less likely to be on medication than non-guardians.[162]\n A 2005 paper states \"recent research has failed to support earlier findings that pet ownership is associated with a reduced risk of cardiovascular disease, a reduced use of general practitioner services, or any psychological or physical benefits on health for community dwelling older people\". Research has pointed to significantly less absenteeism from school through sickness among children who live with pets.\"[159] In one study, new guardians reported a highly significant reduction in minor health problems during the first month following pet acquisition. This effect was sustained in those with dogs through to the end of the study.[163]\n People with pet dogs took considerably more physical exercise than those with cats and those without pets. The results provide evidence that keeping pets may have positive effects on human health and behavior and that for guardians of dogs, these effects are relatively long-term.[163] Pet guardianship has also been associated with increased coronary artery disease survival. Human guardians are significantly less likely to die within one year of an acute myocardial infarction than those who did not own dogs.[164] The association between dog ownership and adult physical activity levels has been reviewed by several authors.[165]\n The health benefits of dogs can result from contact with dogs in general, not solely from having dogs as pets. For example, when in a pet dog's presence, people show reductions in cardiovascular, behavioral and psychological indicators of anxiety.[166] Other health benefits are gained from exposure to immune-stimulating microorganisms, which can protect against allergies and autoimmune diseases according to the hygiene hypothesis. The benefits of contact with a dog also include social support, as dogs cannot only provide companionship and social support themselves but also act as facilitators of social interactions between humans.[167] One study indicated that wheelchair users experience more positive social interactions with strangers when accompanied by a dog than when they are not.[168] In 2015, a study found that pet owners were significantly more likely to get to know people in their neighborhood than non-pet owners.[169]\n Using dogs and other animals as a part of therapy dates back to the late 18th century, when animals were introduced into mental institutions to help socialize patients with mental disorders.[170] Animal-assisted intervention research has shown that animal-assisted therapy with a dog can increase social behaviors, such as smiling and laughing, among people with Alzheimer's disease.[171] One study demonstrated that children with ADHD and conduct disorders who participated in an education program with dogs and other animals showed increased attendance, increased knowledge and skill objectives and decreased antisocial and violent behavior compared with those not in an animal-assisted program.[172]\n Dogs were depicted to symbolize guidance, protection, loyalty, fidelity, faithfulness, alertness, and love.[173] In ancient Mesopotamia, from the Old Babylonian period until the Neo-Babylonian, dogs were the symbol of Ninisina, the goddess of healing and medicine,[174] and her worshippers frequently dedicated small models of seated dogs to her.[174] In the Neo-Assyrian and Neo-Babylonian periods, dogs were used as emblems of magical protection.[174] In China, Korea and Japan, dogs are viewed as kind protectors.[175]\n In mythology, dogs often serve as pets or as watchdogs.[175] Stories of dogs guarding the gates of the underworld recur throughout Indo-European mythologies[176][177] and may originate from Proto-Indo-European religion.[176][177] In Greek mythology, Cerberus is a three-headed, dragon-tailed watchdog who guards the gates of Hades.[175] Dogs are also associated with the Greek goddess Hecate.[178] In Norse mythology, a dog called Garmr guards Hel, a realm of the dead.[175] In Persian mythology, two four-eyed dogs guard the Chinvat Bridge.[175] In Welsh mythology, Annwn is guarded by Cŵn Annwn.[175] In Hindu mythology, Yama, the god of death, owns two watchdogs named Shyama and Sharvara who have four eyes. They are said to watch over the gates of Naraka.[179] A black dog is also considered to be the vahana (vehicle) of Bhairava (an incarnation of Shiva).[180]\n In Christianity, dogs represent faithfulness.[175] Within the Roman Catholic denomination specifically, the iconography of Saint Dominic includes a dog, after the saint's mother dreamt of a dog springing from her womb and becoming pregnant shortly after that.[181] As such, the Dominican Order (Ecclesiastical Latin: Domini canis) means \"dog of the Lord\" or \"hound of the Lord\" (Ecclesiastical Latin: Domini canis).[181] In Christian folklore, a church grim often takes the form of a black dog to guard Christian churches and their churchyards from sacrilege.[182] Jewish law does not prohibit keeping dogs and other pets. Jewish law requires Jews to feed dogs (and other animals that they own) before themselves and make arrangements for feeding them before obtaining them.[citation needed] The view on dogs in Islam is mixed, with some schools of thought viewing them as unclean,[175] although Khaled Abou El Fadl states that this view is based on \"pre-Islamic Arab mythology\" and \"a tradition to be falsely attributed to the Prophet.\"[183] Therefore, Sunni Malaki and Hanafi jurists permit the trade of and keeping of dogs as pets.[184]\n"}
{"key":"Dog","link":"https:\/\/en.wikipedia.org\/wiki\/Main_Page","headline":"Wikipedia, the free encyclopedia","content":"Pinnipeds, including true seals, walruses and sea lions and fur seals, are a widely distributed and diverse clade of semiaquatic, mostly marine mammals of the order Carnivora. There are 34 living species. They have streamlined bodies and four limbs that have evolved into flippers. Males typically mate with more than one female, and the females raise the pups, often born in the spring and summer months. Pinnipeds generally prefer colder waters and spend most of their time in the water, but come ashore to mate, give birth, molt or escape from predators such as sharks and orcas. Humans have hunted seals since at least the Stone Age, and commercial sealing had a devastating effect on some species from the introduction of firearms through the 1960s. Populations have also been reduced or displaced by accidental trapping and marine pollution. All pinniped species are now afforded some protections under international law. (Full article...)\n March 22: World Water Day\n From 1970 to 2023, 279 players have been selected by the Portland Trail Blazers, an American professional basketball team in the National Basketball Association (NBA) based in Portland, Oregon. The franchise was founded in the 1970–71 NBA season and made their first draft pick in the 1970 NBA draft. Portland has held the first overall pick four times, selecting LaRue Martin in 1972, Bill Walton in 1974, Mychal Thompson in 1978, and Greg Oden in 2007. Three players drafted by Portland won the Rookie of the Year Award; Geoff Petrie (pictured), who was the franchise's inaugural draft pick, won in 1971, Sidney Wicks in 1972, and Damian Lillard in 2013. Four drafted players have been elected to the Naismith Memorial Basketball Hall of Fame: Walton, Dražen Petrović, Clyde Drexler, and Arvydas Sabonis, while nine have had their jersey number retired by the team. Walton and Drexler were named to the NBA 50th and 75th anniversary teams in 1996 and 2021, respectively, while Lillard was named to the 75th anniversary team. (Full list...)\n Laocoön and His Sons is an ancient sculpture which was excavated in Rome, Italy, in 1506. It depicts the Trojan priest Laocoön and his sons Antiphantes and Thymbraeus being attacked by sea serpents. The figures in the statue are nearly life-sized, with the entire group measuring just over 2 metres (6 ft 7 in) in height. The statue is likely to be the same one that received praise from the Roman writer Pliny the Elder, who attributed the work (then housed in the palace of the emperor Titus) to three Greek sculptors from the island of Rhodes: Agesander, Athenodoros and Polydorus, but he did not mention the work's date or patron. Modern scholars are not certain of the work's origins; it might be an original work or a copy of an earlier bronze sculpture. After its discovery, Laocoön and His Sons was put on public display in the Vatican Museums, where it remains.\n Sculpture credit: attributed to Agesander, Athenodoros and Polydorus; photographed by Wilfredo Rodríguez Wikipedia is written by volunteer editors and hosted by the Wikimedia Foundation, a non-profit organization that also hosts a range of other volunteer projects:\n This Wikipedia is written in English. Many other Wikipedias are available; some of the largest are listed below.\n"}
{"key":"Dog","link":"https:\/\/en.wikipedia.org\/wiki\/Dog","headline":"Dog - Wikipedia","content":"\n The dog (Canis familiaris[4][5] or Canis lupus familiaris[5]) is a domesticated descendant of the wolf. Also called the domestic dog, it is derived from extinct gray wolves,[6][7] and the gray wolf is the dog's closest living relative.[8] The dog was the first species to be domesticated[9][8] by humans. Experts estimate that hunter-gatherers domesticated dogs more than 15,000 years ago in Oberkassel, Bonn,[7] which was before the development of agriculture.[1] Due to their long association with humans, dogs have expanded to a large number of domestic individuals[10] and gained the ability to thrive on a starch-rich diet that would be inadequate for other canids.[11]\n The dog has been selectively bred over millennia for various behaviors, sensory capabilities, and physical attributes.[12] Dog breeds vary widely in shape, size, and color. They perform many roles for humans, such as hunting, herding, pulling loads, protection, assisting police and the military, companionship, therapy, and aiding disabled people. Over the millennia, dogs became uniquely adapted to human behavior, and the human–canine bond has been a topic of frequent study.[13] This influence on human society has given them the sobriquet of \"man's best friend\".[14]\n In 1758, the Swedish botanist and zoologist Carl Linnaeus published in his Systema Naturae, the two-word naming of species (binomial nomenclature). Canis is the Latin word meaning \"dog\",[15] and under this genus, he listed the domestic dog, the wolf, and the golden jackal. He classified the domestic dog as Canis familiaris and, on the next page, classified the grey wolf as Canis lupus.[2] Linnaeus considered the dog to be a separate species from the wolf because of its upturning tail (cauda recurvata), which is not found in any other canid.[16]\n In 1999, a study of mitochondrial DNA (mtDNA) indicated that the domestic dog may have originated from the grey wolf, with the dingo and New Guinea singing dog breeds having developed at a time when human communities were more isolated from each other.[17] In the third edition of Mammal Species of the World published in 2005, the mammalogist W. Christopher Wozencraft listed under the wolf Canis lupus its wild subspecies and proposed two additional subspecies, which formed the domestic dog clade: familiaris, as named by Linnaeus in 1758 and, dingo named by Meyer in 1793. Wozencraft included hallstromi (the New Guinea singing dog) as another name (junior synonym) for the dingo. Wozencraft referred to the mtDNA study as one of the guides informing his decision.[3] Mammalogists have noted the inclusion of familiaris and dingo together under the \"domestic dog\" clade[18] with some debating it.[19]\n In 2019, a workshop hosted by the IUCN\/Species Survival Commission's Canid Specialist Group considered the dingo and the New Guinea singing dog to be feral Canis familiaris and therefore did not assess them for the IUCN Red List of Threatened Species.[4]\n The earliest remains generally accepted to be those of a domesticated dog were discovered in Bonn-Oberkassel, Germany. Contextual, isotopic, genetic, and morphological evidence shows that this dog was not a local wolf.[20] The dog was dated to 14,223 years ago and was found buried along with a man and a woman, all three having been sprayed with red hematite powder and buried under large, thick basalt blocks. The dog had died of canine distemper.[21] Earlier remains dating back to 30,000 years ago have been described as Paleolithic dogs, but their status as dogs or wolves remains debated[22] because considerable morphological diversity existed among wolves during the Late Pleistocene.[1]\n This timing indicates that the dog was the first species to be domesticated[9][8] in the time of hunter-gatherers,[7] which predates agriculture.[1] DNA sequences show that all ancient and modern dogs share a common ancestry and descended from an ancient, extinct wolf population which was distinct from the modern wolf lineage.[6][7]\n The dog is a classic example of a domestic animal that likely travelled a commensal pathway into domestication.[22][23] The questions of when and where dogs were first domesticated have taxed geneticists and archaeologists for decades.[9] Genetic studies suggest a domestication process commencing over 25,000 years ago, in one or several wolf populations in either Europe, the high Arctic, or eastern Asia.[10] In 2021, a literature review of the current evidence infers that the dog was domesticated in Siberia 23,000 years ago by ancient North Siberians, then later dispersed eastward into the Americas and westward across Eurasia,[20] with dogs likely accompanying the first humans to inhabit the Americas.[24]\n Dogs are the most variable mammal on earth with around 450 globally recognized dog breeds.[10][25] In the Victorian era, directed human selection developed the modern dog breeds, which resulted in a vast range of phenotypes.[8] Most breeds were derived from small numbers of founders within the last 200 years,[8][10] and since then dogs have undergone rapid phenotypic change and were formed into today's modern breeds due to artificial selection imposed by humans. The skull, body, and limb proportions vary significantly between breeds, with dogs displaying more phenotypic diversity than can be found within the entire order of carnivores. These breeds possess distinct traits related to morphology, which include body size, skull shape, tail phenotype, fur type and colour.[8] Their behavioural traits include guarding, herding, and hunting,[8] retrieving, and scent detection. Their personality traits include hypersocial behavior, boldness, and aggression.[8] Present day dogs are dispersed around the world.[10] An example of this dispersal is the numerous modern breeds of European lineage during the Victorian era.[7]\n All healthy dogs, regardless of their size and type, have an identical skeletal structure with the exception of the number of bones in the tail, although there is significant skeletal variation between dogs of different types.[26][27] The dog's skeleton is well adapted for running; the vertebrae on the neck and back have extensions for powerful back muscles to connect to, the long ribs provide plenty of room for the heart and lungs, and the shoulders are unattached to the skeleton allowing great flexibility.[26][27]\n Compared to the dog's wolf-like ancestors, selective breeding since domestication has seen the dog's skeleton greatly enhanced in size for larger types as mastiffs and miniaturised for smaller types such as terriers; dwarfism has been selectively utilised for some types where short legs are advantageous such as dachshunds and corgis.[27] Most dogs naturally have 26 vertebrae in their tails, but some with naturally short tails have as few as three.[26]\n The dog's skull has identical components regardless of breed type, but there is significant divergence in terms of skull shape between types.[27][28] The three basic skull shapes are the elongated dolichocephalic type as seen in sighthounds, the intermediate mesocephalic or mesaticephalic type, and the very short and broad brachycephalic type exemplified by mastiff type skulls.[27][28]\n A dog's senses include vision, hearing, smell, taste, touch. One study suggests that dogs can feel Earth's magnetic field.[29]\n The coats of domestic dogs are of two varieties: \"double\" being familiar with dogs (as well as wolves) originating from colder climates, made up of a coarse guard hair and a soft down hair, or \"single\", with the topcoat only. Breeds may have an occasional \"blaze\", stripe, or \"star\" of white fur on their chest or underside.[30] Premature graying can occur in dogs from as early as one year of age; this is associated with impulsive behaviors, anxiety behaviors, fear of noise, and fear of unfamiliar people or animals.[31]\n As with many canids, one of the primary functions of a dog's tail is to communicate their emotional state, which can be crucial in getting along with others.[32] In some hunting dogs, the tail is traditionally docked to avoid injuries.\n Some breeds of dogs are prone to specific genetic ailments such as elbow and hip dysplasia, blindness, deafness, pulmonic stenosis, a cleft palate, and trick knees. Two severe medical conditions significantly affecting dogs are pyometra, affecting unspayed females of all breeds and ages, and gastric dilatation volvulus (bloat), which affects larger breeds or deep-chested dogs. Both of these are acute conditions and can kill rapidly. Dogs are also susceptible to parasites such as fleas, ticks, mites, hookworms, tapeworms, roundworms, and heartworms, which is a roundworm species that lives in the hearts of dogs.\n Several human foods and household ingestibles are toxic to dogs, including chocolate solids, causing theobromine poisoning, onions and garlic, causing thiosulphate, sulfoxide or disulfide poisoning, grapes and raisins, macadamia nuts, and xylitol.[33] The nicotine in tobacco can also be dangerous to dogs. Signs of ingestion can include copious vomiting (e.g., from eating cigar butts) or diarrhea. Some other symptoms are abdominal pain, loss of coordination, collapse, or death.[34][page needed]\n Dogs are also vulnerable to some of the same health conditions as humans, including diabetes, dental and heart disease, epilepsy, cancer, hypothyroidism, and arthritis.\n The typical lifespan of dogs varies widely among breeds, but the median longevity (the age at which half the dogs in a population have died and half are still alive) ranges from 10 to 13 years.[35][36] The median longevity of mixed-breed dogs, taken as an average of all sizes, is one or more years longer than that of purebred dogs when all breeds are averaged.[35][36][37] For dogs in England, increased body weight has been found to be negatively correlated with longevity (i.e., the heavier the dog, the shorter its lifespan), and mixed-breed dogs live on average 1.2 years longer than purebred dogs.[38]\n In domestic dogs, sexual maturity happens around six months to one year for both males and females, although this can be delayed until up to two years of age for some large breeds, and is the time at which female dogs will have their first estrous cycle. They will experience subsequent estrous cycles semiannually, during which the body prepares for pregnancy. At the peak of the cycle, females will become estrous, mentally and physically receptive to copulation. Because the ova survive and can be fertilized for a week after ovulation, more than one male can sire the same litter.[12] Fertilization typically occurs two to five days after ovulation; 14–16 days after ovulation, the embryo attaches to the uterus, and after seven to eight more days, a heartbeat is detectable.[39][40]\n Dogs bear their litters roughly 58 to 68 days after fertilization,[12][41] with an average of 63 days, although the length of gestation can vary. An average litter consists of about six puppies.[42]\n Neutering is the sterilization of animals, usually by removing the male's testicles or the female's ovaries and uterus, to eliminate the ability to procreate and reduce sex drive. Because of dogs' overpopulation in some countries, many animal control agencies, such as the American Society for the Prevention of Cruelty to Animals (ASPCA), advise that dogs not intended for further breeding should be neutered, so that they do not have undesired puppies that may later be euthanized.[43] According to the Humane Society of the United States, three to four million dogs and cats are euthanized each year.[44] Many more are confined to cages in shelters. Spaying or castrating dogs is considered a major factor in keeping overpopulation down.[45]\n Neutering reduces problems caused by hypersexuality, especially in male dogs.[46] Spayed female dogs are less likely to develop cancers affecting the mammary glands, ovaries, and other reproductive organs.[47][page needed] However, neutering increases the risk of urinary incontinence in female dogs,[48] prostate cancer in males,[49] and osteosarcoma, hemangiosarcoma, cruciate ligament rupture, obesity, and diabetes mellitus in either sex.[50]\n A common breeding practice for pet dogs is mating between close relatives (e.g., between half and full siblings).[51] Inbreeding depression is considered to be due mainly to the expression of homozygous deleterious recessive mutations.[52] Outcrossing between unrelated individuals, including dogs of different breeds, results in the beneficial masking of deleterious recessive mutations in progeny.[53]\n In a study of seven dog breeds (the Bernese Mountain Dog, Basset Hound, Cairn Terrier, Brittany, German Shepherd Dog, Leonberger, and West Highland White Terrier), it was found that inbreeding decreases litter size and survival.[54] Another analysis of data on 42,855 Dachshund litters found that as the inbreeding coefficient increased, litter size decreased and the percentage of stillborn puppies increased, thus indicating inbreeding depression.[55] In a study of Boxer litters, 22% of puppies died before reaching 7 weeks of age. Stillbirth was the most frequent cause of death, followed by infection. Mortality due to infection increased significantly with increases in inbreeding.[56]\n Dog behavior is the internally coordinated responses (actions or inactions) of the domestic dog (individuals or groups) to internal and external stimuli.[57] Dogs' minds have been shaped by millennia of contact with humans; dogs have acquired the ability to understand and communicate with humans; they are uniquely attuned to human behaviors.[13] Behavioral scientists thought that a set of social-cognitive abilities in domestic dogs that are not possessed by the dog's canine relatives or other highly intelligent mammals, such as great apes, are parallel to children's social-cognitive skills.[58]\n Unlike other domestic species selected for production-related traits, dogs were initially selected for their behaviors.[59][60] In 2016, a study found that only 11 fixed genes showed variation between wolves and dogs.[61] These gene variations were unlikely to have been the result of natural evolution and indicate selection on both morphology and behavior during dog domestication. These genes have been shown to affect the catecholamine synthesis pathway, with the majority of the genes affecting the fight-or-flight response[60][62] (i.e., selection for tameness) and emotional processing.[60] Dogs generally show reduced fear and aggression compared with wolves.[60][63] Some of these genes have been associated with aggression in some dog breeds.[60] Traits of high sociability and lack of fear in dogs may include genetic modifications related to Williams-Beuren syndrome in humans, which cause hypersociability at the expense of problem-solving ability.[64]\n Researchers have tested dogs' ability to perceive information, retain it as knowledge, and apply it to solve problems. Studies of two dogs suggest that dogs can learn by inference. A study with Rico, a Border Collie, showed that he knew the labels of over 200 different items. He inferred the names of novel things by exclusion learning and correctly retrieved those new items immediately and after four weeks of the initial exposure. A study of another Border Collie, Chaser, documented that he had learned the names and could associate them by verbal command with over 1,000 words.[65]\n One study of canine cognitive abilities found that dogs' capabilities are no more exceptional than those of other animals, such as horses, chimpanzees, or cats.[66] One limited study of 18 household dogs found that they lacked spatial memory and were more focused on the \"what\" of a task than the \"where\".[67]\n Dogs demonstrate a theory of mind by engaging in deception.[68] An experimental study showed evidence that Australian dingos can outperform domestic dogs in non-social problem-solving, indicating that domestic dogs may have lost much of their original problem-solving abilities once they joined humans.[69] Another study showed that after undergoing training to solve a simple manipulation task, dogs faced with an unsolvable version of the same problem look at humans, while socialized wolves do not.[70]\n Dog communication is how dogs convey information to other dogs, understand messages from humans and translate the information that dogs are transmitting.[71]: xii  Communication behaviors of dogs include eye gaze, facial expression,[72][73] vocalization, body posture (including movements of bodies and limbs), and gustatory communication (scents, pheromones, and taste). Humans communicate to dogs by using vocalization, hand signals, and body posture. As dogs, with their acute sense of hearing, they rely on the auditory aspect of communication for understanding and responding to various cues, including the distinctive barking patterns that convey different messages.\n Dogs commonly mark their territories by scent-marking them by urinating on them. Dogs may also do it to communicate anxiety or frustration. Dogs are more likely to leave urine messages when entering a new environment.[74][75]\n The dog is probably the most widely abundant large carnivoran living in the human environment.[76][77]\nIn 2013, the estimated global dog population was between 700 million[78] and 987 million.[79] About 20% of dogs live as pets in developed countries.[80] In the developing world, dogs are typically feral or communally owned, with pet dogs being uncommon. Most of these dogs live their lives as scavengers and have never been owned by humans, with one study showing their most common response when approached by strangers is to run away (52%) or respond aggressively (11%).[81] The majority of research on dog cognition has focused on pet dogs living in human homes.[82]\n Although dogs are the most abundant and widely distributed terrestrial carnivores, feral and free-ranging dogs' potential to compete with other large carnivores is limited by their strong association with humans.[76] For example, a review of the studies on dogs' competitive effects on sympatric carnivores did not mention any research on competition between dogs and wolves.[83][84] Although wolves are known to kill dogs, they tend to live in pairs or in small packs in areas where they are highly persecuted, giving them a disadvantage when facing large dog groups.[83][85] In some instances, wolves have displayed an uncharacteristic fearlessness of humans and buildings when attacking dogs, to the extent that they have to be beaten off or killed.[86] Although the numbers of dogs killed each year are relatively low, it induces a fear of wolves entering villages and farmyards to take dogs, and losses of dogs to wolves have led to demands for more liberal wolf hunting regulations.[83]\n Coyotes and big cats have also been known to attack dogs. In particular, leopards are known to have a preference for dogs and have been recorded to kill and consume them, no matter their size.[87] Siberian tigers in the Amur River region have killed dogs in the middle of villages. This indicates that the dogs were targeted. Amur tigers will not tolerate wolves as competitors within their territories, and the tigers could be considering dogs in the same way.[88] Striped hyenas are known to kill dogs in their range.[89]\n Dogs have been described as omnivores.[12][90][91] Compared to wolves, dogs from agricultural societies have extra copies of amylase and other genes involved in starch digestion that contribute to an increased ability to thrive on a starch-rich diet.[11] Similar to humans, some dog breeds produce amylase in their saliva and are classified as having a high starch diet.[92] However, more like cats and less like other omnivores, dogs can only produce bile acid with taurine and they cannot produce vitamin D, which they obtain from animal flesh.\nOf the twenty-one amino acids common to all life forms (including selenocysteine), dogs cannot synthesize ten: arginine, histidine, isoleucine, leucine, lysine, methionine, phenylalanine, threonine, tryptophan, and valine.[93][94][95]\nAlso more like cats, dogs require arginine to maintain nitrogen balance. These nutritional requirements place dogs halfway between carnivores and omnivores.[96]\n As a domesticated or semi-domesticated animal, the dog is nearly universal among human societies. Notable exceptions once included:\n Dogs were introduced to Antarctica as sled dogs, but were later outlawed by international agreement due to the possible risk of spreading infections.[105]\n Domestic dogs inherited complex behaviors, such as bite inhibition, from their wolf ancestors, who would have been pack hunters with complex body language. These sophisticated forms of social cognition and communication may account for their trainability, playfulness and ability to fit into human households and social situations,[106] probably also for early human hunter-gatherers.\n Dogs perform many roles for people, such as hunting, herding, pulling loads, protection, assisting police and the military, companionship and aiding disabled individuals. This influence on human society has given them the nickname \"man's best friend\" in the Western world. In some cultures, however, dogs are also a source of meat.[107][108]\n It is estimated that three-quarters of the world's dog population lives in the developing world as feral, village, or community dogs, with pet dogs uncommon.[109][page needed]\n \"The most widespread form of interspecies bonding occurs between humans and dogs\"[110] and the keeping of dogs as companions, particularly by elites, has a long history.[14] Pet dog populations grew significantly after World War II as suburbanization increased.[14] In the 1950s and 1960s, dogs were kept outside more often than they tend to be today[111] (the expression \"in the doghouse\" – recorded since 1932[112] – to describe exclusion from the group implies a distance between the doghouse and the home) and were still primarily functional, acting as a guard, children's playmate, or walking companion. From the 1980s, there have been changes in the pet dog's role, such as the increased role of dogs in the emotional support of their human guardians.[113][page needed] People and their dogs have become increasingly integrated and implicated in each other's lives.[114][page needed][115]\n Within the second half of the 20th century, the first dogs' social status major shift has been \"commodification\", shaping it to conform to social expectations of personality and behavior.[115] The second has been the broadening of the family's concept and the home to include dogs-as-dogs within everyday routines and practices.[115]\n A vast range of commodity forms aim to transform a pet dog into an ideal companion.[116] The list of goods, services, and places available for dogs is enormous.[116] Dog training books, classes, and television programs proliferated as the process of commodifying the pet dog continued.[117] The majority of contemporary dog owners describe their pet as part of the family, although some ambivalence about the relationship is evident in the popular reconceptualization of the dog-human family as a pack.[115] Some dog trainers, such as on the television program Dog Whisperer, have promoted a dominance model of dog-human relationships. However, it has been disputed that \"trying to achieve status\" is characteristic of dog-human interactions.[118] The idea of the \"alpha dog\" trying to be dominant is based on a controversial theory about wolf packs.[119][120] Increasingly, human family-members engage in activities centered on the dog's perceived needs and interests, or in which the dog is an integral partner, such as dog dancing and dog yoga.[116]\n According to statistics published by the American Pet Products Manufacturers Association in the National Pet Owner Survey in 2009–2010, an estimated 77.5 million people in the United States have pet dogs.[121] The same source shows that nearly 40% of American households own at least one dog, of which 67% own just one dog, 25% two dogs and nearly 9% more than two dogs. There does not seem to be any sex preference among dogs as pets, as the statistical data reveal an equal number of male and female pet dogs. Although several programs promote pet adoption, less than one-fifth of the owned dogs come from shelters.[121]\n A study using magnetic resonance imaging (MRI) to compare humans and dogs showed that dogs have the same response to voices and use the same parts of the brain as humans do. This gives dogs the ability to recognize human emotional sounds.[122][123][124]\n In addition to dogs' role as companion animals, dogs have been bred for herding livestock (collies, sheepdogs),[125][page needed][12] hunting (hounds, pointers)[126][page needed] and rodent control (terriers).[12] Other types of working dogs include search and rescue dogs,[127] detection dogs trained to detect illicit drugs[128] or chemical weapons;[129] guard dogs; dogs who assist fishermen with the use of nets; and dogs that pull loads.[12] In 1957, the dog Laika became the first animal to be launched into Earth orbit, aboard the Soviets' Sputnik 2; she died during the flight.[130][131]\n Various kinds of service dogs and assistance dogs, including guide dogs, hearing dogs, mobility assistance dogs and psychiatric service dogs, assist individuals with disabilities.[132][133] Some dogs owned by people with epilepsy have been shown to alert their handler when the handler shows signs of an impending seizure, sometimes well in advance of onset.[134]\n People often enter their dogs in competitions, such as breed-conformation shows or sports, including racing, sledding and agility competitions. In conformation shows, also referred to as breed shows, a judge familiar with the specific dog breed evaluates individual purebred dogs for conformity with their established breed type as described in the breed standard. As the breed standard only deals with the dog's externally observable qualities (such as appearance, movement and temperament), separately tested qualities (such as ability or health) are not part of the judging in conformation shows.\n Dog meat is consumed in some East Asian countries, including Korea,[135][page needed] China,[107] Vietnam[108] and the Philippines,[136] which dates back to antiquity.[137] Based on limited data, it is estimated that 13–16 million dogs are killed and consumed in Asia every year.[138] In China, debates have ensued over banning the consumption of dog meat.[139]  Following the Sui and Tang dynasties of the first millennium, however, people living on northern China's plains began to eschew eating dogs, which is likely due to Buddhism and Islam's spread, two religions that forbade the consumption of certain animals, including the dog. As members of the upper classes shunned dog meat, it gradually became a social taboo to eat it, even though the general population continued to consume it for centuries afterward.[citation needed] Dog meat is also consumed in some parts of Switzerland.[140] Other cultures, such as Polynesia and pre-Columbian Mexico, also consumed dog meat in their history. In some parts of Poland[141][142] and Central Asia,[143][144] dog fat is reportedly believed to be beneficial for the lungs. Proponents of eating dog meat have argued that placing a distinction between livestock and dogs is Western hypocrisy and that there is no difference in eating different animals' meat.[145][146][147][148]\n In Korea, the primary dog breed raised for meat, the Nureongi, differs from those breeds raised for pets that Koreans may keep in their homes.[149]\n The most popular Korean dog dish is called bosintang, a spicy stew meant to balance the body's heat during the summer months. Followers of the custom claim this is done to ensure good health by balancing one's gi, or the body's vital energy. A 19th-century version of bosintang explains that the dish is prepared by boiling dog meat with scallions and chili powder. Dog is not as widely consumed as beef, pork and chicken.[149]\n In 2018, the WHO reported that 59,000 people died globally from rabies, with 59.6% in Asia and 36.4% in Africa. Rabies is a disease for which dogs are the most important vector.[150] Dog bites affect tens of millions of people globally each year. Children in mid-to-late childhood are the largest percentage bitten by dogs, with a greater risk of injury to the head and neck. They are more likely to need medical treatment and have the highest death rate.[151] Sharp claws can lacerate flesh that can lead to serious infections.[152] In the U.S., cats and dogs are a factor in more than 86,000 falls each year.[153] It has been estimated that around 2% of dog-related injuries treated in U.K. hospitals are domestic accidents. The same study found that while dog involvement in road traffic accidents was difficult to quantify, dog-associated road accidents involving injury more commonly involved two-wheeled vehicles.[154]\n Toxocara canis (dog roundworm) eggs in dog feces can cause toxocariasis. In the United States, about 10,000 cases of Toxocara infection are reported in humans each year, and almost 14% of the U.S. population is infected.[155] Untreated toxocariasis can cause retinal damage and decreased vision.[156] Dog feces can also contain hookworms that cause cutaneous larva migrans in humans.[157][158]\n Dogs suffer from the same common disorders as humans; these include cancer, diabetes, heart disease and neurologic disorders. Their pathology is similar to humans, as is their response to treatment and their outcomes. Researchers are identifying the genes associated with dog diseases similar to human disorders, but lack mouse models to find cures for both dogs and humans. The genes involved in canine obsessive-compulsive disorders led to the detection of four genes in humans' related pathways.[10]\n The scientific evidence is mixed as to whether a dog's companionship can enhance human physical health and psychological well-being.[159] Studies suggest that there are benefits to physical health and psychological well-being[160] have been criticized for being poorly controlled.[161] It states that \"the health of elderly people is related to their health habits and social supports but not to their ownership of, or attachment to, a companion animal.\" Earlier studies have shown that people who keep pet dogs or cats exhibit better mental and physical health than those who do not, making fewer visits to the doctor and being less likely to be on medication than non-guardians.[162]\n A 2005 paper states \"recent research has failed to support earlier findings that pet ownership is associated with a reduced risk of cardiovascular disease, a reduced use of general practitioner services, or any psychological or physical benefits on health for community dwelling older people\". Research has pointed to significantly less absenteeism from school through sickness among children who live with pets.\"[159] In one study, new guardians reported a highly significant reduction in minor health problems during the first month following pet acquisition. This effect was sustained in those with dogs through to the end of the study.[163]\n People with pet dogs took considerably more physical exercise than those with cats and those without pets. The results provide evidence that keeping pets may have positive effects on human health and behavior and that for guardians of dogs, these effects are relatively long-term.[163] Pet guardianship has also been associated with increased coronary artery disease survival. Human guardians are significantly less likely to die within one year of an acute myocardial infarction than those who did not own dogs.[164] The association between dog ownership and adult physical activity levels has been reviewed by several authors.[165]\n The health benefits of dogs can result from contact with dogs in general, not solely from having dogs as pets. For example, when in a pet dog's presence, people show reductions in cardiovascular, behavioral and psychological indicators of anxiety.[166] Other health benefits are gained from exposure to immune-stimulating microorganisms, which can protect against allergies and autoimmune diseases according to the hygiene hypothesis. The benefits of contact with a dog also include social support, as dogs cannot only provide companionship and social support themselves but also act as facilitators of social interactions between humans.[167] One study indicated that wheelchair users experience more positive social interactions with strangers when accompanied by a dog than when they are not.[168] In 2015, a study found that pet owners were significantly more likely to get to know people in their neighborhood than non-pet owners.[169]\n Using dogs and other animals as a part of therapy dates back to the late 18th century, when animals were introduced into mental institutions to help socialize patients with mental disorders.[170] Animal-assisted intervention research has shown that animal-assisted therapy with a dog can increase social behaviors, such as smiling and laughing, among people with Alzheimer's disease.[171] One study demonstrated that children with ADHD and conduct disorders who participated in an education program with dogs and other animals showed increased attendance, increased knowledge and skill objectives and decreased antisocial and violent behavior compared with those not in an animal-assisted program.[172]\n Dogs were depicted to symbolize guidance, protection, loyalty, fidelity, faithfulness, alertness, and love.[173] In ancient Mesopotamia, from the Old Babylonian period until the Neo-Babylonian, dogs were the symbol of Ninisina, the goddess of healing and medicine,[174] and her worshippers frequently dedicated small models of seated dogs to her.[174] In the Neo-Assyrian and Neo-Babylonian periods, dogs were used as emblems of magical protection.[174] In China, Korea and Japan, dogs are viewed as kind protectors.[175]\n In mythology, dogs often serve as pets or as watchdogs.[175] Stories of dogs guarding the gates of the underworld recur throughout Indo-European mythologies[176][177] and may originate from Proto-Indo-European religion.[176][177] In Greek mythology, Cerberus is a three-headed, dragon-tailed watchdog who guards the gates of Hades.[175] Dogs are also associated with the Greek goddess Hecate.[178] In Norse mythology, a dog called Garmr guards Hel, a realm of the dead.[175] In Persian mythology, two four-eyed dogs guard the Chinvat Bridge.[175] In Welsh mythology, Annwn is guarded by Cŵn Annwn.[175] In Hindu mythology, Yama, the god of death, owns two watchdogs named Shyama and Sharvara who have four eyes. They are said to watch over the gates of Naraka.[179] A black dog is also considered to be the vahana (vehicle) of Bhairava (an incarnation of Shiva).[180]\n In Christianity, dogs represent faithfulness.[175] Within the Roman Catholic denomination specifically, the iconography of Saint Dominic includes a dog, after the saint's mother dreamt of a dog springing from her womb and becoming pregnant shortly after that.[181] As such, the Dominican Order (Ecclesiastical Latin: Domini canis) means \"dog of the Lord\" or \"hound of the Lord\" (Ecclesiastical Latin: Domini canis).[181] In Christian folklore, a church grim often takes the form of a black dog to guard Christian churches and their churchyards from sacrilege.[182] Jewish law does not prohibit keeping dogs and other pets. Jewish law requires Jews to feed dogs (and other animals that they own) before themselves and make arrangements for feeding them before obtaining them.[citation needed] The view on dogs in Islam is mixed, with some schools of thought viewing them as unclean,[175] although Khaled Abou El Fadl states that this view is based on \"pre-Islamic Arab mythology\" and \"a tradition to be falsely attributed to the Prophet.\"[183] Therefore, Sunni Malaki and Hanafi jurists permit the trade of and keeping of dogs as pets.[184]\n"}
{"key":"Dog","link":"https:\/\/en.wikipedia.org\/wiki\/Megaannum","headline":"Year - Wikipedia","content":"\n \n A year is the time taken for astronomical objects to complete one orbit. For example, a year on Earth is the time taken for Earth to revolve around the Sun. Generally, a year is taken to mean a calendar year, but the word is also used for periods loosely associated with the calendar or astronomical year, such as the seasonal year, the fiscal year, the academic year, etc. The term can also be used in reference to any long period or cycle, such as the Great Year.[1]\n Due to the Earth's axial tilt, the course of a year sees the passing of the seasons, marked by change in weather, the hours of daylight, and, consequently, vegetation and soil fertility. In temperate and subpolar regions around the planet, four seasons are generally recognized: spring, summer, autumn and winter. In tropical and subtropical regions, several geographical sectors do not present defined seasons; but in the seasonal tropics, the annual wet and dry seasons are recognized and tracked.\n A calendar year is an approximation of the number of days of the Earth's orbital period, as counted in a given calendar. The Gregorian calendar, or modern calendar, presents its calendar year to be either a common year of 365 days or a leap year of 366 days, as do the Julian calendars. For the Gregorian calendar, the average length of the calendar year (the mean year) across the complete leap cycle of 400 years is 365.2425 days (97 out of 400 years are leap years).[2]\n In English, the unit of time for year is commonly abbreviated as \"y\" or \"yr\". The symbol \"a\" (for Latin: annus, year) is sometimes used in scientific literature, though its exact duration may be inconsistent.\n English year (via West Saxon ġēar (\/jɛar\/), Anglian ġēr) continues Proto-Germanic *jǣran (*jē₁ran). Cognates are German Jahr, Old High German jār, Old Norse ár and Gothic jer, from the Proto-Indo-European noun *yeh₁r-om \"year, season\". Cognates also descended from the same Proto-Indo-European noun (with variation in suffix ablaut) are Avestan yārǝ \"year\", Greek ὥρα (hṓra) \"year, season, period of time\" (whence \"hour\"), Old Church Slavonic jarŭ, and Latin hornus \"of this year\".[citation needed]\n Latin annus (a 2nd declension masculine noun; annum is the accusative singular; annī is genitive singular and nominative plural; annō the dative and ablative singular) is from a PIE noun *h₂et-no-, which also yielded Gothic aþn \"year\" (only the dative plural aþnam is attested).\n Although most languages treat the word as thematic *yeh₁r-o-, there is evidence for an original derivation with an *-r\/n suffix, *yeh₁-ro-. Both Indo-European words for year, *yeh₁-ro- and *h₂et-no-, would then be derived from verbal roots meaning \"to go, move\", *h₁ey- and *h₂et-, respectively (compare Vedic Sanskrit éti \"goes\", atasi \"thou goest, wanderest\"). A number of English words are derived from Latin annus, such as annual, annuity, anniversary, etc.; per annum means \"each year\", annō Dominī means \"in the year of the Lord\".\n The Greek word for \"year\", ἔτος, is cognate with Latin vetus \"old\", from the PIE word *wetos- \"year\", also preserved in this meaning in Sanskrit vat-sa-ras \"year\" and vat-sa- \"yearling (calf)\", the latter also reflected in Latin vitulus \"bull calf\", English wether \"ram\" (Old English weðer, Gothic wiþrus \"lamb\").\n In some languages, it is common to count years by referencing to one season, as in \"summers\", or \"winters\", or \"harvests\". Examples include Chinese 年 \"year\", originally 秂, an ideographic compound of a person carrying a bundle of wheat denoting \"harvest\". Slavic besides godŭ \"time period; year\" uses lěto \"summer; year\".\n Astronomical years do not have an integer number of days or lunar months. Any calendar that follows an astronomical year must have a system of intercalation such as leap years.\n In the Julian calendar, the average (mean) length of a year is 365.25 days. In a non-leap year, there are 365 days, in a leap year there are 366 days. A leap year occurs every fourth year during which a leap day is intercalated into the month of February. The name \"Leap Day\" is applied to the added day.\n In astronomy, the Julian year is a unit of time defined as 365.25 days, each of exactly 86,400 seconds (SI base unit), totaling exactly 31,557,600 seconds in the Julian astronomical year.[3][4]\n The Revised Julian calendar, proposed in 1923 and used in some Eastern Orthodox Churches, has 218 leap years every 900 years, for the average (mean) year length of 365.2422222 days, close to the length of the mean tropical year, 365.24219 days (relative error of 9·10). In the year 2800 CE, the Gregorian and Revised Julian calendars will begin to differ by one calendar day.[5]\n The Gregorian calendar aims to ensure that the northward equinox falls on or shortly before March 21 and hence it follows the northward equinox year, or tropical year.[6] Because 97 out of 400 years are leap years, the mean length of the Gregorian calendar year is 365.2425 days; with a relative error below one ppm (8·10) relative to the current length of the mean tropical year (365.24219 days) and even closer to the current March equinox year of 365.242374 days that it aims to match. \n Historically, lunisolar calendars intercalated entire leap months on an observational basis. Lunisolar calendars have mostly fallen out of use except for liturgical reasons (Hebrew calendar, various Hindu calendars).\n A modern adaptation of the historical Jalali calendar, known as the Solar Hijri calendar (1925), is a purely solar calendar with an irregular pattern of leap days based on observation (or astronomical computation), aiming to place new year (Nowruz) on the day of vernal equinox (for the time zone of Tehran), as opposed to using an algorithmic system of leap years.\n A calendar era assigns a cardinal number to each sequential year, using a reference event in the past (called the epoch) as the beginning of the era.\n The Gregorian calendar era is the world's most widely used civil calendar.[7] Its epoch is a 6th century estimate of the date of birth of Jesus of Nazareth. Two notations are used to indicate year numbering in the Gregorian calendar: the Christian \"Anno Domini\" (meaning \"in the year of the Lord\"), abbreviated AD; and \"Common Era\", abbreviated CE, preferred by many of other faiths and none.  Year numbers are based on inclusive counting, so that there is no \"year zero\".  Years before the epoch are abbreviated BC for Before Christ or BCE for Before the Common Era. In Astronomical year numbering, positive numbers indicate years AD\/CE, the number 0 designates 1 BC\/BCE, −1 designates 2 BC\/BCE, and so on.\n Other eras include that of Ancient Rome, Ab Urbe Condita (\"from the foundation of the city), abbreviated AUC; Anno Mundi (\"year of the world\"), used for the Hebrew calendar and abbreviated AM; and the Japanese imperial eras. The Islamic Hijri year, (year of the Hijrah, Anno Hegirae abbreviated AH), is a lunar calendar of twelve lunar months and thus is shorter than a solar year.\n Financial and scientific calculations often use a 365-day calendar to simplify daily rates.\n A fiscal year or financial year is a 12-month period used for calculating annual financial statements in businesses and other organizations. In many jurisdictions, regulations regarding accounting require such reports once per twelve months, but do not require that the twelve months constitute a calendar year.\n For example, in Canada and India the fiscal year runs from April 1; in the United Kingdom it runs from April 1 for purposes of corporation tax and government financial statements, but from April 6 for purposes of personal taxation and payment of state benefits; in Australia it runs from July 1; while in the United States the fiscal year of the federal government runs from October 1.\n An academic year is the annual period during which a student attends an educational institution. The academic year may be divided into academic terms, such as semesters or quarters. The school year in many countries starts in August or September and ends in May, June or July. In Israel the academic year begins around October or November, aligned with the second month of the Hebrew calendar.\n Some schools in the UK, Canada and the United States divide the academic year into three roughly equal-length terms (called trimesters or quarters in the United States), roughly coinciding with autumn, winter, and spring. At some, a shortened summer session, sometimes considered part of the regular academic year, is attended by students on a voluntary or elective basis. Other schools break the year into two main semesters, a first (typically August through December) and a second semester (January through May). Each of these main semesters may be split in half by mid-term exams, and each of the halves is referred to as a quarter (or term in some countries). There may also be a voluntary summer session or a short January session.\n Some other schools, including some in the United States, have four marking periods. Some schools in the United States, notably Boston Latin School, may divide the year into five or more marking periods. Some state in defense of this that there is perhaps a positive correlation between report frequency and academic achievement.\n There are typically 180 days of teaching each year in schools in the US, excluding weekends and breaks, while there are 190 days for pupils in state schools in Canada, New Zealand and the United Kingdom, and 200 for pupils in Australia.\n In India the academic year normally starts from June 1 and ends on May 31. Though schools start closing from mid-March, the actual academic closure is on May 31 and in Nepal it starts from July 15.[citation needed]\n Schools and universities in Australia typically have academic years that roughly align with the calendar year (i.e., starting in February or March and ending in October to December), as the southern hemisphere experiences summer from December to February.\n The Julian year, as used in astronomy and other sciences, is a time unit defined as exactly 365.25 days of 86,400 SI seconds each (\"ephemeris days\"). This is the normal meaning of the unit \"year\" used in various scientific contexts. The Julian century of 36525 ephemeris days and the Julian millennium of 365250 ephemeris days are used in astronomical calculations. Fundamentally, expressing a time interval in Julian years is a way to precisely specify an amount of time (not how many \"real\" years), for long time intervals where stating the number of ephemeris days would be unwieldy and unintuitive. By convention, the Julian year is used in the computation of the distance covered by a light-year.\n In the Unified Code for Units of Measure (but not according to the International Union of Pure and Applied Physics or the International Union of Geological Sciences, see below), the symbol a (without subscript) always refers to the Julian year, aj, of exactly 31557600 seconds.\n The SI multiplier prefixes may be applied to it to form \"ka\", \"Ma\", etc.[8]\n Each of these three years can be loosely called an astronomical year.\n The sidereal year is the time taken for the Earth to complete one revolution of its orbit, as measured against a fixed frame of reference (such as the fixed stars, Latin sidera, singular sidus). Its average duration is 365.256363004 days (365 d 6 h 9 min 9.76 s) (at the epoch J2000.0 = January 1, 2000, 12:00:00 TT).[9]\n Today the mean tropical year is defined as the period of time for the mean ecliptic longitude of the Sun to increase by 360 degrees.[10] Since the Sun's ecliptic longitude is measured with respect to the equinox,[11] the tropical year comprises a complete cycle of the seasons and is the basis of solar calendars such as the internationally used Gregorian calendar. The modern definition of mean tropical year differs from the actual time between passages of, e.g., the northward equinox, by a minute or two, for several reasons explained below. Because of the Earth's axial precession, this year is about 20 minutes shorter than the sidereal year. The mean tropical year is approximately 365 days, 5 hours, 48 minutes, 45 seconds, using the modern definition[12] ( = 365.24219 d × 86 400 s). The length of the tropical year varies a bit over thousands of years because the rate of axial precession is not constant.\n The anomalistic year is the time taken for the Earth to complete one revolution with respect to its apsides. The orbit of the Earth is elliptical; the extreme points, called apsides, are the perihelion, where the Earth is closest to the Sun, and the aphelion, where the Earth is farthest from the Sun. The anomalistic year is usually defined as the time between perihelion passages. Its average duration is 365.259636 days (365 d 6 h 13 min 52.6 s) (at the epoch J2011.0).[13]\n The draconic year, draconitic year, eclipse year, or ecliptic year is the time taken for the Sun (as seen from the Earth) to complete one revolution with respect to the same lunar node (a point where the Moon's orbit intersects the ecliptic). The year is associated with eclipses: these occur only when both the Sun and the Moon are near these nodes; so eclipses occur within about a month of every half eclipse year. Hence there are two eclipse seasons every eclipse year. The average duration of the eclipse year is\n This term is sometimes erroneously used for the draconic or nodal period of lunar precession, that is the period of a complete revolution of the Moon's ascending node around the ecliptic: 18.612815932 Julian years (6798.331019 days; at the epoch J2000.0).\n The full moon cycle is the time for the Sun (as seen from the Earth) to complete one revolution with respect to the perigee of the Moon's orbit. This period is associated with the apparent size of the full moon, and also with the varying duration of the synodic month. The duration of one full moon cycle is:\n The lunar year comprises twelve full cycles of the phases of the Moon, as seen from Earth. It has a duration of approximately 354.37 days. Muslims use this for celebrating their Eids and for marking the start of the fasting month of Ramadan. A Muslim calendar year is based on the lunar cycle. The Jewish calendar is also essentially lunar, except that an intercalary lunar month is added once every two or three years, in order to keep the calendar synchronized with the solar cycle as well. Thus, a lunar year on the Jewish (Hebrew) calendar consists of either twelve or thirteen lunar months.\n The vague year, from annus vagus or wandering year, is an integral approximation to the year equaling 365 days, which wanders in relation to more exact years. Typically the vague year is divided into 12 schematic months of 30 days each plus 5 epagomenal days. The vague year was used in the calendars of Ethiopia, Ancient Egypt, Iran, Armenia and in Mesoamerica among the Aztecs and Maya.[14] It is still used by many Zoroastrian communities.\n A heliacal year is the interval between the heliacal risings of a star. It differs from the sidereal year for stars away from the ecliptic due mainly to the precession of the equinoxes.\n The Sothic year is the heliacal year, the interval between heliacal risings, of the star Sirius. It is currently less than the sidereal year and its duration is very close to the Julian year of 365.25 days.\n The Gaussian year is the sidereal year for a planet of negligible mass (relative to the Sun) and unperturbed by other planets that is governed by the Gaussian gravitational constant. Such a planet would be slightly closer to the Sun than Earth's mean distance. Its length is:\n The Besselian year is a tropical year that starts when the (fictitious) mean Sun reaches an ecliptic longitude of 280°. This is currently on or close to January 1. It is named after the 19th-century German astronomer and mathematician Friedrich Bessel. The following equation can be used to compute the current Besselian epoch (in years):[15]\n The TT subscript indicates that for this formula, the Julian date should use the Terrestrial Time scale, or its predecessor, ephemeris time.\n The exact length of an astronomical year changes over time.\n Numerical value of year variation\nMean year lengths in this section are calculated for 2000, and differences in year lengths, compared to 2000, are given for past and future years. In the tables a day is 86,400 SI seconds long.[16][17][18][19]\n Some of the year lengths in this table are in average solar days, which are slowly getting longer (at a rate that cannot be exactly predicted in advance) and are now around 86,400.002 SI seconds.\n An average Gregorian year may be said to be 365.2425 days (52.1775 weeks, and if an hour is defined as one twenty-fourth of a day, 8765.82 hours, 525949.2 minutes or 31556952 seconds). Note however that in absolute time the average Gregorian year is not adequately defined unless the period of the averaging (start and end dates) is stated, because each period of 400 years is longer (by more than 1000 seconds) than the preceding one as the rotation of the Earth slows. In this calendar, a common year is 365 days (8760 hours, 525600 minutes or 31536000 seconds), and a leap year is 366 days (8784 hours, 527040 minutes or 31622400 seconds). The 400-year civil cycle of the Gregorian calendar has 146097 days and hence exactly 20871 weeks.\n The Great Year, or equinoctial cycle, corresponds to a complete revolution of the equinoxes around the ecliptic. Its length is about 25,700 years.[20][21]\n The Galactic year is the time it takes Earth's Solar System to revolve once around the Galactic Center. It comprises roughly 230 million Earth years.[22]\n A seasonal year is the time between successive recurrences of a seasonal event such as the flooding of a river, the migration of a species of bird, the flowering of a species of plant, the first frost, or the first scheduled game of a certain sport. All of these events can have wide variations of more than a month from year to year.\n A common symbol for the year as a unit of time is \"a\", taken from the Latin word annus. \nFor example, the U.S. National Institute of Standards and Technology (NIST) Guide for the Use of the International System of Units (SI) supports the symbol \"a\" as the unit of time for a year.[23]\n In English, the abbreviations \"y\" or \"yr\" are more commonly used in non-scientific literature.[24] In some Earth sciences branches (geology and paleontology), \"kyr, myr, byr\" (thousands, millions, and billions of years, respectively) and similar abbreviations are used to denote intervals of time remote from the present.[25][26] In astronomy the abbreviations kyr, Myr and Gyr are in common use for kiloyears, megayears and gigayears.[27][28]\n The Unified Code for Units of Measure (UCUM) disambiguates the varying symbologies of ISO 1000, ISO 2955 and ANSI X3.50 by using:[8]\n In the UCUM, the symbol \"a\", without any qualifier, equals 1 aj.\nThe UCUM also minimizes confusion with are, a unit of area, by using the abbreviation \"ar\".\n Since 1989, the International Astronomical Union (IAU) recognizes the symbol \"a\" rather than \"yr\" for a year, notes the different kinds of year, and recommends adopting the Julian year of 365.25 days, unless otherwise specified (IAU Style Manual).[29][30]\n Since 1987, the International Union of Pure and Applied Physics (IUPAP) notes \"a\" as the general symbol for the time unit year (IUPAP Red Book).[31]\nSince 1993, the International Union of Pure and Applied Chemistry (IUPAC) Green Book also uses the same symbol \"a\", notes the difference between Gregorian year and Julian year, and adopts the former (a=365.2425 days),[32] also noted in the IUPAC Gold Book.[33]\n In 2011, the IUPAC and the International Union of Geological Sciences jointly recommended defining the \"annus\", with symbol \"a\", as the length of the tropical year in the year 2000:[34]\n This differs from the above definition of 365.25 days by about 20 parts per million. The joint document says that definitions such as the Julian year \"bear an inherent, pre-programmed obsolescence because of the variability of Earth's orbital movement\", but then proposes using the length of the tropical year as of 2000 AD (specified down to the millisecond), which suffers from the same problem.[35] (The tropical year oscillates with time by more than a minute.)\n The notation has proved controversial as it conflicts with an earlier convention among geoscientists to use \"a\" specifically for \"years ago\" (e.g. 1 Ma for 1 million years ago), and \"y\" or \"yr\" for a one-year time period.[35][36]\nHowever, this historical practice does not comply with the NIST Guide,[23] considering the unacceptability of mixing information concerning the physical quantity being measured (in this case, time intervals or points in time) with the units and also the unnaceptability of using abbreviations for units.\nFurthermore, according to the UK Metric Association (UKMA), language-independent symbols are more universally understood (UKMA Style guide).[37]\n For the following, there are alternative forms that elide the consecutive vowels, such as kilannus, megannus, etc. The exponents and exponential notations are typically used for calculating and in displaying calculations, and for conserving space, as in tables of data.\n \n In geology and paleontology, a distinction sometimes is made between abbreviation \"yr\" for years and \"ya\" for years ago, combined with prefixes for thousand, million, or billion.[25][41] In archaeology, dealing with more recent periods, normally expressed dates, e.g. \"10,000 BC\", may be used as a more traditional form than Before Present (\"BP\").\n These abbreviations include:\n Around 200 kyaAround 60 kyaAround 20 kyaAround 10 kya\n Use of \"mya\" and \"bya\" is deprecated in modern geophysics, the recommended usage being \"Ma\" and \"Ga\" for dates Before Present, but \"m.y.\" for the durations of epochs.[25][26] This ad hoc distinction between \"absolute\" time and time intervals is somewhat controversial amongst members of the Geological Society of America.[43]\n"}
{"key":"Dog","link":"https:\/\/en.wikipedia.org\/wiki\/Precambrian","headline":"Precambrian - Wikipedia","content":"The Precambrian ( \/priˈkæmbri.ən, -ˈkeɪm-\/ pree-KAM-bree-ən, -⁠KAYM-;[2] or Pre-Cambrian, sometimes abbreviated pꞒ, or Cryptozoic) is the earliest part of Earth's history, set before the current Phanerozoic Eon. The Precambrian is so named because it preceded the Cambrian, the first period of the Phanerozoic Eon, which is named after Cambria, the Latinised name for Wales, where rocks from this age were first studied. The Precambrian accounts for 88% of the Earth's geologic time.\n The Precambrian is an informal unit of geologic time,[3] subdivided into three eons (Hadean, Archean, Proterozoic) of the geologic time scale. It spans from the formation of Earth about 4.6 billion years ago (Ga) to the beginning of the Cambrian Period, about 538.8 million years ago (Ma), when hard-shelled creatures first appeared in abundance.\n Relatively little is known about the Precambrian, despite it making up roughly seven-eighths of the Earth's history, and what is known has largely been discovered from the 1960s onwards. The Precambrian fossil record is poorer than that of the succeeding Phanerozoic, and fossils from the Precambrian (e.g. stromatolites) are of limited biostratigraphic use.[4] This is because many Precambrian rocks have been heavily metamorphosed, obscuring their origins, while others have been destroyed by erosion, or remain deeply buried beneath Phanerozoic strata.[4][5][6]\n It is thought that the Earth coalesced from material in orbit around the Sun at roughly 4,543 Ma, and may have been struck by another planet called Theia shortly after it formed, splitting off material that formed the Moon (see Giant-impact hypothesis). A stable crust was apparently in place by 4,433 Ma, since zircon crystals from Western Australia have been dated at 4,404 ± 8 Ma.[7][8]\n The term \"Precambrian\" is used by geologists and paleontologists for general discussions not requiring a more specific eon name. However, both the United States Geological Survey[9] and the International Commission on Stratigraphy regard the term as informal.[10] Because the span of time falling under the Precambrian consists of three eons (the Hadean, the Archean, and the Proterozoic), it is sometimes described as a supereon,[11][12] but this is also an informal term, not defined by the ICS in its chronostratigraphic guide.[13]\n Eozoic (from eo- \"earliest\") was a synonym for pre-Cambrian,[14][15] or more specifically Archean.[16]\n A specific date for the origin of life has not been determined. Carbon found in 3.8 billion-year-old rocks (Archean Eon) from islands off western Greenland may be of organic origin. Well-preserved microscopic fossils of bacteria older than 3.46 billion years have been found in Western Australia.[17] Probable fossils 100 million years older have been found in the same area. However, there is evidence that life could have evolved over 4.280 billion years ago.[18][19][20][21] There is a fairly solid record of bacterial life throughout the remainder (Proterozoic Eon) of the Precambrian.\n Complex multicellular organisms may have appeared as early as 2100 Ma.[22] However, the interpretation of ancient fossils is problematic, and \"... some definitions of multicellularity encompass everything from simple bacterial colonies to badgers.\"[23] Other possible early complex multicellular organisms include a possible 2450 Ma red alga from the Kola Peninsula,[24] 1650 Ma carbonaceous biosignatures in north China,[25] the 1600 Ma Rafatazmia,[26] and a possible 1047 Ma Bangiomorpha red alga from the Canadian Arctic.[27] The earliest fossils widely accepted as complex multicellular organisms date from the Ediacaran Period.[28][29] A very diverse collection of soft-bodied forms is found in a variety of locations worldwide and date to between 635 and 542 Ma. These are referred to as Ediacaran or Vendian biota. Hard-shelled creatures appeared toward the end of that time span, marking the beginning of the Phanerozoic Eon. By the middle of the following Cambrian Period, a very diverse fauna is recorded in the Burgess Shale, including some which may represent stem groups of modern taxa. The increase in diversity of lifeforms during the early Cambrian is called the Cambrian explosion of life.[30][31]\n While land seems to have been devoid of plants and animals, cyanobacteria and other microbes formed prokaryotic mats that covered terrestrial areas.[32]\n Tracks from an animal with leg-like appendages have been found in what was mud 551 million years ago.[33][34]\n The RNA World hypothesis assumes that RNA evolved before coded proteins and DNA genomes.[35]  During the Hadean Eon (4,567–4,031 Ma) abundant geothermal microenvironments were present that may have had the potential to support the synthesis and replication of RNA and thus possibly the evolution of a primitive life form.[36]  It was shown that porous rock systems comprising heated air-water interfaces could allow ribozyme catalyzed RNA replication of sense and antisense strands that could be followed by strand-dissociation, thus enabling combined synthesis, release and folding of active ribozymes.[36]  This primitive RNA replicative system also may have been able to undergo template strand switching during replication  (genetic recombination) as is known to occur during the RNA replication of extant coronaviruses.[37]\n Evidence of the details of plate motions and other tectonic activity in the Precambrian has been poorly preserved. It is generally believed that small proto-continents existed before 4280 Ma, and that most of the Earth's landmasses collected into a single supercontinent around 1130 Ma. The supercontinent, known as Rodinia, broke up around 750 Ma. A number of glacial periods have been identified going as far back as the Huronian epoch, roughly 2400–2100 Ma. One of the best studied is the Sturtian-Varangian glaciation, around 850–635 Ma, which may have brought glacial conditions all the way to the equator, resulting in a \"Snowball Earth\".\n The atmosphere of the early Earth is not well understood. Most geologists believe it was composed primarily of nitrogen, carbon dioxide, and other relatively inert gases, and was lacking in free oxygen. There is, however, evidence that an oxygen-rich atmosphere existed since the early Archean.[38]\n At present, it is still believed that molecular oxygen was not a significant fraction of Earth's atmosphere until after photosynthetic life forms evolved and began to produce it in large quantities as a byproduct of their metabolism. This radical shift from a chemically inert to an oxidizing atmosphere caused an ecological crisis, sometimes called the oxygen catastrophe. At first, oxygen would have quickly combined with other elements in Earth's crust, primarily iron, removing it from the atmosphere. After the supply of oxidizable surfaces ran out, oxygen would have begun to accumulate in the atmosphere, and the modern high-oxygen atmosphere would have developed. Evidence for this lies in older rocks that contain massive banded iron formations that were laid down as iron oxides.\n A terminology has evolved covering the early years of the Earth's existence, as radiometric dating has allowed absolute dates to be assigned to specific formations and features.[39] The Precambrian is divided into three eons: the Hadean (4567.3–4031 Ma), Archean (4031-2500 Ma) and Proterozoic (2500-538.8 Ma). See Timetable of the Precambrian.\n It has been proposed that the Precambrian should be divided into eons and eras that reflect stages of planetary evolution, rather than the current scheme based upon numerical ages. Such a system could rely on events in the stratigraphic record and be demarcated by GSSPs. The Precambrian could be divided into five \"natural\" eons, characterized as follows:[42]\n The movement of Earth's plates has caused the formation and break-up of continents over time, including occasional formation of a supercontinent containing most or all of the landmass. The earliest known supercontinent was Vaalbara. It formed from proto-continents and was a supercontinent 3.636 billion years ago. Vaalbara broke up c. 2.845–2.803 Ga ago. The supercontinent Kenorland was formed c. 2.72 Ga ago and then broke sometime after 2.45–2.1 Ga into the proto-continent cratons called Laurentia, Baltica, Yilgarn craton and Kalahari. The supercontinent Columbia, or Nuna, formed 2.1–1.8 billion years ago and broke up about 1.3–1.2 billion years ago.[43][44] The supercontinent Rodinia is thought to have formed about 1300-900 Ma, to have embodied most or all of Earth's continents and to have broken up into eight continents around 750–600 million years ago.[45]\n"}
{"key":"Dog","link":"https:\/\/en.wikipedia.org\/wiki\/Cambrian","headline":"Cambrian - Wikipedia","content":"\n The Cambrian ( \/ˈkæmbri.ən, ˈkeɪm-\/ KAM-bree-ən, KAYM-; sometimes symbolized Ꞓ) is the first geological period of the Paleozoic Era, and of the Phanerozoic Eon.[5] The Cambrian lasted 53.4 million years from the end of the preceding Ediacaran period 538.8 million years ago (mya) to the beginning of the Ordovician period 485.4 mya.[6] Its subdivisions, and its base, are somewhat in flux. \n The period was established as \"Cambrian series\" by Adam Sedgwick,[5] who named it after Cambria, the Latin name for 'Cymru' (Wales), where Britain's Cambrian rocks are best exposed.[7][8][9] Sedgwick identified the layer as part of his task, along with Roderick Murchison, to subdivide the large \"Transition Series\", although the two geologists disagreed for a while on the appropriate categorization.[5]\n The Cambrian is unique in its unusually high proportion of lagerstätte sedimentary deposits, sites of exceptional preservation where \"soft\" parts of organisms are preserved as well as their more resistant shells. As a result, scientific understanding of the Cambrian biology surpasses that of some later periods.[10]\n The Cambrian marked a profound change in life on Earth: prior to the Cambrian, the majority of living organisms on the whole were small, unicellular, and simple (Ediacaran fauna and earlier Tonian Huainan biota being notable exceptions). Complex, multicellular organisms gradually became more common in the millions of years immediately preceding the Cambrian, but it was not until this period that mineralized – hence readily fossilized – organisms became common.[11]\n The rapid diversification of lifeforms in the Cambrian, known as the Cambrian explosion, produced the first representatives of most modern animal phyla. Phylogenetic analysis has supported the view that before the Cambrian radiation, in the Cryogenian[12][13][14] or Tonian,[15] animals (metazoans) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.[16]\nAlthough diverse life forms prospered in the oceans, the land is thought to have been comparatively barren – with nothing more complex than a microbial soil crust[17] and a few mollusks and arthropods (albeit not terrestrial) that emerged to graze on the microbial biofilm.[18]\n By the end of the Cambrian, myriapods,[19][20] arachnids,[21] and hexapods[22] started adapting to the land, along with the first plants.[23][24] Most of the continents were probably dry and rocky due to a lack of vegetation. Shallow seas flanked the margins of several continents created during the breakup of the supercontinent Pannotia. The seas were relatively warm, and polar ice was absent for much of the period.\n The Cambrian Period followed the Ediacaran Period and preceded the Ordovician Period.\n The base of the Cambrian lies atop a complex assemblage of trace fossils known as the Treptichnus pedum assemblage.[25]\nThe use of Treptichnus pedum, a reference ichnofossil to mark the lower boundary of the Cambrian, is problematic because very similar trace fossils belonging to the Treptichnids group are found well below T. pedum in Namibia, Spain, Newfoundland, and possibly in the western US. The stratigraphic range of T. pedum overlaps the range of the Ediacaran fossils in Namibia, and probably in Spain.[26][27]\n The Cambrian is divided into four epochs (series) and ten ages (stages). Currently only three series and six stages are named and have a GSSP (an internationally agreed-upon stratigraphic reference point).\n Because the international stratigraphic subdivision is not yet complete, many local subdivisions are still widely used. In some of these subdivisions the Cambrian is divided into three epochs with locally differing names – the Early Cambrian (Caerfai or Waucoban, 538.8 ± 0.2 to 509 ± 1.9 mya), Middle Cambrian (St Davids or Albertan, 509 ± 0.2 to 497 ± 1.9 mya) and Late Cambrian (Merioneth or Croixan, 497 ± 0.2 to 485.4 ± 1.9 mya). \nTrilobite zones allow biostratigraphic correlation in the Cambrian. Rocks of these epochs are referred to as belonging to the Lower, Middle, or Upper Cambrian.\n Each of the local series is divided into several stages. The Cambrian is divided into several regional faunal stages of which the Russian-Kazakhian system is most used in international parlance:\n *Most Russian paleontologists define the lower boundary of the Cambrian at the base of the Tommotian Stage, characterized by diversification and global distribution of organisms with mineral skeletons and the appearance of the first Archaeocyath bioherms.[29][30][31]\n The International Commission on Stratigraphy lists the Cambrian Period as beginning at 538.8 million years ago and ending at 485.4 million years ago.\n The lower boundary of the Cambrian was originally held to represent the first appearance of complex life, represented by trilobites. The recognition of small shelly fossils before the first trilobites, and Ediacara biota substantially earlier, has led to calls for a more precisely defined base to the Cambrian Period.[32]\n Despite the long recognition of its distinction from younger Ordovician rocks and older Precambrian rocks, it was not until 1994 that the Cambrian system\/period was internationally ratified. After decades of careful consideration, a continuous sedimentary sequence at Fortune Head, Newfoundland was settled upon as a formal base of the Cambrian Period, which was to be correlated worldwide by the earliest appearance of Treptichnus pedum.[32] Discovery of this fossil a few metres below the GSSP led to the refinement of this statement, and it is the T. pedum ichnofossil assemblage that is now formally used to correlate the base of the Cambrian.[32][33]\n This formal designation allowed radiometric dates to be obtained from samples across the globe that corresponded to the base of the Cambrian. An early date of 570 million years ago quickly gained favour,[32] though the methods used to obtain this number are now considered to be unsuitable and inaccurate. A more precise date using modern radiometric dating yield a date of 538.8 ± 0.2 million years ago.[6] The ash horizon in Oman from which this date was recovered corresponds to a marked fall in the abundance of carbon-13 that correlates to equivalent excursions elsewhere in the world, and to the disappearance of distinctive Ediacaran fossils (Namacalathus, Cloudina). Nevertheless, there are arguments that the dated horizon in Oman does not correspond to the Ediacaran-Cambrian boundary, but represents a facies change from marine to evaporite-dominated strata – which would mean that dates from other sections, ranging from 544 to 542 Ma, are more suitable.[32]\n Plate reconstructions suggest a global supercontinent, Pannotia, was in the process of breaking up early in the Cambrian,[34][35] with Laurentia (North America), Baltica, and Siberia having separated from the main supercontinent of Gondwana to form isolated land masses.[36] Most continental land was clustered in the Southern Hemisphere at this time, but was drifting north.[36] Large, high-velocity rotational movement of Gondwana appears to have occurred in the Early Cambrian.[37]\n With a lack of sea ice – the great glaciers of the Marinoan Snowball Earth were long melted[38] – the sea level was high, which led to large areas of the continents being flooded in warm, shallow seas ideal for sea life. The sea levels fluctuated somewhat, suggesting there were \"ice ages\", associated with pulses of expansion and contraction of a south polar ice cap.[39]\n In Baltoscandia a Lower Cambrian transgression transformed large swathes of the Sub-Cambrian peneplain into an epicontinental sea.[40]\n Glaciers likely existed during the earliest Cambrian at high and possibly even at middle palaeolatitudes,[41] possibly due to the ancient continent of Gondwana covering the South Pole and cutting off polar ocean currents. Middle Terreneuvian deposits, corresponding to the boundary between the Fortunian and Stage 2, show evidence of glaciation.[42] However, other authors believe these very early, pretrilobitic glacial deposits may not even be of Cambrian age at all but instead date back to the Neoproterozoic, an era characterised by numerous severe icehouse periods.[43]\n The beginning of Stage 3 was relatively cool, with the period between 521 and 517 Ma being known as the Cambrian Arthropod Radiation Cool Event (CARCE).[44] The Earth was generally very warm during Stage 4; its climate was comparable to the hot greenhouse of the Late Cretaceous and Early Palaeogene, as evidenced by a maximum in continental weathering rates over the last 900 million years and the presence of tropical, lateritic palaeosols at high palaeolatitudes during this time.[43]\n The Archaecyathid Extinction Warm Event (AEWE), lasting from 511 to 510.5 Ma, was particularly warm. Another warm event, the Redlichiid-Olenid Extinction Warm Event, occurred at the beginning of the Wuliuan.[44] It became even warmer towards the end of the period, and sea levels rose dramatically. This warming trend continued into the Early Ordovician, the start of which was characterised by an extremely hot global climate.[45]\n The Cambrian flora was little different from the Ediacaran. The principal taxa were the marine macroalgae Fuxianospira, Sinocylindra, and Marpolia. No calcareous macroalgae are known from the period.[46]\n No land plant (embryophyte) fossils are known from the Cambrian. However, biofilms and microbial mats were well developed on Cambrian tidal flats and beaches 500 mya,[17] and microbes forming microbial Earth ecosystems, comparable with modern soil crust of desert regions, contributing to soil formation.[47][48] Although molecular clock estimates suggest terrestrial plants may have first emerged during the Middle or Late Cambrian, the consequent large-scale removal of the greenhouse gas CO2 from the atmosphere through sequestration did not begin until the Ordovician.[49]\n The Cambrian explosion was a period of rapid multicellular growth. Most animal life during the Cambrian was aquatic. Trilobites were once assumed to be the dominant life form at that time,[50] but this has proven to be incorrect. Arthropods were by far the most dominant animals in the ocean, but trilobites were only a minor part of the total arthropod diversity. What made them so apparently abundant was their heavy armor reinforced by calcium carbonate (CaCO3), which fossilized far more easily than the fragile chitinous exoskeletons of other arthropods, leaving numerous preserved remains.[51]\n The period marked a steep change in the diversity and composition of Earth's biosphere. The Ediacaran biota suffered a mass extinction at the start of the Cambrian Period, which corresponded with an increase in the abundance and complexity of burrowing behaviour. This behaviour had a profound and irreversible effect on the substrate which transformed the seabed ecosystems. Before the Cambrian, the sea floor was covered by microbial mats. By the end of the Cambrian, burrowing animals had destroyed the mats in many areas through bioturbation. As a consequence, many of those organisms that were dependent on the mats became extinct, while the other species adapted to the changed environment that now offered new ecological niches.[52] Around the same time there was a seemingly rapid appearance of representatives of all the mineralized phyla, including the Bryozoa,[53] which were once thought to have only appeared in the Lower Ordovician.[54] However, many of those phyla were represented only by stem-group forms; and since mineralized phyla generally have a benthic origin, they may not be a good proxy for (more abundant) non-mineralized phyla.[55]\n While the early Cambrian showed such diversification that it has been named the Cambrian Explosion, this changed later in the period, when there occurred a sharp drop in biodiversity. About 515 million years ago, the number of species going extinct exceeded the number of new species appearing. Five million years later, the number of genera had dropped from an earlier peak of about 600 to just 450. Also, the speciation rate in many groups was reduced to between a fifth and a third of previous levels. 500 million years ago, oxygen levels fell dramatically in the oceans, leading to hypoxia, while the level of poisonous hydrogen sulfide simultaneously increased, causing another extinction. The later half of Cambrian was surprisingly barren and showed evidence of several rapid extinction events; the stromatolites which had been replaced by reef building sponges known as Archaeocyatha, returned once more as the archaeocyathids became extinct. This declining trend did not change until the Great Ordovician Biodiversification Event.[57][58]\n Some Cambrian organisms ventured onto land, producing the trace fossils Protichnites and Climactichnites. Fossil evidence suggests that euthycarcinoids, an extinct group of arthropods, produced at least some of the Protichnites.[59] Fossils of the track-maker of Climactichnites have not been found; however, fossil trackways and resting traces suggest a large, slug-like mollusc.[60]\n In contrast to later periods, the Cambrian fauna was somewhat restricted; free-floating organisms were rare, with the majority living on or close to the sea floor;[61] and mineralizing animals were rarer than in future periods, in part due to the unfavourable ocean chemistry.[61]\n Many modes of preservation are unique to the Cambrian, and some preserve soft body parts, resulting in an abundance of Lagerstätten. These include Sirius Passet,[62][63] the Sinsk Algal Lens,[64] the Maotianshan Shales,[65] the Emu Bay Shale,[66] and the Burgess Shale,[67][68][69].\n The United States Federal Geographic Data Committee uses a \"barred capital C\" ⟨Ꞓ⟩ character to represent the Cambrian Period.[70]\nThe Unicode character is U+A792 Ꞓ LATIN CAPITAL LETTER C WITH BAR.[71][72]\n"}
{"key":"Cat","link":"https:\/\/en.wikipedia.org\/wiki\/Cat","headline":"Cat - Wikipedia","content":"\n The cat (Felis catus), commonly referred to as the domestic cat or house cat, is the only domesticated species in the family Felidae. Recent advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC. It is commonly kept as a house pet and farm cat, but also ranges freely as a feral cat avoiding human contact. It is valued by humans for companionship and its ability to kill vermin. Because of its retractable claws, it is adapted to killing small prey like mice and rats. It has a strong, flexible body, quick reflexes, sharp teeth, and its night vision and sense of smell are well developed. It is a social species, but a solitary hunter and a crepuscular predator. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling, and grunting as well as cat body language. It can hear sounds too faint or too high in frequency for human ears, such as those made by small mammals. It also secretes and perceives pheromones.\n Female domestic cats can have kittens from spring to late autumn in temperate zones and throughout the year in equatorial regions, with litter sizes often ranging from two to five kittens. Domestic cats are bred and shown at events as registered pedigreed cats, a hobby known as cat fancy. Animal population control of cats may be achieved by spaying and neutering, but their proliferation and the abandonment of pets has resulted in large numbers of feral cats worldwide, contributing to the extinction of bird, mammal and reptile species.\n As of 2017,[update] the domestic cat was the second most popular pet in the United States, with 95.6 million cats owned and around 42 million households owning at least one cat. In the United Kingdom, 26% of adults have a cat, with an estimated population of 10.9 million pet cats as of 2020.[update] As of 2021,[update] there were an estimated 220 million owned and 480 million stray cats in the world.\n The origin of the English word cat, Old English catt, is thought to be the Late Latin word cattus, which was first used at the beginning of the 6th century.[4] The Late Latin word may be derived from an unidentified African language.[5] The Nubian word kaddîska 'wildcat' and Nobiin kadīs are possible sources or cognates.[6] The Nubian word may be a loan from Arabic قَطّ‎ qaṭṭ ~ قِطّ qiṭṭ.[citation needed]\n The forms might also have derived from an ancient Germanic word that was imported into Latin and then into Greek, Syriac, and Arabic.[7] The word may be derived from Germanic and Northern European languages, and ultimately be borrowed from Uralic, cf. Northern Sámi gáđfi, 'female stoat', and Hungarian hölgy, 'lady, female stoat'; from Proto-Uralic *käďwä, 'female (of a furred animal)'.[8]\n The English puss, extended as pussy and pussycat, is attested from the 16th century and may have been introduced from Dutch poes or from Low German puuskatte, related to Swedish kattepus, or Norwegian pus, pusekatt. Similar forms exist in Lithuanian puižė and Irish puisín or puiscín. The etymology of this word is unknown, but it may have arisen from a sound used to attract a cat.[9][10]\n A male cat is called a tom or tomcat[11] (or a gib,[12] if neutered). A female is called a queen[13] or a molly,[14][user-generated source?] if spayed, especially in a cat-breeding context. A juvenile cat is referred to as a kitten. In Early Modern English, the word kitten was interchangeable with the now-obsolete word catling.[15]\n A group of cats can be referred to as a clowder or a glaring.[16]\n The scientific name Felis catus was proposed by Carl Linnaeus in 1758 for a domestic cat.[1][2] Felis catus domesticus was proposed by Johann Christian Polycarp Erxleben in 1777.[3] Felis daemon proposed by Konstantin Satunin in 1904 was a black cat from the Transcaucasus, later identified as a domestic cat.[17][18]\n In 2003, the International Commission on Zoological Nomenclature ruled that the domestic cat is a distinct species, namely Felis catus.[19][20]\nIn 2007, the modern domesticated subspecies F. silvestris catus sampled worldwide was considered to have likely descended from the Near Eastern wildcat (F. lybica) following results of phylogenetic research.[21][22][a] In 2017, the IUCN Cat Classification Taskforce followed the recommendation of the ICZN in regarding the domestic cat as a distinct species, Felis catus.[23]\n The domestic cat is a member of the Felidae, a family that had a common ancestor about 10 to 15 million years ago.[24] The evolutionary radiation of the Felidae began in Asia during the Miocene around 8.38 to 14.45 million years ago.[25] Analysis of mitochondrial DNA of all Felidae species indicates a radiation at 6.46 to 16.76 million years ago.[26]\nThe genus Felis genetically diverged from other Felidae around 6 to 7 million years ago.[25] \nResults of phylogenetic research shows that the wild members of this genus evolved through sympatric or parapatric speciation, whereas the domestic cat evolved through artificial selection.[27]\nThe domestic cat and its closest wild ancestor are diploid and both possess 38 chromosomes[28] and roughly 20,000 genes.[29]\n Pantherinae\n other Felinae lineages\n Jungle cat (F. chaus) \n Black-footed cat (F. nigripes)\n Sand cat (F. margarita)\n Chinese mountain cat (F. bieti)\n African wildcat (F. lybica)\n European wildcat (F. silvestris) \n Domestic cat \n Sand cat (F. margarita)\n Chinese mountain cat (F. bieti)\n European wildcat (F. silvestris) \n Southern African wildcat (F. l. cafra)\n Asiatic wildcat (F. l. ornata)\n Near Eastern wildcat\n Domestic cat \n It was long thought that the domestication of the cat began in ancient Egypt, where cats were venerated from around 3100 BC,[31][32]\nHowever, the earliest known indication for the taming of an African wildcat was excavated close by a human Neolithic grave in Shillourokambos, southern Cyprus, dating to about 7500–7200 BC. Since there is no evidence of native mammalian fauna on Cyprus, the inhabitants of this Neolithic village most likely brought the cat and other wild mammals to the island from the Middle Eastern mainland.[33] Scientists therefore assume that African wildcats were attracted to early human settlements in the Fertile Crescent by rodents, in particular the house mouse (Mus musculus), and were tamed by Neolithic farmers. This mutual relationship between early farmers and tamed cats lasted thousands of years. As agricultural practices spread, so did tame and domesticated cats.[30][34] Wildcats of Egypt contributed to the maternal gene pool of the domestic cat at a later time.[35]\n The earliest known evidence for the occurrence of the domestic cat in Greece dates to around 1200 BC. Greek, Phoenician, Carthaginian and Etruscan traders introduced domestic cats to southern Europe.[36] During the Roman Empire they were introduced to Corsica and Sardinia before the beginning of the 1st millennium.[37] By the 5th century BC, they were familiar animals around settlements in Magna Graecia and Etruria.[38] By the end of the Western Roman Empire in the 5th century, the Egyptian domestic cat lineage had arrived in a Baltic Sea port in northern Germany.[35]\n The leopard cat (Prionailurus bengalensis) was tamed independently in China around 5500 BC. This line of partially domesticated cats leaves no trace in the domestic cat populations of today.[39]\n During domestication, cats have undergone only minor changes in anatomy and behavior, and they are still capable of surviving in the wild. Several natural behaviors and characteristics of wildcats may have pre-adapted them for domestication as pets. These traits include their small size, social nature, obvious body language, love of play, and high intelligence. Since they practice rigorous grooming habits and have an instinctual drive to bury and hide their urine and feces, they are generally much less messy than other domesticated animals. Captive Leopardus cats may also display affectionate behavior toward humans but were not domesticated.[40] House cats often mate with feral cats.[41] Hybridisation between domestic and other Felinae species is also possible, producing hybrids such as the Kellas cat in Scotland.[42][43]\n Development of cat breeds started in the mid 19th century.[44] An analysis of the domestic cat genome revealed that the ancestral wildcat genome was significantly altered in the process of domestication, as specific mutations were selected to develop cat breeds.[45] Most breeds are founded on random-bred domestic cats. Genetic diversity of these breeds varies between regions, and is lowest in purebred populations, which show more than 20 deleterious genetic disorders.[46]\n The domestic cat has a smaller skull and shorter bones than the European wildcat.[47] It averages about 46 cm (18 in) in head-to-body length and 23–25 cm (9.1–9.8 in) in height, with about 30 cm (12 in) long tails. Males are larger than females.[48]\nAdult domestic cats typically weigh 4–5 kg (8.8–11.0 lb).[27]\n Cats have seven cervical vertebrae (as do most mammals); 13 thoracic vertebrae (humans have 12); seven lumbar vertebrae (humans have five); three sacral vertebrae (as do most mammals, but humans have five); and a variable number of caudal vertebrae in the tail (humans have only three to five vestigial caudal vertebrae, fused into an internal coccyx).[49]: 11  The extra lumbar and thoracic vertebrae account for the cat's spinal mobility and flexibility. Attached to the spine are 13 ribs, the shoulder, and the pelvis.[49]: 16  Unlike human arms, cat forelimbs are attached to the shoulder by free-floating clavicle bones which allow them to pass their body through any space into which they can fit their head.[50]\n The cat skull is unusual among mammals in having very large eye sockets and a powerful specialized jaw.[51]: 35  Within the jaw, cats have teeth adapted for killing prey and tearing meat. When it overpowers its prey, a cat delivers a lethal neck bite with its two long canine teeth, inserting them between two of the prey's vertebrae and severing its spinal cord, causing irreversible paralysis and death.[52] Compared to other felines, domestic cats have narrowly spaced canine teeth relative to the size of their jaw, which is an adaptation to their preferred prey of small rodents, which have small vertebrae.[52]\n The premolar and first molar together compose the carnassial pair on each side of the mouth, which efficiently shears meat into small pieces, like a pair of scissors. These are vital in feeding, since cats' small molars cannot chew food effectively, and cats are largely incapable of mastication.[51]: 37  Cats tend to have better teeth than most humans, with decay generally less likely because of a thicker protective layer of enamel, a less damaging saliva, less retention of food particles between teeth, and a diet mostly devoid of sugar. Nonetheless, they are subject to occasional tooth loss and infection.[53]\n Cats have protractible and retractable claws.[54] In their normal, relaxed position, the claws are sheathed with the skin and fur around the paw's toe pads. This keeps the claws sharp by preventing wear from contact with the ground and allows for the silent stalking of prey. The claws on the forefeet are typically sharper than those on the hindfeet.[55] Cats can voluntarily extend their claws on one or more paws. They may extend their claws in hunting or self-defense, climbing, kneading, or for extra traction on soft surfaces. Cats shed the outside layer of their claw sheaths when scratching rough surfaces.[56]\n Most cats have five claws on their front paws and four on their rear paws. The dewclaw is proximal to the other claws. More proximally is a protrusion which appears to be a sixth \"finger\". This special feature of the front paws on the inside of the wrists has no function in normal walking but is thought to be an antiskidding device used while jumping. Some cat breeds are prone to having extra digits (\"polydactyly\").[57] Polydactylous cats occur along North America's northeast coast and in Great Britain.[58]\n The cat is digitigrade. It walks on the toes, with the bones of the feet making up the lower part of the visible leg.[59] Unlike most mammals, it uses a \"pacing\" gait and moves both legs on one side of the body before the legs on the other side. It registers directly by placing each hind paw close to the track of the corresponding fore paw, minimizing noise and visible tracks. This also provides sure footing for hind paws when navigating rough terrain. As it speeds up from walking to trotting, its gait changes to a \"diagonal\" gait: The diagonally opposite hind and fore legs move simultaneously.[60]\n Cats are generally fond of sitting in high places or perching. A higher place may serve as a concealed site from which to hunt; domestic cats strike prey by pouncing from a perch such as a tree branch. Another possible explanation is that height gives the cat a better observation point, allowing it to survey its territory. A cat falling from heights of up to 3 m (9.8 ft) can right itself and land on its paws.[61]\n During a fall from a high place, a cat reflexively twists its body and rights itself to land on its feet using its acute sense of balance and flexibility. This reflex is known as the cat righting reflex.[62] A cat always rights itself in the same way during a fall, if it has enough time to do so, which is the case in falls of 90 cm (3.0 ft) or more.[63] How cats are able to right themselves when falling has been investigated as the \"falling cat problem\".[64]\n The cat family (Felidae) can pass down many colors and patterns to their offspring. The domestic cat genes MC1R and ASIP allow for the variety of color in coats. The feline ASIP gene consists of three coding exons.[65] Three novel microsatellite markers linked to ASIP were isolated from a domestic cat BAC clone containing this gene and were used to perform linkage analysis in a pedigree of 89 domestic cats that segregated for melanism.[citation needed]\n Cats have excellent night vision and can see at one sixth the light level required for human vision.[51]: 43  This is partly the result of cat eyes having a tapetum lucidum, which reflects any light that passes through the retina back into the eye, thereby increasing the eye's sensitivity to dim light.[66] Large pupils are an adaptation to dim light. The domestic cat has slit pupils, which allow it to focus bright light without chromatic aberration.[67] At low light, a cat's pupils expand to cover most of the exposed surface of its eyes.[68] The domestic cat has rather poor color vision and only two types of cone cells, optimized for sensitivity to blue and yellowish green; its ability to distinguish between red and green is limited.[69] A response to middle wavelengths from a system other than the rod cells might be due to a third type of cone. This appears to be an adaptation to low light levels rather than representing true trichromatic vision.[70] Cats also have a nictitating membrane, allowing them to blink without hindering their vision.\n The domestic cat's hearing is most acute in the range of 500 Hz to 32 kHz.[71] It can detect an extremely broad range of frequencies ranging from 55 Hz to 79 kHz, whereas humans can only detect frequencies between 20 Hz and 20 kHz. It can hear a range of 10.5 octaves, while humans and dogs can hear ranges of about 9 octaves.[72][73]\nIts hearing sensitivity is enhanced by its large movable outer ears, the pinnae, which amplify sounds and help detect the location of a noise. It can detect ultrasound, which enables it to detect ultrasonic calls made by rodent prey.[74][75] Recent research has shown that cats have socio-spatial cognitive abilities to create mental maps of owners' locations based on hearing owners' voices.[76]\n Cats have an acute sense of smell, due in part to their well-developed olfactory bulb and a large surface of olfactory mucosa, about 5.8 cm2 (0.90 in2) in area, which is about twice that of humans.[77] Cats and many other animals have a Jacobson's organ in their mouths that is used in the behavioral process of flehmening. It allows them to sense certain aromas in a way that humans cannot. Cats are sensitive to pheromones such as 3-mercapto-3-methylbutan-1-ol,[78] which they use to communicate through urine spraying and marking with scent glands.[79] Many cats also respond strongly to plants that contain nepetalactone, especially catnip, as they can detect that substance at less than one part per billion.[80] About 70–80% of cats are affected by nepetalactone.[81] This response is also produced by other plants, such as silver vine (Actinidia polygama) and the herb valerian; it may be caused by the smell of these plants mimicking a pheromone and stimulating cats' social or sexual behaviors.[82]\n Cats have relatively few taste buds compared to humans (470 or so versus more than 9,000 on the human tongue).[83] Domestic and wild cats share a taste receptor gene mutation that keeps their sweet taste buds from binding to sugary molecules, leaving them with no ability to taste sweetness.[84] They, however, possess taste bud receptors specialized for acids, amino acids like protein, and bitter tastes.[85] Their taste buds possess the receptors needed to detect umami. However, these receptors contain molecular changes that make the cat taste of umami different from that of humans. In humans, they detect the amino acids of glutamic acid and aspartic acid, but in cats they instead detect inosine monophosphate and l-Histidine.[86] These molecules are particularly enriched in tuna.[86] This has been argued is why cats find tuna so palatable: as put by researchers into cat taste, \"the specific combination of the high IMP and free l-Histidine contents of tuna\" .. \"produces a strong umami taste synergy that is highly preferred by cats\".[86] One of the researchers involved in this research has further claimed, \"I think umami is as important for cats as sweet is for humans\".[87]\n Cats also have a distinct temperature preference for their food, preferring food with a temperature around 38 °C (100 °F) which is similar to that of a fresh kill; some cats reject cold food (which would signal to the cat that the \"prey\" item is long dead and therefore possibly toxic or decomposing).[83]\n To aid with navigation and sensation, cats have dozens of movable whiskers (vibrissae) over their body, especially their faces. These provide information on the width of gaps and on the location of objects in the dark, both by touching objects directly and by sensing air currents; they also trigger protective blink reflexes to protect the eyes from damage.[51]: 47 \n Outdoor cats are active both day and night, although they tend to be slightly more active at night.[88] Domestic cats spend the majority of their time in the vicinity of their homes but can range many hundreds of meters from this central point. They establish territories that vary considerably in size, in one study ranging 7–28 ha (17–69 acres).[89] The timing of cats' activity is quite flexible and varied but being low-light predators, they are generally crepuscular, which means they tend to be more active near dawn and dusk. However, house cats' behavior is also influenced by human activity and they may adapt to their owners' sleeping patterns to some extent.[90][91]\n Cats conserve energy by sleeping more than most animals, especially as they grow older. The daily duration of sleep varies, usually between 12 and 16 hours, with 13 and 14 being the average. Some cats can sleep as much as 20 hours. The term \"cat nap\" for a short rest refers to the cat's tendency to fall asleep (lightly) for a brief period. While asleep, cats experience short periods of rapid eye movement sleep often accompanied by muscle twitches, which suggests they are dreaming.[92]\n The social behavior of the domestic cat ranges from widely dispersed individuals to feral cat colonies that gather around a food source, based on groups of co-operating females.[93][94] Within such groups, one cat is usually dominant over the others.[95] Each cat in a colony holds a distinct territory, with sexually active males having the largest territories, which are about 10 times larger than those of female cats and may overlap with several females' territories. These territories are marked by urine spraying, by rubbing objects at head height with secretions from facial glands, and by defecation.[79] Between these territories are neutral areas where cats watch and greet one another without territorial conflicts. Outside these neutral areas, territory holders usually chase away stranger cats, at first by staring, hissing, and growling and, if that does not work, by short but noisy and violent attacks. Despite this colonial organization, cats do not have a social survival strategy or a herd behavior, and always hunt alone.[96]\n Life in proximity to humans and other domestic animals has led to a symbiotic social adaptation in cats, and cats may express great affection toward humans or other animals. Ethologically, a cat's human keeper functions as if a mother surrogate.[97] Adult cats live their lives in a kind of extended kittenhood, a form of behavioral neoteny. Their high-pitched sounds may mimic the cries of a hungry human infant, making them particularly difficult for humans to ignore.[98] Some pet cats are poorly socialized. In particular, older cats show aggressiveness toward newly arrived kittens, which include biting and scratching; this type of behavior is known as feline asocial aggression.[99]\n Redirected aggression is a common form of aggression which can occur in multiple cat households. In redirected aggression there is usually something that agitates the cat: this could be a sight, sound, or another source of stimuli which causes a heightened level of anxiety or arousal. If the cat cannot attack the stimuli, it may direct anger elsewhere by attacking or directing aggression to the nearest cat, dog, human or other being.[100][101]\n Domestic cats' scent rubbing behavior toward humans or other cats is thought to be a feline means for social bonding.[102]\n Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling\/snarling, grunting, and several different forms of meowing.[103] Their body language, including position of ears and tail, relaxation of the whole body, and kneading of the paws, are all indicators of mood. The tail and ears are particularly important social signal mechanisms in cats. A raised tail indicates a friendly greeting, and flattened ears indicate hostility. Tail-raising also indicates the cat's position in the group's social hierarchy, with dominant individuals raising their tails less often than subordinate ones.[104] Feral cats are generally silent.[105]: 208  Nose-to-nose touching is also a common greeting and may be followed by social grooming, which is solicited by one of the cats raising and tilting its head.[93]\n Purring may have developed as an evolutionary advantage as a signaling mechanism of reassurance between mother cats and nursing kittens, who are thought to use it as a care-soliciting signal.[106]\nPost-nursing cats also often purr as a sign of contentment: when being petted, becoming relaxed,[107][108] or eating. Even though purring is popularly interpreted as indicative of pleasure, it has been recorded in a wide variety of circumstances, most of which involve physical contact between the cat and another, presumably trusted individual.[106] Some cats have been observed to purr continuously when chronically ill or in apparent pain.[109]\n The exact mechanism by which cats purr has long been elusive, but it has been proposed that purring is generated via a series of sudden build-ups and releases of pressure as the glottis is opened and closed, which causes the vocal folds to separate forcefully. The laryngeal muscles in control of the glottis are thought to be driven by a neural oscillator which generates a cycle of contraction and release every 30–40 milliseconds (giving a frequency of 33 to 25 Hz).[106][110][111]\n Domestic cats observed in a rescue facility have total of 276 distinct facial expressions based on 26 different facial movements; each facial expression corresponds to different social functions that are likely influenced by domestication.[112]\n Cats are known for spending considerable amounts of time licking their coats to keep them clean.[113][114] The cat's tongue has backward-facing spines about 500 μm long, which are called papillae. These contain keratin which makes them rigid[115] so the papillae act like a hairbrush. Some cats, particularly longhaired cats, occasionally regurgitate hairballs of fur that have collected in their stomachs from grooming. These clumps of fur are usually sausage-shaped and about 2–3 cm (0.79–1.18 in) long. Hairballs can be prevented with remedies that ease elimination of the hair through the gut, as well as regular grooming of the coat with a comb or stiff brush.[113]\n Among domestic cats, males are more likely to fight than females.[116] Among feral cats, the most common reason for cat fighting is competition between two males to mate with a female. In such cases, most fights are won by the heavier male.[117] Another common reason for fighting in domestic cats is the difficulty of establishing territories within a small home.[116] Female cats also fight over territory or to defend their kittens. Neutering will decrease or eliminate this behavior in many cases, suggesting that the behavior is linked to sex hormones.[118]\n When cats become aggressive, they try to make themselves appear larger and more threatening by raising their fur, arching their backs, turning sideways and hissing or spitting.[119] Often, the ears are pointed down and back to avoid damage to the inner ear and potentially listen for any changes behind them while focused forward. Cats may also vocalize loudly and bare their teeth in an effort to further intimidate their opponents. Fights usually consist of grappling and delivering powerful slaps to the face and body with the forepaws as well as bites. Cats also throw themselves to the ground in a defensive posture to rake their opponent's belly with their powerful hind legs.[120]\n Serious damage is rare, as the fights are usually short in duration, with the loser running away with little more than a few scratches to the face and ears. Fights for mating rights are typically more severe and injuries may include deep puncture wounds and lacerations. Normally, serious injuries from fighting are limited to infections of scratches and bites, though these can occasionally kill cats if untreated. In addition, bites are probably the main route of transmission of feline immunodeficiency virus.[121] Sexually active males are usually involved in many fights during their lives, and often have decidedly battered faces with obvious scars and cuts to their ears and nose.[122] Cats are willing to threaten animals larger than them to defend their territory, such as dogs and foxes.[123]\n The shape and structure of cats' cheeks is insufficient to allow them to take in liquids using suction. Therefore, when drinking they lap with the tongue to draw liquid upward into their mouths. Lapping at a rate of four times a second, the cat touches the smooth tip of its tongue to the surface of the water, and quickly retracts it like a corkscrew, drawing water upward.[124][125]\n Feral cats and free-fed house cats consume several small meals in a day. The frequency and size of meals varies between individuals. They select food based on its temperature, smell and texture; they dislike chilled foods and respond most strongly to moist foods rich in amino acids, which are similar to meat. Cats reject novel flavors (a response termed neophobia) and learn quickly to avoid foods that have tasted unpleasant in the past.[96][126] It is also a common misconception that cats like milk\/cream, as they tend to avoid sweet food and milk. Most adult cats are lactose intolerant; the sugar in milk is not easily digested and may cause soft stools or diarrhea.[127] Some also develop odd eating habits and like to eat or chew on things like wool, plastic, cables, paper, string, aluminum foil, or even coal. This condition, pica, can threaten their health, depending on the amount and toxicity of the items eaten.[128]\n Cats hunt small prey, primarily birds and rodents,[129] and are often used as a form of pest control.[130][131] Other common small creatures such as lizards and snakes may also become prey.[132] Cats use two hunting strategies, either stalking prey actively, or waiting in ambush until an animal comes close enough to be captured.[133] The strategy used depends on the prey species in the area, with cats waiting in ambush outside burrows, but tending to actively stalk birds.[134]: 153  Domestic cats are a major predator of wildlife in the United States, killing an estimated 1.3 to 4.0 billion birds and 6.3 to 22.3 billion mammals annually.[135]\n Certain species appear more susceptible than others; in one English village, for example, 30% of house sparrow mortality was linked to the domestic cat.[136] In the recovery of ringed robins (Erithacus rubecula) and dunnocks (Prunella modularis) in Britain, 31% of deaths were a result of cat predation.[137] In parts of North America, the presence of larger carnivores such as coyotes which prey on cats and other small predators reduces the effect of predation by cats and other small predators such as opossums and raccoons on bird numbers and variety.[138]\n Perhaps the best-known element of cats' hunting behavior, which is commonly misunderstood and often appalls cat owners because it looks like torture, is that cats often appear to \"play\" with prey by releasing and recapturing it. This cat and mouse behavior is due to an instinctive imperative to ensure that the prey is weak enough to be killed without endangering the cat.[139]\n Another poorly understood element of cat hunting behavior is the presentation of prey to human guardians. One explanation is that cats adopt humans into their social group and share excess kill with others in the group according to the dominance hierarchy, in which humans are reacted to as if they are at or near the top.[140] Another explanation is that they attempt to teach their guardians to hunt or to help their human as if feeding \"an elderly cat, or an inept kitten\".[141] This hypothesis is inconsistent with the fact that male cats also bring home prey, despite males having negligible involvement in raising kittens.[134]: 153 \n Domestic cats, especially young kittens, are known for their love of play. This behavior mimics hunting and is important in helping kittens learn to stalk, capture, and kill prey.[142] Cats also engage in play fighting, with each other and with humans. This behavior may be a way for cats to practice the skills needed for real combat, and might also reduce any fear they associate with launching attacks on other animals.[143]\n Cats also tend to play with toys more when they are hungry.[144] Owing to the close similarity between play and hunting, cats prefer to play with objects that resemble prey, such as small furry toys that move rapidly, but rapidly lose interest. They become habituated to a toy they have played with before.[145] String is often used as a toy, but if it is eaten, it can become caught at the base of the cat's tongue and then move into the intestines, a medical emergency which can cause serious illness, even death.[146] Owing to the risks posed by cats eating string, it is sometimes replaced with a laser pointer's dot, which cats may chase.[147]\n The cat secretes and perceives pheromones.[148]\nFemale cats, called queens, are polyestrous with several estrus cycles during a year, lasting usually 21 days. They are usually ready to mate between early February and August[149] in northern temperate zones and throughout the year in equatorial regions.[150]\n Several males, called tomcats, are attracted to a female in heat. They fight over her, and the victor wins the right to mate. At first, the female rejects the male, but eventually, the female allows the male to mate. The female utters a loud yowl as the male pulls out of her because a male cat's penis has a band of about 120–150 backward-pointing penile spines, which are about 1 mm (0.039 in) long; upon withdrawal of the penis, the spines may provide the female with increased sexual stimulation, which acts to induce ovulation.[151]\n After mating, the female cleans her vulva thoroughly. If a male attempts to mate with her at this point, the female attacks him. After about 20 to 30 minutes, once the female is finished grooming, the cycle will repeat.[152] Because ovulation is not always triggered by a single mating, females may not be impregnated by the first male with which they mate.[153] Furthermore, cats are superfecund; that is, a female may mate with more than one male when she is in heat, with the result that different kittens in a litter may have different fathers.[152]\n The morula forms 124 hours after conception. At 148 hours, early blastocysts form. At 10–12 days, implantation occurs.[154] The gestation of queens lasts between 64 and 67 days, with an average of 65 days.[149][155]\n Data on the reproductive capacity of more than 2,300 free-ranging queens were collected during a study between May 1998 and October 2000. They had one to six kittens per litter, with an average of three kittens. They produced a mean of 1.4 litters per year, but a maximum of three litters in a year. Of 169 kittens, 127 died before they were six months old due to a trauma caused in most cases by dog attacks and road accidents.[156]\nThe first litter is usually smaller than subsequent litters. Kittens are weaned between six and seven weeks of age. Queens normally reach sexual maturity at 5–10 months, and males at 5–7 months. This varies depending on breed.[152] Kittens reach puberty at the age of 9–10 months.[149]\n Cats are ready to go to new homes at about 12 weeks of age, when they are ready to leave their mother.[157] They can be surgically sterilized (spayed or castrated) as early as seven weeks to limit unwanted reproduction.[158] This surgery also prevents undesirable sex-related behavior, such as aggression, territory marking (spraying urine) in males and yowling (calling) in females. Traditionally, this surgery was performed at around six to nine months of age, but it is increasingly being performed before puberty, at about three to six months.[159] In the United States, about 80% of household cats are neutered.[160]\n The average lifespan of pet cats has risen in recent decades. In the early 1980s, it was about seven years,[161]: 33 [162] rising to 9.4 years in 1995[161]: 33  and an average of about 13 years as of 2014 and 2023.[163][164] Some cats have been reported as surviving into their 30s,[165] with the oldest known cat dying at a verified age of 38.[166]\n Neutering increases life expectancy: one study found castrated male cats live twice as long as intact males, while spayed female cats live 62% longer than intact females.[161]: 35  Having a cat neutered confers some health benefits such as greater life expectancy and decreased incidence of neoplasia;[167] however neutering decreases metabolism and,[168][169][170] increases food intake,[170][171] both of these cause obesity to be a predisposition for neutered cats.[172]\n About 250 heritable genetic disorders have been identified in cats, many similar to human inborn errors of metabolism.[173] The high level of similarity among the metabolism of mammals allows many of these feline diseases to be diagnosed using genetic tests that were originally developed for use in humans, as well as the use of cats as animal models in the study of the human diseases.[174][175] Diseases affecting domestic cats include acute infections, parasitic infestations, injuries, and chronic diseases such as kidney disease, thyroid disease, and arthritis. Vaccinations are available for many infectious diseases, as are treatments to eliminate parasites such as worms, ticks, and fleas.[176]\n The domestic cat is a cosmopolitan species and occurs across much of the world.[46] It is adaptable and now present on all continents except Antarctica, and on 118 of the 131 main groups of islands, even on the isolated Kerguelen Islands.[177][178] Due to its ability to thrive in almost any terrestrial habitat, it is among the world's most invasive species.[179] It lives on small islands with no human inhabitants.[180] Feral cats can live in forests, grasslands, tundra, coastal areas, agricultural land, scrublands, urban areas, and wetlands.[181]\n The unwantedness that leads to the domestic cat being treated as an invasive species is twofold. On one hand, as it is little altered from the wildcat, it can readily interbreed with the wildcat. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary, possibly also the Iberian Peninsula, and where protected natural areas are close to human-dominated landscapes, such as Kruger National Park in South Africa.[182][43] However, its introduction to places where no native felines are present also contributes to the decline of native species.[183]\n Feral cats are domestic cats that were born in or have reverted to a wild state. They are unfamiliar with and wary of humans and roam freely in urban and rural areas.[184] The numbers of feral cats is not known, but estimates of the United States feral population range from 25 to 60 million.[184] Feral cats may live alone, but most are found in large colonies, which occupy a specific territory and are usually associated with a source of food.[185] Famous feral cat colonies are found in Rome around the Colosseum and Forum Romanum, with cats at some of these sites being fed and given medical attention by volunteers.[186]\n Public attitudes toward feral cats vary widely, from seeing them as free-ranging pets to regarding them as vermin.[187]\n Some feral cats can be successfully socialized and 're-tamed' for adoption; young cats, especially kittens[188] and cats that have had prior experience and contact with humans are the most receptive to these efforts.\n On islands, birds can contribute as much as 60% of a cat's diet.[189] In nearly all cases, the cat cannot be identified as the sole cause for reducing the numbers of island birds, and in some instances, eradication of cats has caused a \"mesopredator release\" effect;[190] where the suppression of top carnivores creates an abundance of smaller predators that cause a severe decline in their shared prey. Domestic cats are a contributing factor to the decline of many species, a factor that has ultimately led, in some cases, to extinction. The South Island piopio, Chatham rail,[137] and the New Zealand merganser[191] are a few from a long list, with the most extreme case being the flightless Lyall's wren, which was driven to extinction only a few years after its discovery.[192][193]\nOne feral cat in New Zealand killed 102 New Zealand lesser short-tailed bats in seven days.[194] In the US, feral and free-ranging domestic cats kill an estimated 6.3 – 22.3 billion mammals annually.[135]\n In Australia one study found feral cats to kill 466 million reptiles per year. More than 258 reptile species were identified as being predated by cats.[195] Cats have contributed to the extinction of the Navassa curly-tailed lizard and Chioninia coctei.[183]\n Cats are common pets throughout the world, and their worldwide population as of 2007 exceeded 500 million.[196]\nAs of 2017,[update] the domestic cat was the second most popular pet in the United States, with 95.6 million cats owned[197][198] and around 42 million households owning at least one cat.[199] In the United Kingdom, 26% of adults have a cat, with an estimated population of 10.9 million pet cats as of 2020.[update][200] As of 2021,[update] there were an estimated 220 million owned and 480 million stray cats in the world.[201][202][203]\n Cats have been used for millennia to control rodents, notably around grain stores and aboard ships, and both uses extend to the present day.[204][205]\n As well as being kept as pets, cats are also used in the international fur trade[206] and leather industries for making coats, hats, blankets, stuffed toys,[207] shoes, gloves, and musical instruments.[208] About 24 cats are needed to make a cat-fur coat.[209] This use has been outlawed in the United States since 2000 and in the European Union (as well as the United Kingdom) since 2007.[210]\n Cat pelts have been used for superstitious purposes as part of the practice of witchcraft,[211] and are still made into blankets in Switzerland as traditional medicine thought to cure rheumatism.[212]\n A few attempts to build a cat census have been made over the years, both through associations or national and international organizations (such as that of the Canadian Federation of Humane Societies[213]) and over the Internet,[214][215] but such a task does not seem simple to achieve. General estimates for the global population of domestic cats range widely from anywhere between 200 million to 600 million.[216][217][218][219][220] Walter Chandoha made his career photographing cats after his 1949 images of Loco, an especially charming stray taken in, were published around the world. He is reported to have photographed 90,000 cats during his career and maintained an archive of 225,000 images that he drew from for publications during his lifetime.[221]\n A cat show is a judged event in which the owners of cats compete to win titles in various cat-registering organizations by entering their cats to be judged after a breed standard.[222] It is often required that a cat must be healthy and vaccinated in order to participate in a cat show.[222] Both pedigreed and non-purebred companion (\"moggy\") cats are admissible, although the rules differ depending on the organization. Competing cats are compared to the applicable breed standard, and assessed for temperament.[222]\n Cats can be infected or infested with viruses, bacteria, fungus, protozoans, arthropods or worms that can transmit diseases to humans.[223] In some cases, the cat exhibits no symptoms of the disease.[224] The same disease can then become evident in a human.[225] The likelihood that a person will become diseased depends on the age and immune status of the person. Humans who have cats living in their home or in close association are more likely to become infected. Others might also acquire infections from cat feces and parasites exiting the cat's body.[223][226] Some of the infections of most concern include salmonella, cat-scratch disease and toxoplasmosis.[224]\n In ancient Egypt, cats were revered, and the goddess Bastet often depicted in cat form, sometimes taking on the war-like aspect of a lioness. The Greek historian Herodotus reported that killing a cat was forbidden, and when a household cat died, the entire family mourned and shaved their eyebrows. Families took their dead cats to the sacred city of Bubastis, where they were embalmed and buried in sacred repositories. Herodotus expressed astonishment at the domestic cats in Egypt, because he had only ever seen wildcats.[227]\n Ancient Greeks and Romans kept weasels as pets, which were seen as the ideal rodent-killers. The earliest unmistakable evidence of the Greeks having domestic cats comes from two coins from Magna Graecia dating to the mid-fifth century BC showing Iokastos and Phalanthos, the legendary founders of Rhegion and Taras respectively, playing with their pet cats. The usual ancient Greek word for 'cat' was ailouros, meaning 'thing with the waving tail'. Cats are rarely mentioned in ancient Greek literature. Aristotle remarked in his History of Animals that \"female cats are naturally lecherous.\" The Greeks later syncretized their own goddess Artemis with the Egyptian goddess Bastet, adopting Bastet's associations with cats and ascribing them to Artemis. In Ovid's Metamorphoses, when the deities flee to Egypt and take animal forms, the goddess Diana turns into a cat.[228][229]\n Cats eventually displaced weasels as the pest control of choice because they were more pleasant to have around the house and were more enthusiastic hunters of mice. During the Middle Ages, many of Artemis's associations with cats were grafted onto the Virgin Mary. Cats are often shown in icons of Annunciation and of the Holy Family and, according to Italian folklore, on the same night that Mary gave birth to Jesus, a cat in Bethlehem gave birth to a kitten.[230] Domestic cats were spread throughout much of the rest of the world during the Age of Discovery, as ships' cats were carried on sailing ships to control shipboard rodents and as good-luck charms.[36]\n Several ancient religions believed cats are exalted souls, companions or guides for humans, that are all-knowing but mute so they cannot influence decisions made by humans. In Japan, the maneki neko cat is a symbol of good fortune.[231] In Norse mythology, Freyja, the goddess of love, beauty, and fertility, is depicted as riding a chariot drawn by cats.[232] In Jewish legend, the first cat was living in the house of the first man Adam as a pet that got rid of mice. The cat was once partnering with the first dog before the latter broke an oath they had made which resulted in enmity between the descendants of these two animals. It is also written that neither cats nor foxes are represented in the water, while every other animal has an incarnation species in the water.[233] Although no species are sacred in Islam, cats are revered by Muslims. Some Western writers have stated Muhammad had a favorite cat, Muezza.[234] He is reported to have loved cats so much, \"he would do without his cloak rather than disturb one that was sleeping on it\".[235] The story has no origin in early Muslim writers, and seems to confuse a story of a later Sufi saint, Ahmed ar-Rifa'i, centuries after Muhammad.[236] One of the companions of Muhammad was known as Abu Hurayrah (\"father of the kitten\"), in reference to his documented affection to cats.[237]\n Many cultures have negative superstitions about cats. An example would be the belief that encountering a black cat (\"crossing one's path\") leads to bad luck, or that cats are witches' familiars used to augment a witch's powers and skills. The killing of cats in Medieval Ypres, Belgium, is commemorated in the innocuous present-day Kattenstoet (cat parade).[239] In mid-16th century France, cats would allegedly be burnt alive as a form of entertainment, particularly during midsummer festivals. According to Norman Davies, the assembled people \"shrieked with laughter as the animals, howling with pain, were singed, roasted, and finally carbonized\".[240] The remaining ashes were sometimes taken back home by the people for good luck.[241]\n According to a myth in many cultures, cats have multiple lives. In many countries, they are believed to have nine lives, but in Italy, Germany, Greece, Brazil and some Spanish-speaking regions, they are said to have seven lives,[242][243] while in Arabic traditions, the number of lives is six.[244] An early mention of the myth can be found in John Heywood's The Proverbs of John Heywood (1546):[245]\n Husband, (quoth she), ye studie, be merrie now,\nAnd even as ye thinke now, so come to yow.\nNay not so, (quoth he), for my thought to tell right,\nI thinke how you lay groning, wife, all last night.\nHusband, a groning horse and a groning wife\nNever faile their master, (quoth she), for my life.\nNo wife, a woman hath nine lives like a cat.\n The myth is attributed to the natural suppleness and swiftness cats exhibit to escape life-threatening situations.[246] Also lending credence to this myth is the fact that falling cats often land on their feet, using an instinctive righting reflex to twist their bodies around. Nonetheless, cats can still be injured or killed by a high fall.[247]\n"}
{"key":"Cat","link":"https:\/\/en.wikipedia.org\/wiki\/Cat","headline":"Cat - Wikipedia","content":"\n The cat (Felis catus), commonly referred to as the domestic cat or house cat, is the only domesticated species in the family Felidae. Recent advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC. It is commonly kept as a house pet and farm cat, but also ranges freely as a feral cat avoiding human contact. It is valued by humans for companionship and its ability to kill vermin. Because of its retractable claws, it is adapted to killing small prey like mice and rats. It has a strong, flexible body, quick reflexes, sharp teeth, and its night vision and sense of smell are well developed. It is a social species, but a solitary hunter and a crepuscular predator. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling, and grunting as well as cat body language. It can hear sounds too faint or too high in frequency for human ears, such as those made by small mammals. It also secretes and perceives pheromones.\n Female domestic cats can have kittens from spring to late autumn in temperate zones and throughout the year in equatorial regions, with litter sizes often ranging from two to five kittens. Domestic cats are bred and shown at events as registered pedigreed cats, a hobby known as cat fancy. Animal population control of cats may be achieved by spaying and neutering, but their proliferation and the abandonment of pets has resulted in large numbers of feral cats worldwide, contributing to the extinction of bird, mammal and reptile species.\n As of 2017,[update] the domestic cat was the second most popular pet in the United States, with 95.6 million cats owned and around 42 million households owning at least one cat. In the United Kingdom, 26% of adults have a cat, with an estimated population of 10.9 million pet cats as of 2020.[update] As of 2021,[update] there were an estimated 220 million owned and 480 million stray cats in the world.\n The origin of the English word cat, Old English catt, is thought to be the Late Latin word cattus, which was first used at the beginning of the 6th century.[4] The Late Latin word may be derived from an unidentified African language.[5] The Nubian word kaddîska 'wildcat' and Nobiin kadīs are possible sources or cognates.[6] The Nubian word may be a loan from Arabic قَطّ‎ qaṭṭ ~ قِطّ qiṭṭ.[citation needed]\n The forms might also have derived from an ancient Germanic word that was imported into Latin and then into Greek, Syriac, and Arabic.[7] The word may be derived from Germanic and Northern European languages, and ultimately be borrowed from Uralic, cf. Northern Sámi gáđfi, 'female stoat', and Hungarian hölgy, 'lady, female stoat'; from Proto-Uralic *käďwä, 'female (of a furred animal)'.[8]\n The English puss, extended as pussy and pussycat, is attested from the 16th century and may have been introduced from Dutch poes or from Low German puuskatte, related to Swedish kattepus, or Norwegian pus, pusekatt. Similar forms exist in Lithuanian puižė and Irish puisín or puiscín. The etymology of this word is unknown, but it may have arisen from a sound used to attract a cat.[9][10]\n A male cat is called a tom or tomcat[11] (or a gib,[12] if neutered). A female is called a queen[13] or a molly,[14][user-generated source?] if spayed, especially in a cat-breeding context. A juvenile cat is referred to as a kitten. In Early Modern English, the word kitten was interchangeable with the now-obsolete word catling.[15]\n A group of cats can be referred to as a clowder or a glaring.[16]\n The scientific name Felis catus was proposed by Carl Linnaeus in 1758 for a domestic cat.[1][2] Felis catus domesticus was proposed by Johann Christian Polycarp Erxleben in 1777.[3] Felis daemon proposed by Konstantin Satunin in 1904 was a black cat from the Transcaucasus, later identified as a domestic cat.[17][18]\n In 2003, the International Commission on Zoological Nomenclature ruled that the domestic cat is a distinct species, namely Felis catus.[19][20]\nIn 2007, the modern domesticated subspecies F. silvestris catus sampled worldwide was considered to have likely descended from the Near Eastern wildcat (F. lybica) following results of phylogenetic research.[21][22][a] In 2017, the IUCN Cat Classification Taskforce followed the recommendation of the ICZN in regarding the domestic cat as a distinct species, Felis catus.[23]\n The domestic cat is a member of the Felidae, a family that had a common ancestor about 10 to 15 million years ago.[24] The evolutionary radiation of the Felidae began in Asia during the Miocene around 8.38 to 14.45 million years ago.[25] Analysis of mitochondrial DNA of all Felidae species indicates a radiation at 6.46 to 16.76 million years ago.[26]\nThe genus Felis genetically diverged from other Felidae around 6 to 7 million years ago.[25] \nResults of phylogenetic research shows that the wild members of this genus evolved through sympatric or parapatric speciation, whereas the domestic cat evolved through artificial selection.[27]\nThe domestic cat and its closest wild ancestor are diploid and both possess 38 chromosomes[28] and roughly 20,000 genes.[29]\n Pantherinae\n other Felinae lineages\n Jungle cat (F. chaus) \n Black-footed cat (F. nigripes)\n Sand cat (F. margarita)\n Chinese mountain cat (F. bieti)\n African wildcat (F. lybica)\n European wildcat (F. silvestris) \n Domestic cat \n Sand cat (F. margarita)\n Chinese mountain cat (F. bieti)\n European wildcat (F. silvestris) \n Southern African wildcat (F. l. cafra)\n Asiatic wildcat (F. l. ornata)\n Near Eastern wildcat\n Domestic cat \n It was long thought that the domestication of the cat began in ancient Egypt, where cats were venerated from around 3100 BC,[31][32]\nHowever, the earliest known indication for the taming of an African wildcat was excavated close by a human Neolithic grave in Shillourokambos, southern Cyprus, dating to about 7500–7200 BC. Since there is no evidence of native mammalian fauna on Cyprus, the inhabitants of this Neolithic village most likely brought the cat and other wild mammals to the island from the Middle Eastern mainland.[33] Scientists therefore assume that African wildcats were attracted to early human settlements in the Fertile Crescent by rodents, in particular the house mouse (Mus musculus), and were tamed by Neolithic farmers. This mutual relationship between early farmers and tamed cats lasted thousands of years. As agricultural practices spread, so did tame and domesticated cats.[30][34] Wildcats of Egypt contributed to the maternal gene pool of the domestic cat at a later time.[35]\n The earliest known evidence for the occurrence of the domestic cat in Greece dates to around 1200 BC. Greek, Phoenician, Carthaginian and Etruscan traders introduced domestic cats to southern Europe.[36] During the Roman Empire they were introduced to Corsica and Sardinia before the beginning of the 1st millennium.[37] By the 5th century BC, they were familiar animals around settlements in Magna Graecia and Etruria.[38] By the end of the Western Roman Empire in the 5th century, the Egyptian domestic cat lineage had arrived in a Baltic Sea port in northern Germany.[35]\n The leopard cat (Prionailurus bengalensis) was tamed independently in China around 5500 BC. This line of partially domesticated cats leaves no trace in the domestic cat populations of today.[39]\n During domestication, cats have undergone only minor changes in anatomy and behavior, and they are still capable of surviving in the wild. Several natural behaviors and characteristics of wildcats may have pre-adapted them for domestication as pets. These traits include their small size, social nature, obvious body language, love of play, and high intelligence. Since they practice rigorous grooming habits and have an instinctual drive to bury and hide their urine and feces, they are generally much less messy than other domesticated animals. Captive Leopardus cats may also display affectionate behavior toward humans but were not domesticated.[40] House cats often mate with feral cats.[41] Hybridisation between domestic and other Felinae species is also possible, producing hybrids such as the Kellas cat in Scotland.[42][43]\n Development of cat breeds started in the mid 19th century.[44] An analysis of the domestic cat genome revealed that the ancestral wildcat genome was significantly altered in the process of domestication, as specific mutations were selected to develop cat breeds.[45] Most breeds are founded on random-bred domestic cats. Genetic diversity of these breeds varies between regions, and is lowest in purebred populations, which show more than 20 deleterious genetic disorders.[46]\n The domestic cat has a smaller skull and shorter bones than the European wildcat.[47] It averages about 46 cm (18 in) in head-to-body length and 23–25 cm (9.1–9.8 in) in height, with about 30 cm (12 in) long tails. Males are larger than females.[48]\nAdult domestic cats typically weigh 4–5 kg (8.8–11.0 lb).[27]\n Cats have seven cervical vertebrae (as do most mammals); 13 thoracic vertebrae (humans have 12); seven lumbar vertebrae (humans have five); three sacral vertebrae (as do most mammals, but humans have five); and a variable number of caudal vertebrae in the tail (humans have only three to five vestigial caudal vertebrae, fused into an internal coccyx).[49]: 11  The extra lumbar and thoracic vertebrae account for the cat's spinal mobility and flexibility. Attached to the spine are 13 ribs, the shoulder, and the pelvis.[49]: 16  Unlike human arms, cat forelimbs are attached to the shoulder by free-floating clavicle bones which allow them to pass their body through any space into which they can fit their head.[50]\n The cat skull is unusual among mammals in having very large eye sockets and a powerful specialized jaw.[51]: 35  Within the jaw, cats have teeth adapted for killing prey and tearing meat. When it overpowers its prey, a cat delivers a lethal neck bite with its two long canine teeth, inserting them between two of the prey's vertebrae and severing its spinal cord, causing irreversible paralysis and death.[52] Compared to other felines, domestic cats have narrowly spaced canine teeth relative to the size of their jaw, which is an adaptation to their preferred prey of small rodents, which have small vertebrae.[52]\n The premolar and first molar together compose the carnassial pair on each side of the mouth, which efficiently shears meat into small pieces, like a pair of scissors. These are vital in feeding, since cats' small molars cannot chew food effectively, and cats are largely incapable of mastication.[51]: 37  Cats tend to have better teeth than most humans, with decay generally less likely because of a thicker protective layer of enamel, a less damaging saliva, less retention of food particles between teeth, and a diet mostly devoid of sugar. Nonetheless, they are subject to occasional tooth loss and infection.[53]\n Cats have protractible and retractable claws.[54] In their normal, relaxed position, the claws are sheathed with the skin and fur around the paw's toe pads. This keeps the claws sharp by preventing wear from contact with the ground and allows for the silent stalking of prey. The claws on the forefeet are typically sharper than those on the hindfeet.[55] Cats can voluntarily extend their claws on one or more paws. They may extend their claws in hunting or self-defense, climbing, kneading, or for extra traction on soft surfaces. Cats shed the outside layer of their claw sheaths when scratching rough surfaces.[56]\n Most cats have five claws on their front paws and four on their rear paws. The dewclaw is proximal to the other claws. More proximally is a protrusion which appears to be a sixth \"finger\". This special feature of the front paws on the inside of the wrists has no function in normal walking but is thought to be an antiskidding device used while jumping. Some cat breeds are prone to having extra digits (\"polydactyly\").[57] Polydactylous cats occur along North America's northeast coast and in Great Britain.[58]\n The cat is digitigrade. It walks on the toes, with the bones of the feet making up the lower part of the visible leg.[59] Unlike most mammals, it uses a \"pacing\" gait and moves both legs on one side of the body before the legs on the other side. It registers directly by placing each hind paw close to the track of the corresponding fore paw, minimizing noise and visible tracks. This also provides sure footing for hind paws when navigating rough terrain. As it speeds up from walking to trotting, its gait changes to a \"diagonal\" gait: The diagonally opposite hind and fore legs move simultaneously.[60]\n Cats are generally fond of sitting in high places or perching. A higher place may serve as a concealed site from which to hunt; domestic cats strike prey by pouncing from a perch such as a tree branch. Another possible explanation is that height gives the cat a better observation point, allowing it to survey its territory. A cat falling from heights of up to 3 m (9.8 ft) can right itself and land on its paws.[61]\n During a fall from a high place, a cat reflexively twists its body and rights itself to land on its feet using its acute sense of balance and flexibility. This reflex is known as the cat righting reflex.[62] A cat always rights itself in the same way during a fall, if it has enough time to do so, which is the case in falls of 90 cm (3.0 ft) or more.[63] How cats are able to right themselves when falling has been investigated as the \"falling cat problem\".[64]\n The cat family (Felidae) can pass down many colors and patterns to their offspring. The domestic cat genes MC1R and ASIP allow for the variety of color in coats. The feline ASIP gene consists of three coding exons.[65] Three novel microsatellite markers linked to ASIP were isolated from a domestic cat BAC clone containing this gene and were used to perform linkage analysis in a pedigree of 89 domestic cats that segregated for melanism.[citation needed]\n Cats have excellent night vision and can see at one sixth the light level required for human vision.[51]: 43  This is partly the result of cat eyes having a tapetum lucidum, which reflects any light that passes through the retina back into the eye, thereby increasing the eye's sensitivity to dim light.[66] Large pupils are an adaptation to dim light. The domestic cat has slit pupils, which allow it to focus bright light without chromatic aberration.[67] At low light, a cat's pupils expand to cover most of the exposed surface of its eyes.[68] The domestic cat has rather poor color vision and only two types of cone cells, optimized for sensitivity to blue and yellowish green; its ability to distinguish between red and green is limited.[69] A response to middle wavelengths from a system other than the rod cells might be due to a third type of cone. This appears to be an adaptation to low light levels rather than representing true trichromatic vision.[70] Cats also have a nictitating membrane, allowing them to blink without hindering their vision.\n The domestic cat's hearing is most acute in the range of 500 Hz to 32 kHz.[71] It can detect an extremely broad range of frequencies ranging from 55 Hz to 79 kHz, whereas humans can only detect frequencies between 20 Hz and 20 kHz. It can hear a range of 10.5 octaves, while humans and dogs can hear ranges of about 9 octaves.[72][73]\nIts hearing sensitivity is enhanced by its large movable outer ears, the pinnae, which amplify sounds and help detect the location of a noise. It can detect ultrasound, which enables it to detect ultrasonic calls made by rodent prey.[74][75] Recent research has shown that cats have socio-spatial cognitive abilities to create mental maps of owners' locations based on hearing owners' voices.[76]\n Cats have an acute sense of smell, due in part to their well-developed olfactory bulb and a large surface of olfactory mucosa, about 5.8 cm2 (0.90 in2) in area, which is about twice that of humans.[77] Cats and many other animals have a Jacobson's organ in their mouths that is used in the behavioral process of flehmening. It allows them to sense certain aromas in a way that humans cannot. Cats are sensitive to pheromones such as 3-mercapto-3-methylbutan-1-ol,[78] which they use to communicate through urine spraying and marking with scent glands.[79] Many cats also respond strongly to plants that contain nepetalactone, especially catnip, as they can detect that substance at less than one part per billion.[80] About 70–80% of cats are affected by nepetalactone.[81] This response is also produced by other plants, such as silver vine (Actinidia polygama) and the herb valerian; it may be caused by the smell of these plants mimicking a pheromone and stimulating cats' social or sexual behaviors.[82]\n Cats have relatively few taste buds compared to humans (470 or so versus more than 9,000 on the human tongue).[83] Domestic and wild cats share a taste receptor gene mutation that keeps their sweet taste buds from binding to sugary molecules, leaving them with no ability to taste sweetness.[84] They, however, possess taste bud receptors specialized for acids, amino acids like protein, and bitter tastes.[85] Their taste buds possess the receptors needed to detect umami. However, these receptors contain molecular changes that make the cat taste of umami different from that of humans. In humans, they detect the amino acids of glutamic acid and aspartic acid, but in cats they instead detect inosine monophosphate and l-Histidine.[86] These molecules are particularly enriched in tuna.[86] This has been argued is why cats find tuna so palatable: as put by researchers into cat taste, \"the specific combination of the high IMP and free l-Histidine contents of tuna\" .. \"produces a strong umami taste synergy that is highly preferred by cats\".[86] One of the researchers involved in this research has further claimed, \"I think umami is as important for cats as sweet is for humans\".[87]\n Cats also have a distinct temperature preference for their food, preferring food with a temperature around 38 °C (100 °F) which is similar to that of a fresh kill; some cats reject cold food (which would signal to the cat that the \"prey\" item is long dead and therefore possibly toxic or decomposing).[83]\n To aid with navigation and sensation, cats have dozens of movable whiskers (vibrissae) over their body, especially their faces. These provide information on the width of gaps and on the location of objects in the dark, both by touching objects directly and by sensing air currents; they also trigger protective blink reflexes to protect the eyes from damage.[51]: 47 \n Outdoor cats are active both day and night, although they tend to be slightly more active at night.[88] Domestic cats spend the majority of their time in the vicinity of their homes but can range many hundreds of meters from this central point. They establish territories that vary considerably in size, in one study ranging 7–28 ha (17–69 acres).[89] The timing of cats' activity is quite flexible and varied but being low-light predators, they are generally crepuscular, which means they tend to be more active near dawn and dusk. However, house cats' behavior is also influenced by human activity and they may adapt to their owners' sleeping patterns to some extent.[90][91]\n Cats conserve energy by sleeping more than most animals, especially as they grow older. The daily duration of sleep varies, usually between 12 and 16 hours, with 13 and 14 being the average. Some cats can sleep as much as 20 hours. The term \"cat nap\" for a short rest refers to the cat's tendency to fall asleep (lightly) for a brief period. While asleep, cats experience short periods of rapid eye movement sleep often accompanied by muscle twitches, which suggests they are dreaming.[92]\n The social behavior of the domestic cat ranges from widely dispersed individuals to feral cat colonies that gather around a food source, based on groups of co-operating females.[93][94] Within such groups, one cat is usually dominant over the others.[95] Each cat in a colony holds a distinct territory, with sexually active males having the largest territories, which are about 10 times larger than those of female cats and may overlap with several females' territories. These territories are marked by urine spraying, by rubbing objects at head height with secretions from facial glands, and by defecation.[79] Between these territories are neutral areas where cats watch and greet one another without territorial conflicts. Outside these neutral areas, territory holders usually chase away stranger cats, at first by staring, hissing, and growling and, if that does not work, by short but noisy and violent attacks. Despite this colonial organization, cats do not have a social survival strategy or a herd behavior, and always hunt alone.[96]\n Life in proximity to humans and other domestic animals has led to a symbiotic social adaptation in cats, and cats may express great affection toward humans or other animals. Ethologically, a cat's human keeper functions as if a mother surrogate.[97] Adult cats live their lives in a kind of extended kittenhood, a form of behavioral neoteny. Their high-pitched sounds may mimic the cries of a hungry human infant, making them particularly difficult for humans to ignore.[98] Some pet cats are poorly socialized. In particular, older cats show aggressiveness toward newly arrived kittens, which include biting and scratching; this type of behavior is known as feline asocial aggression.[99]\n Redirected aggression is a common form of aggression which can occur in multiple cat households. In redirected aggression there is usually something that agitates the cat: this could be a sight, sound, or another source of stimuli which causes a heightened level of anxiety or arousal. If the cat cannot attack the stimuli, it may direct anger elsewhere by attacking or directing aggression to the nearest cat, dog, human or other being.[100][101]\n Domestic cats' scent rubbing behavior toward humans or other cats is thought to be a feline means for social bonding.[102]\n Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling\/snarling, grunting, and several different forms of meowing.[103] Their body language, including position of ears and tail, relaxation of the whole body, and kneading of the paws, are all indicators of mood. The tail and ears are particularly important social signal mechanisms in cats. A raised tail indicates a friendly greeting, and flattened ears indicate hostility. Tail-raising also indicates the cat's position in the group's social hierarchy, with dominant individuals raising their tails less often than subordinate ones.[104] Feral cats are generally silent.[105]: 208  Nose-to-nose touching is also a common greeting and may be followed by social grooming, which is solicited by one of the cats raising and tilting its head.[93]\n Purring may have developed as an evolutionary advantage as a signaling mechanism of reassurance between mother cats and nursing kittens, who are thought to use it as a care-soliciting signal.[106]\nPost-nursing cats also often purr as a sign of contentment: when being petted, becoming relaxed,[107][108] or eating. Even though purring is popularly interpreted as indicative of pleasure, it has been recorded in a wide variety of circumstances, most of which involve physical contact between the cat and another, presumably trusted individual.[106] Some cats have been observed to purr continuously when chronically ill or in apparent pain.[109]\n The exact mechanism by which cats purr has long been elusive, but it has been proposed that purring is generated via a series of sudden build-ups and releases of pressure as the glottis is opened and closed, which causes the vocal folds to separate forcefully. The laryngeal muscles in control of the glottis are thought to be driven by a neural oscillator which generates a cycle of contraction and release every 30–40 milliseconds (giving a frequency of 33 to 25 Hz).[106][110][111]\n Domestic cats observed in a rescue facility have total of 276 distinct facial expressions based on 26 different facial movements; each facial expression corresponds to different social functions that are likely influenced by domestication.[112]\n Cats are known for spending considerable amounts of time licking their coats to keep them clean.[113][114] The cat's tongue has backward-facing spines about 500 μm long, which are called papillae. These contain keratin which makes them rigid[115] so the papillae act like a hairbrush. Some cats, particularly longhaired cats, occasionally regurgitate hairballs of fur that have collected in their stomachs from grooming. These clumps of fur are usually sausage-shaped and about 2–3 cm (0.79–1.18 in) long. Hairballs can be prevented with remedies that ease elimination of the hair through the gut, as well as regular grooming of the coat with a comb or stiff brush.[113]\n Among domestic cats, males are more likely to fight than females.[116] Among feral cats, the most common reason for cat fighting is competition between two males to mate with a female. In such cases, most fights are won by the heavier male.[117] Another common reason for fighting in domestic cats is the difficulty of establishing territories within a small home.[116] Female cats also fight over territory or to defend their kittens. Neutering will decrease or eliminate this behavior in many cases, suggesting that the behavior is linked to sex hormones.[118]\n When cats become aggressive, they try to make themselves appear larger and more threatening by raising their fur, arching their backs, turning sideways and hissing or spitting.[119] Often, the ears are pointed down and back to avoid damage to the inner ear and potentially listen for any changes behind them while focused forward. Cats may also vocalize loudly and bare their teeth in an effort to further intimidate their opponents. Fights usually consist of grappling and delivering powerful slaps to the face and body with the forepaws as well as bites. Cats also throw themselves to the ground in a defensive posture to rake their opponent's belly with their powerful hind legs.[120]\n Serious damage is rare, as the fights are usually short in duration, with the loser running away with little more than a few scratches to the face and ears. Fights for mating rights are typically more severe and injuries may include deep puncture wounds and lacerations. Normally, serious injuries from fighting are limited to infections of scratches and bites, though these can occasionally kill cats if untreated. In addition, bites are probably the main route of transmission of feline immunodeficiency virus.[121] Sexually active males are usually involved in many fights during their lives, and often have decidedly battered faces with obvious scars and cuts to their ears and nose.[122] Cats are willing to threaten animals larger than them to defend their territory, such as dogs and foxes.[123]\n The shape and structure of cats' cheeks is insufficient to allow them to take in liquids using suction. Therefore, when drinking they lap with the tongue to draw liquid upward into their mouths. Lapping at a rate of four times a second, the cat touches the smooth tip of its tongue to the surface of the water, and quickly retracts it like a corkscrew, drawing water upward.[124][125]\n Feral cats and free-fed house cats consume several small meals in a day. The frequency and size of meals varies between individuals. They select food based on its temperature, smell and texture; they dislike chilled foods and respond most strongly to moist foods rich in amino acids, which are similar to meat. Cats reject novel flavors (a response termed neophobia) and learn quickly to avoid foods that have tasted unpleasant in the past.[96][126] It is also a common misconception that cats like milk\/cream, as they tend to avoid sweet food and milk. Most adult cats are lactose intolerant; the sugar in milk is not easily digested and may cause soft stools or diarrhea.[127] Some also develop odd eating habits and like to eat or chew on things like wool, plastic, cables, paper, string, aluminum foil, or even coal. This condition, pica, can threaten their health, depending on the amount and toxicity of the items eaten.[128]\n Cats hunt small prey, primarily birds and rodents,[129] and are often used as a form of pest control.[130][131] Other common small creatures such as lizards and snakes may also become prey.[132] Cats use two hunting strategies, either stalking prey actively, or waiting in ambush until an animal comes close enough to be captured.[133] The strategy used depends on the prey species in the area, with cats waiting in ambush outside burrows, but tending to actively stalk birds.[134]: 153  Domestic cats are a major predator of wildlife in the United States, killing an estimated 1.3 to 4.0 billion birds and 6.3 to 22.3 billion mammals annually.[135]\n Certain species appear more susceptible than others; in one English village, for example, 30% of house sparrow mortality was linked to the domestic cat.[136] In the recovery of ringed robins (Erithacus rubecula) and dunnocks (Prunella modularis) in Britain, 31% of deaths were a result of cat predation.[137] In parts of North America, the presence of larger carnivores such as coyotes which prey on cats and other small predators reduces the effect of predation by cats and other small predators such as opossums and raccoons on bird numbers and variety.[138]\n Perhaps the best-known element of cats' hunting behavior, which is commonly misunderstood and often appalls cat owners because it looks like torture, is that cats often appear to \"play\" with prey by releasing and recapturing it. This cat and mouse behavior is due to an instinctive imperative to ensure that the prey is weak enough to be killed without endangering the cat.[139]\n Another poorly understood element of cat hunting behavior is the presentation of prey to human guardians. One explanation is that cats adopt humans into their social group and share excess kill with others in the group according to the dominance hierarchy, in which humans are reacted to as if they are at or near the top.[140] Another explanation is that they attempt to teach their guardians to hunt or to help their human as if feeding \"an elderly cat, or an inept kitten\".[141] This hypothesis is inconsistent with the fact that male cats also bring home prey, despite males having negligible involvement in raising kittens.[134]: 153 \n Domestic cats, especially young kittens, are known for their love of play. This behavior mimics hunting and is important in helping kittens learn to stalk, capture, and kill prey.[142] Cats also engage in play fighting, with each other and with humans. This behavior may be a way for cats to practice the skills needed for real combat, and might also reduce any fear they associate with launching attacks on other animals.[143]\n Cats also tend to play with toys more when they are hungry.[144] Owing to the close similarity between play and hunting, cats prefer to play with objects that resemble prey, such as small furry toys that move rapidly, but rapidly lose interest. They become habituated to a toy they have played with before.[145] String is often used as a toy, but if it is eaten, it can become caught at the base of the cat's tongue and then move into the intestines, a medical emergency which can cause serious illness, even death.[146] Owing to the risks posed by cats eating string, it is sometimes replaced with a laser pointer's dot, which cats may chase.[147]\n The cat secretes and perceives pheromones.[148]\nFemale cats, called queens, are polyestrous with several estrus cycles during a year, lasting usually 21 days. They are usually ready to mate between early February and August[149] in northern temperate zones and throughout the year in equatorial regions.[150]\n Several males, called tomcats, are attracted to a female in heat. They fight over her, and the victor wins the right to mate. At first, the female rejects the male, but eventually, the female allows the male to mate. The female utters a loud yowl as the male pulls out of her because a male cat's penis has a band of about 120–150 backward-pointing penile spines, which are about 1 mm (0.039 in) long; upon withdrawal of the penis, the spines may provide the female with increased sexual stimulation, which acts to induce ovulation.[151]\n After mating, the female cleans her vulva thoroughly. If a male attempts to mate with her at this point, the female attacks him. After about 20 to 30 minutes, once the female is finished grooming, the cycle will repeat.[152] Because ovulation is not always triggered by a single mating, females may not be impregnated by the first male with which they mate.[153] Furthermore, cats are superfecund; that is, a female may mate with more than one male when she is in heat, with the result that different kittens in a litter may have different fathers.[152]\n The morula forms 124 hours after conception. At 148 hours, early blastocysts form. At 10–12 days, implantation occurs.[154] The gestation of queens lasts between 64 and 67 days, with an average of 65 days.[149][155]\n Data on the reproductive capacity of more than 2,300 free-ranging queens were collected during a study between May 1998 and October 2000. They had one to six kittens per litter, with an average of three kittens. They produced a mean of 1.4 litters per year, but a maximum of three litters in a year. Of 169 kittens, 127 died before they were six months old due to a trauma caused in most cases by dog attacks and road accidents.[156]\nThe first litter is usually smaller than subsequent litters. Kittens are weaned between six and seven weeks of age. Queens normally reach sexual maturity at 5–10 months, and males at 5–7 months. This varies depending on breed.[152] Kittens reach puberty at the age of 9–10 months.[149]\n Cats are ready to go to new homes at about 12 weeks of age, when they are ready to leave their mother.[157] They can be surgically sterilized (spayed or castrated) as early as seven weeks to limit unwanted reproduction.[158] This surgery also prevents undesirable sex-related behavior, such as aggression, territory marking (spraying urine) in males and yowling (calling) in females. Traditionally, this surgery was performed at around six to nine months of age, but it is increasingly being performed before puberty, at about three to six months.[159] In the United States, about 80% of household cats are neutered.[160]\n The average lifespan of pet cats has risen in recent decades. In the early 1980s, it was about seven years,[161]: 33 [162] rising to 9.4 years in 1995[161]: 33  and an average of about 13 years as of 2014 and 2023.[163][164] Some cats have been reported as surviving into their 30s,[165] with the oldest known cat dying at a verified age of 38.[166]\n Neutering increases life expectancy: one study found castrated male cats live twice as long as intact males, while spayed female cats live 62% longer than intact females.[161]: 35  Having a cat neutered confers some health benefits such as greater life expectancy and decreased incidence of neoplasia;[167] however neutering decreases metabolism and,[168][169][170] increases food intake,[170][171] both of these cause obesity to be a predisposition for neutered cats.[172]\n About 250 heritable genetic disorders have been identified in cats, many similar to human inborn errors of metabolism.[173] The high level of similarity among the metabolism of mammals allows many of these feline diseases to be diagnosed using genetic tests that were originally developed for use in humans, as well as the use of cats as animal models in the study of the human diseases.[174][175] Diseases affecting domestic cats include acute infections, parasitic infestations, injuries, and chronic diseases such as kidney disease, thyroid disease, and arthritis. Vaccinations are available for many infectious diseases, as are treatments to eliminate parasites such as worms, ticks, and fleas.[176]\n The domestic cat is a cosmopolitan species and occurs across much of the world.[46] It is adaptable and now present on all continents except Antarctica, and on 118 of the 131 main groups of islands, even on the isolated Kerguelen Islands.[177][178] Due to its ability to thrive in almost any terrestrial habitat, it is among the world's most invasive species.[179] It lives on small islands with no human inhabitants.[180] Feral cats can live in forests, grasslands, tundra, coastal areas, agricultural land, scrublands, urban areas, and wetlands.[181]\n The unwantedness that leads to the domestic cat being treated as an invasive species is twofold. On one hand, as it is little altered from the wildcat, it can readily interbreed with the wildcat. This hybridization poses a danger to the genetic distinctiveness of some wildcat populations, particularly in Scotland and Hungary, possibly also the Iberian Peninsula, and where protected natural areas are close to human-dominated landscapes, such as Kruger National Park in South Africa.[182][43] However, its introduction to places where no native felines are present also contributes to the decline of native species.[183]\n Feral cats are domestic cats that were born in or have reverted to a wild state. They are unfamiliar with and wary of humans and roam freely in urban and rural areas.[184] The numbers of feral cats is not known, but estimates of the United States feral population range from 25 to 60 million.[184] Feral cats may live alone, but most are found in large colonies, which occupy a specific territory and are usually associated with a source of food.[185] Famous feral cat colonies are found in Rome around the Colosseum and Forum Romanum, with cats at some of these sites being fed and given medical attention by volunteers.[186]\n Public attitudes toward feral cats vary widely, from seeing them as free-ranging pets to regarding them as vermin.[187]\n Some feral cats can be successfully socialized and 're-tamed' for adoption; young cats, especially kittens[188] and cats that have had prior experience and contact with humans are the most receptive to these efforts.\n On islands, birds can contribute as much as 60% of a cat's diet.[189] In nearly all cases, the cat cannot be identified as the sole cause for reducing the numbers of island birds, and in some instances, eradication of cats has caused a \"mesopredator release\" effect;[190] where the suppression of top carnivores creates an abundance of smaller predators that cause a severe decline in their shared prey. Domestic cats are a contributing factor to the decline of many species, a factor that has ultimately led, in some cases, to extinction. The South Island piopio, Chatham rail,[137] and the New Zealand merganser[191] are a few from a long list, with the most extreme case being the flightless Lyall's wren, which was driven to extinction only a few years after its discovery.[192][193]\nOne feral cat in New Zealand killed 102 New Zealand lesser short-tailed bats in seven days.[194] In the US, feral and free-ranging domestic cats kill an estimated 6.3 – 22.3 billion mammals annually.[135]\n In Australia one study found feral cats to kill 466 million reptiles per year. More than 258 reptile species were identified as being predated by cats.[195] Cats have contributed to the extinction of the Navassa curly-tailed lizard and Chioninia coctei.[183]\n Cats are common pets throughout the world, and their worldwide population as of 2007 exceeded 500 million.[196]\nAs of 2017,[update] the domestic cat was the second most popular pet in the United States, with 95.6 million cats owned[197][198] and around 42 million households owning at least one cat.[199] In the United Kingdom, 26% of adults have a cat, with an estimated population of 10.9 million pet cats as of 2020.[update][200] As of 2021,[update] there were an estimated 220 million owned and 480 million stray cats in the world.[201][202][203]\n Cats have been used for millennia to control rodents, notably around grain stores and aboard ships, and both uses extend to the present day.[204][205]\n As well as being kept as pets, cats are also used in the international fur trade[206] and leather industries for making coats, hats, blankets, stuffed toys,[207] shoes, gloves, and musical instruments.[208] About 24 cats are needed to make a cat-fur coat.[209] This use has been outlawed in the United States since 2000 and in the European Union (as well as the United Kingdom) since 2007.[210]\n Cat pelts have been used for superstitious purposes as part of the practice of witchcraft,[211] and are still made into blankets in Switzerland as traditional medicine thought to cure rheumatism.[212]\n A few attempts to build a cat census have been made over the years, both through associations or national and international organizations (such as that of the Canadian Federation of Humane Societies[213]) and over the Internet,[214][215] but such a task does not seem simple to achieve. General estimates for the global population of domestic cats range widely from anywhere between 200 million to 600 million.[216][217][218][219][220] Walter Chandoha made his career photographing cats after his 1949 images of Loco, an especially charming stray taken in, were published around the world. He is reported to have photographed 90,000 cats during his career and maintained an archive of 225,000 images that he drew from for publications during his lifetime.[221]\n A cat show is a judged event in which the owners of cats compete to win titles in various cat-registering organizations by entering their cats to be judged after a breed standard.[222] It is often required that a cat must be healthy and vaccinated in order to participate in a cat show.[222] Both pedigreed and non-purebred companion (\"moggy\") cats are admissible, although the rules differ depending on the organization. Competing cats are compared to the applicable breed standard, and assessed for temperament.[222]\n Cats can be infected or infested with viruses, bacteria, fungus, protozoans, arthropods or worms that can transmit diseases to humans.[223] In some cases, the cat exhibits no symptoms of the disease.[224] The same disease can then become evident in a human.[225] The likelihood that a person will become diseased depends on the age and immune status of the person. Humans who have cats living in their home or in close association are more likely to become infected. Others might also acquire infections from cat feces and parasites exiting the cat's body.[223][226] Some of the infections of most concern include salmonella, cat-scratch disease and toxoplasmosis.[224]\n In ancient Egypt, cats were revered, and the goddess Bastet often depicted in cat form, sometimes taking on the war-like aspect of a lioness. The Greek historian Herodotus reported that killing a cat was forbidden, and when a household cat died, the entire family mourned and shaved their eyebrows. Families took their dead cats to the sacred city of Bubastis, where they were embalmed and buried in sacred repositories. Herodotus expressed astonishment at the domestic cats in Egypt, because he had only ever seen wildcats.[227]\n Ancient Greeks and Romans kept weasels as pets, which were seen as the ideal rodent-killers. The earliest unmistakable evidence of the Greeks having domestic cats comes from two coins from Magna Graecia dating to the mid-fifth century BC showing Iokastos and Phalanthos, the legendary founders of Rhegion and Taras respectively, playing with their pet cats. The usual ancient Greek word for 'cat' was ailouros, meaning 'thing with the waving tail'. Cats are rarely mentioned in ancient Greek literature. Aristotle remarked in his History of Animals that \"female cats are naturally lecherous.\" The Greeks later syncretized their own goddess Artemis with the Egyptian goddess Bastet, adopting Bastet's associations with cats and ascribing them to Artemis. In Ovid's Metamorphoses, when the deities flee to Egypt and take animal forms, the goddess Diana turns into a cat.[228][229]\n Cats eventually displaced weasels as the pest control of choice because they were more pleasant to have around the house and were more enthusiastic hunters of mice. During the Middle Ages, many of Artemis's associations with cats were grafted onto the Virgin Mary. Cats are often shown in icons of Annunciation and of the Holy Family and, according to Italian folklore, on the same night that Mary gave birth to Jesus, a cat in Bethlehem gave birth to a kitten.[230] Domestic cats were spread throughout much of the rest of the world during the Age of Discovery, as ships' cats were carried on sailing ships to control shipboard rodents and as good-luck charms.[36]\n Several ancient religions believed cats are exalted souls, companions or guides for humans, that are all-knowing but mute so they cannot influence decisions made by humans. In Japan, the maneki neko cat is a symbol of good fortune.[231] In Norse mythology, Freyja, the goddess of love, beauty, and fertility, is depicted as riding a chariot drawn by cats.[232] In Jewish legend, the first cat was living in the house of the first man Adam as a pet that got rid of mice. The cat was once partnering with the first dog before the latter broke an oath they had made which resulted in enmity between the descendants of these two animals. It is also written that neither cats nor foxes are represented in the water, while every other animal has an incarnation species in the water.[233] Although no species are sacred in Islam, cats are revered by Muslims. Some Western writers have stated Muhammad had a favorite cat, Muezza.[234] He is reported to have loved cats so much, \"he would do without his cloak rather than disturb one that was sleeping on it\".[235] The story has no origin in early Muslim writers, and seems to confuse a story of a later Sufi saint, Ahmed ar-Rifa'i, centuries after Muhammad.[236] One of the companions of Muhammad was known as Abu Hurayrah (\"father of the kitten\"), in reference to his documented affection to cats.[237]\n Many cultures have negative superstitions about cats. An example would be the belief that encountering a black cat (\"crossing one's path\") leads to bad luck, or that cats are witches' familiars used to augment a witch's powers and skills. The killing of cats in Medieval Ypres, Belgium, is commemorated in the innocuous present-day Kattenstoet (cat parade).[239] In mid-16th century France, cats would allegedly be burnt alive as a form of entertainment, particularly during midsummer festivals. According to Norman Davies, the assembled people \"shrieked with laughter as the animals, howling with pain, were singed, roasted, and finally carbonized\".[240] The remaining ashes were sometimes taken back home by the people for good luck.[241]\n According to a myth in many cultures, cats have multiple lives. In many countries, they are believed to have nine lives, but in Italy, Germany, Greece, Brazil and some Spanish-speaking regions, they are said to have seven lives,[242][243] while in Arabic traditions, the number of lives is six.[244] An early mention of the myth can be found in John Heywood's The Proverbs of John Heywood (1546):[245]\n Husband, (quoth she), ye studie, be merrie now,\nAnd even as ye thinke now, so come to yow.\nNay not so, (quoth he), for my thought to tell right,\nI thinke how you lay groning, wife, all last night.\nHusband, a groning horse and a groning wife\nNever faile their master, (quoth she), for my life.\nNo wife, a woman hath nine lives like a cat.\n The myth is attributed to the natural suppleness and swiftness cats exhibit to escape life-threatening situations.[246] Also lending credence to this myth is the fact that falling cats often land on their feet, using an instinctive righting reflex to twist their bodies around. Nonetheless, cats can still be injured or killed by a high fall.[247]\n"}
{"key":"Cat","link":"https:\/\/en.wikipedia.org\/wiki\/Felidae","headline":"Felidae - Wikipedia","content":"\n Felidae (\/ˈfɛlɪdiː\/) is the family of mammals in the order Carnivora colloquially referred to as cats. A member of this family is also called a felid (\/ˈfiːlɪd\/).[3][4][5][6] The term \"cat\" refers both to felids in general and specifically to the domestic cat (Felis catus).[7]\n The 41 extant Felidae species exhibit the most diversity in fur patterns of all terrestrial carnivores.[8] Cats have retractile claws, slender muscular bodies and strong flexible forelimbs. Their teeth and facial muscles allow for a powerful bite. They are all obligate carnivores, and most are solitary predators ambushing or stalking their prey. Wild cats occur in Africa, Europe, Asia and the Americas. Some wild cat species are adapted to forest and savanna habitats, some to arid environments, and a few also to wetlands and mountainous terrain. Their activity patterns range from nocturnal and crepuscular to diurnal, depending on their preferred prey species.[9]\n Reginald Innes Pocock divided the extant Felidae into three subfamilies: the Pantherinae, the Felinae and the Acinonychinae, differing from each other by the ossification of the hyoid apparatus and by the cutaneous sheaths which protect their claws.[10]\nThis concept has been revised following developments in molecular biology and techniques for the analysis of morphological data. Today, the living Felidae are divided into two subfamilies: the Pantherinae and Felinae, with the Acinonychinae subsumed into the latter. Pantherinae includes five Panthera and two Neofelis species, while Felinae includes the other 34 species in 12 genera.[11]\n The first cats emerged during the Oligocene about 25 million years ago, with the appearance of Proailurus and Pseudaelurus. The latter species complex was ancestral to two main lines of felids: the cats in the extant subfamilies and a group of extinct \"saber-tooth\" felids of the subfamily Machairodontinae, which range from the type genus Machairodus of the late Miocene to Smilodon of the Pleistocene. The \"false saber-toothed cats\", the Barbourofelidae and Nimravidae, are not true cats but are closely related. Together with the Felidae, Viverridae, hyenas and mongooses, they constitute the Feliformia.[7]\n All members of the cat family have the following characteristics in common:\n The colour, length and density of their fur are very diverse. Fur colour covers the gamut from white to black, and fur patterns from distinctive small spots, and stripes to small blotches and rosettes. Most cat species are born with spotted fur, except the jaguarundi (Herpailurus yagouaroundi), Asian golden cat (Catopuma temminckii) and caracal (Caracal caracal). The spotted fur of lion (Panthera leo) and cougar (Puma concolor) cubs change to uniform fur during their ontogeny.[8] Those living in cold environments have thick fur with long hair, like the snow leopard (Panthera uncia) and the Pallas's cat (Otocolobus manul).[14] Those living in tropical and hot climate zones have short fur.[9] Several species exhibit melanism with all-black individuals.[25]\n In the great majority of cat species, the tail is between a third and a half of the body length, although with some exceptions, like the Lynx species and margay (Leopardus wiedii).[9] Cat species vary greatly in body and skull sizes, and weights:\n Most cat species have a haploid number of 18 or 19. Central and South American cats have a haploid number of 18, possibly due to the combination of two smaller chromosomes into a larger one.[30]\n Most cat species are also induced ovulators, although the margay appears to be a spontaneous ovulator.[15]\n Felidae have type IIx muscle fibers three times more powerful than the muscle fibers of human athletes.[31]\n The family Felidae is part of the Feliformia, a suborder that diverged probably about 50.6 to 35 million years ago into several families.[32] The Felidae and the Asiatic linsangs are considered a sister group, which split about 35.2 to 31.9 million years ago.[33]\n The earliest cats probably appeared about 35 to 28.5 million years ago. Proailurus is the oldest known cat that occurred after the Eocene–Oligocene extinction event about 33.9 million years ago; fossil remains were excavated in France and Mongolia's Hsanda Gol Formation.[7] Fossil occurrences indicate that the Felidae arrived in North America around 18.5 million years ago. This is about 20 million years later than the Ursidae and the Nimravidae, and about 10 million years later than the Canidae.[34]\n In the Early Miocene about 20 to 16.6 million years ago, Pseudaelurus lived in Africa. Its fossil jaws were also excavated in geological formations of Europe's Vallesian, Asia's Middle Miocene and North America's late Hemingfordian to late Barstovian epochs.[35]\n In the Early or Middle Miocene, the saber-toothed Machairodontinae evolved in Africa and migrated northwards in the Late Miocene.[36] With their large upper canines, they were adapted to prey on large-bodied megaherbivores.[37][38] Miomachairodus is the oldest known member of this subfamily. Metailurus lived in Africa and Eurasia about 8 to 6 million years ago. Several Paramachaerodus skeletons were found in Spain. Homotherium appeared in Africa, Eurasia and North America around 3.5 million years ago, and Megantereon about 3 million years ago. Smilodon lived in North and South America from about 2.5 million years ago. This subfamily became extinct in the Late Pleistocene.[36]\n Results of mitochondrial analysis indicate that the living Felidae species descended from a common ancestor, which originated in Asia in the Late Miocene epoch. They migrated to Africa, Europe and the Americas in the course of at least 10 migration waves during the past ~11 million years. Low sea levels and interglacial and glacial periods facilitated these migrations.[39] Panthera blytheae is the oldest known pantherine cat dated to the late Messinian to early Zanclean ages about 5.95 to 4.1 million years ago. A fossil skull was excavated in 2010 in Zanda County on the Tibetan Plateau.[40] Panthera palaeosinensis from North China probably dates to the Late Miocene or Early Pliocene. The skull of the holotype is similar to that of a lion or leopard.[41] Panthera zdanskyi dates to the Gelasian about 2.55 to 2.16 million years ago. Several fossil skulls and jawbones were excavated in northwestern China.[42] Panthera gombaszoegensis is the earliest known pantherine cat that lived in Europe about 1.95 to 1.77 million years ago.[43]\n Living felids fall into eight evolutionary lineages or species clades.[44][45] Genotyping of the nuclear DNA of all 41 felid species revealed that hybridization between species occurred in the course of evolution within the majority of the eight lineages.[46]\n Modelling of felid coat pattern transformations revealed that nearly all patterns evolved from small spots.[47]\n Traditionally, five subfamilies had been distinguished within the Felidae based on phenotypical features: the Pantherinae, the Felinae, the Acinonychinae,[10] and the extinct Machairodontinae and Proailurinae.[48] Acinonychinae used to only contain the genus Acinonyx but this genus is now within the Felinae subfamily.[11]\n The following cladogram based on Piras et al. (2013) depicts the phylogeny of basal living and extinct groups.[49]\n\n †Proailurus bourbonnensis\n †Proailurus lemanensis\n †Proailurus major\n †Pseudaelurus quadridentatus\n †Pseudaelurus cuspidatus\n †Pseudaelurus guangheesis\n †Machairodontinae \n †Hyperailurictis intrepidus\n †Hyperailurictis marshi\n †Hyperailurictis stouti\n †Hyperailurictis validus\n †Hyperailurictis skinneri\n †Sivaelurus chinjiensis\n †Styriofelis turnauensis\n †Styriofelis romieviensis\n Felinae \n †Miopanthera lorteti\n †Miopanthera pamiri\n Pantherinae \n The phylogenetic relationships of living felids are shown in the following cladogram:[46]\n\n Leopard (P. pardus)\n Lion (P. leo)\n Jaguar (P. onca)\n Snow leopard (P. uncia)\n Tiger (P. tigris)\n Clouded leopard (N. nebulosa)\n Sunda clouded leopard (N. diardi)\n Caracal (C. caracal)\n African golden cat (C. aurata)\n Serval (L. serval)\n Geoffroy's cat (L. geoffroyi)\n Kodkod (L. guigna)\n Southern tiger cat (L. guttulus)\n Oncilla (Northern tiger cat, L. tigrina)\n Pampas cat (L. colocola)\n Andean mountain cat (L. jacobita)\n Ocelot (L. pardalis)\n Margay (L. wiedii)\n Bay cat (C. badia)\n Asian golden cat (C. temminckii)\n Marbled cat (P. marmorata)\n Eurasian lynx (L. lynx)\n Iberian lynx (L. pardinus)\n Canada lynx (L. canadensis)\n Bobcat (L. rufus)\n Cougar (P. concolor)\n Jaguarundi (H. yagouaroundi)\n Cheetah (A. jubatus)\n Sunda leopard cat (P. javanensis)\n Leopard cat (P. bengalensis)\n Fishing cat (P. viverrinus)\n Flat-headed cat (P. planiceps)\n Rusty-spotted cat (P. rubiginosus)\n Pallas's cat (O. manul)\n Jungle cat (F. chaus)\n Black-footed cat (F. nigripes)\n Sand cat (F. margarita)\n Chinese mountain cat (F. bieti)\n African wildcat (F. lybica)\n European wildcat (F. silvestris)\n Domestic cat (F. catus)\n"}
{"key":"Cat","link":"https:\/\/en.wikipedia.org\/wiki\/Conservation_status","headline":"Conservation status - Wikipedia","content":"The conservation status of a group of organisms (for instance, a species) indicates whether the group still exists and how likely the group is to become extinct in the near future. Many factors are taken into account when assessing conservation status: not simply the number of individuals remaining, but the overall increase or decrease in the population over time, breeding success rates, and known threats. Various systems of conservation status are in use at international, multi-country, national and local levels, as well as for consumer use such as sustainable seafood advisory lists and certification. The two international systems are by the International Union for Conservation of Nature (IUCN) and The Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES).\n The IUCN Red List of Threatened Species  by the International Union for Conservation of Nature is the best known worldwide conservation status listing and ranking system. Species are classified by the IUCN Red List into nine groups set through criteria such as rate of decline, population size, area of geographic distribution, and degree of population and distribution fragmentation.[1][2]\n Also included are species that have gone extinct since 1500 CE.[3] When discussing the IUCN Red List, the official term \"threatened\" is a grouping of three categories: critically endangered, endangered, and vulnerable.\n The Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES) went into force in 1975. It aims to ensure that international trade in specimens of wild animals and plants does not threaten their survival. Many countries require CITES permits when importing plants and animals listed on CITES.\n In the European Union (EU), the Birds Directive and Habitats Directive are the legal instruments which evaluate the conservation status within the EU of species and habitats.\n NatureServe conservation status focuses on Latin America, the United States, Canada, and the Caribbean. It has been developed by scientists from NatureServe, The Nature Conservancy, and a network of natural heritage programs and data centers. It is increasingly integrated with the IUCN Red List system. Its categories for species include: presumed extinct (GX), possibly extinct (GH), critically imperiled (G1), imperiled (G2), vulnerable (G3), apparently secure (G4), and secure (G5).[4] The system also allows ambiguous or uncertain ranks including inexact numeric ranks (e.g. G2?), and range ranks (e.g. G2G3) for when the exact rank is uncertain. NatureServe adds a qualifier for captive or cultivated only (C), which has a similar meaning to the IUCN Red List extinct in the wild (EW) status.\n The Red Data Book of the Russian Federation is used within the Russian Federation, and also accepted in parts of Africa.\n In Australia, the Environment Protection and Biodiversity Conservation Act 1999 (EPBC Act) describes lists of threatened species, ecological communities and threatening processes. The categories resemble those of the 1994 IUCN Red List Categories & Criteria (version 2.3). Prior to the EPBC Act, a simpler classification system was used by the Endangered Species Protection Act 1992. Some state and territory governments also have their own systems for conservation status. The codes for the Western Australian conservation system are given at Declared Rare and Priority Flora List (abbreviated to DECF when using in a taxobox).\n In Belgium, the Flemish Research Institute for Nature and Forest publishes an online set of more than 150 nature indicators in Dutch.[5]\n In Canada, the Committee on the Status of Endangered Wildlife in Canada (COSEWIC) is a group of experts that assesses and designates which wild species are in some danger of disappearing from Canada.[6] Under the Species at Risk Act (SARA), it is up to the federal government, which is politically accountable, to legally protect species assessed by COSEWIC.\n In China, the State, provinces and some counties have determined their key protected wildlife species. There is the China red data book.\n In Finland, many species are protected under the Nature Conservation Act, and through the EU Habitats Directive and EU Birds Directive.[7]\n In Germany, the Federal Agency for Nature Conservation publishes \"red lists of endangered species\".\n India has the Wild Life Protection Act, 1972, Amended 2003 and the Biological Diversity Act, 2002.\n In Japan, the Ministry of Environment publishes a Threatened Wildlife of Japan Red Data Book.[8]\n In the Netherlands, the Dutch Ministry of Agriculture, Nature and Food Quality publishes a list of threatened species, and conservation is enforced by the Nature Conservation Act 1998. Species are also protected through the Wild Birds and Habitats Directives.\n In New Zealand, the Department of Conservation publishes the New Zealand Threat Classification System lists.  As of January 2008[update] threatened species or subspecies are assigned one of seven categories: Nationally Critical, Nationally Endangered, Nationally Vulnerable, Declining, Recovering, Relict, or Naturally Uncommon.[9]  While the classification looks only at a national level, many species are unique to New Zealand, and species which are secure overseas are noted as such.\n In Russia, the Red Book of Russian Federation came out in 2001, it contains categories defining preservation status for different species. In it there are 8 taxa of amphibians, 21 taxa of reptiles, 128 taxa of birds, and 74 taxa of mammals, in total 231. There are also more than 30 regional red books, for example the red book of the Altaic region which came out in 1994.\n In South Africa, the South African National Biodiversity Institute, established under the National Environmental Management: Biodiversity Act, 2004,[10] is responsible for drawing up lists of affected species, and monitoring compliance with CITES decisions. It is envisaged that previously diverse Red lists would be more easily kept current, both technically and financially.\n In Thailand, the Wild Animal Reservation and Protection Act of BE 2535 defines fifteen reserved animal species and two classes of protected species, of which hunting, breeding, possession, and trade are prohibited or restricted by law. The National Park, Wildlife and Plant Conservation Department of the Ministry of Natural Resources and Environment is responsible for the regulation of these activities.\n In Ukraine, the Ministry of Environment Protection maintains list of endangered species (divided into seven categories from \"0\" - extinct to \"VI\" - rehabilitated) and publishes it in the Red Book of Ukraine.\n In the United States of America, the Endangered Species Act of 1973 created the Endangered Species List.\n Some consumer guides for seafood, such as Seafood Watch, divide fish and other sea creatures into three categories, analogous to conservation status categories:\n The categories do not simply reflect the imperilment of individual species, but also consider the environmental impacts of how and where they are fished, such as through bycatch or ocean bottom trawlers. Often groups of species are assessed rather than individual species (e.g. squid, prawns).\n The Marine Conservation Society has five levels of ratings for seafood species, as displayed on their FishOnline website.[12]\n"}
{"key":"Cat","link":"https:\/\/en.wikipedia.org\/wiki\/Eukaryote","headline":"Eukaryote - Wikipedia","content":"\n \n The eukaryotes (\/juːˈkærioʊts, -əts\/ yoo-KARR-ee-ohts, -⁠əts) constitute the domain of Eukarya, organisms whose cells have a membrane-bound nucleus. All animals, plants, fungi, and many unicellular organisms are eukaryotes. They constitute a major group of life forms alongside the two groups of prokaryotes: the Bacteria and the Archaea. Eukaryotes represent a small minority of the number of organisms, but given their generally much larger size, their collective global biomass is much larger than that of prokaryotes.\n The eukaryotes seemingly emerged in the Archaea, within the Asgard archaea. This implies that there are only two domains of life, Bacteria and Archaea, with eukaryotes incorporated among the Archaea. Eukaryotes first emerged during the Paleoproterozoic, likely as flagellated cells. The leading evolutionary theory is they were created by symbiogenesis between an anaerobic Asgard archaean and an aerobic proteobacterium, which formed the mitochondria. A second episode of symbiogenesis with a cyanobacterium created the plants, with chloroplasts.\n Eukaryotic cells contain membrane-bound organelles such as the nucleus, the endoplasmic reticulum, and the Golgi apparatus. Eukaryotes may be either unicellular or multicellular. In comparison, prokaryotes are typically unicellular. Unicellular eukaryotes are sometimes called protists. Eukaryotes can reproduce both asexually through mitosis and sexually through meiosis and gamete fusion (fertilization).\n Eukaryotes are organisms that range from microscopic single cells, such as picozoans under 3 micrometres across,[5] to animals like the blue whale, weighing up to 190 tonnes and measuring up to 33.6 metres (110 ft) long,[6] or plants like the coast redwood, up to 120 metres (390 ft) tall.[7] Many eukaryotes are unicellular; the informal grouping called protists includes many of these, with some multicellular forms like the giant kelp up to 200 feet (61 m) long.[8] The multicellular eukaryotes include the animals, plants, and fungi, but again, these groups too contain many unicellular species.[9] Eukaryotic cells are typically much larger than those of prokaryotes—the bacteria and the archaea—having a volume of around 10,000 times greater.[10][11] Eukaryotes represent a small minority of the number of organisms, but, as many of them are much larger, their collective global biomass (468 gigatons) is far larger than that of prokaryotes (77 gigatons), with plants alone accounting for over 81% of the total biomass of Earth.[12]\n The eukaryotes are a diverse lineage, consisting mainly of microscopic organisms.[13] Multicellularity in some form has evolved independently at least 25 times within the eukaryotes.[14][15] Complex multicellular organisms, not counting the aggregation of amoebae to form slime molds, have evolved within only six eukaryotic lineages: animals, symbiomycotan fungi, brown algae, red algae, green algae, and land plants.[16] Eukaryotes are grouped by genomic similarities, so that groups often lack visible shared characteristics.[13]\n The defining feature of eukaryotes is that their cells have nuclei. This gives them their name, from the Greek εὖ (eu, \"well\" or \"good\") and κάρυον (karyon, \"nut\" or \"kernel\", here meaning \"nucleus\").[17] Eukaryotic cells have a variety of internal membrane-bound structures, called organelles, and a cytoskeleton which defines the cell's organization and shape. The nucleus stores the cell's DNA, which is divided into linear bundles called chromosomes;[18] these are separated into two matching sets by a microtubular spindle during nuclear division, in the distinctively eukaryotic process of mitosis.[19]\n Eukaryotes differ from prokaryotes in multiple ways, with unique biochemical pathways such as sterane synthesis.[20] The eukaryotic signature proteins have no homology to proteins in other domains of life, but appear to be universal among eukaryotes. They include the proteins of the cytoskeleton, the complex transcription machinery, the membrane-sorting systems, the nuclear pore, and some enzymes in the biochemical pathways.[21]\n Eukaryote cells include a variety of membrane-bound structures, together forming the endomembrane system.[22] Simple compartments, called vesicles and vacuoles, can form by budding off other membranes. Many cells ingest food and other materials through a process of endocytosis, where the outer membrane invaginates and then pinches off to form a vesicle.[23] Some cell products can leave in a vesicle through exocytosis.[24]\n The nucleus is surrounded by a double membrane known as the nuclear envelope, with nuclear pores that allow material to move in and out.[25] Various tube- and sheet-like extensions of the nuclear membrane form the endoplasmic reticulum, which is involved in protein transport and maturation. It includes the rough endoplasmic reticulum, covered in ribosomes which synthesize proteins; these enter the interior space or lumen. Subsequently, they generally enter vesicles, which bud off from the smooth endoplasmic reticulum.[26] In most eukaryotes, these protein-carrying vesicles are released and further modified in stacks of flattened vesicles (cisternae), the Golgi apparatus.[27]\n Vesicles may be specialized; for instance, lysosomes contain digestive enzymes that break down biomolecules in the cytoplasm.[28]\n Mitochondria are organelles in eukaryotic cells. The mitochondrion is commonly called \"the powerhouse of the cell\",[29] for its function providing energy by oxidising sugars or fats to produce the energy-storing molecule ATP.[30][31] Mitochondria have two surrounding membranes, each a phospholipid bilayer; the inner of which is folded into invaginations called cristae where aerobic respiration takes place.[32]\n Mitochondria contain their own DNA, which has close structural similarities to bacterial DNA, from which it originated, and which encodes rRNA and tRNA genes that produce RNA which is closer in structure to bacterial RNA than to eukaryote RNA.[33]\n Some eukaryotes, such as the metamonads Giardia and Trichomonas, and the amoebozoan Pelomyxa, appear to lack mitochondria, but all contain mitochondrion-derived organelles, like hydrogenosomes or mitosomes, having lost their mitochondria secondarily.[34] They obtain energy by enzymatic action in the cytoplasm.[35][34]\n Plants and various groups of algae have plastids as well as mitochondria. Plastids, like mitochondria, have their own DNA and are developed from endosymbionts, in this case cyanobacteria. They usually take the form of chloroplasts which, like cyanobacteria, contain chlorophyll and produce organic compounds (such as glucose) through photosynthesis. Others are involved in storing food. Although plastids probably had a single origin, not all plastid-containing groups are closely related. Instead, some eukaryotes have obtained them from others through secondary endosymbiosis or ingestion.[36] The capture and sequestering of photosynthetic cells and chloroplasts, kleptoplasty, occurs in many types of modern eukaryotic organisms.[37][38]\n The cytoskeleton provides stiffening structure and points of \nattachment for motor structures that enable the cell to move, change shape, or transport materials. The motor structures are microfilaments  of actin and actin-binding proteins, including α-actinin, fimbrin, and filamin are present in submembranous cortical layers and bundles. Motor proteins of microtubules, dynein and kinesin, and myosin of actin filaments, provide dynamic character of the network.[39][40]\n Many eukaryotes have long slender motile cytoplasmic projections, called flagella, or multiple shorter structures called cilia. These organelles are variously involved in movement, feeding, and sensation. They are composed mainly of tubulin, and are entirely distinct from prokaryotic flagella. They are supported by a bundle of microtubules arising from a centriole, characteristically arranged as nine doublets surrounding two singlets. Flagella may have hairs (mastigonemes), as in many Stramenopiles. Their interior is continuous with the cell's cytoplasm.[41][42]\n Centrioles are often present, even in cells and groups that do not have flagella, but conifers and flowering plants have neither. They generally occur in groups that give rise to various microtubular roots. These form a primary component of the cytoskeleton, and are often assembled over the course of several cell divisions, with one flagellum retained from the parent and the other derived from it. Centrioles produce the spindle during nuclear division.[43]\n The cells of plants, algae, fungi and most chromalveolates, but not animals, are surrounded by a cell wall. This is a layer outside the cell membrane, providing the cell with structural support, protection, and a filtering mechanism. The cell wall also prevents over-expansion when water enters the cell.[44]\n The major polysaccharides making up the primary cell wall of land plants are cellulose, hemicellulose, and pectin. The cellulose microfibrils are linked together with hemicellulose, embedded in a pectin matrix. The most common hemicellulose in the primary cell wall is xyloglucan.[45]\n Eukaryotes have a life cycle that involves sexual reproduction, alternating between a haploid phase, where only one copy of each chromosome is present in each cell, and a diploid phase, with two copies of each chromosome in each cell. The diploid phase is formed by fusion of two haploid gametes, such as eggs and spermatozoa, to form a zygote; this may grow into a body, with its cells dividing by mitosis, and at some stage produce haploid gametes through meiosis, a division that reduces the number of chromosomes and creates genetic variability.[46] There is considerable variation in this pattern. Plants have both haploid and diploid multicellular phases.[47] Eukaryotes have lower metabolic rates and longer generation times than prokaryotes, because they are larger and therefore have a smaller surface area to volume ratio.[48]\n The evolution of sexual reproduction may be a primordial characteristic of eukaryotes. Based on a phylogenetic analysis, Dacks and Roger have proposed that facultative sex was present in the group's common ancestor.[49] A core set of genes that function in meiosis is present in both Trichomonas vaginalis and Giardia intestinalis, two organisms previously thought to be asexual.[50][51] Since these two species are descendants of lineages that diverged early from the eukaryotic evolutionary tree, core meiotic genes, and hence sex, were likely present in the common ancestor of eukaryotes.[50][51] Species once thought to be asexual, such as Leishmania parasites, have a sexual cycle.[52] Amoebae, previously regarded as asexual, are anciently sexual; present-day asexual groups likely arose recently.[53]\n In antiquity, the two lineages of animals and plants were recognized by Aristotle and Theophrastus. The lineages were given the taxonomic rank of Kingdom by Linnaeus in the 18th century. Though he included the fungi with plants with some reservations, it was later realized that they are quite distinct and warrant a separate kingdom.[55] The various single-cell eukaryotes were originally placed with plants or animals when they became known. In 1818, the German biologist Georg A. Goldfuss coined the word protozoa to refer to organisms such as ciliates,[56] and this group was expanded until Ernst Haeckel made it a kingdom encompassing all single-celled eukaryotes, the Protista, in 1866.[57][58][59] The eukaryotes thus came to be seen as four kingdoms:\n The protists were at that time thought to be \"primitive forms\", and thus an evolutionary grade, united by their primitive unicellular nature.[58] Understanding of the oldest branchings in the tree of life only developed substantially with DNA sequencing, leading to a system of domains rather than kingdoms as top level rank being put forward by Carl Woese, Otto Kandler, and Mark Wheelis in 1990, uniting all the eukaryote kingdoms in the domain \"Eucarya\", stating, however, that \"'eukaryotes' will continue to be an acceptable common synonym\".[3][60] In 1996, the evolutionary biologist Lynn Margulis proposed to replace Kingdoms and Domains with \"inclusive\" names to create a \"symbiosis-based phylogeny\", giving the description \"Eukarya (symbiosis-derived nucleated organisms)\".[4]\n \n By 2014, a rough consensus started to emerge from the phylogenomic studies of the previous two decades.[9][61] The majority of eukaryotes can be placed in one of two large clades dubbed Amorphea (similar in composition to the unikont hypothesis) and the Diphoda (formerly bikonts), which includes plants and most algal lineages. A third major grouping, the Excavata, has been abandoned as a formal group as it is paraphyletic.[2] The proposed phylogeny below includes only one group of excavates (Discoba),[62] and incorporates the 2021 proposal that picozoans are close relatives of rhodophytes.[63] The Provora are a group of microbial predators discovered in 2022.[1]\n \n\n Ancyromonadida \n Malawimonada \n CRuMs \n Amoebozoa \n Breviatea \n Apusomonadida \n Holomycota (inc. fungi) \n Holozoa (inc. animals) \n ? Metamonada \n Discoba \n Cryptista \n Rhodophyta (red algae) \n Picozoa \n Glaucophyta \n Viridiplantae (plants) \n Hemimastigophora \n Provora \n Haptista \n Telonemia \n Rhizaria \n Alveolata \n Stramenopiles  \n One view of the great kingdoms and their stem groups.[62][64][65][13] The Metamonada are hard to place, being sister possibly to Discoba, possibly to Malawimonada.[13]\n \n The origin of the eukaryotic cell, or eukaryogenesis, is a milestone in the evolution of life, since eukaryotes include all complex cells and almost all multicellular organisms. The last eukaryotic common ancestor (LECA) is the hypothetical origin of all living eukaryotes,[67] and was most likely a biological population, not a single individual.[68] The LECA is believed to have been a protist with a nucleus, at least one centriole and flagellum, facultatively aerobic mitochondria, sex (meiosis and syngamy), a dormant cyst with a cell wall of chitin or cellulose, and peroxisomes.[69][70][71]\n An endosymbiotic union between a motile anaerobic archaean and an aerobic alphaproteobacterium gave rise to the LECA and all eukaryotes, with mitochondria. A second, much later endosymbiosis with a cyanobacterium gave rise to the ancestor of plants, with chloroplasts.[66]\n The presence of eukaryotic biomarkers in archaea points towards an archaeal origin. The genomes of Asgard archaea have plenty of Eukaryotic signature protein genes, which play a crucial role in the development of the cytoskeleton and complex cellular structures characteristic of eukaryotes. In 2022, cryo-electron tomography demonstrated that Asgard archaea have a complex actin-based cytoskeleton, providing the first direct visual evidence of the archaeal ancestry of eukaryotes.[72]\n The timing of the origin of eukaryotes is hard to determine but the discovery of Qingshania magnificia, the earliest multicelluar eukaryote from North China which lived during 1.635 billion years ago, suggests that the crown group eukaryotes would have originated from the late Paleoproterozoic (Statherian); the earliest unequivocal unicellular eukaryotes which lived during approximately 1.65 billion years ago are also discovered from North China: Tappania plana, Shuiyousphaeridium macroreticulatum, Dictyosphaera macroreticulata, Germinosphaera alveolata, and Valeria lophostriata.[73]\n Some acritarchs are known from at least 1.65 billion years ago, and a fossil, Grypania, which may be an alga, is as much as 2.1 billion years old.[74][75] The \"problematic\"[76] fossil Diskagma has been found in paleosols 2.2 billion years old.[76]\n Structures proposed to represent \"large colonial organisms\" have been found in the black shales of the Palaeoproterozoic such as the Francevillian B Formation, in Gabon, dubbed the \"Francevillian biota\" which is dated at 2.1 billion years old.[77][78] However, the status of  these structures as fossils is contested, with other authors suggesting that they might represent pseudofossils.[79] The oldest fossils than can unambiguously be assigned to eukaryotes are from the Ruyang Group of China, dating to approximately 1.8-1.6 billion years ago.[80] Fossils that are clearly related to modern groups start appearing an estimated 1.2 billion years ago, in the form of red algae, though recent work suggests the existence of fossilized filamentous algae in the Vindhya basin dating back perhaps to 1.6 to 1.7 billion years ago.[81]\n The presence of steranes, eukaryotic-specific biomarkers, in Australian shales previously indicated that eukaryotes were present in these rocks dated at 2.7 billion years old,[20][82] but these Archaean biomarkers have been rebutted as later contaminants.[83] The oldest valid biomarker records are only around 800 million years old.[84] In contrast, a molecular clock analysis suggests the emergence of sterol biosynthesis as early as 2.3 billion years ago.[85] The nature of steranes as eukaryotic biomarkers is further complicated by the production of sterols by some bacteria.[86][87]\n Whenever their origins, eukaryotes may not have become ecologically dominant until much later; a massive increase in the zinc composition of marine sediments 800 million years ago has been attributed to the rise of substantial populations of eukaryotes, which preferentially consume and incorporate zinc relative to prokaryotes, approximately a billion years after their origin (at the latest).[88]\n"}
{"key":"Cow","link":"https:\/\/en.wikipedia.org\/wiki\/Cow","headline":"Cattle - Wikipedia","content":"\n Cattle (Bos taurus) are large, domesticated, bovid ungulates widely kept as livestock. They are prominent modern members of the subfamily Bovinae and the most widespread species of the genus Bos. Mature female cattle are called cows and mature male cattle are bulls. Young female cattle are called heifers, young male cattle are oxen or bullocks, and castrated male cattle are known as steers.\n Cattle are commonly raised for meat, for dairy products, and for leather. As draft animals, they pull carts and farm implements. In  India, cattle are sacred animals. Small breeds such as the miniature Zebu are kept as pets.\n Taurine cattle are widely distributed across Europe and temperate areas of Asia, the Americas, and Australia. Zebus are found mainly in India and tropical areas of Asia, America, and Australia. Sanga cattle are found primarily in sub-Saharan Africa. These types, sometimes classified as separate species or subspecies, are further divided into over 1,000 recognized breeds.\n Around 10,500 years ago, taurine cattle were domesticated from wild aurochs progenitors in central Anatolia, the Levant and Western Iran. A separate domestication event occurred in the Indian subcontinent, which gave rise to zebu. There were over 940 million cattle in the world in 2022. Cattle are responsible for around 10% of global greenhouse gas emissions. Cattle were one of the first domesticated animals to have a fully-mapped genome.\n The term Cattle was borrowed from Anglo-Norman catel, itself from medieval Latin capitale 'principal sum of money, capital', itself derived in turn from Latin caput 'head'. Cattle originally meant movable personal property, especially livestock of any kind, as opposed to real property (the land, which also included wild or small free-roaming animals such as chickens—they were sold as part of the land).[1] The word is a variant of chattel (a unit of personal property) and closely related to capital in the economic sense.[2][1] The word cow came via Anglo-Saxon cū (plural cȳ), from Common Indo-European gʷōus (genitive gʷowés) 'a bovine animal', cf. Persian: gâv, Sanskrit: go-, Welsh: buwch.[3] In older English sources such as the King James Version of the Bible, cattle means livestock, as opposed to deer which are wild.[1]\n Cattle are large artiodactyls, mammals with cloven hooves,[4] kept on farms to produce meat, milk, and leather, and sometimes to pull carts or farm implements.[5] Bulls are larger than cows of the same breed by up to a few hundred kilograms. British Hereford cows, for example, weigh 600–800 kg (1,300–1,800 lb), while the bulls weigh 1,000–1,200 kg (2,200–2,600 lb).[6] Before 1790, beef cattle averaged only 160 kg (350 lb) net. Thereafter, weights climbed steadily.[7][8] The natural life of domestic cattle is some 25–30 years. Beef cattle go to slaughter at around 18 months, and dairy cows at about five years.[9]\n Cattle are ruminants, meaning their digestive system is highly specialized for processing plant material such as grass rich in cellulose, a tough carbohydrate polymer which many animals cannot digest. They do this in symbiosis with micro-organisms – bacteria, fungi, and protozoa – that possess cellulases, enzymes that split cellulose into its constituent sugars. Among the many bacteria that contribute are Fibrobacter succinogenes,  Ruminococcus flavefaciens, and Ruminococcus albus. Cellulolytic fungi include several species of Neocallimastix, while the protozoa include the ciliates Eudiplodinium maggie and Ostracodinium album.[11] If the animal's feed changes over time, the composition of this microbiome changes in response.[10]\n Cattle have one large stomach with four compartments; the rumen, reticulum, omasum, and abomasum. The rumen is the largest compartment and it harbours the most important parts of the microbiome.[10] The reticulum, the smallest compartment, is known as the \"honeycomb\". The omasum's main function is to absorb water and nutrients from the digestible feed. The abomasum has a similar function to the human stomach.[12]\n Cattle regurgitate and re-chew their food in the process of chewing the cud, like most ruminants. While feeding, cows swallow their food without chewing; it goes into the rumen for storage. Later, the food is regurgitated to the mouth, a mouthful at a time, where the cud is chewed by the molars, grinding down the coarse vegetation to small particles. The cud is then swallowed again and further digested by the micro-organisms in the cow's stomach.[12]\n The gestation period for a cow is about nine months long. The ratio of male to female offspring at birth is approximately 52:48.[13] A cow's udder has two pairs of mammary glands or teats.[14] Farms often use artificial insemination, the artificial deposition of semen in the female's genital tract; this allows farmers to choose from a wide range of bulls to breed their cattle. Estrus too may be artificially induced to facilitate the process.[15]\n Cows seek secluded areas for calving.[16] Semi-wild Highland cattle heifers first give birth at 2 or 3 years of age, and the timing of birth is synchronized with increases in natural food quality. Average calving interval is 391 days, and calving mortality within the first year of life is 5%.[17] Beef calves suckle an average of 5 times per day, spending some 46 minutes suckling. There is a diurnal rhythm in suckling, peaking at roughly 6am, 11:30am, and 7pm.[18] Under natural conditions, calves stay with their mother until weaning at 8 to 11 months. Heifer and bull calves are equally attached to their mothers in the first few months of life.[19]\n Cattle have a variety of cognitive abilities. They can memorize the locations of multiple food sources,[21] and can retain memories for at least 48 days.[22] Young cattle learn more quickly than adults,[23] and calves are capable of discrimination learning,[24] distinguishing familiar and unfamiliar animals,[25] and between humans, using faces and other cues.[26] Calves prefer their own mother's vocalizations to those of an unfamiliar mother.[27] Vocalizations provide information on the age, sex, dominance status and reproductive status of the caller, and may indicate estrus in cows and competitive display in bulls.[28] Cows can categorize images as familiar and unfamiliar individuals.[25] Cloned calves from the same donor form subgroups, suggesting that kin discrimination may be a basis of grouping behaviour.[29] Cattle use visual\/brain lateralisation when scanning novel and familiar stimuli.[30] They prefer to view novel stimuli with the left eye (using the right brain hemisphere), but the right eye for familiar stimuli.[31] Individual cattle have also been observed to display different personality traits, such as fearfulness and sociability.[20]\n Vision is the dominant sense; cattle obtain almost half of their information visually.[32] Being prey animals, cattle evolved to look out for predators almost all round, with eyes that are on the sides of their head rather than the front. This gives them a field of view of 330°, but limits binocular vision (and therefore stereopsis) to some 30° to 50°, compared to 140° in humans.[25] They are dichromatic, like most mammals.[33] \nCattle avoid bitter-tasting foods, selecting sweet foods for energy. Their sensitivity to sour-tasting foods helps them to maintain optimal ruminal pH.[32] They seek out salty foods by taste and smell to maintain their electrolyte balance.[34] Their hearing is better than that of horses[35] but less good at localising sounds than goats, and much less good than dogs or humans.[36] They can distinguish between live and recorded human speech.[37] Olfaction probably plays a large role in their social life, indicating social and reproductive status.[32][38] Cattle can tell when other animals are stressed by smelling the alarm chemicals in their urine.[39] Cattle can be trained to recognise conspecific individuals using olfaction only.[38]\n Cattle live in a dominance hierarchy. This is maintained in several ways. Cattle often engage in mock fights where they test each other's strength in a non-aggressive way. Licking is primarily performed by subordinates and received by dominant animals. Mounting is a playful behavior shown by calves of both sexes and by bulls and sometimes by cows in estrus,[40] however, this is not a dominance related behavior as has been found in other species.[17] Dominance-associated aggressiveness does not correlate with rank position, but is closely related to rank distance between individuals.[17] The horns of cattle are honest signals used in mate selection. Horned cattle attempt to keep greater distances between themselves and have fewer physical interactions than hornless cattle, resulting in more stable social relationships.[41] In calves, agonistic behavior becomes less frequent as space allowance increases, but not as group size changes, whereas in adults, the number of agonistic encounters increases with group size.[42]\n Dominance relationships in semi-wild highland cattle are very firm, with few overt aggressive conflicts: most disputes are settled by agonistic (non-aggressive, competitive) behaviors with no physical contact between opponents, reducing the risk of injury. Dominance status depends on age and sex, with older animals usually dominant to young ones and males dominant to females. Young bulls gain superior dominance status over adult cows when they reach about 2 years of age.[17]\n Cattle eat mixed diets, but prefer to eat approximately 70% clover and 30% grass. This preference has a diurnal pattern, with a stronger preference for clover in the morning, and the proportion of grass increasing towards the evening.[43] When grazing, cattle vary several aspects of their bite, i.e. tongue and jaw movements, depending on characteristics of the plant they are eating. Bite area decreases with the density of the plants but increases with their height. Bite area is determined by the sweep of the tongue; in one study observing 750-kilogram (1,650 lb) steers, bite area reached a maximum of approximately 170 cm2 (30 sq in). Bite depth increases with the height of the plants. By adjusting their behavior, cattle obtain heavier bites in swards that are tall and sparse compared with short, dense swards of equal mass\/area.[44] Cattle adjust other aspects of their grazing behavior in relation to the available food; foraging velocity decreases and intake rate increases in areas of abundant palatable forage.[45] Cattle avoid grazing areas contaminated by the faeces of other cattle more strongly than they avoid areas contaminated by sheep,[46] but they do not avoid pasture contaminated by rabbits.[47]\n In cattle, temperament or behavioral disposition can affect productivity, overall health, and reproduction.[49] Five underlying categories of temperament traits have been proposed: shyness–boldness, exploration–avoidance, activity, aggressiveness, and sociability.[50] Different studies have found many indicators of emotion in cattle. Holstein–Friesian heifers that had made clear improvements in a learning experiment had higher heart rates, indicating an emotional reaction to their own learning.[51] After separation from their mothers, Holstein calves reacted negatively, indicating low mood.[52] Similarly, after hot-iron dehorning, calves reacted negatively to the post-operative pain.[53] The position of the ears has been used as an indicator of emotional state.[25] Cattle can tell when other cattle are stressed by the chemicals in their urine.[39] Cattle are gregarious, and even short-term isolation causes psychological stress. When heifers are isolated, vocalizations, heart rate and plasma cortisol all increase. When visual contact is re-instated, vocalizations rapidly decline; heart rate decreases more rapidly if the returning cattle are familiar to the previously isolated individual.[54] Mirrors have been used to reduce stress in isolated cattle.[55]\n The average sleep time of a domestic cow is about 4 hours a day.[56] Cattle do have a stay apparatus,[57] but do not sleep standing up;[58] they lie down to sleep deeply.[59]\n On 24 April 2009, edition of the journal Science, a team of researchers led by the National Institutes of Health and the US Department of Agriculture reported having mapped the bovine genome.[61] Cattle have some 22,000 genes, of which 80% are shared with humans; they have about 1000 genes that they share with dogs and rodents, but not with humans. Using this bovine \"HapMap\", researchers can track the differences between the breeds that affect the quality of meat and milk yields.[62] In 2022, it was found that because the previous research was entirely focused on Hereford genetic sequences, it missed 4.2% of the overall cattle genome, and an updated genetic map was created to redress this issue.[60]\n Behavioral traits of cattle can be as heritable as some production traits, and often, the two can be related.[63]  The heritability of temperament (response to isolation during handling) has been calculated as 0.36 and 0.46 for habituation to handling.[64] Rangeland assessments show that the heritability of aggressiveness in cattle is around 0.36.[65]\n Quantitative trait loci have been found for a range of production and behavioral characteristics for both dairy and beef cattle.[66]\n Cattle have played a key role in human history, having been domesticated since at least the early neolithic age. Archaeozoological and genetic data indicate that cattle were first domesticated from wild aurochs (Bos primigenius) approximately 10,500 years ago. There were two major areas of domestication: one in central Anatolia, the Levant and Western Iran, giving rise to the taurine line, and a second in the area that is now Pakistan, resulting in the indicine line.[67] Modern mitochondrial DNA variation indicates the taurine line may have arisen from as few as 80 aurochs tamed in the upper reaches of Mesopotamia near the villages of Çayönü Tepesi in what is now southeastern Turkey, and Dja'de el-Mughara in what is now northern Syria.[68]\n Although European cattle are largely descended from the taurine lineage, gene flow from African cattle (partially of indicine origin) contributed substantial genomic components to both southern European cattle breeds and their New World descendants.[67] A study on 134 breeds showed that modern taurine cattle originated from Africa, Asia, North and South America, Australia, and Europe.[69] Some researchers have suggested that African taurine cattle are derived from a third independent domestication from the North African aurochs.[67] Whether there have been two or three domestications, European, African, and Asian cattle share much of their genomes both through their species ancestry and through repeated migrations of livestock and genetic material between species, as shown in the diagram.[70]\n Cattle were originally identified as three separate species: Bos taurus, the European or \"taurine\" cattle (including similar types from Africa and Asia); Bos indicus, the Indicine or \"zebu\"; and the extinct Bos primigenius, the aurochs. The aurochs is ancestral to both zebu and taurine cattle.[71] They were later reclassified as one species, Bos taurus, with the aurochs (B. t. primigenius), zebu (B. t. indicus), and taurine (B. t. taurus) cattle as subspecies.[72] However, this taxonomy is contentious, and authorities such as the American Society of Mammalogists treat these taxa as separate species.[73][74]\n Complicating the matter is the ability of cattle to interbreed with other closely related species. Hybrid individuals and even breeds exist, not only between taurine cattle and zebu (such as the sanga cattle (Bos taurus africanus x Bos indicus), but also between one or both of these and some other members of the genus Bos – yaks (the dzo or yattle[75]), banteng, and gaur. Hybrids such as the beefalo breed can even occur between taurine cattle and either species of bison, leading some authors to consider them part of the genus Bos, as well.[76] The hybrid origin of some types may not be obvious – for example, genetic testing of the Dwarf Lulu breed, the only taurine-type cattle in Nepal, found them to be a mix of taurine cattle, zebu, and yak.[77]\n The aurochs originally ranged throughout Europe, North Africa, and much of Asia. In historical times, its range became restricted to Europe, and the last known individual died in Mazovia, Poland, in about 1627.[78] Breeders have attempted to recreate a similar appearance to the aurochs by crossing traditional types of domesticated cattle, producing the Heck breed.[79]\n A group of taurine-type cattle exist in Africa; they either represent an independent domestication event or were the result of crossing taurines domesticated elsewhere with local aurochs, but they are genetically distinct;[80] some authors name them as a separate subspecies, Bos taurus africanus.[81] The only pure African taurine breeds remaining are the N'Dama, Kuri and some varieties of the West African Shorthorn.[82]\n Feral cattle are those that have been allowed to go wild.[83] Populations exist in many parts of the world,[84][85] sometimes on small islands.[86] Some, such as Amsterdam Island cattle,[72] Chillingham cattle,[87] and Aleutian wild cattle have become sufficiently distinct to be described as breeds.[88]\n Cattle are often raised by allowing herds to graze on the grasses of large tracts of rangeland. Raising cattle extensively in this manner allows the use of land that might be unsuitable for growing crops. The most common interactions with cattle involve daily feeding, cleaning and milking. Many routine husbandry practices involve ear tagging, dehorning, loading, medical operations, artificial insemination, vaccinations and hoof care, as well as training for agricultural shows and preparations. Around the world, Fulani husbandry rests on behavioural techniques, whereas in Europe, cattle are controlled primarily by physical means, such as fences.[90] Breeders use cattle husbandry to reduce tuberculosis susceptibility by selective breeding and maintaining herd health to avoid concurrent disease.[91]\n \nIn the United States, many cattle are raised intensively, kept in concentrated animal feeding operations, meaning there are at least 700 mature dairy cows or at least 1000 other cattle stabled or confined in a feedlot for \"45 days or more in a 12-month period\".[89] Historically, the cattle population of Britain rose from 9.8 million in 1878 to 11.7 million in 1908, but beef consumption rose much faster. Britain became the \"stud farm of the world\" exporting livestock to countries where there were no indigenous cattle. In 1929 80% of the meat trade of the world was products of what were originally English breeds. There were nearly 70 million cattle in the US by the early 1930s.[92]\n Cattle have the largest biomass of any animal species on Earth, at roughly 400 million tonnes, followed closely by Antarctic krill at 379 million tonnes and humans at 373 million tonnes.[93] In 2023, the countries with the most cattle were India with 307.5 million (32.6% of the total), Brazil with 194.4 million, and China with 101.5 million, out of a total of 942.6 million in the world.[94]\n The meat of adult cattle is known as beef, and that of calves is veal. Other animal parts are used as food products, including blood, liver, kidney, heart and oxtail. Approximately 300 million cattle, including dairy animals, are slaughtered each year for food.[95] About a quarter of the world's meat comes from cattle.[96] World cattle meat production in 2021 was 72.3 million tons.[97]\n Certain breeds of cattle, such as the Holstein-Friesian, are used to produce milk,[98][99] much of which is processed into dairy products such as butter, cheese, and yogurt. Dairy cattle are usually kept on specialized dairy farms designed for milk production. Most cows are milked twice per day, with milk processed at a dairy, which may be onsite at the farm or the milk may be shipped to a dairy plant for eventual sale of a dairy product.[100] Lactation is induced in heifers and spayed cows by a combination of physical and psychological stimulation, by drugs, or by a combination of those methods.[101] For mother cows to continue producing milk, they give birth to one calf per year. If the calf is male, it generally is slaughtered at a young age to produce veal.[102] They will continue to produce milk until three weeks before birth.[99] Over the last fifty years, dairy farming has become more intensive to increase the yield of milk produced by each cow. The Holstein-Friesian is the breed of dairy cow most common in the UK, Europe and the United States. It has been bred selectively to produce the highest yields of milk of any cow. Around 22 litres per day is average in the UK.[98][99]\n Dairy is a large industry worldwide. In 2023, the 27 European Union countries produced 143 million tons of cow's milk; the United States 104.1 million tons; and India 99.5 million tons.[103] India further produces 94.4 million tons of buffalo milk,[104] making it (in 2023) the world's largest milk producer; its dairy industry employs some 80 million people.[105]\n Oxen are cattle trained as draft animals. Oxen can pull heavier loads and for a longer period of time than horses.[106] Oxen are used worldwide, especially in developing countries. There are some 11 million draft oxen in sub-Saharan Africa,[107] while in 1998 India had over 65 million oxen.[108] At the start of the 21st century, about half the world's crop production depended on land preparation by draft animals.[109]\n Cattle are not often kept solely for hides, and they are usually a by-product of beef production. Hides are used mainly for leather products such as shoes. In 2012, India was the world's largest producer of cattle hides.[110] Cattle hides account for around 65% of the world's leather production.[111][112]\n Cattle are subject to pests including arthropod parasites such as ticks (which can in turn transmit diseases caused by bacteria and protozoa),[113] and diseases caused by pathogens including bacteria and viruses. Some viral diseases are spread by insects - i.e. bluetongue disease is spread by midges. Psoroptic mange is a disabling skin condition caused by mites. Bovine tuberculosis is caused by a bacterium; it causes disease in humans and in wild animals such as deer and badgers.[114] Foot-and-mouth disease is caused by a virus, affects a range of hoofed livestock and is highly contagious, being transmissible in a variety of ways.[115] Bovine spongiform encephalopathy is a neurodegenerative disease spread by a prion, a misfolded brain protein, in contaminated meat.[116] Among the intestinal parasites of cattle are Paramphistomum flukes, affecting the rumen, and hookworms in the small intestine.[117]\n Climate change is expected to exacerbate heat stress in cattle, and for longer periods.[119] Heat-stressed cattle may experience accelerated breakdown of adipose tissue by the liver, causing lipidosis.[120] Cattle eat less when heat stressed, resulting in ruminal acidosis, which can lead to laminitis. Cattle can attempt to deal with higher temperatures by panting more often; this rapidly decreases carbon dioxide concentrations at the price of increasing pH, respiratory alkalosis. To deal with this, cattle are forced to shed bicarbonate through urination, at the expense of rumen buffering. These two pathologies can both cause lameness.[120] Another specific risk is mastitis.[120] This worsens as Calliphora blowflies increase in number with continued warming, spreading mastitis-causing bacteria.[121] Ticks too are likely to increase in temperate zones as the climate warms, increasing the risk of tick-borne diseases.[122] Both beef and milk production are likely to experience declines due to climate change.[118][123]\n Cattle health is at once a veterinary issue (for animal welfare and productivity), a public health issue (to limit the spread of disease), and a food safety issue (to ensure meat and dairy products are safe to eat). These concerns are reflected in farming regulations.[124] These rules can become political matters, as when it was proposed in the UK in 2011 that milk from tuberculosis-infected cattle should be allowed to enter the food chain.[125] Cattle disease attracted attention in the 1980s and 1990s when bovine spongiform encephalopathy (mad cow disease) broke out in the United Kingdom. BSE can cross into humans as the deadly variant Creutzfeldt–Jakob disease; 178 people in the UK died from it.[126]\n The gut flora of cattle produce methane, a powerful[127] greenhouse gas, as a byproduct of enteric fermentation, which cattle belch out.[128] Livestock production creates around 14.5% of greenhouse gas emissions, some 65% of which is due to cattle.[129] Additional methane is produced by anaerobic fermentation of stored manure.[130]\n Concentrated animal feeding operations in particular produce substantial amounts of wastewater and manure,[131][132] which can cause environmental harms such as soil erosion, human and animal exposure to toxic chemicals, development of antibiotic resistant bacteria and an increase in E. coli contamination.[133][134] In many world regions, overgrazing by cattle has reduced biodiversity of the grazed plants and of animals at different trophic levels in the ecosystem.[135]\n Cattle husbandry practices including branding,[136] castration,[137] dehorning,[138] ear tagging,[139] nose ringing,[140] restraint,[141] tail docking,[142] the use of veal crates,[143] and cattle prods[144] have raised welfare concerns.[145]\n Stocking density is the number of animals within a specified area. High stocking density can affect cattle health, welfare, productivity,[146] and feeding behaviour.[147] Densely-stocked cattle feed more rapidly and lie down sooner, increasing the risk of teat infection, mastitis, and embryo loss.[148][149] The stress and negative health impacts induced by high stocking density such as in concentrated animal feeding operations or feedlots, auctions, and transport may be detrimental to cattle welfare.[150]\n To produce milk from dairy cattle, most calves are separated from their mothers soon after birth and fed milk replacement in order to retain the cows' milk for human consumption.[151] Animal welfare advocates are critical of this practice, stating that this breaks the natural bond between the mother and her calf.[151] The welfare of veal calves is also a concern.[152]\n Two sports involving cattle are thought to be cruel by animal welfare groups: rodeos and bullfighting. Such groups oppose rodeo activities including bull riding, calf roping and steer roping, stating that rodeos are unnecessary and cause stress, injury, and death to the animals.[153] In Spain, the Running of the bulls faces opposition due to the stress and injuries incurred by the bulls during the event.[154]\n From early in civilisation, cattle have been used in barter; an advantage of using cattle as currency is that it allows the seller to set a fixed price.[155][156]\n Cattle play a part in several religions. Veneration of the cow is a symbol of Hindu community identity.[157] Slaughter of cows (including oxen, bulls and calves) is forbidden by law in several states of the Indian Union.[158]\nIn Christian art, the Evangelist St. Luke is symbolised as an ox.[159] The second and longest surah of the Quran is named Al-Baqara (\"The Cow\"); it mentions cows seven times.[160][161] The ox is one of the 12-year cycle of animals which appear in the Chinese zodiac. The astrological sign Taurus is represented as a bull in the Western zodiac.[162] The akabeko (赤べこ, red cow) is a traditional toy from the Aizu region of Japan, thought to ward off illness.[163] In the Jewish religion, cattle appear as the golden calf, the idol that the Israelites made when Moses was on Mount Sinai,[164] and as the red heifer, used for certain ritual purifications.[165]\n"}
{"key":"Cow","link":"https:\/\/en.wikipedia.org\/wiki\/Cattle","headline":"Cattle - Wikipedia","content":"\n Cattle (Bos taurus) are large, domesticated, bovid ungulates widely kept as livestock. They are prominent modern members of the subfamily Bovinae and the most widespread species of the genus Bos. Mature female cattle are called cows and mature male cattle are bulls. Young female cattle are called heifers, young male cattle are oxen or bullocks, and castrated male cattle are known as steers.\n Cattle are commonly raised for meat, for dairy products, and for leather. As draft animals, they pull carts and farm implements. In  India, cattle are sacred animals. Small breeds such as the miniature Zebu are kept as pets.\n Taurine cattle are widely distributed across Europe and temperate areas of Asia, the Americas, and Australia. Zebus are found mainly in India and tropical areas of Asia, America, and Australia. Sanga cattle are found primarily in sub-Saharan Africa. These types, sometimes classified as separate species or subspecies, are further divided into over 1,000 recognized breeds.\n Around 10,500 years ago, taurine cattle were domesticated from wild aurochs progenitors in central Anatolia, the Levant and Western Iran. A separate domestication event occurred in the Indian subcontinent, which gave rise to zebu. There were over 940 million cattle in the world in 2022. Cattle are responsible for around 10% of global greenhouse gas emissions. Cattle were one of the first domesticated animals to have a fully-mapped genome.\n The term Cattle was borrowed from Anglo-Norman catel, itself from medieval Latin capitale 'principal sum of money, capital', itself derived in turn from Latin caput 'head'. Cattle originally meant movable personal property, especially livestock of any kind, as opposed to real property (the land, which also included wild or small free-roaming animals such as chickens—they were sold as part of the land).[1] The word is a variant of chattel (a unit of personal property) and closely related to capital in the economic sense.[2][1] The word cow came via Anglo-Saxon cū (plural cȳ), from Common Indo-European gʷōus (genitive gʷowés) 'a bovine animal', cf. Persian: gâv, Sanskrit: go-, Welsh: buwch.[3] In older English sources such as the King James Version of the Bible, cattle means livestock, as opposed to deer which are wild.[1]\n Cattle are large artiodactyls, mammals with cloven hooves,[4] kept on farms to produce meat, milk, and leather, and sometimes to pull carts or farm implements.[5] Bulls are larger than cows of the same breed by up to a few hundred kilograms. British Hereford cows, for example, weigh 600–800 kg (1,300–1,800 lb), while the bulls weigh 1,000–1,200 kg (2,200–2,600 lb).[6] Before 1790, beef cattle averaged only 160 kg (350 lb) net. Thereafter, weights climbed steadily.[7][8] The natural life of domestic cattle is some 25–30 years. Beef cattle go to slaughter at around 18 months, and dairy cows at about five years.[9]\n Cattle are ruminants, meaning their digestive system is highly specialized for processing plant material such as grass rich in cellulose, a tough carbohydrate polymer which many animals cannot digest. They do this in symbiosis with micro-organisms – bacteria, fungi, and protozoa – that possess cellulases, enzymes that split cellulose into its constituent sugars. Among the many bacteria that contribute are Fibrobacter succinogenes,  Ruminococcus flavefaciens, and Ruminococcus albus. Cellulolytic fungi include several species of Neocallimastix, while the protozoa include the ciliates Eudiplodinium maggie and Ostracodinium album.[11] If the animal's feed changes over time, the composition of this microbiome changes in response.[10]\n Cattle have one large stomach with four compartments; the rumen, reticulum, omasum, and abomasum. The rumen is the largest compartment and it harbours the most important parts of the microbiome.[10] The reticulum, the smallest compartment, is known as the \"honeycomb\". The omasum's main function is to absorb water and nutrients from the digestible feed. The abomasum has a similar function to the human stomach.[12]\n Cattle regurgitate and re-chew their food in the process of chewing the cud, like most ruminants. While feeding, cows swallow their food without chewing; it goes into the rumen for storage. Later, the food is regurgitated to the mouth, a mouthful at a time, where the cud is chewed by the molars, grinding down the coarse vegetation to small particles. The cud is then swallowed again and further digested by the micro-organisms in the cow's stomach.[12]\n The gestation period for a cow is about nine months long. The ratio of male to female offspring at birth is approximately 52:48.[13] A cow's udder has two pairs of mammary glands or teats.[14] Farms often use artificial insemination, the artificial deposition of semen in the female's genital tract; this allows farmers to choose from a wide range of bulls to breed their cattle. Estrus too may be artificially induced to facilitate the process.[15]\n Cows seek secluded areas for calving.[16] Semi-wild Highland cattle heifers first give birth at 2 or 3 years of age, and the timing of birth is synchronized with increases in natural food quality. Average calving interval is 391 days, and calving mortality within the first year of life is 5%.[17] Beef calves suckle an average of 5 times per day, spending some 46 minutes suckling. There is a diurnal rhythm in suckling, peaking at roughly 6am, 11:30am, and 7pm.[18] Under natural conditions, calves stay with their mother until weaning at 8 to 11 months. Heifer and bull calves are equally attached to their mothers in the first few months of life.[19]\n Cattle have a variety of cognitive abilities. They can memorize the locations of multiple food sources,[21] and can retain memories for at least 48 days.[22] Young cattle learn more quickly than adults,[23] and calves are capable of discrimination learning,[24] distinguishing familiar and unfamiliar animals,[25] and between humans, using faces and other cues.[26] Calves prefer their own mother's vocalizations to those of an unfamiliar mother.[27] Vocalizations provide information on the age, sex, dominance status and reproductive status of the caller, and may indicate estrus in cows and competitive display in bulls.[28] Cows can categorize images as familiar and unfamiliar individuals.[25] Cloned calves from the same donor form subgroups, suggesting that kin discrimination may be a basis of grouping behaviour.[29] Cattle use visual\/brain lateralisation when scanning novel and familiar stimuli.[30] They prefer to view novel stimuli with the left eye (using the right brain hemisphere), but the right eye for familiar stimuli.[31] Individual cattle have also been observed to display different personality traits, such as fearfulness and sociability.[20]\n Vision is the dominant sense; cattle obtain almost half of their information visually.[32] Being prey animals, cattle evolved to look out for predators almost all round, with eyes that are on the sides of their head rather than the front. This gives them a field of view of 330°, but limits binocular vision (and therefore stereopsis) to some 30° to 50°, compared to 140° in humans.[25] They are dichromatic, like most mammals.[33] \nCattle avoid bitter-tasting foods, selecting sweet foods for energy. Their sensitivity to sour-tasting foods helps them to maintain optimal ruminal pH.[32] They seek out salty foods by taste and smell to maintain their electrolyte balance.[34] Their hearing is better than that of horses[35] but less good at localising sounds than goats, and much less good than dogs or humans.[36] They can distinguish between live and recorded human speech.[37] Olfaction probably plays a large role in their social life, indicating social and reproductive status.[32][38] Cattle can tell when other animals are stressed by smelling the alarm chemicals in their urine.[39] Cattle can be trained to recognise conspecific individuals using olfaction only.[38]\n Cattle live in a dominance hierarchy. This is maintained in several ways. Cattle often engage in mock fights where they test each other's strength in a non-aggressive way. Licking is primarily performed by subordinates and received by dominant animals. Mounting is a playful behavior shown by calves of both sexes and by bulls and sometimes by cows in estrus,[40] however, this is not a dominance related behavior as has been found in other species.[17] Dominance-associated aggressiveness does not correlate with rank position, but is closely related to rank distance between individuals.[17] The horns of cattle are honest signals used in mate selection. Horned cattle attempt to keep greater distances between themselves and have fewer physical interactions than hornless cattle, resulting in more stable social relationships.[41] In calves, agonistic behavior becomes less frequent as space allowance increases, but not as group size changes, whereas in adults, the number of agonistic encounters increases with group size.[42]\n Dominance relationships in semi-wild highland cattle are very firm, with few overt aggressive conflicts: most disputes are settled by agonistic (non-aggressive, competitive) behaviors with no physical contact between opponents, reducing the risk of injury. Dominance status depends on age and sex, with older animals usually dominant to young ones and males dominant to females. Young bulls gain superior dominance status over adult cows when they reach about 2 years of age.[17]\n Cattle eat mixed diets, but prefer to eat approximately 70% clover and 30% grass. This preference has a diurnal pattern, with a stronger preference for clover in the morning, and the proportion of grass increasing towards the evening.[43] When grazing, cattle vary several aspects of their bite, i.e. tongue and jaw movements, depending on characteristics of the plant they are eating. Bite area decreases with the density of the plants but increases with their height. Bite area is determined by the sweep of the tongue; in one study observing 750-kilogram (1,650 lb) steers, bite area reached a maximum of approximately 170 cm2 (30 sq in). Bite depth increases with the height of the plants. By adjusting their behavior, cattle obtain heavier bites in swards that are tall and sparse compared with short, dense swards of equal mass\/area.[44] Cattle adjust other aspects of their grazing behavior in relation to the available food; foraging velocity decreases and intake rate increases in areas of abundant palatable forage.[45] Cattle avoid grazing areas contaminated by the faeces of other cattle more strongly than they avoid areas contaminated by sheep,[46] but they do not avoid pasture contaminated by rabbits.[47]\n In cattle, temperament or behavioral disposition can affect productivity, overall health, and reproduction.[49] Five underlying categories of temperament traits have been proposed: shyness–boldness, exploration–avoidance, activity, aggressiveness, and sociability.[50] Different studies have found many indicators of emotion in cattle. Holstein–Friesian heifers that had made clear improvements in a learning experiment had higher heart rates, indicating an emotional reaction to their own learning.[51] After separation from their mothers, Holstein calves reacted negatively, indicating low mood.[52] Similarly, after hot-iron dehorning, calves reacted negatively to the post-operative pain.[53] The position of the ears has been used as an indicator of emotional state.[25] Cattle can tell when other cattle are stressed by the chemicals in their urine.[39] Cattle are gregarious, and even short-term isolation causes psychological stress. When heifers are isolated, vocalizations, heart rate and plasma cortisol all increase. When visual contact is re-instated, vocalizations rapidly decline; heart rate decreases more rapidly if the returning cattle are familiar to the previously isolated individual.[54] Mirrors have been used to reduce stress in isolated cattle.[55]\n The average sleep time of a domestic cow is about 4 hours a day.[56] Cattle do have a stay apparatus,[57] but do not sleep standing up;[58] they lie down to sleep deeply.[59]\n On 24 April 2009, edition of the journal Science, a team of researchers led by the National Institutes of Health and the US Department of Agriculture reported having mapped the bovine genome.[61] Cattle have some 22,000 genes, of which 80% are shared with humans; they have about 1000 genes that they share with dogs and rodents, but not with humans. Using this bovine \"HapMap\", researchers can track the differences between the breeds that affect the quality of meat and milk yields.[62] In 2022, it was found that because the previous research was entirely focused on Hereford genetic sequences, it missed 4.2% of the overall cattle genome, and an updated genetic map was created to redress this issue.[60]\n Behavioral traits of cattle can be as heritable as some production traits, and often, the two can be related.[63]  The heritability of temperament (response to isolation during handling) has been calculated as 0.36 and 0.46 for habituation to handling.[64] Rangeland assessments show that the heritability of aggressiveness in cattle is around 0.36.[65]\n Quantitative trait loci have been found for a range of production and behavioral characteristics for both dairy and beef cattle.[66]\n Cattle have played a key role in human history, having been domesticated since at least the early neolithic age. Archaeozoological and genetic data indicate that cattle were first domesticated from wild aurochs (Bos primigenius) approximately 10,500 years ago. There were two major areas of domestication: one in central Anatolia, the Levant and Western Iran, giving rise to the taurine line, and a second in the area that is now Pakistan, resulting in the indicine line.[67] Modern mitochondrial DNA variation indicates the taurine line may have arisen from as few as 80 aurochs tamed in the upper reaches of Mesopotamia near the villages of Çayönü Tepesi in what is now southeastern Turkey, and Dja'de el-Mughara in what is now northern Syria.[68]\n Although European cattle are largely descended from the taurine lineage, gene flow from African cattle (partially of indicine origin) contributed substantial genomic components to both southern European cattle breeds and their New World descendants.[67] A study on 134 breeds showed that modern taurine cattle originated from Africa, Asia, North and South America, Australia, and Europe.[69] Some researchers have suggested that African taurine cattle are derived from a third independent domestication from the North African aurochs.[67] Whether there have been two or three domestications, European, African, and Asian cattle share much of their genomes both through their species ancestry and through repeated migrations of livestock and genetic material between species, as shown in the diagram.[70]\n Cattle were originally identified as three separate species: Bos taurus, the European or \"taurine\" cattle (including similar types from Africa and Asia); Bos indicus, the Indicine or \"zebu\"; and the extinct Bos primigenius, the aurochs. The aurochs is ancestral to both zebu and taurine cattle.[71] They were later reclassified as one species, Bos taurus, with the aurochs (B. t. primigenius), zebu (B. t. indicus), and taurine (B. t. taurus) cattle as subspecies.[72] However, this taxonomy is contentious, and authorities such as the American Society of Mammalogists treat these taxa as separate species.[73][74]\n Complicating the matter is the ability of cattle to interbreed with other closely related species. Hybrid individuals and even breeds exist, not only between taurine cattle and zebu (such as the sanga cattle (Bos taurus africanus x Bos indicus), but also between one or both of these and some other members of the genus Bos – yaks (the dzo or yattle[75]), banteng, and gaur. Hybrids such as the beefalo breed can even occur between taurine cattle and either species of bison, leading some authors to consider them part of the genus Bos, as well.[76] The hybrid origin of some types may not be obvious – for example, genetic testing of the Dwarf Lulu breed, the only taurine-type cattle in Nepal, found them to be a mix of taurine cattle, zebu, and yak.[77]\n The aurochs originally ranged throughout Europe, North Africa, and much of Asia. In historical times, its range became restricted to Europe, and the last known individual died in Mazovia, Poland, in about 1627.[78] Breeders have attempted to recreate a similar appearance to the aurochs by crossing traditional types of domesticated cattle, producing the Heck breed.[79]\n A group of taurine-type cattle exist in Africa; they either represent an independent domestication event or were the result of crossing taurines domesticated elsewhere with local aurochs, but they are genetically distinct;[80] some authors name them as a separate subspecies, Bos taurus africanus.[81] The only pure African taurine breeds remaining are the N'Dama, Kuri and some varieties of the West African Shorthorn.[82]\n Feral cattle are those that have been allowed to go wild.[83] Populations exist in many parts of the world,[84][85] sometimes on small islands.[86] Some, such as Amsterdam Island cattle,[72] Chillingham cattle,[87] and Aleutian wild cattle have become sufficiently distinct to be described as breeds.[88]\n Cattle are often raised by allowing herds to graze on the grasses of large tracts of rangeland. Raising cattle extensively in this manner allows the use of land that might be unsuitable for growing crops. The most common interactions with cattle involve daily feeding, cleaning and milking. Many routine husbandry practices involve ear tagging, dehorning, loading, medical operations, artificial insemination, vaccinations and hoof care, as well as training for agricultural shows and preparations. Around the world, Fulani husbandry rests on behavioural techniques, whereas in Europe, cattle are controlled primarily by physical means, such as fences.[90] Breeders use cattle husbandry to reduce tuberculosis susceptibility by selective breeding and maintaining herd health to avoid concurrent disease.[91]\n \nIn the United States, many cattle are raised intensively, kept in concentrated animal feeding operations, meaning there are at least 700 mature dairy cows or at least 1000 other cattle stabled or confined in a feedlot for \"45 days or more in a 12-month period\".[89] Historically, the cattle population of Britain rose from 9.8 million in 1878 to 11.7 million in 1908, but beef consumption rose much faster. Britain became the \"stud farm of the world\" exporting livestock to countries where there were no indigenous cattle. In 1929 80% of the meat trade of the world was products of what were originally English breeds. There were nearly 70 million cattle in the US by the early 1930s.[92]\n Cattle have the largest biomass of any animal species on Earth, at roughly 400 million tonnes, followed closely by Antarctic krill at 379 million tonnes and humans at 373 million tonnes.[93] In 2023, the countries with the most cattle were India with 307.5 million (32.6% of the total), Brazil with 194.4 million, and China with 101.5 million, out of a total of 942.6 million in the world.[94]\n The meat of adult cattle is known as beef, and that of calves is veal. Other animal parts are used as food products, including blood, liver, kidney, heart and oxtail. Approximately 300 million cattle, including dairy animals, are slaughtered each year for food.[95] About a quarter of the world's meat comes from cattle.[96] World cattle meat production in 2021 was 72.3 million tons.[97]\n Certain breeds of cattle, such as the Holstein-Friesian, are used to produce milk,[98][99] much of which is processed into dairy products such as butter, cheese, and yogurt. Dairy cattle are usually kept on specialized dairy farms designed for milk production. Most cows are milked twice per day, with milk processed at a dairy, which may be onsite at the farm or the milk may be shipped to a dairy plant for eventual sale of a dairy product.[100] Lactation is induced in heifers and spayed cows by a combination of physical and psychological stimulation, by drugs, or by a combination of those methods.[101] For mother cows to continue producing milk, they give birth to one calf per year. If the calf is male, it generally is slaughtered at a young age to produce veal.[102] They will continue to produce milk until three weeks before birth.[99] Over the last fifty years, dairy farming has become more intensive to increase the yield of milk produced by each cow. The Holstein-Friesian is the breed of dairy cow most common in the UK, Europe and the United States. It has been bred selectively to produce the highest yields of milk of any cow. Around 22 litres per day is average in the UK.[98][99]\n Dairy is a large industry worldwide. In 2023, the 27 European Union countries produced 143 million tons of cow's milk; the United States 104.1 million tons; and India 99.5 million tons.[103] India further produces 94.4 million tons of buffalo milk,[104] making it (in 2023) the world's largest milk producer; its dairy industry employs some 80 million people.[105]\n Oxen are cattle trained as draft animals. Oxen can pull heavier loads and for a longer period of time than horses.[106] Oxen are used worldwide, especially in developing countries. There are some 11 million draft oxen in sub-Saharan Africa,[107] while in 1998 India had over 65 million oxen.[108] At the start of the 21st century, about half the world's crop production depended on land preparation by draft animals.[109]\n Cattle are not often kept solely for hides, and they are usually a by-product of beef production. Hides are used mainly for leather products such as shoes. In 2012, India was the world's largest producer of cattle hides.[110] Cattle hides account for around 65% of the world's leather production.[111][112]\n Cattle are subject to pests including arthropod parasites such as ticks (which can in turn transmit diseases caused by bacteria and protozoa),[113] and diseases caused by pathogens including bacteria and viruses. Some viral diseases are spread by insects - i.e. bluetongue disease is spread by midges. Psoroptic mange is a disabling skin condition caused by mites. Bovine tuberculosis is caused by a bacterium; it causes disease in humans and in wild animals such as deer and badgers.[114] Foot-and-mouth disease is caused by a virus, affects a range of hoofed livestock and is highly contagious, being transmissible in a variety of ways.[115] Bovine spongiform encephalopathy is a neurodegenerative disease spread by a prion, a misfolded brain protein, in contaminated meat.[116] Among the intestinal parasites of cattle are Paramphistomum flukes, affecting the rumen, and hookworms in the small intestine.[117]\n Climate change is expected to exacerbate heat stress in cattle, and for longer periods.[119] Heat-stressed cattle may experience accelerated breakdown of adipose tissue by the liver, causing lipidosis.[120] Cattle eat less when heat stressed, resulting in ruminal acidosis, which can lead to laminitis. Cattle can attempt to deal with higher temperatures by panting more often; this rapidly decreases carbon dioxide concentrations at the price of increasing pH, respiratory alkalosis. To deal with this, cattle are forced to shed bicarbonate through urination, at the expense of rumen buffering. These two pathologies can both cause lameness.[120] Another specific risk is mastitis.[120] This worsens as Calliphora blowflies increase in number with continued warming, spreading mastitis-causing bacteria.[121] Ticks too are likely to increase in temperate zones as the climate warms, increasing the risk of tick-borne diseases.[122] Both beef and milk production are likely to experience declines due to climate change.[118][123]\n Cattle health is at once a veterinary issue (for animal welfare and productivity), a public health issue (to limit the spread of disease), and a food safety issue (to ensure meat and dairy products are safe to eat). These concerns are reflected in farming regulations.[124] These rules can become political matters, as when it was proposed in the UK in 2011 that milk from tuberculosis-infected cattle should be allowed to enter the food chain.[125] Cattle disease attracted attention in the 1980s and 1990s when bovine spongiform encephalopathy (mad cow disease) broke out in the United Kingdom. BSE can cross into humans as the deadly variant Creutzfeldt–Jakob disease; 178 people in the UK died from it.[126]\n The gut flora of cattle produce methane, a powerful[127] greenhouse gas, as a byproduct of enteric fermentation, which cattle belch out.[128] Livestock production creates around 14.5% of greenhouse gas emissions, some 65% of which is due to cattle.[129] Additional methane is produced by anaerobic fermentation of stored manure.[130]\n Concentrated animal feeding operations in particular produce substantial amounts of wastewater and manure,[131][132] which can cause environmental harms such as soil erosion, human and animal exposure to toxic chemicals, development of antibiotic resistant bacteria and an increase in E. coli contamination.[133][134] In many world regions, overgrazing by cattle has reduced biodiversity of the grazed plants and of animals at different trophic levels in the ecosystem.[135]\n Cattle husbandry practices including branding,[136] castration,[137] dehorning,[138] ear tagging,[139] nose ringing,[140] restraint,[141] tail docking,[142] the use of veal crates,[143] and cattle prods[144] have raised welfare concerns.[145]\n Stocking density is the number of animals within a specified area. High stocking density can affect cattle health, welfare, productivity,[146] and feeding behaviour.[147] Densely-stocked cattle feed more rapidly and lie down sooner, increasing the risk of teat infection, mastitis, and embryo loss.[148][149] The stress and negative health impacts induced by high stocking density such as in concentrated animal feeding operations or feedlots, auctions, and transport may be detrimental to cattle welfare.[150]\n To produce milk from dairy cattle, most calves are separated from their mothers soon after birth and fed milk replacement in order to retain the cows' milk for human consumption.[151] Animal welfare advocates are critical of this practice, stating that this breaks the natural bond between the mother and her calf.[151] The welfare of veal calves is also a concern.[152]\n Two sports involving cattle are thought to be cruel by animal welfare groups: rodeos and bullfighting. Such groups oppose rodeo activities including bull riding, calf roping and steer roping, stating that rodeos are unnecessary and cause stress, injury, and death to the animals.[153] In Spain, the Running of the bulls faces opposition due to the stress and injuries incurred by the bulls during the event.[154]\n From early in civilisation, cattle have been used in barter; an advantage of using cattle as currency is that it allows the seller to set a fixed price.[155][156]\n Cattle play a part in several religions. Veneration of the cow is a symbol of Hindu community identity.[157] Slaughter of cows (including oxen, bulls and calves) is forbidden by law in several states of the Indian Union.[158]\nIn Christian art, the Evangelist St. Luke is symbolised as an ox.[159] The second and longest surah of the Quran is named Al-Baqara (\"The Cow\"); it mentions cows seven times.[160][161] The ox is one of the 12-year cycle of animals which appear in the Chinese zodiac. The astrological sign Taurus is represented as a bull in the Western zodiac.[162] The akabeko (赤べこ, red cow) is a traditional toy from the Aizu region of Japan, thought to ward off illness.[163] In the Jewish religion, cattle appear as the golden calf, the idol that the Israelites made when Moses was on Mount Sinai,[164] and as the red heifer, used for certain ritual purifications.[165]\n"}
{"key":"Cow","link":"https:\/\/en.wikipedia.org\/wiki\/Taurus_Project","headline":"Taurus Project - Wikipedia","content":"The Taurus Project of the German Arbeitsgemeinschaft Biologischer Umweltschutz aims to re-create the extinct aurochs, the wild ancestor of domestic cattle, by cross-breeding Heck cattle (themselves bred in the 1920s and 1930s in an attempt to replicate the aurochs) with aurochs-like cattle, mostly from Southern Europe. Herds of these cross-bred Taurus cattle have been established in Germany, Denmark, Hungary and Latvia, and are used in conservation of natural landscapes and biodiversity.[1][2]\n In 1996 the conservation group Arbeitsgemeinschaft Biologischer Umweltschutz in Germany started to crossbreed Heck cattle with primitive cattle from Southern Europe such as Chianina, Sayaguesa Cattle and the Spanish fighting bull in the Lippeaue reserve near the town of Soest. The purpose was and is an increased resemblance to the extinct aurochs, because they considered Heck cattle not satisfying. For example, they write in one of their publications: \"The 'recreations' by the Heck brothers are too small, too short-legged, not elegant and their horns are not satisfying\".[3] Therefore, the goal is to breed cattle that are considerably larger, more long-legged and long-snouted and have horns curving forwards, in addition to possessing the wild type colour scheme that was already present in the population.[4][5] In 2003 breeding herds were started in Hungary and Denmark, and in 2004 one was begun in Latvia.[2]\n In Germany, Taurus cattle herds are crossed with Chianina and Sayaguesa, two very tall breeds, and initially also the Spanish fighting bull (Toro de Lidia). The crossbred animals in the Lippeaue reserve, the most important breeding location, are composed of 47% Sayaguesa, 29% Heck cattle, 20% Chianina and 4% Lidia on average.[6]\n Taurus cattle are listed in the herdbook X of the German Heck cattle association VFA. There is an increasing interest of Heck cattle breeders in using Taurus cattle because of their larger resemblance to the aurochs, so that there is a continuum between Taurus cattle and un-crossed Heck cattle.[4]\n Hortobágy National Park in Hungary has the largest herd of Taurus cattle so far, counting 500–600 individuals of which around 200 are mature cows.[7] In addition to crossbred cattle that were imported from Germany, Ankole-Watusi, Hungarian Grey cattle crosses and one half-Holstein Friesian cow are used.[8] There are two sub-herds, a main herd at Pentezug and another one at Karácsonyfok.[9] Studies in the national park showed that cattle are less adapted to dry, cold grassland than Przewalski horses, and until a few years ago the cattle were supplementary fed.[9][10] The winter of 2011 was the first winter in which additional food was not necessary.[11]\n Taurus breeding was initiated in Lille Vildmose Nature Reserve under the name Projekt Urokse (\"Project Aurochs\").[12]  The founding herd consisted of one Chianina × Heck bull, four Heck cows and one Sayaguesa × Heck cow, and in 2009 three Sayaguesa bulls were added; by 2010 the herd had grown to 56 individuals.[13]\n In Latvia, Taurus cattle are being bred by WWF Latvia.[2] In February 2004, besides two German animals 21 head of Dutch Heck cattle were brought to Pape Nature Reserve,[14] in October another 18 head of Dutch Heck cattle followed.[15]\n Most Taurus cattle are long-legged and comparatively slender. An increase in size was achieved, as large Taurus bulls measure 165 cm at the withers while normal Heck bulls measure only 140 cm. Particularly, the Sayaguesa-influenced animals are long-snouted.[4]\n"}
{"key":"Cow","link":"https:\/\/en.wikipedia.org\/wiki\/Fleckvieh","headline":"Fleckvieh - Wikipedia","content":"The Fleckvieh is a breed of dual-purpose cattle suitable for both milk and meat production. It originated in Central Europe in the 19th century from cross-breeding of local stock with Simmental cattle imported from Switzerland. Today, the worldwide population is 41 million animals. \n The Fleckvieh originated in the Austrian Empire and the Kingdom of Bavaria from cross-breeding of local stock with Simmental cattle imported from Switzerland from about 1830. The Simmental had good milk-producing and draught qualities, and the resulting crosses were triple-purpose animals with milk, meat, and draught capabilities. The Fleckvieh is now a dual-purpose breed; it may be used for the production of beef or milk, or be crossed with dairy breeds or with beef breeds.[3]\n It is reported from several European countries, including Austria,[2] Belgium,[4] Germany,[5][6] the Netherlands,[7] and Spain,[8] and also, since 2009, from Switzerland;[9] in Hungary, the Fleckvieh is present on many small farms and its importance is growing steadily.[10] It is also reported from other countries of the world, including  Australia, Paraguay,[11] Peru,[12] and Uruguay.[13]\n A comparison was made between the rates of muscle growth and energy use of Fleckvieh bulls as compared to German Black Pied (Schwarzbunte) bulls. The Fleckvieh bulls had faster growth rates, the carcasses had a smaller proportion of fat, especially abdominal fat, and the animals could be slaughtered at an earlier date on similar diets.[14]\n"}
{"key":"Cow","link":"https:\/\/en.wikipedia.org\/wiki\/Cowbell","headline":"Cowbell - Wikipedia","content":"A cowbell (or cow bell) is a bell worn around the neck of free-roaming livestock so herders can keep track of an animal via the sound of the bell when the animal is grazing out of view in hilly landscapes or vast plains.[1] Although they are typically referred to as \"cow bells\" due to their extensive use with cattle, the bells are used on a wide variety of animals.\n The bell and clapper are commonly crafted from iron, bronze, brass, copper, or wood. The collar used to hold the bell is traditionally made with leather and wood fibers. The craftsmanship of cow bells varies by geographic location and culture. Most cow bells are made of thin, flat pieces of plated sheet metal. Plating causes the sheet metal to have a surface which can be decorated or left plain. The ornaments on the cow bell and the collar are usually decorative although some cultures believe that certain ornaments provide or enhance magical protections such as the power to prevent or cure fever and other illnesses.[citation needed] Different bells can have specific sounds to identify important characteristics of the animals, such as age, sex, and species. Some cultures have even developed names to differentiate between bells and their tones; for example, in Spanish, \"truco\" refers to stud males, \"esquila\" to female goats or ewes, and \"esquileta\" for pregnant females and immature animals. Each of these bells possess unique sounds, shapes, and sizes.\n Bells are used to keep track of grazing animal herds such as goats, reindeer, sheep and cows. They are mainly used in Europe, Mediterranean areas and Latin America, but are also used worldwide by those who practice transhumance, including nomadic pastoral tribes in Africa and Asia. Some people put bells on their livestock because they believe the foreign sound of the bell scares off predators,[2] however, some studies have shown that the sound of the bell has the opposite effect and leads predators to livestock because predators develop a learned association between the sound of the bell and the presence of a prey animal.[3]\n Cowbells are often rung by human spectators at Alpine skiing events, and also, particularly in the US, at cyclo-cross races. They are usually held in the hand rather than worn around the neck.\n In the 1960s, Earl W. Terrell and Ralph L. Reeves welded handles onto cowbells at Mississippi State University for students. Today that has led to 60,000 cowbells ringing around Mississippi State Bulldog Athletics. Mississippi State now holds the world record for most cowbells ringing simultaneously. \n [4][5]\n Archaeological evidence of bells dates back to more than 5000 years ago, from the 3rd millennium BC in Neolithic China.[6] During this era, there is evidence of early forms of pottery cowbells, which were likely used to track goats, sheep, and cattle.[7] The pottery bells were later replaced by metal bells. In West Asia, the first bells appeared in 1000 BC.[6] The earliest metal bells, one found in the Taosi site, and four in the Erlitou site, are dated to about 2000 BC.[8]\n Bells for shepherding were expanded from the fertile crescent to Celtic, Carthaginian, Greek and Roman cultures. The earliest depictions of bells used for livestock in Britain appear on Pictish carved stones of the 7th to 9th centuries AD at Eassie, Angus[9] and Fowlis Wester, Perthshire.[10] Small iron bells of 8th or 9th century date, argued to be for cow or sheep, have been excavated from upland farm settlements at Crummack Dale and Gauber High Pasture in the Yorkshire Dales.[11] An early depiction of a bellwether, the leading sheep of a flock, on whose neck a bell is hung, is in the Carolingian Stuttgart Psalter of the ninth century.[12]\n In Europe, the earliest written evidence of bells used for livestock dates to the late 14th to early 15th century. Grimm's Deutsches Wörterbuch s.v. \"Kuhschelle\" points to a 1410 mention in a Frankfurt archive; the OED lists 1440 as the earliest attestation of a 'bell-wether'. The OED also attributes the phrase \"to bear the bell\" in the sense \"to take the first place\" as originally referring to the leading cow or sheep of a drove or flock to Chaucer's Troilus and Criseyde, 1374. In 15th-century Germany, a cow bell was worn only by the best and leading piece of livestock.[citation needed] The wider distribution of the bell worn by livestock was a gradual process of the Early Modern period. In France in the mid-16th century, Francois Rabelais makes this practice explicit in his Gargantua and Pantagruel, stating that\n such was the custom, to appear on the field wearing jingling garment, as the high priest wears when entering the sacristy; since the tournaments, that is, the contest of nobility, have been abolished, carters have taken the bells and hung them on their hacks. The importance of the cow bell is highlighted in Swiss folklore, which reflects a period when a great Trychel, or large cow bell, was a rare and much-coveted item. The legend of the Simmental tells how a young cowherd strays inside a mountain, and is offered by a beautiful woman the choice between a treasure of gold coins, a golden Trychel, or the fairy herself. He chooses the Trychel.[13]\n As opposed to regular cast-metal bells, 'trychlen' are made of hammered sheet metal. This results in a clanking, less crisp sound, but at the same time results in a bell that is lighter and thus easier to carry.\n Modern-day manufacturing of cow bells continues today in Korea, Indonesia, and India, many created as village handicrafts. Despite a May 2012 fire that destroyed its factory, the Bevin Brothers Manufacturing Company continues to make cow bell bells in East Hampton, CT, as it has since its founding in 1832; it is the only remaining U.S. company making just bells.[14]\n In Western Europe, when the snow has melted in the spring, villages send the cows to the high alpine meadows to graze. This event, called Alpaufzug, is celebrated in each village with a procession through the village to the high pastures. The cows are decorated with floral wreaths woven through the horns. The best milk-producing cow in the village leads the procession and wears the largest bell. The bells are made in various sizes, and are awarded to the cows according to their milk production that year.[15]\n In the fall, the event is repeated, but is called an \"Alpabzug\", as the animals return from the high meadow. The best cows (each referred to as a 'Kranzkuh', \"crown[ed] cow\", after the ornamental headwear with which it is adorned) from each herd again lead the procession. The traditional festival is called Viehscheid in Southern Germany, and has other names in the Alpine regions.[16]\n Cow bells can be as loud as 113 decibels, and it has been suggested that this may cause pain or deafness in animals wearing them.[17] A study[18] published in 2015 found that wearing a bell over three days caused cows to spend less time feeding, ruminating, and lying down. Animal rights campaigners, including the German Animal Welfare Society, have called for a ban on using cow bells.[19]\n"}
{"key":"Tiger","link":"https:\/\/en.wikipedia.org\/wiki\/Tiger","headline":"Tiger - Wikipedia","content":"\n The tiger (Panthera tigris) is the largest living cat species and a member of the genus Panthera. It is most recognisable for its black stripes on orange fur with a white underside. An apex predator, it primarily preys on ungulates, such as deer and wild boar. It is territorial and generally a solitary but social predator, requiring large contiguous areas of habitat to support its requirements for prey and rearing of its offspring. Tiger cubs stay with their mother for about two years and then become independent, leaving their mother's home range to establish their own.\n The tiger was first scientifically described in 1758. It once ranged widely from the Eastern Anatolia Region in the west to the Amur River basin in the east, and in the south from the foothills of the Himalayas to Bali in the Sunda Islands. Since the early 20th century, tiger populations have lost at least 93% of their historic range and have been extirpated from Western and Central Asia, the islands of Java and Bali, and in large areas of Southeast and South Asia and China. What remains of the range where tigers still roam free is fragmented, stretching in spots from Siberian temperate forests to subtropical and tropical forests on the Indian subcontinent, Indochina and a single Indonesian island, Sumatra.\n The tiger is listed as Endangered on the IUCN Red List. India hosts the largest tiger population. Major reasons for population decline are habitat destruction, habitat fragmentation and poaching. Tigers are also victims of human–wildlife conflict, due to encroachment in countries with a high human population density.\n The tiger is among the most recognisable and popular of the world's charismatic megafauna. It featured prominently in the ancient mythology and folklore of cultures throughout its historic range and continues to be depicted in modern films and literature, appearing on many flags, coats of arms and as mascots for sporting teams. The tiger is the national animal of India, Bangladesh, Malaysia and South Korea.\n The Old English tigras derive from Old French tigre, from Latin tigris. This was a borrowing of Classical Greek τίγρις 'tigris'.[4][5] The ultimate origin of the word is uncertain.[6] Ancient Greek geographer Strabo suggested an Armenian origin.[7] One popular idea, believed in the 16th and 17th centuries, is tiger was a transliteration of the Middle Persian tigr, meaning 'arrow', from which the name of the river Tigris may also have been derived. Thus, the animal and the river may have both been associated with speed. The connection between the two words is doubted in modern times, and they are likely to be Latin homonyms.[6]\n In 1758, Carl Linnaeus described the tiger in his work Systema Naturae and gave it the scientific name Felis tigris.[2] In 1929, the British taxonomist Reginald Innes Pocock subordinated the species under the genus Panthera using the scientific name Panthera tigris.[8][9]\n \nFollowing Linnaeus's first descriptions of the species, several tiger zoological specimens were described and proposed as subspecies.[10] The validity of several tiger subspecies was questioned in 1999. Most putative subspecies described in the 19th and 20th centuries were distinguished on the basis of fur length and colouration, striping patterns and body size, hence characteristics that vary widely within populations. Morphologically, tigers from different regions vary little, and gene flow between populations in those regions is considered to have been possible during the Pleistocene. Therefore, it was proposed to recognize only two tiger subspecies as valid, namely P. t. tigris in mainland Asia, and P. t. sondaica in the Greater Sunda Islands. Mainland tigers are described as being larger in size with generally lighter fur and fewer stripes, while island tigers are smaller due to insular dwarfism, with darker coats and more numerous stripes.[11] The stripes of island tigers may break up into spotted patterns.[12]\n This two-subspecies proposal was reaffirmed in 2015 by a comprehensive analysis of morphological, ecological and molecular traits of all putative tiger subspecies using a combined approach. The authors proposed recognition of only two subspecies, namely P. t. tigris comprising the Bengal, Malayan, Indochinese, South Chinese, Siberian and Caspian tiger populations of continental Asia, and P. t. sondaica comprising the Javan, Bali and Sumatran tiger populations of the Sunda Islands. The continental nominate subspecies P. t. tigris constitutes two clades: a northern clade composed of the Siberian and Caspian tiger populations, and a southern clade composed of all other mainland populations. The authors noted that this two-subspecies reclassification will impact tiger conservation management.[13] It would make captive breeding programs and future re-wilding of zoo-born tigers easier, as one tiger population could then be used to reinforce another. However, there is the risk that the loss of subspecies uniqueness could lead to less protection efforts for specific populations.[14]\n In 2017, the Cat Classification Task Force of the IUCN Cat Specialist Group revised felid taxonomy in accordance with the two-subspecies proposal of the comprehensive 2015 study, and recognized the tiger populations in continental Asia as P. t. tigris, and those in the Sunda Islands as P. t. sondaica.[15] This two-subspecies view is still disputed by researchers, since the currently recognized six living subspecies can be distinguished genetically.[14] Results of a 2018 whole-genome sequencing of 32 samples support six monophyletic tiger clades corresponding with the six living proposed subspecies and indicate they descended from a common ancestor around 110,000 years ago.[16] Studies in 2021 and 2023 also affirmed the genetic distinctiveness and separation of these tigers.[17][18]\n The following tables are based on the classification of the species Panthera tigris provided in Mammal Species of the World,[10] and also reflect the classification used by the Cat Classification Task Force in 2017:[15]\n The tiger shares the genus Panthera with the lion, leopard, jaguar and snow leopard. Results of genetic analysis indicate that the tiger and snow leopard are sister species and about 2.88 million years ago, the tiger and the snow leopard lineages diverged from the other Panthera species.[36][40]\n The fossil species Panthera palaeosinensis of early Pleistocene northern China was described as a possible tiger ancestor when it was discovered in 1924, but modern cladistics place it as basal to modern Panthera.[41][39] Panthera zdanskyi, which lived around the same time and place, was suggested to be a sister taxon of the modern tiger when it was examined in 2014.[39] However, as of 2023, at least two recent studies considered P. zdanskyi likely to be a synonym of P. palaeosinensis, noting that its proposed differences from that species fell within the range of individual variation.[42][43] The earliest appearance of the modern tiger species in the fossil record are jaw fragments from Lantion in China that are dated to the early Pleistocene.[39] Middle to late Pleistocene tiger fossils were found throughout China, Sumatra and Java. Prehistoric subspecies include Panthera tigris trinilensis and P. t. soloensis of Java and Sumatra, and P. t. acutidens of China; late Pleistocene and early Holocene fossils of tigers were also found in Borneo and Palawan, Philippines.[44]\n Results of a phylogeographic study indicate that all living tigers had a common ancestor 108,000 to 72,000 years ago.[30] A 2022 paleogenomic study of a Pleistocene tiger basal to living tigers concluded that modern tiger populations spread across Asia no earlier than 94,000 years ago. There is evidence of interbreeding between the lineage of modern mainland tigers and these ancient tigers.[45] The potential tiger range during the late Pleistocene and Holocene was predicted applying ecological niche modelling based on more than 500 tiger locality records combined with bioclimatic data. The resulting model shows a contiguous tiger range at the Last Glacial Maximum, indicating gene flow between tiger populations in mainland Asia. The tiger populations on the Sunda Islands and mainland Asia were possibly separated during interglacial periods.[46]\n The tiger's full genome sequence was published in 2013. It was found to have repeat compositions much as other cat genomes and \"an appreciably conserved synteny\".[47]\n Captive tigers were bred with lions to create hybrids called liger and tigon. The former born to a female tiger and male lion and the latter the result of a male tiger and female lion. They share physical and behavioural qualities of both parent species.[48] Because the lion sire passes on a growth-promoting gene, but the corresponding growth-inhibiting gene from the female tiger is absent, ligers grow far larger than either parent species. By contrast, the male tiger does not pass on a growth-promoting gene and the lioness passes on a growth inhibiting gene, hence tigons are around the same size as either species.[49] Breeding hybrids is now discouraged due to the emphasis on conservation.[48]\n The tiger is considered to be the largest living felid species.[12] However, there is some debate over averages compared to the lion. Since tiger populations vary greatly in size, the \"average\" size for a tiger may be less than a lion, while the biggest tigers are bigger than their lion counterparts.[44] The Siberian and Bengal tigers, along with the extinct Caspian are considered to be the largest of the species.[12] Bengal tigers average a total length of 3 m (9.8 ft), with males weighing 200–260 kg (440–570 lb) and females weighing 100–160 kg (220–350 lb).[50] Island tigers are the smallest, the Sumatran tigers have a total length of 2.2–2.5 m (7 ft 3 in – 8 ft 2 in) with a weight of 100–140 kg (220–310 lb) for males and 75–110 kg (165–243 lb) for females.[50] The extinct Bali tiger was even smaller.[12] It has been hypothesised that body sizes of different tiger populations may be correlated with climate and be explained by thermoregulation and Bergmann's rule.[12][11]\n The tiger has a typical felid morphology. It has a muscular body with strong forelimbs, a large head and a tail that is about half the length of the rest of its body. There are five digits on the front feet and four on the back, all of which have retractable claws which are compact and curved. The ears are rounded, while the eyes have a round pupil.[12] The tiger's skull is large and robust, with a constricted front region, proportionally small, elliptical orbits, long nasal bones, and a lengthened cranium with a large sagittal crest.[51][12] It resembles a lion's skull; with the structure of the lower jaw and length of the nasals being the most reliable indicators for species identification.[51] The tiger has fairly robust teeth and its somewhat curved canines are the longest in the cat family at 6.4–7.6 cm (2.5–3.0 in).[12][52] It has an average bite force at the canine tips of 1234.3 Newton.[53]\n Tiger fur tends to be short, except in the northern-living Siberian tiger. It has a mane-like heavy growth of fur around the neck and jaws and long whiskers, especially in males.[12] Its colouration is generally orange, but can vary from light yellow to dark red.[12][44][54] White fur covers the ventral surface, along with parts of the face.[12][55] It also has a prominent white spot on the back of their ears which are surrounded by black.[12] The tiger is marked with distinctive black or dark brown stripes; the patterns of which are unique in each individual.[12][56] The stripes are mostly vertical, but those on the limbs and forehead are horizonal. They are more concentrated towards the posterior and those on the trunk may or may not reach under the belly. The tips of stripes are generally sharp and some have gaps within them. Tail stripes are thick bands and a black tip marks the end.[57]\n Stripes are likely advantageous for camouflage in vegetation with vertical patterns of light and shade, such as trees and long grass.[56] This is supported by a 1987 Fourier analysis study which concluded that the spatial frequencies of tiger stripes line up with their environment.[58] The tiger is one of only a few striped cat species; it is not known why spotted patterns and rosettes are the more common camouflage pattern among felids.[59] The orange colour may also aid in concealment as the tiger's prey are dichromats, and thus may perceive the cat as green and blended in with the vegetation.[60] The white dots on the ear may play a role in communication.[12]\n Three colour variants – white, golden and nearly stripeless snow white are now virtually non-existent in the wild due to the reduction of wild tiger populations, but continue in captive populations. The white tiger has a white background colour with sepia-brown stripes. The golden tiger is pale golden with reddish-brown stripes. The snow white tiger is a morph with extremely faint stripes and a pale reddish-brown ringed tail. White and golden morphs are the result of an autosomal recessive trait with a white locus and a wideband locus respectively. The snow white variation is caused by polygenes with both the white and wideband loci.[61]\nThe breeding of white tigers is controversial, as they have no use for conservation. Only 0.001% of wild tigers have the genes for this colour morph, and the overrepresentation of white tigers in captivity is the result of inbreeding. Hence their continued breeding will risk both inbreeding depression and loss of genetic variability in captive tigers.[62]\n Pseudo-melanistic tigers with thick, merged stripes have been recorded in Simlipal National Park and three Indian zoos; population genetic analysis of Indian tiger samples revealed that this phenotype is caused by a mutation of a transmembrane aminopeptidase gene. Around 37% of the Simlipal tiger population has this feature, which has been linked to genetic isolation.[63]\n The tiger historically ranged from eastern Turkey and northern Afghanistan to Indochina, and from southeastern Siberia to Sumatra, Java and Bali.[12] As of 2022, it inhabits less than 7% of its historical distribution, and has a scattered range that includes the Indian subcontinent, the Indochinese Peninsula, Sumatra, the Russian Far East and northeastern China.[1]\nAs of 2020, India had the largest extent of global tiger habitat with 300,508 km2 (116,027 sq mi), followed by Russia with 195,819 km2 (75,606 sq mi).[64]\n The tiger mainly lives in forest habitats and is highly adaptable.[50] Records in Central Asia indicate that it occurred foremost in Tugay riverine forests and inhabited hilly and lowland forests in the Caucasus.[65] In the Amur-Ussuri region, it inhabits Korean pine and temperate broadleaf and mixed forests; riparian forests serve as dispersal corridors, providing food and water for both tiger and ungulates.[66] On the Indian subcontinent, it inhabits mainly tropical and subtropical moist broadleaf forests, moist evergreen forests, tropical dry forests, alluvial plains and the mangrove forests of the Sundarbans.[67] In the Eastern Himalayas, tigers were documented in temperate forest up to an elevation of 4,200 m (13,800 ft) in Bhutan and of 3,630 m (11,910 ft) in the Mishmi Hills.[68][69] In Thailand, it lives in deciduous and evergreen forests.[70] In Sumatra, tigers range from lowland peat swamp forests to rugged montane forests.[71]\n Camera trap data show that tigers in Chitwan National Park avoided locations frequented by people and were more active at night than by day.[72]\nIn Sundarbans National Park, six radio-collared tigers were most active in the early morning with a peak around dawn and moved an average distance of 4.6 km (2.9 mi) per day.[73]\nA three-year long camera trap survey in Shuklaphanta National Park revealed that tigers were most active from dusk until midnight.[74]\nIn northeastern China, tigers were crepuscular and active at night with activity peaking at dawn and at dusk; they exhibited a high temporal overlap with ungulate species.[75]\n As with other felid species, tigers groom themselves, maintaining their coats by licking them and spreading oil from their sebaceous glands.[76] It will take to water, particularly on hot days. It is a powerful swimmer and easily transverses across rivers as wide as 8 km (5.0 mi).[56] Adults only occasionally climbs trees, but have been recorded climbing 10 m (33 ft) up a smooth pipal tree.[12] In general, tigers are less capable tree climbers than many other cats due to their size, but cubs under 16 months old may routinely do so.[77]\n Adult tigers lead largely solitary lives. They establish and maintain home ranges, the size of which mainly depends on prey abundance, geographic area and sex of the individual. Males and females defend their home ranges from those of the same sex, and the home range of a male encompasses that of multiple females.[12][56] Two females in the Sundarbans had home ranges of 10.6 and 14.1 km2 (4.1 and 5.4 sq mi).[78] In Panna Tiger Reserve, the home ranges of five reintroduced females varied from 53–67 km2 (20–26 sq mi) in winter to 55–60 km2 (21–23 sq mi) in summer and to 46–94 km2 (18–36 sq mi) during monsoon; three males had 84–147 km2 (32–57 sq mi) large home ranges in winter, 82–98 km2 (32–38 sq mi) in summer and 81–118 km2 (31–46 sq mi) during monsoon seasons.[79] In Huai Kha Khaeng Wildlife Sanctuary, seven resident females had home ranges of 44.1–122.3 km2 (17.0–47.2 sq mi) and four resident males of 174.8–417.5 km2 (67.5–161.2 sq mi).[80]\nFour male problem tigers in Sumatra were translocated to national parks and needed 6–17 weeks to establish new home ranges of 37.5–188.1 km2 (14.5–72.6 sq mi).[81]\nTen solitary females in Sikhote-Alin Biosphere Reserve had home ranges of 413.5 ± 77.6 km2 (159.7 ± 30.0 sq mi); when they had cubs of up to 4 months of age, their home ranges declined to 177.3 ± 53.5 km2 (68.5 ± 20.7 sq mi) and steadily grew to 403.3 ± 105.1 km2 (155.7 ± 40.6 sq mi) until the cubs were 13–18 months old.[82]\n The tiger is a long-ranging species, and individuals disperse over distances of up to 650 km (400 mi) to reach tiger populations in other areas.[83] Young tigresses establish their first territories close to their mother's. Males, however, migrate further than their female counterparts and set out at a younger age to mark out their own area.[84] Four radio-collared females in Chitwan dispersed between 0 and 43.2 km (0.0 and 26.8 mi), and 10 males between 9.5 and 65.7 km (5.9 and 40.8 mi).[85] A young male may have to live as a transient in another male's territory until he is older and strong enough to challenge the resident male. Young males thus have an annual mortality rate of up to 35%. By contrast, young female tigers die at a rate of only around 5%.[84] Tigers mark their territories by spraying urine on vegetation and rocks, clawing or scent rubbing trees, and marking trails with feces, anal gland secretions and ground scrapings.[56][86][87][88] Scent markings also allow an individual to pick up information on another's identity. A tigress in oestrus will signal her availability by scent marking more frequently and increasing her vocalisations. Unclaimed territories, particularly those that belonged to a decreased individual, can be taken over in days or weeks.[56]\n Male tigers are generally less tolerant of other males within their territories than females are of other females. Territory disputes are usually solved by intimidation rather than outright violence. Once dominance has been established, a male may tolerate a subordinate within his range, as long as they do not live in too close quarters. The most serious disputes tend to occur between two males competing for a female in oestrus.[89] Though tigers mostly live alone, relationships between individuals can be complex. Tigers are particularly social at kills, and a male tiger will share a carcass with the females and cubs within this territory and unlike male lions, will allow them to feed on the kill before he is finished with it. Though the female and male act amicably, females are more tense towards each other at a kill.[90][91]\n During friendly encounters and bonding, tigers rub against each others' bodies.[92] Facial expressions include the \"defense threat\", which involves a wrinkled face, bared teeth, pulled-back ears, and widened pupils.[92][12] Both males and females show a flehmen response, a characteristic grimace, when sniffing urine markings. Males also use the flehman to detect the markings made by tigresses in oestrus.[12] Tigers also use their tails to signal their mood. To show cordiality, the tail sticks up and sways slowly, while an apprehensive tiger lowers its tail or wags it side-to-side. When calm, the tail hangs low.[93]\n Tigers are normally silent but can produce numerous vocalisations.[94][95] They roar to signal their presence to other individuals over long distances. This vocalisation is forced through an open mouth as it closes and can be heard 3 km (1.9 mi) away. A tiger may roar three or four times in a row, and others may respond in kind. Tigers also roar during mating, and a mother will roar to call her cubs to her. When tense, tigers will moan, a sound similar to a roar but softer and made when the mouth is at least partially closed. Moaning can be heard 400 m (1,300 ft) away.[12][96]\n Aggressive encounters involve growling, snarling and hissing.[97] An explosive \"coughing roar\" or \"coughing snarl\" is emitted through an open mouth and exposed teeth.[12][97][98] Chuffing—soft, low-frequency snorting similar to purring in smaller cats—is heard in more friendly situations.[99] Mother tigers communicate with their cubs by grunting, while cubs call back with miaows.[100] A \"woof\" sound is produced when the animal is startled. It has also been recording emitting a deer-like \"pok\" sound for unknown reasons, but most often at kills.[101][102]\n The tiger is a carnivore and an apex predator feeding mainly on ungulates, with a particular preference for sambar deer, Manchurian wapiti, barasingha and wild boar. Tigers kill large prey like gaur,[103] but opportunistically kill much smaller prey like monkeys, peafowl and other ground-based birds, porcupines and fish.[12][56] Tiger attacks on adult Asian elephants and Indian rhinoceros have also been reported.[104][105][106] More often, tigers take the more vulnerable small calves.[107] When in close proximity to humans, tigers sometimes prey on domestic livestock and dogs.[12] Tigers occasionally consume vegetation, fruit and minerals for dietary fibre.[108]\n Tigers learn to hunt from their mothers, which is important but not necessary for their success.[109] They usually hunt alone, but families hunt together when cubs are old enough.[110] A tiger travels up to 19.3 km (12.0 mi) per day in search of prey, using vision and hearing to find a target.[111] It also waits at a watering hole for prey to come by, particularly during hot summer days.[112][113] It is an ambush predator and when approaching potential prey, the tiger crouches, with head lowered, and hides in foliage. It switches between creeping forward and staying still. Tigers have been recorded dozing off while in still mode, and can stay in the same spot for as long as a day waiting for prey and launches an attack, when the prey is close enough,[114] usually within 30 m (98 ft).[50] If the prey spots it before then, the cat does not pursue further.[112] Tigers can sprint 56 km\/h (35 mph) and leap 10 m (33 ft);[115][116] they are not long distance runners and give up a chase if prey outpaces them over a certain distance.[112]\n The tiger attacks from behind or at the sides and tries to knock the target off balance. It latches onto prey with its forelimbs, twisting and turning during the struggle. The tiger generally applies a bite to the throat until its target dies of strangulation.[12][117][118] Holding onto the throat puts the cat out of reach of the horns, antlers, tusks and hooves.[117][119] Tigers are adaptable killers and may use other methods, including ripping the throat or breaking the neck. Large prey may be disabled by a bite to the back of the hock, severing the tendon. Swipes from the large paws are capable of stunning or breaking to skull of a water buffalo.[120] They kill small prey with a bite to the back of the neck or skull.[121][50] Estimates of the success rate for hunting tigers ranges from a low 5% to a high of 50%. They are sometimes killed or injured by large or dangerous prey like gaur, buffalo and gaur.[50]\n The tiger typically drags its kill for 183–549 m (600–1,801 ft) to a hidden, usually vegetated spot before eating. The tiger has the strength to drag the carcass of a fully grown buffalo for some distance, a feat three men struggle with. It rests for a while before eating and can consume as much as 50 kg (110 lb) of meat in one session, but feeds on a carcass for several days, leaving very little for scavengers.[122]\n Tigers may kill and even prey on other predators they coexist with.[123] In much of their range, tigers share habitat with leopards and dholes. They typically dominate both of them, though large packs of dholes can drive away a tiger,[124] or even kill it.[125] Tigers appear to inhabit the deep parts of a forest while these smaller predators are pushed closer to the fringes.[126] The three predators coexist by hunting different prey.[127] In one study, tigers were found to have killed prey that weighed an average of 91.5 kg (202 lb), in contrast to 37.6 kg (83 lb) for the leopard and 43.4 kg (96 lb) for the dhole.[128] Leopards can live successfully in tiger habitat when there is abundant food and vegetation cover, and there is no evidence of competitive exclusion.[127] Nevertheless, leopards avoid areas were tigers roam and are less common where tigers are numerous.[123][129][130]\n Tigers tend to be wary of sloth bears, with their sharp claws, quickness and ability to stand on two legs. Tiger do sometimes prey on sloth bears by ambushing them when they are feeding at termite mounds.[131] Siberian tigers may attack, kill and prey on Ussuri brown and Ussuri black bears.[12] In turn, some studies show that brown bears frequently track down tigers to usurp their kills, with occasional fatal outcomes for the tiger.[132][133][134]\n The tiger mates all year round, but most cubs are born between March and June, with another peak in September.[135] A tigress is in oestrus for three to six days, inbetween three to nine week intervals.[12] A resident male mates with all the females within his territory, who signal their receptiveness by roaring and marking.[136][137] Younger, transient males are also attracted, leading to a fight in which the more dominant male drives the usurper off.[135][136] During courtship, the male is cautious with the female as he waits for her to show signs she is ready to mate. She signals to him by positioning herself in lordosis with their tail to the side. Copulation is generally 20 to 25 seconds long, with the male biting the female by the scruff of her neck. After it is finished, the male quickly pulls away as the female may turn and slap him.[136] Tiger pairs may stay together for up to four days and mate multiple times.[138] Gestation ranges from 93 to 114 days, with an average of 103 to 105 days.[135]\n A tigress gives birth in a secluded location, be it in dense vegetation, in a cave or under a rocky shelter.[139] Litters consist of as many as seven cubs, but two or three are more typical.[135][139] Newborn cubs weigh 785–1,610 g (27.7–56.8 oz), and are blind and altricial.[139] The mother licks and cleans her cubs, suckles them and viscously defends them from any potential threat.[135] She will only leave them alone to hunt, and even then does not travel far.[140] When a mother suspects an area is no longer safe, she moves her cubs to a new spot, transporting them one by one by grabbing them by the scruff of the neck with her mouth. The mortality rate for tiger cubs can reach 50% during these early months, causes of death include predators like dholes, leopards and pythons.[141] Young are able to see in a week, can leave the denning site in two months and around the same time they start eating meat.[135][142]\n After around two months, the cubs are able to follow their mother. They still hide in vegetation when she goes hunting, and she will guide them to the kill. Cubs bond though play fighting and practice stalking. A hierarchy develops in the litter, with the biggest cub, often a male, being the most dominant and the first to eat its fill at a kill.[143] Around the age of six months, cubs are fully weaned and have more freedom to explore their environment. Between eight and ten months, they accompany their mother on hunts.[141] A cub can make a kill as early as 11 months, and reach independence around 18 to 24 months of age, males becoming independent earlier than females.[144] Radio-collared tigers in Chitwan started dispersing from their natal areas earliest at the age of 19 months.[85] Young females are sexual mature at three to four years, whereas males are at four to five years. Tigers may live up to 26 years.[12]\n Tiger fathers play no role in raising the young, but he may encounter and interact with them. The resident male appears to visit the female-cub families within his territory. They may socalise and even share kills with them.[145][146] One male was recorded looking after cubs whose mother had died.[147] By defending his territory, the male is also protecting the females and cubs from other males.[148] When a new male takes over a territory, dependent cubs are at risk of being killed, as the male would want to sire his own young with the females. Older female cubs are tolerated but males may be treated as potential competitors.[149]\n Major threats to the tiger include habitat destruction, habitat fragmentation and poaching for fur and body parts, which have simultaneously greatly reduced tiger populations in the wild.[1]\n During 2001–2020, landscapes where tigers live declined from 1,025,488 km2 (395,943 sq mi) to 911,901 km2 (352,087 sq mi).[64]\nIn the Tanintharyi Region of southern Myanmar, deforestation coupled with mining activities and a high hunting pressure threatens the tiger population in the area.[150] Between March 2017 and January 2020, 630 activities of hunters using snares, drift nets, hunting platforms and hunting dogs were discovered in a reserve forest of about 1,000 km2 (390 sq mi).[151]\nIn Thailand, nine of 15 protected areas hosting tigers are isolated and fragmented offering a low probability for dispersal between them; and four of these do not harbour tigers any more at least since 2013.[152]\nIn Peninsular Malaysia, an area of 8,315.7 km2 (3,210.7 sq mi) tiger habitat was cleared during 1988–2012, most of it for industrial plantations.[153]\nLarge-scale land acquisitions of about 23,000 km2 (8,900 sq mi) for commercial agriculture and timber extraction in Cambodia contributed to the fragmentation of potential tiger habitat, especially in the Eastern Plains.[154]\nIn China, tigers became the target of large-scale 'anti-pest' campaigns in the early 1950s, where suitable habitats were fragmented following deforestation and resettlement of people to rural areas, who hunted tigers and prey species. Though tiger hunting was prohibited in 1977, the population continued to decline and is considered extinct in southern China since 2001.[155][156]\n Nam Et-Phou Louey National Park was considered the last important site for the tiger in Laos, but it has not been recorded since there at least since 2013; this population likely fell victim to indiscriminate snaring.[157]\nTiger populations in India have been targetted by poachers since the 1990s and were extirpated in two tiger reserves in 2005 and 2009.[158]\n During the years 2000–2022, at least 3,377 tigers were confiscated in 2,205 seizures in 28 countries; seizures encompassed 665 live and 654 dead individuals, 1,313 whole tiger skins and 16,214 body parts like bones, teeth, paws, claws, whiskers, and 1.1 t (1.1 long tons; 1.2 short tons) of meat; 759 seizures were reported in India encompassing body parts of 893 tigers, and 403 seizures in Thailand involved mostly captive-bred tigers.[159]\n Demand for tiger parts for use in traditional Chinese medicine has also been cited as a major threat to tiger populations.[160]\nInterviews with local people in the Bangladeshi Sundarbans revealed that they kill tigers for local consumption and trade of skins, bones and meat, in retaliation for attacks by tigers, and for excitement.[161]\nTiger body parts like skins, bones, teeth and hair are consumed locally by wealthy Bangladeshis and are illegally trafficked from Bangladesh to 15 countries including India, China, Malaysia, Korea, Vietnam, Cambodia, Japan and the United Kingdom via land borders, airports and seaports.[162]\nTiger bone glue is the prevailing tiger product purchased for medicinal purposes in Hanoi and Ho Chi Minh City.[163]\n Internationally, the tiger is protected under CITES Appendix I, banning trade of live tigers and their body parts.[1]\nIn India, it has been protected since 1972 under Schedule I of the Wild Life (Protection) Act, 1972.[164] In 1973,  the National Tiger Conservation Authority and Project Tiger were founded to gain public support for tiger conservation.[158] Since then, 53 tiger reserves covering an area of 75,796 km2 (29,265 sq mi) have been established in the country until 2022.[165]\nIn Nepal, it has been protected since 1973 under the National Parks and Wildlife Conservation Act, 1973.[164]\nIn Bhutan, it has been protected since 1969; the first Tiger Action Plan implemented during 2006–2015 revolved around habitat conservation, human–wildlife conflict management, education and awareness; the second Action Plan aimed at increasing the country’s tiger population by 20% until 2023 compared to 2015.[166]\nIn Bangladesh, it has been protected since 1973 under the Wildlife (Preservation) Act and the Wildlife (Conservation and Security) Act, 2012.[162] In 2009, the Bangladesh Tiger Action Plan was initiated to stabilize the country's tiger population, maintain habitat and a sufficient prey base, improve law enforcement and cooperation between governmental agencies responsible for tiger conservation.[167]\nMyanmar’s national tiger conservation strategy developed in 2003 comprises management tasks such as restoration of degraded habitats, increasing the extent of protected areas and wildlife corridors, protecting tiger prey species, thwarting of tiger killing and illegal trade of its body parts, and promoting public awareness through wildlife education programs.[168]\nIn China, the trade in tiger body parts was banned in 1993, which helped to reduce the use of tiger bones in traditional Chinese medicine.[169]\n In the 1990s, a new approach to tiger conservation was developed: Tiger Conservation Units (TCUs), which are blocks of habitat that have the potential to host tiger populations in 15 habitat types within five bioregions. Altogether 143 TCUs were identified and prioritized based on size and integrity of habitat, poaching pressure and population status. They range in size from 33 to 155,829 km2 (13 to 60,166 sq mi).[67]\n In the 1940s, the tiger was on the brink of extinction in Russia. Anti-poaching controls were initialised, and a network of protected areas, so-called zapovedniks, were instituted, leading to a rise in the tiger population.[179] Increases in patrol effort during 2011–2014 in four protected areas contributed to reducing poaching, stabilising the tiger population and improving protection of ungulate populations.[180]\n In 1994, the Indonesian Sumatran Tiger Conservation Strategy addressed the potential crisis that tigers faced in Sumatra. The Sumatran Tiger Project (STP) was initiated in June 1995 in and around the Way Kambas National Park to ensure the long-term viability of wild Sumatran tigers and to accumulate data on tiger life-history characteristics vital for the management of wild populations.[181] By August 1999, the teams of the STP had evaluated 52 sites of potential tiger habitat in Lampung Province, of which only 15 these were intact enough to contain tigers.[182] In the framework of the STP a community-based conservation program was initiated to document the tiger-human dimension in the park to enable conservation authorities to resolve tiger-human conflicts based on a comprehensive database rather than anecdotes and opinions.[183]\n The Wildlife Conservation Society and Panthera Corporation formed the collaboration Tigers Forever, with field sites including the world's largest tiger reserve, the 21,756 km2 (8,400 sq mi) Hukaung Valley in Myanmar. Other reserves were located in India, Thailand, Laos, Cambodia and the Russian Far East covering in total about 260,000 km2 (100,000 sq mi).[184]\n Tigers have been studied in the wild using a variety of techniques. Tiger population have been estimated using plaster casts of their pugmarks, although this method was criticized as being inaccurate.[185] More recent techniques include the use of camera traps and studies of DNA from tiger scat, while radio-collaring has been used to track tigers in the wild.[186] Tiger spray has been found to be just as good, or better, as a source of DNA than scat.[187]\n A tiger hunt is painted on the Bhimbetka rock shelters in India and dated to 5,000–6,000 years ago. Thousands of years later, Emperor Samudragupta was depicted slaying tigers on coins. Tiger hunting became an established sport under the Mughal Empire in the 16th century. The cats were chased on horseback and killed with spears. Emperor Akbar participated in such activities and one of his hunts is the subject of a painting from the Akbarnama. Following Akbar, Emperor Jahangir will introduce firearms to tiger hunts and eventually, elephant would be ridden. The British East India Company would pay for bounties on tigers as early as 1757 and tiger hunting would continue under British Raj.[188] Tiger killings were particularly high in the 19th and early 20th centuries; as an estimated 80,000 cats were killed between 1875 and 1925.[189][190] King George V on his visit to Colonial India in 1911 killed 39 tigers in a matter of 10 days.[191]\n Tigers are said to have directly killed more people than any other wild mammal.[192] In most areas, the big cats typically avoid humans, but attacks are a risk wherever people coexist with them.[193][194] Dangerous encounters are more likely to occur in edge habitats, between wild and agricultural areas.[193] Most attacks on humans are defensive, including protection of young. However, tigers do sometimes see people as prey.[194] They hunt people the same way they hunt other prey, by ambush and with a killing bite to the neck. A tiger inflicted wound also carries the risk of infection.[193] Man-eating tigers tend to be old and disabled.[56] Those they have been driven from their home ranges and territories are also at risk of turning to man-eating.[195]\n At the beginning of the 20th century, the Champawat Tiger was responsible for over 430 human deaths in Nepal and India before she was shot by famed hunter Jim Corbett.[196] Corbett recorded that the tigress suffered from broken teeth and thus unable to kill normal prey. Modern authors speculate that feeding on meagre human flesh forced the cat to kill more and more.[197] Tiger attacks were particularly high in Singapore during the mid-19th century, when plantations expanded into the animal's habitat.[198] The number of deaths ranged from 200 to 300 annually in the 1840s.[199]\n Tiger predation on humans is highest in the Sundarbans. An estimated 129 people were killed between 1969 and 1971. In the 10 years prior to that period, about 100 attacks per year in the Sundarbans.[192] Victims of tigers attacks are local villagers who enter the tiger's domain to collect resources like wood and honey. Fishermen have been particularly common targets. Methods to counter tiger attacks have included face-masks (worn backwards), protective clothes, sticks and carefully stationed electric dummies. These tools have been credited with reducing tiger attacks to only 22 per year in the 1980s.[200]  Because of rapid habitat loss attributed to climate change, tiger attacks have increased in the Sundarbans in the 21st century.[201]\n Tigers have been kept in captivity since ancient times. In ancient Rome, tigers were displayed in amphitheaters; they were slaughtered in hunts and used for public executions of criminals. Mongol Emperor Kublai Khan is reported to have kept tigers in the 13th century. Starting in the Middle Ages, tigers were being kept in European menageries. In 1830, two tigers and a lion were accidentally put in the same exhibit at the Tower of London. This lead to a fight between them and, after they were separated, the lion died of its wounds.[202] Tigers and other exotic animals were mainly used for the entertainment of elites but from the 19th century onward, they were exhibited more to the public. Tigers were particularly big attractions, and their captive population soared.[203]\n Tigers have played prominent roles in circuses and other live performances. Ringling Bros included many tiger tamers in the 20th century including Mabel Stark, who became a big draw and had a long career. She was well known for being able to control the big cats despite being a small woman; using \"manly\" tools like whips and guns. Another trainer was Clyde Beatty, who used chairs, whips and guns to provoke tigers and other beasts into acting fierce and allowed him to appear courageous. He would perform with as many as 40 tigers and lions in one act. From the 1960s onward trainers like Gunther Gebel-Williams would use gentler methods to control their animals. Tiger trainer Sara Houckle was dubbed \"the Tiger Whisperer\", as she trained the cats to obey her by whispering to them.[204] Siegfried & Roy became famous for performing with white tigers in Las Vegas. The act ended in 2003 when a tiger named Mantacore attacked Roy during a performance.[205] The use of tigers and other animals in shows would eventually decline in many countries due to pressure from animal rights groups and greater desires from the public to see them in more natural settings. Several countries would restrict or ban such acts.[206] According to a 2009 analysis, tigers were the most traded circus animals.[207]\n Tigers have become popular in the exotic pet trade, particularly in the United States.[208] The World Wide Fund for Nature (WWF) estimated that in the US, 5,000 tigers were kept in captivity in 2020, with only 6% of them being in zoos and other facilities approved by the Association of Zoos and Aquariums. The WWF argues that private collectors are ill-equipped to provide proper care for tigers, which compromises their welfare. They can also threaten public safety by allowing people to interact with them.[209] The keeping of tigers and other big cats by private individuals was banned in the US in 2022 under the Big Cat Public Safety Act. Those who owned big cats at the time of the signing were expected to register with the United States Fish and Wildlife Service before 18 June 2023.[210] The WWF also estimated in 2020 that 7,000–8,000 tigers were held in \"tiger farm\" facilities in China and Southeast Asia. These tigers are bred to be used for traditional medicine and appear to pose a threat to wild populations by rising demand for tiger parts.[209]\n The tiger is among the most famous of charismatic megafauna. It has been labelled as \"a rare combination of courage, ferocity and brilliant colour\".[135] In a 2004 online poll conducted by cable television channel Animal Planet, involving more than 50,000 viewers from 73 countries, the tiger was voted the world's favourite animal with 21% of the vote, narrowly beating the dog.[211] Likewise, a 2018 study found the tiger to be the most popular wild animal based on surveys, as well as appearances on websites of major zoos and posters of some animated movies.[212]\n While the lion represented royalty and power in Western culture, the tiger filled such a role in Asia. In ancient China, the tiger was seen as the \"king of the forest\" and symbolised the power of the emperor.[213] In Chinese astrology, the tiger is the third out of 12 symbols in the zodiac and controls the period of the day between 3 am and 5 am. The Year of the Tiger is thought to bring \"dramatic and extreme events\". The White Tiger is one of the Four Symbols of the Chinese constellations, representing the west along with the yin and the season of autumn. It is the counterpart to the Azure Dragon, which conversely symbolises the east, yang and springtime.[214] The tiger is one of the animals displayed on the Pashupati seal of the Indus Valley civilisation. The big cat was depicted on seals and coins during the Chola Dynasty of southern India, as it was the official emblem.[215]\n Tigers have had religious significance, even being worshiped. In Buddhism, the tiger, monkey and deer are Three Senseless Creatures, the tiger symbolising anger.[216] In Bhutan, the tiger is venerated as one of the four powerful animals called the \"four dignities\", and a tigress is believed to have carried Padmasambhava from Singye Dzong to the Paro Taktsang monastery in the late 8th century.[166] In Hinduism, the tiger is the vehicle for the goddess of feminine power and peace, Durga, whom the gods created to fight demons. Similarly, in the Greco-Roman world, the tiger was depicted being ridden by the god Dionysus. In Korean mythology, tigers are messengers of the Mountain Gods.[217] The Warli of western India worship the tiger-like god Waghoba. The Warli believe that shrines and sacrifices to the deity will lead to better coexistence with the local big cats, both tigers and leopards, and that Waghoba will protect them when they enter the forests.[218]\n In both Chinese and Korean culture, tigers are seen as protectors against evil spirits, and their image was used to decorate homes and tombs.[213][219] In the folklore of Malaysia and Indonesia, \"tiger shamans\" heal the sick by invoking the big cat. People turning into tigers and the inverse has also been widespread, in particular weretigers are people who could change into tigers and back again. The Mnong people of Indochina believed that tigers could transform into humans.[220] Among some indigenous peoples of Siberia, it was believed that men would seduce women by transforming into tigers.[213]\n The tiger's cultural reputation is generally that of a fierce and powerful animal. William Blake's 1794 poem \"The Tyger\" portrays the animal as the duality of beauty and ferocity. It is the sister poem to \"The Lamb\" in Blake's Songs of Innocence and of Experience and he ponders why God would create such different creatures. The tiger is featured in the medieval Chinese novel Water Margin, where the cat battles and is slain by the bandit Wu Song, while the tiger Shere Khan in Rudyard Kipling's 1894 The Jungle Book is the mortal enemy of the human protagonist Mowgli. The image of the friendly tame tiger has also existed in culture, notably Tigger, the Winnie-the-Pooh character and Tony the Tiger, the Kellogg's cereal mascot.[221]\n"}
{"key":"Tiger","link":"https:\/\/en.wikipedia.org\/wiki\/Tiger","headline":"Tiger - Wikipedia","content":"\n The tiger (Panthera tigris) is the largest living cat species and a member of the genus Panthera. It is most recognisable for its black stripes on orange fur with a white underside. An apex predator, it primarily preys on ungulates, such as deer and wild boar. It is territorial and generally a solitary but social predator, requiring large contiguous areas of habitat to support its requirements for prey and rearing of its offspring. Tiger cubs stay with their mother for about two years and then become independent, leaving their mother's home range to establish their own.\n The tiger was first scientifically described in 1758. It once ranged widely from the Eastern Anatolia Region in the west to the Amur River basin in the east, and in the south from the foothills of the Himalayas to Bali in the Sunda Islands. Since the early 20th century, tiger populations have lost at least 93% of their historic range and have been extirpated from Western and Central Asia, the islands of Java and Bali, and in large areas of Southeast and South Asia and China. What remains of the range where tigers still roam free is fragmented, stretching in spots from Siberian temperate forests to subtropical and tropical forests on the Indian subcontinent, Indochina and a single Indonesian island, Sumatra.\n The tiger is listed as Endangered on the IUCN Red List. India hosts the largest tiger population. Major reasons for population decline are habitat destruction, habitat fragmentation and poaching. Tigers are also victims of human–wildlife conflict, due to encroachment in countries with a high human population density.\n The tiger is among the most recognisable and popular of the world's charismatic megafauna. It featured prominently in the ancient mythology and folklore of cultures throughout its historic range and continues to be depicted in modern films and literature, appearing on many flags, coats of arms and as mascots for sporting teams. The tiger is the national animal of India, Bangladesh, Malaysia and South Korea.\n The Old English tigras derive from Old French tigre, from Latin tigris. This was a borrowing of Classical Greek τίγρις 'tigris'.[4][5] The ultimate origin of the word is uncertain.[6] Ancient Greek geographer Strabo suggested an Armenian origin.[7] One popular idea, believed in the 16th and 17th centuries, is tiger was a transliteration of the Middle Persian tigr, meaning 'arrow', from which the name of the river Tigris may also have been derived. Thus, the animal and the river may have both been associated with speed. The connection between the two words is doubted in modern times, and they are likely to be Latin homonyms.[6]\n In 1758, Carl Linnaeus described the tiger in his work Systema Naturae and gave it the scientific name Felis tigris.[2] In 1929, the British taxonomist Reginald Innes Pocock subordinated the species under the genus Panthera using the scientific name Panthera tigris.[8][9]\n \nFollowing Linnaeus's first descriptions of the species, several tiger zoological specimens were described and proposed as subspecies.[10] The validity of several tiger subspecies was questioned in 1999. Most putative subspecies described in the 19th and 20th centuries were distinguished on the basis of fur length and colouration, striping patterns and body size, hence characteristics that vary widely within populations. Morphologically, tigers from different regions vary little, and gene flow between populations in those regions is considered to have been possible during the Pleistocene. Therefore, it was proposed to recognize only two tiger subspecies as valid, namely P. t. tigris in mainland Asia, and P. t. sondaica in the Greater Sunda Islands. Mainland tigers are described as being larger in size with generally lighter fur and fewer stripes, while island tigers are smaller due to insular dwarfism, with darker coats and more numerous stripes.[11] The stripes of island tigers may break up into spotted patterns.[12]\n This two-subspecies proposal was reaffirmed in 2015 by a comprehensive analysis of morphological, ecological and molecular traits of all putative tiger subspecies using a combined approach. The authors proposed recognition of only two subspecies, namely P. t. tigris comprising the Bengal, Malayan, Indochinese, South Chinese, Siberian and Caspian tiger populations of continental Asia, and P. t. sondaica comprising the Javan, Bali and Sumatran tiger populations of the Sunda Islands. The continental nominate subspecies P. t. tigris constitutes two clades: a northern clade composed of the Siberian and Caspian tiger populations, and a southern clade composed of all other mainland populations. The authors noted that this two-subspecies reclassification will impact tiger conservation management.[13] It would make captive breeding programs and future re-wilding of zoo-born tigers easier, as one tiger population could then be used to reinforce another. However, there is the risk that the loss of subspecies uniqueness could lead to less protection efforts for specific populations.[14]\n In 2017, the Cat Classification Task Force of the IUCN Cat Specialist Group revised felid taxonomy in accordance with the two-subspecies proposal of the comprehensive 2015 study, and recognized the tiger populations in continental Asia as P. t. tigris, and those in the Sunda Islands as P. t. sondaica.[15] This two-subspecies view is still disputed by researchers, since the currently recognized six living subspecies can be distinguished genetically.[14] Results of a 2018 whole-genome sequencing of 32 samples support six monophyletic tiger clades corresponding with the six living proposed subspecies and indicate they descended from a common ancestor around 110,000 years ago.[16] Studies in 2021 and 2023 also affirmed the genetic distinctiveness and separation of these tigers.[17][18]\n The following tables are based on the classification of the species Panthera tigris provided in Mammal Species of the World,[10] and also reflect the classification used by the Cat Classification Task Force in 2017:[15]\n The tiger shares the genus Panthera with the lion, leopard, jaguar and snow leopard. Results of genetic analysis indicate that the tiger and snow leopard are sister species and about 2.88 million years ago, the tiger and the snow leopard lineages diverged from the other Panthera species.[36][40]\n The fossil species Panthera palaeosinensis of early Pleistocene northern China was described as a possible tiger ancestor when it was discovered in 1924, but modern cladistics place it as basal to modern Panthera.[41][39] Panthera zdanskyi, which lived around the same time and place, was suggested to be a sister taxon of the modern tiger when it was examined in 2014.[39] However, as of 2023, at least two recent studies considered P. zdanskyi likely to be a synonym of P. palaeosinensis, noting that its proposed differences from that species fell within the range of individual variation.[42][43] The earliest appearance of the modern tiger species in the fossil record are jaw fragments from Lantion in China that are dated to the early Pleistocene.[39] Middle to late Pleistocene tiger fossils were found throughout China, Sumatra and Java. Prehistoric subspecies include Panthera tigris trinilensis and P. t. soloensis of Java and Sumatra, and P. t. acutidens of China; late Pleistocene and early Holocene fossils of tigers were also found in Borneo and Palawan, Philippines.[44]\n Results of a phylogeographic study indicate that all living tigers had a common ancestor 108,000 to 72,000 years ago.[30] A 2022 paleogenomic study of a Pleistocene tiger basal to living tigers concluded that modern tiger populations spread across Asia no earlier than 94,000 years ago. There is evidence of interbreeding between the lineage of modern mainland tigers and these ancient tigers.[45] The potential tiger range during the late Pleistocene and Holocene was predicted applying ecological niche modelling based on more than 500 tiger locality records combined with bioclimatic data. The resulting model shows a contiguous tiger range at the Last Glacial Maximum, indicating gene flow between tiger populations in mainland Asia. The tiger populations on the Sunda Islands and mainland Asia were possibly separated during interglacial periods.[46]\n The tiger's full genome sequence was published in 2013. It was found to have repeat compositions much as other cat genomes and \"an appreciably conserved synteny\".[47]\n Captive tigers were bred with lions to create hybrids called liger and tigon. The former born to a female tiger and male lion and the latter the result of a male tiger and female lion. They share physical and behavioural qualities of both parent species.[48] Because the lion sire passes on a growth-promoting gene, but the corresponding growth-inhibiting gene from the female tiger is absent, ligers grow far larger than either parent species. By contrast, the male tiger does not pass on a growth-promoting gene and the lioness passes on a growth inhibiting gene, hence tigons are around the same size as either species.[49] Breeding hybrids is now discouraged due to the emphasis on conservation.[48]\n The tiger is considered to be the largest living felid species.[12] However, there is some debate over averages compared to the lion. Since tiger populations vary greatly in size, the \"average\" size for a tiger may be less than a lion, while the biggest tigers are bigger than their lion counterparts.[44] The Siberian and Bengal tigers, along with the extinct Caspian are considered to be the largest of the species.[12] Bengal tigers average a total length of 3 m (9.8 ft), with males weighing 200–260 kg (440–570 lb) and females weighing 100–160 kg (220–350 lb).[50] Island tigers are the smallest, the Sumatran tigers have a total length of 2.2–2.5 m (7 ft 3 in – 8 ft 2 in) with a weight of 100–140 kg (220–310 lb) for males and 75–110 kg (165–243 lb) for females.[50] The extinct Bali tiger was even smaller.[12] It has been hypothesised that body sizes of different tiger populations may be correlated with climate and be explained by thermoregulation and Bergmann's rule.[12][11]\n The tiger has a typical felid morphology. It has a muscular body with strong forelimbs, a large head and a tail that is about half the length of the rest of its body. There are five digits on the front feet and four on the back, all of which have retractable claws which are compact and curved. The ears are rounded, while the eyes have a round pupil.[12] The tiger's skull is large and robust, with a constricted front region, proportionally small, elliptical orbits, long nasal bones, and a lengthened cranium with a large sagittal crest.[51][12] It resembles a lion's skull; with the structure of the lower jaw and length of the nasals being the most reliable indicators for species identification.[51] The tiger has fairly robust teeth and its somewhat curved canines are the longest in the cat family at 6.4–7.6 cm (2.5–3.0 in).[12][52] It has an average bite force at the canine tips of 1234.3 Newton.[53]\n Tiger fur tends to be short, except in the northern-living Siberian tiger. It has a mane-like heavy growth of fur around the neck and jaws and long whiskers, especially in males.[12] Its colouration is generally orange, but can vary from light yellow to dark red.[12][44][54] White fur covers the ventral surface, along with parts of the face.[12][55] It also has a prominent white spot on the back of their ears which are surrounded by black.[12] The tiger is marked with distinctive black or dark brown stripes; the patterns of which are unique in each individual.[12][56] The stripes are mostly vertical, but those on the limbs and forehead are horizonal. They are more concentrated towards the posterior and those on the trunk may or may not reach under the belly. The tips of stripes are generally sharp and some have gaps within them. Tail stripes are thick bands and a black tip marks the end.[57]\n Stripes are likely advantageous for camouflage in vegetation with vertical patterns of light and shade, such as trees and long grass.[56] This is supported by a 1987 Fourier analysis study which concluded that the spatial frequencies of tiger stripes line up with their environment.[58] The tiger is one of only a few striped cat species; it is not known why spotted patterns and rosettes are the more common camouflage pattern among felids.[59] The orange colour may also aid in concealment as the tiger's prey are dichromats, and thus may perceive the cat as green and blended in with the vegetation.[60] The white dots on the ear may play a role in communication.[12]\n Three colour variants – white, golden and nearly stripeless snow white are now virtually non-existent in the wild due to the reduction of wild tiger populations, but continue in captive populations. The white tiger has a white background colour with sepia-brown stripes. The golden tiger is pale golden with reddish-brown stripes. The snow white tiger is a morph with extremely faint stripes and a pale reddish-brown ringed tail. White and golden morphs are the result of an autosomal recessive trait with a white locus and a wideband locus respectively. The snow white variation is caused by polygenes with both the white and wideband loci.[61]\nThe breeding of white tigers is controversial, as they have no use for conservation. Only 0.001% of wild tigers have the genes for this colour morph, and the overrepresentation of white tigers in captivity is the result of inbreeding. Hence their continued breeding will risk both inbreeding depression and loss of genetic variability in captive tigers.[62]\n Pseudo-melanistic tigers with thick, merged stripes have been recorded in Simlipal National Park and three Indian zoos; population genetic analysis of Indian tiger samples revealed that this phenotype is caused by a mutation of a transmembrane aminopeptidase gene. Around 37% of the Simlipal tiger population has this feature, which has been linked to genetic isolation.[63]\n The tiger historically ranged from eastern Turkey and northern Afghanistan to Indochina, and from southeastern Siberia to Sumatra, Java and Bali.[12] As of 2022, it inhabits less than 7% of its historical distribution, and has a scattered range that includes the Indian subcontinent, the Indochinese Peninsula, Sumatra, the Russian Far East and northeastern China.[1]\nAs of 2020, India had the largest extent of global tiger habitat with 300,508 km2 (116,027 sq mi), followed by Russia with 195,819 km2 (75,606 sq mi).[64]\n The tiger mainly lives in forest habitats and is highly adaptable.[50] Records in Central Asia indicate that it occurred foremost in Tugay riverine forests and inhabited hilly and lowland forests in the Caucasus.[65] In the Amur-Ussuri region, it inhabits Korean pine and temperate broadleaf and mixed forests; riparian forests serve as dispersal corridors, providing food and water for both tiger and ungulates.[66] On the Indian subcontinent, it inhabits mainly tropical and subtropical moist broadleaf forests, moist evergreen forests, tropical dry forests, alluvial plains and the mangrove forests of the Sundarbans.[67] In the Eastern Himalayas, tigers were documented in temperate forest up to an elevation of 4,200 m (13,800 ft) in Bhutan and of 3,630 m (11,910 ft) in the Mishmi Hills.[68][69] In Thailand, it lives in deciduous and evergreen forests.[70] In Sumatra, tigers range from lowland peat swamp forests to rugged montane forests.[71]\n Camera trap data show that tigers in Chitwan National Park avoided locations frequented by people and were more active at night than by day.[72]\nIn Sundarbans National Park, six radio-collared tigers were most active in the early morning with a peak around dawn and moved an average distance of 4.6 km (2.9 mi) per day.[73]\nA three-year long camera trap survey in Shuklaphanta National Park revealed that tigers were most active from dusk until midnight.[74]\nIn northeastern China, tigers were crepuscular and active at night with activity peaking at dawn and at dusk; they exhibited a high temporal overlap with ungulate species.[75]\n As with other felid species, tigers groom themselves, maintaining their coats by licking them and spreading oil from their sebaceous glands.[76] It will take to water, particularly on hot days. It is a powerful swimmer and easily transverses across rivers as wide as 8 km (5.0 mi).[56] Adults only occasionally climbs trees, but have been recorded climbing 10 m (33 ft) up a smooth pipal tree.[12] In general, tigers are less capable tree climbers than many other cats due to their size, but cubs under 16 months old may routinely do so.[77]\n Adult tigers lead largely solitary lives. They establish and maintain home ranges, the size of which mainly depends on prey abundance, geographic area and sex of the individual. Males and females defend their home ranges from those of the same sex, and the home range of a male encompasses that of multiple females.[12][56] Two females in the Sundarbans had home ranges of 10.6 and 14.1 km2 (4.1 and 5.4 sq mi).[78] In Panna Tiger Reserve, the home ranges of five reintroduced females varied from 53–67 km2 (20–26 sq mi) in winter to 55–60 km2 (21–23 sq mi) in summer and to 46–94 km2 (18–36 sq mi) during monsoon; three males had 84–147 km2 (32–57 sq mi) large home ranges in winter, 82–98 km2 (32–38 sq mi) in summer and 81–118 km2 (31–46 sq mi) during monsoon seasons.[79] In Huai Kha Khaeng Wildlife Sanctuary, seven resident females had home ranges of 44.1–122.3 km2 (17.0–47.2 sq mi) and four resident males of 174.8–417.5 km2 (67.5–161.2 sq mi).[80]\nFour male problem tigers in Sumatra were translocated to national parks and needed 6–17 weeks to establish new home ranges of 37.5–188.1 km2 (14.5–72.6 sq mi).[81]\nTen solitary females in Sikhote-Alin Biosphere Reserve had home ranges of 413.5 ± 77.6 km2 (159.7 ± 30.0 sq mi); when they had cubs of up to 4 months of age, their home ranges declined to 177.3 ± 53.5 km2 (68.5 ± 20.7 sq mi) and steadily grew to 403.3 ± 105.1 km2 (155.7 ± 40.6 sq mi) until the cubs were 13–18 months old.[82]\n The tiger is a long-ranging species, and individuals disperse over distances of up to 650 km (400 mi) to reach tiger populations in other areas.[83] Young tigresses establish their first territories close to their mother's. Males, however, migrate further than their female counterparts and set out at a younger age to mark out their own area.[84] Four radio-collared females in Chitwan dispersed between 0 and 43.2 km (0.0 and 26.8 mi), and 10 males between 9.5 and 65.7 km (5.9 and 40.8 mi).[85] A young male may have to live as a transient in another male's territory until he is older and strong enough to challenge the resident male. Young males thus have an annual mortality rate of up to 35%. By contrast, young female tigers die at a rate of only around 5%.[84] Tigers mark their territories by spraying urine on vegetation and rocks, clawing or scent rubbing trees, and marking trails with feces, anal gland secretions and ground scrapings.[56][86][87][88] Scent markings also allow an individual to pick up information on another's identity. A tigress in oestrus will signal her availability by scent marking more frequently and increasing her vocalisations. Unclaimed territories, particularly those that belonged to a decreased individual, can be taken over in days or weeks.[56]\n Male tigers are generally less tolerant of other males within their territories than females are of other females. Territory disputes are usually solved by intimidation rather than outright violence. Once dominance has been established, a male may tolerate a subordinate within his range, as long as they do not live in too close quarters. The most serious disputes tend to occur between two males competing for a female in oestrus.[89] Though tigers mostly live alone, relationships between individuals can be complex. Tigers are particularly social at kills, and a male tiger will share a carcass with the females and cubs within this territory and unlike male lions, will allow them to feed on the kill before he is finished with it. Though the female and male act amicably, females are more tense towards each other at a kill.[90][91]\n During friendly encounters and bonding, tigers rub against each others' bodies.[92] Facial expressions include the \"defense threat\", which involves a wrinkled face, bared teeth, pulled-back ears, and widened pupils.[92][12] Both males and females show a flehmen response, a characteristic grimace, when sniffing urine markings. Males also use the flehman to detect the markings made by tigresses in oestrus.[12] Tigers also use their tails to signal their mood. To show cordiality, the tail sticks up and sways slowly, while an apprehensive tiger lowers its tail or wags it side-to-side. When calm, the tail hangs low.[93]\n Tigers are normally silent but can produce numerous vocalisations.[94][95] They roar to signal their presence to other individuals over long distances. This vocalisation is forced through an open mouth as it closes and can be heard 3 km (1.9 mi) away. A tiger may roar three or four times in a row, and others may respond in kind. Tigers also roar during mating, and a mother will roar to call her cubs to her. When tense, tigers will moan, a sound similar to a roar but softer and made when the mouth is at least partially closed. Moaning can be heard 400 m (1,300 ft) away.[12][96]\n Aggressive encounters involve growling, snarling and hissing.[97] An explosive \"coughing roar\" or \"coughing snarl\" is emitted through an open mouth and exposed teeth.[12][97][98] Chuffing—soft, low-frequency snorting similar to purring in smaller cats—is heard in more friendly situations.[99] Mother tigers communicate with their cubs by grunting, while cubs call back with miaows.[100] A \"woof\" sound is produced when the animal is startled. It has also been recording emitting a deer-like \"pok\" sound for unknown reasons, but most often at kills.[101][102]\n The tiger is a carnivore and an apex predator feeding mainly on ungulates, with a particular preference for sambar deer, Manchurian wapiti, barasingha and wild boar. Tigers kill large prey like gaur,[103] but opportunistically kill much smaller prey like monkeys, peafowl and other ground-based birds, porcupines and fish.[12][56] Tiger attacks on adult Asian elephants and Indian rhinoceros have also been reported.[104][105][106] More often, tigers take the more vulnerable small calves.[107] When in close proximity to humans, tigers sometimes prey on domestic livestock and dogs.[12] Tigers occasionally consume vegetation, fruit and minerals for dietary fibre.[108]\n Tigers learn to hunt from their mothers, which is important but not necessary for their success.[109] They usually hunt alone, but families hunt together when cubs are old enough.[110] A tiger travels up to 19.3 km (12.0 mi) per day in search of prey, using vision and hearing to find a target.[111] It also waits at a watering hole for prey to come by, particularly during hot summer days.[112][113] It is an ambush predator and when approaching potential prey, the tiger crouches, with head lowered, and hides in foliage. It switches between creeping forward and staying still. Tigers have been recorded dozing off while in still mode, and can stay in the same spot for as long as a day waiting for prey and launches an attack, when the prey is close enough,[114] usually within 30 m (98 ft).[50] If the prey spots it before then, the cat does not pursue further.[112] Tigers can sprint 56 km\/h (35 mph) and leap 10 m (33 ft);[115][116] they are not long distance runners and give up a chase if prey outpaces them over a certain distance.[112]\n The tiger attacks from behind or at the sides and tries to knock the target off balance. It latches onto prey with its forelimbs, twisting and turning during the struggle. The tiger generally applies a bite to the throat until its target dies of strangulation.[12][117][118] Holding onto the throat puts the cat out of reach of the horns, antlers, tusks and hooves.[117][119] Tigers are adaptable killers and may use other methods, including ripping the throat or breaking the neck. Large prey may be disabled by a bite to the back of the hock, severing the tendon. Swipes from the large paws are capable of stunning or breaking to skull of a water buffalo.[120] They kill small prey with a bite to the back of the neck or skull.[121][50] Estimates of the success rate for hunting tigers ranges from a low 5% to a high of 50%. They are sometimes killed or injured by large or dangerous prey like gaur, buffalo and gaur.[50]\n The tiger typically drags its kill for 183–549 m (600–1,801 ft) to a hidden, usually vegetated spot before eating. The tiger has the strength to drag the carcass of a fully grown buffalo for some distance, a feat three men struggle with. It rests for a while before eating and can consume as much as 50 kg (110 lb) of meat in one session, but feeds on a carcass for several days, leaving very little for scavengers.[122]\n Tigers may kill and even prey on other predators they coexist with.[123] In much of their range, tigers share habitat with leopards and dholes. They typically dominate both of them, though large packs of dholes can drive away a tiger,[124] or even kill it.[125] Tigers appear to inhabit the deep parts of a forest while these smaller predators are pushed closer to the fringes.[126] The three predators coexist by hunting different prey.[127] In one study, tigers were found to have killed prey that weighed an average of 91.5 kg (202 lb), in contrast to 37.6 kg (83 lb) for the leopard and 43.4 kg (96 lb) for the dhole.[128] Leopards can live successfully in tiger habitat when there is abundant food and vegetation cover, and there is no evidence of competitive exclusion.[127] Nevertheless, leopards avoid areas were tigers roam and are less common where tigers are numerous.[123][129][130]\n Tigers tend to be wary of sloth bears, with their sharp claws, quickness and ability to stand on two legs. Tiger do sometimes prey on sloth bears by ambushing them when they are feeding at termite mounds.[131] Siberian tigers may attack, kill and prey on Ussuri brown and Ussuri black bears.[12] In turn, some studies show that brown bears frequently track down tigers to usurp their kills, with occasional fatal outcomes for the tiger.[132][133][134]\n The tiger mates all year round, but most cubs are born between March and June, with another peak in September.[135] A tigress is in oestrus for three to six days, inbetween three to nine week intervals.[12] A resident male mates with all the females within his territory, who signal their receptiveness by roaring and marking.[136][137] Younger, transient males are also attracted, leading to a fight in which the more dominant male drives the usurper off.[135][136] During courtship, the male is cautious with the female as he waits for her to show signs she is ready to mate. She signals to him by positioning herself in lordosis with their tail to the side. Copulation is generally 20 to 25 seconds long, with the male biting the female by the scruff of her neck. After it is finished, the male quickly pulls away as the female may turn and slap him.[136] Tiger pairs may stay together for up to four days and mate multiple times.[138] Gestation ranges from 93 to 114 days, with an average of 103 to 105 days.[135]\n A tigress gives birth in a secluded location, be it in dense vegetation, in a cave or under a rocky shelter.[139] Litters consist of as many as seven cubs, but two or three are more typical.[135][139] Newborn cubs weigh 785–1,610 g (27.7–56.8 oz), and are blind and altricial.[139] The mother licks and cleans her cubs, suckles them and viscously defends them from any potential threat.[135] She will only leave them alone to hunt, and even then does not travel far.[140] When a mother suspects an area is no longer safe, she moves her cubs to a new spot, transporting them one by one by grabbing them by the scruff of the neck with her mouth. The mortality rate for tiger cubs can reach 50% during these early months, causes of death include predators like dholes, leopards and pythons.[141] Young are able to see in a week, can leave the denning site in two months and around the same time they start eating meat.[135][142]\n After around two months, the cubs are able to follow their mother. They still hide in vegetation when she goes hunting, and she will guide them to the kill. Cubs bond though play fighting and practice stalking. A hierarchy develops in the litter, with the biggest cub, often a male, being the most dominant and the first to eat its fill at a kill.[143] Around the age of six months, cubs are fully weaned and have more freedom to explore their environment. Between eight and ten months, they accompany their mother on hunts.[141] A cub can make a kill as early as 11 months, and reach independence around 18 to 24 months of age, males becoming independent earlier than females.[144] Radio-collared tigers in Chitwan started dispersing from their natal areas earliest at the age of 19 months.[85] Young females are sexual mature at three to four years, whereas males are at four to five years. Tigers may live up to 26 years.[12]\n Tiger fathers play no role in raising the young, but he may encounter and interact with them. The resident male appears to visit the female-cub families within his territory. They may socalise and even share kills with them.[145][146] One male was recorded looking after cubs whose mother had died.[147] By defending his territory, the male is also protecting the females and cubs from other males.[148] When a new male takes over a territory, dependent cubs are at risk of being killed, as the male would want to sire his own young with the females. Older female cubs are tolerated but males may be treated as potential competitors.[149]\n Major threats to the tiger include habitat destruction, habitat fragmentation and poaching for fur and body parts, which have simultaneously greatly reduced tiger populations in the wild.[1]\n During 2001–2020, landscapes where tigers live declined from 1,025,488 km2 (395,943 sq mi) to 911,901 km2 (352,087 sq mi).[64]\nIn the Tanintharyi Region of southern Myanmar, deforestation coupled with mining activities and a high hunting pressure threatens the tiger population in the area.[150] Between March 2017 and January 2020, 630 activities of hunters using snares, drift nets, hunting platforms and hunting dogs were discovered in a reserve forest of about 1,000 km2 (390 sq mi).[151]\nIn Thailand, nine of 15 protected areas hosting tigers are isolated and fragmented offering a low probability for dispersal between them; and four of these do not harbour tigers any more at least since 2013.[152]\nIn Peninsular Malaysia, an area of 8,315.7 km2 (3,210.7 sq mi) tiger habitat was cleared during 1988–2012, most of it for industrial plantations.[153]\nLarge-scale land acquisitions of about 23,000 km2 (8,900 sq mi) for commercial agriculture and timber extraction in Cambodia contributed to the fragmentation of potential tiger habitat, especially in the Eastern Plains.[154]\nIn China, tigers became the target of large-scale 'anti-pest' campaigns in the early 1950s, where suitable habitats were fragmented following deforestation and resettlement of people to rural areas, who hunted tigers and prey species. Though tiger hunting was prohibited in 1977, the population continued to decline and is considered extinct in southern China since 2001.[155][156]\n Nam Et-Phou Louey National Park was considered the last important site for the tiger in Laos, but it has not been recorded since there at least since 2013; this population likely fell victim to indiscriminate snaring.[157]\nTiger populations in India have been targetted by poachers since the 1990s and were extirpated in two tiger reserves in 2005 and 2009.[158]\n During the years 2000–2022, at least 3,377 tigers were confiscated in 2,205 seizures in 28 countries; seizures encompassed 665 live and 654 dead individuals, 1,313 whole tiger skins and 16,214 body parts like bones, teeth, paws, claws, whiskers, and 1.1 t (1.1 long tons; 1.2 short tons) of meat; 759 seizures were reported in India encompassing body parts of 893 tigers, and 403 seizures in Thailand involved mostly captive-bred tigers.[159]\n Demand for tiger parts for use in traditional Chinese medicine has also been cited as a major threat to tiger populations.[160]\nInterviews with local people in the Bangladeshi Sundarbans revealed that they kill tigers for local consumption and trade of skins, bones and meat, in retaliation for attacks by tigers, and for excitement.[161]\nTiger body parts like skins, bones, teeth and hair are consumed locally by wealthy Bangladeshis and are illegally trafficked from Bangladesh to 15 countries including India, China, Malaysia, Korea, Vietnam, Cambodia, Japan and the United Kingdom via land borders, airports and seaports.[162]\nTiger bone glue is the prevailing tiger product purchased for medicinal purposes in Hanoi and Ho Chi Minh City.[163]\n Internationally, the tiger is protected under CITES Appendix I, banning trade of live tigers and their body parts.[1]\nIn India, it has been protected since 1972 under Schedule I of the Wild Life (Protection) Act, 1972.[164] In 1973,  the National Tiger Conservation Authority and Project Tiger were founded to gain public support for tiger conservation.[158] Since then, 53 tiger reserves covering an area of 75,796 km2 (29,265 sq mi) have been established in the country until 2022.[165]\nIn Nepal, it has been protected since 1973 under the National Parks and Wildlife Conservation Act, 1973.[164]\nIn Bhutan, it has been protected since 1969; the first Tiger Action Plan implemented during 2006–2015 revolved around habitat conservation, human–wildlife conflict management, education and awareness; the second Action Plan aimed at increasing the country’s tiger population by 20% until 2023 compared to 2015.[166]\nIn Bangladesh, it has been protected since 1973 under the Wildlife (Preservation) Act and the Wildlife (Conservation and Security) Act, 2012.[162] In 2009, the Bangladesh Tiger Action Plan was initiated to stabilize the country's tiger population, maintain habitat and a sufficient prey base, improve law enforcement and cooperation between governmental agencies responsible for tiger conservation.[167]\nMyanmar’s national tiger conservation strategy developed in 2003 comprises management tasks such as restoration of degraded habitats, increasing the extent of protected areas and wildlife corridors, protecting tiger prey species, thwarting of tiger killing and illegal trade of its body parts, and promoting public awareness through wildlife education programs.[168]\nIn China, the trade in tiger body parts was banned in 1993, which helped to reduce the use of tiger bones in traditional Chinese medicine.[169]\n In the 1990s, a new approach to tiger conservation was developed: Tiger Conservation Units (TCUs), which are blocks of habitat that have the potential to host tiger populations in 15 habitat types within five bioregions. Altogether 143 TCUs were identified and prioritized based on size and integrity of habitat, poaching pressure and population status. They range in size from 33 to 155,829 km2 (13 to 60,166 sq mi).[67]\n In the 1940s, the tiger was on the brink of extinction in Russia. Anti-poaching controls were initialised, and a network of protected areas, so-called zapovedniks, were instituted, leading to a rise in the tiger population.[179] Increases in patrol effort during 2011–2014 in four protected areas contributed to reducing poaching, stabilising the tiger population and improving protection of ungulate populations.[180]\n In 1994, the Indonesian Sumatran Tiger Conservation Strategy addressed the potential crisis that tigers faced in Sumatra. The Sumatran Tiger Project (STP) was initiated in June 1995 in and around the Way Kambas National Park to ensure the long-term viability of wild Sumatran tigers and to accumulate data on tiger life-history characteristics vital for the management of wild populations.[181] By August 1999, the teams of the STP had evaluated 52 sites of potential tiger habitat in Lampung Province, of which only 15 these were intact enough to contain tigers.[182] In the framework of the STP a community-based conservation program was initiated to document the tiger-human dimension in the park to enable conservation authorities to resolve tiger-human conflicts based on a comprehensive database rather than anecdotes and opinions.[183]\n The Wildlife Conservation Society and Panthera Corporation formed the collaboration Tigers Forever, with field sites including the world's largest tiger reserve, the 21,756 km2 (8,400 sq mi) Hukaung Valley in Myanmar. Other reserves were located in India, Thailand, Laos, Cambodia and the Russian Far East covering in total about 260,000 km2 (100,000 sq mi).[184]\n Tigers have been studied in the wild using a variety of techniques. Tiger population have been estimated using plaster casts of their pugmarks, although this method was criticized as being inaccurate.[185] More recent techniques include the use of camera traps and studies of DNA from tiger scat, while radio-collaring has been used to track tigers in the wild.[186] Tiger spray has been found to be just as good, or better, as a source of DNA than scat.[187]\n A tiger hunt is painted on the Bhimbetka rock shelters in India and dated to 5,000–6,000 years ago. Thousands of years later, Emperor Samudragupta was depicted slaying tigers on coins. Tiger hunting became an established sport under the Mughal Empire in the 16th century. The cats were chased on horseback and killed with spears. Emperor Akbar participated in such activities and one of his hunts is the subject of a painting from the Akbarnama. Following Akbar, Emperor Jahangir will introduce firearms to tiger hunts and eventually, elephant would be ridden. The British East India Company would pay for bounties on tigers as early as 1757 and tiger hunting would continue under British Raj.[188] Tiger killings were particularly high in the 19th and early 20th centuries; as an estimated 80,000 cats were killed between 1875 and 1925.[189][190] King George V on his visit to Colonial India in 1911 killed 39 tigers in a matter of 10 days.[191]\n Tigers are said to have directly killed more people than any other wild mammal.[192] In most areas, the big cats typically avoid humans, but attacks are a risk wherever people coexist with them.[193][194] Dangerous encounters are more likely to occur in edge habitats, between wild and agricultural areas.[193] Most attacks on humans are defensive, including protection of young. However, tigers do sometimes see people as prey.[194] They hunt people the same way they hunt other prey, by ambush and with a killing bite to the neck. A tiger inflicted wound also carries the risk of infection.[193] Man-eating tigers tend to be old and disabled.[56] Those they have been driven from their home ranges and territories are also at risk of turning to man-eating.[195]\n At the beginning of the 20th century, the Champawat Tiger was responsible for over 430 human deaths in Nepal and India before she was shot by famed hunter Jim Corbett.[196] Corbett recorded that the tigress suffered from broken teeth and thus unable to kill normal prey. Modern authors speculate that feeding on meagre human flesh forced the cat to kill more and more.[197] Tiger attacks were particularly high in Singapore during the mid-19th century, when plantations expanded into the animal's habitat.[198] The number of deaths ranged from 200 to 300 annually in the 1840s.[199]\n Tiger predation on humans is highest in the Sundarbans. An estimated 129 people were killed between 1969 and 1971. In the 10 years prior to that period, about 100 attacks per year in the Sundarbans.[192] Victims of tigers attacks are local villagers who enter the tiger's domain to collect resources like wood and honey. Fishermen have been particularly common targets. Methods to counter tiger attacks have included face-masks (worn backwards), protective clothes, sticks and carefully stationed electric dummies. These tools have been credited with reducing tiger attacks to only 22 per year in the 1980s.[200]  Because of rapid habitat loss attributed to climate change, tiger attacks have increased in the Sundarbans in the 21st century.[201]\n Tigers have been kept in captivity since ancient times. In ancient Rome, tigers were displayed in amphitheaters; they were slaughtered in hunts and used for public executions of criminals. Mongol Emperor Kublai Khan is reported to have kept tigers in the 13th century. Starting in the Middle Ages, tigers were being kept in European menageries. In 1830, two tigers and a lion were accidentally put in the same exhibit at the Tower of London. This lead to a fight between them and, after they were separated, the lion died of its wounds.[202] Tigers and other exotic animals were mainly used for the entertainment of elites but from the 19th century onward, they were exhibited more to the public. Tigers were particularly big attractions, and their captive population soared.[203]\n Tigers have played prominent roles in circuses and other live performances. Ringling Bros included many tiger tamers in the 20th century including Mabel Stark, who became a big draw and had a long career. She was well known for being able to control the big cats despite being a small woman; using \"manly\" tools like whips and guns. Another trainer was Clyde Beatty, who used chairs, whips and guns to provoke tigers and other beasts into acting fierce and allowed him to appear courageous. He would perform with as many as 40 tigers and lions in one act. From the 1960s onward trainers like Gunther Gebel-Williams would use gentler methods to control their animals. Tiger trainer Sara Houckle was dubbed \"the Tiger Whisperer\", as she trained the cats to obey her by whispering to them.[204] Siegfried & Roy became famous for performing with white tigers in Las Vegas. The act ended in 2003 when a tiger named Mantacore attacked Roy during a performance.[205] The use of tigers and other animals in shows would eventually decline in many countries due to pressure from animal rights groups and greater desires from the public to see them in more natural settings. Several countries would restrict or ban such acts.[206] According to a 2009 analysis, tigers were the most traded circus animals.[207]\n Tigers have become popular in the exotic pet trade, particularly in the United States.[208] The World Wide Fund for Nature (WWF) estimated that in the US, 5,000 tigers were kept in captivity in 2020, with only 6% of them being in zoos and other facilities approved by the Association of Zoos and Aquariums. The WWF argues that private collectors are ill-equipped to provide proper care for tigers, which compromises their welfare. They can also threaten public safety by allowing people to interact with them.[209] The keeping of tigers and other big cats by private individuals was banned in the US in 2022 under the Big Cat Public Safety Act. Those who owned big cats at the time of the signing were expected to register with the United States Fish and Wildlife Service before 18 June 2023.[210] The WWF also estimated in 2020 that 7,000–8,000 tigers were held in \"tiger farm\" facilities in China and Southeast Asia. These tigers are bred to be used for traditional medicine and appear to pose a threat to wild populations by rising demand for tiger parts.[209]\n The tiger is among the most famous of charismatic megafauna. It has been labelled as \"a rare combination of courage, ferocity and brilliant colour\".[135] In a 2004 online poll conducted by cable television channel Animal Planet, involving more than 50,000 viewers from 73 countries, the tiger was voted the world's favourite animal with 21% of the vote, narrowly beating the dog.[211] Likewise, a 2018 study found the tiger to be the most popular wild animal based on surveys, as well as appearances on websites of major zoos and posters of some animated movies.[212]\n While the lion represented royalty and power in Western culture, the tiger filled such a role in Asia. In ancient China, the tiger was seen as the \"king of the forest\" and symbolised the power of the emperor.[213] In Chinese astrology, the tiger is the third out of 12 symbols in the zodiac and controls the period of the day between 3 am and 5 am. The Year of the Tiger is thought to bring \"dramatic and extreme events\". The White Tiger is one of the Four Symbols of the Chinese constellations, representing the west along with the yin and the season of autumn. It is the counterpart to the Azure Dragon, which conversely symbolises the east, yang and springtime.[214] The tiger is one of the animals displayed on the Pashupati seal of the Indus Valley civilisation. The big cat was depicted on seals and coins during the Chola Dynasty of southern India, as it was the official emblem.[215]\n Tigers have had religious significance, even being worshiped. In Buddhism, the tiger, monkey and deer are Three Senseless Creatures, the tiger symbolising anger.[216] In Bhutan, the tiger is venerated as one of the four powerful animals called the \"four dignities\", and a tigress is believed to have carried Padmasambhava from Singye Dzong to the Paro Taktsang monastery in the late 8th century.[166] In Hinduism, the tiger is the vehicle for the goddess of feminine power and peace, Durga, whom the gods created to fight demons. Similarly, in the Greco-Roman world, the tiger was depicted being ridden by the god Dionysus. In Korean mythology, tigers are messengers of the Mountain Gods.[217] The Warli of western India worship the tiger-like god Waghoba. The Warli believe that shrines and sacrifices to the deity will lead to better coexistence with the local big cats, both tigers and leopards, and that Waghoba will protect them when they enter the forests.[218]\n In both Chinese and Korean culture, tigers are seen as protectors against evil spirits, and their image was used to decorate homes and tombs.[213][219] In the folklore of Malaysia and Indonesia, \"tiger shamans\" heal the sick by invoking the big cat. People turning into tigers and the inverse has also been widespread, in particular weretigers are people who could change into tigers and back again. The Mnong people of Indochina believed that tigers could transform into humans.[220] Among some indigenous peoples of Siberia, it was believed that men would seduce women by transforming into tigers.[213]\n The tiger's cultural reputation is generally that of a fierce and powerful animal. William Blake's 1794 poem \"The Tyger\" portrays the animal as the duality of beauty and ferocity. It is the sister poem to \"The Lamb\" in Blake's Songs of Innocence and of Experience and he ponders why God would create such different creatures. The tiger is featured in the medieval Chinese novel Water Margin, where the cat battles and is slain by the bandit Wu Song, while the tiger Shere Khan in Rudyard Kipling's 1894 The Jungle Book is the mortal enemy of the human protagonist Mowgli. The image of the friendly tame tiger has also existed in culture, notably Tigger, the Winnie-the-Pooh character and Tony the Tiger, the Kellogg's cereal mascot.[221]\n"}
{"key":"Tiger","link":"https:\/\/en.wikipedia.org\/wiki\/Ordovician","headline":"Ordovician - Wikipedia","content":"The Ordovician (\/ɔːrdəˈvɪʃi.ən, -doʊ-, -ˈvɪʃən\/ or-də-VISH-ee-ən, -⁠doh-, -⁠VISH-ən)[9] is a geologic period and system, the second of six periods of the Paleozoic Era. The Ordovician spans 41.6 million years from the end of the Cambrian Period 485.4 Ma (million years ago) to the start of the Silurian Period 443.8 Ma.[10]\n The Ordovician, named after the Welsh tribe of the Ordovices, was defined by Charles Lapworth in 1879 to resolve a dispute between followers of Adam Sedgwick and Roderick Murchison, who were placing the same rock beds in North Wales in the Cambrian and Silurian systems, respectively.[11] Lapworth recognized that the fossil fauna in the disputed strata were different from those of either the Cambrian or the Silurian systems, and placed them in a system of their own. The Ordovician received international approval in 1960 (forty years after Lapworth's death), when it was adopted as an official period of the Paleozoic Era by the International Geological Congress.\n Life continued to flourish during the Ordovician as it did in the earlier Cambrian Period, although the end of the period was marked by the Ordovician–Silurian extinction events. Invertebrates, namely molluscs and arthropods, dominated the oceans, with members of the latter group probably starting their establishment on land during this time, becoming fully established by the Devonian. The first land plants are known from this period. The Great Ordovician Biodiversification Event considerably increased the diversity of life. Fish, the world's first true vertebrates, continued to evolve, and those with jaws may have first appeared late in the period. About 100 times as many meteorites struck the Earth per year during the Ordovician compared with today.[12]\n \nA number of regional terms have been used to subdivide the Ordovician Period. In 2008, the ICS erected a formal international system of subdivisions.[13] There exist Baltoscandic, British, Siberian, North American, Australian, Chinese, Mediterranean and North-Gondwanan regional stratigraphic schemes.[14]\n The Ordovician Period in Britain was traditionally broken into Early (Tremadocian and Arenig), Middle (Llanvirn (subdivided into Abereiddian and Llandeilian) and Llandeilo) and Late (Caradoc and Ashgill) epochs. The corresponding rocks of the Ordovician System are referred to as coming from the Lower, Middle, or Upper part of the column.\n The Tremadoc corresponds to the (modern) Tremadocian. The Floian corresponds to the early Arenig; the Arenig continues until the early Darriwilian, subsuming the Dapingian. The Llanvirn occupies the rest of the Darriwilian, and terminates with it at the start of the Late Ordovician.\nThe Sandbian represents the first half of the Caradoc; the Caradoc ends in the mid-Katian, and the Ashgill represents the last half of the Katian, plus the Hirnantian.\n The British ages (subdivisions of epochs) from youngest to oldest are:\n \"Late Ordovician\"\n \"Middle Ordovician\"\n \"Early Ordovician\"\n The Tremadoc corresponds to the (modern) Tremadocian. The Floian corresponds to the lower Arenig; the Arenig continues until the early Darriwilian, subsuming the Dapingian. The Llanvirn occupies the rest of the Darriwilian, and terminates with it at the base of the Late Ordovician.\nThe Sandbian represents the first half of the Caradoc; the Caradoc ends in the mid-Katian, and the Ashgill represents the last half of the Katian, plus the Hirnantian.[15]\n During the Ordovician, the southern continents were assembled into Gondwana, which reached from north of the equator to the South Pole. The Panthalassic Ocean, centered in the northern hemisphere, covered over half the globe.[17] At the start of the period, the continents of Laurentia (in present-day North America), Siberia, and Baltica (present-day northern Europe)  were separated from Gondwana by over 5,000 kilometres (3,100 mi) of ocean. These smaller continents were also sufficiently widely separated from each other to develop distinct communities of benthic organisms.[18] The small continent of Avalonia had just rifted from Gondwana and began to move north towards Baltica and Laurentia, opening the Rheic Ocean between Gondwana and Avalonia.[19][20][21] Avalonia collided with Baltica towards the end of Ordovician.[22][23]\n Other geographic features of the Ordovician world included the Tornquist Sea, which separated Avalonia from Baltica;[18] the Aegir Ocean, which separated Baltica from Siberia;[24] and an oceanic area between Siberia, Baltica, and Gondwana which expanded to become the Paleoasian Ocean in Carboniferous time. The Mongol-Okhotsk Ocean formed a deep embayment between Siberia and the Central Mongolian terranes. Most of the terranes of central Asia were part of an equatorial archipelago whose geometry is poorly constrained by the available evidence.[25]\n The period was one of extensive, widespread tectonism and volcanism. However, orogenesis (mountain-building) was not primarily due to continent-continent collisions. Instead, mountains arose along active continental margins during accretion of arc terranes or ribbon microcontinents. Accretion of new crust was limited to the Iapetus margin of Laurentia; elsewhere, the pattern was of rifting in back-arc basins followed by remerger. This reflected episodic switching from extension to compression. The initiation of new subduction reflected a global reorganization of tectonic plates centered on the amalgamation of Gondwana.[26][18]\n The Taconic orogeny, a major mountain-building episode, was well under way in Cambrian times.[27] This continued into the Ordovician, when at least two volcanic island arcs collided with Laurentia to form the Appalachian Mountains. Laurentia was otherwise tectonically stable. An island arc accreted to South China during the period, while subduction along north China (Sulinheer) resulted in the emplacement of ophiolites.[28]\n The ash fall of the Millburg\/Big Bentonite bed, at about 454 Ma, was the largest in the last 590 million years. This had a dense rock equivalent volume of as much as 1,140 cubic kilometres (270 cu mi). Remarkably, this appears to have had little impact on life.[29]\n There was vigorous tectonic activity along northwest margin of Gondwana during the Floian, 478 Ma, recorded in the Central Iberian Zone of Spain. The activity reached as far as Turkey by the end of Ordovician. The opposite margin of Gondwana, in Australia, faced a set of island arcs.[18] The accretion of these arcs to the eastern margin of Gondwana was responsible for the Benambran Orogeny of eastern Australia.[30][31] Subduction also took place along what is now Argentina (Famatinian Orogeny) at 450 Ma.[32] This involved significant back arc rifting.[18] The interior of Gondwana was tectonically quiet until the Triassic.[18]\n Towards the end of the period, Gondwana began to drift across the South Pole. This contributed to the Hibernian glaciation and the associated extinction event.[33]\n The Ordovician meteor event is a proposed shower of meteors that occurred during the Middle Ordovician Epoch, about 467.5 ± 0.28 million years ago, due to the break-up of the L chondrite parent body.[34] It is not associated with any major extinction event.[35][36][37]\n The Ordovician was a time of calcite sea geochemistry in which low-magnesium calcite was the primary inorganic marine precipitate of calcium carbonate.[38] Carbonate hardgrounds were thus very common, along with calcitic ooids, calcitic cements, and invertebrate faunas with dominantly calcitic skeletons. Biogenic aragonite, like that composing the shells of most molluscs, dissolved rapidly on the sea floor after death.[39][40]\n Unlike Cambrian times, when calcite production was dominated by microbial and non-biological processes, animals (and macroalgae) became a dominant source of calcareous material in Ordovician deposits.[41]\n The Early Ordovician climate was very hot,[42] with intense greenhouse conditions and sea surface temperatures comparable to those during the Early Eocene Climatic Optimum.[43] Carbon dioxide levels were very high at the Ordovician period's beginning.[44] By the late Early Ordovician, the Earth cooled,[45] giving way to a more temperate climate in the Middle Ordovician,[46] with the Earth likely entering the Early Palaeozoic Ice Age during the Sandbian,[47][48] and possibly as early as the Darriwilian[49] or even the Floian.[45] Evidence suggests that global temperatures rose briefly in the early Katian (Boda Event), depositing bioherms and radiating fauna across Europe.[50] Further cooling during the Hirnantian, at the end of the Ordovician, led to the Late Ordovician glaciation.[51]\n The Ordovician saw the highest sea levels of the Paleozoic, and the low relief of the continents led to many shelf deposits being formed under hundreds of metres of water.[41] The sea level rose more or less continuously throughout the Early Ordovician, leveling off somewhat during the middle of the period.[41] Locally, some regressions occurred, but the sea level rise continued in the beginning of the Late Ordovician. Sea levels fell steadily due to the cooling temperatures for about 3 million years leading up to the Hirnantian glaciation. During this icy stage, sea level seems to have risen and dropped somewhat. Despite much study, the details remain unresolved.[41] In particular, some researches interpret the fluctuations in sea level as pre-Hibernian glaciation,[52] but sedimentary evidence of glaciation is lacking until the end of the period.[23] There is evidence of glaciers during the Hirnantian on the land we now know as Africa and South America, which were near the South Pole at the time, facilitating the formation of the ice caps of the Hirnantian glaciation.\n As with North America and Europe, Gondwana was largely covered with shallow seas during the Ordovician. Shallow clear waters over continental shelves encouraged the growth of organisms that deposit calcium carbonates in their shells and hard parts. The Panthalassic Ocean covered much of the Northern Hemisphere, and other minor oceans included Proto-Tethys, Paleo-Tethys, Khanty Ocean, which was closed off by the Late Ordovician, Iapetus Ocean, and the new Rheic Ocean.\n For most of the Late Ordovician life continued to flourish, but at and near the end of the period there were mass-extinction events that seriously affected conodonts and planktonic forms like graptolites. The trilobites Agnostida and Ptychopariida completely died out, and the Asaphida were much reduced. Brachiopods, bryozoans and echinoderms were also heavily affected, and the endocerid cephalopods died out completely, except for possible rare Silurian forms. The Ordovician–Silurian extinction events may have been caused by an ice age that occurred at the end of the Ordovician Period, due to the expansion of the first terrestrial plants,[53] as the end of the Late Ordovician was one of the coldest times in the last 600 million years of Earth's history.\n \nOn the whole, the fauna that emerged in the Ordovician were the template for the remainder of the Palaeozoic. The fauna was dominated by tiered communities of suspension feeders, mainly with short food chains. The ecological system reached a new grade of complexity far beyond that of the Cambrian fauna, which has persisted until the present day.[41] Though less famous than the Cambrian explosion, the Ordovician radiation (also known as the Great Ordovician Biodiversification Event)[18] was no less remarkable; marine faunal genera increased fourfold, resulting in 12% of all known Phanerozoic marine fauna.[54] Several animals also went through a miniaturization process, becoming much smaller than their Cambrian counterparts.[citation needed] Another change in the fauna was the strong increase in filter-feeding organisms.[55] The trilobite, inarticulate brachiopod, archaeocyathid, and eocrinoid faunas of the Cambrian were succeeded by those that dominated the rest of the Paleozoic, such as articulate brachiopods, cephalopods, and crinoids. Articulate brachiopods, in particular, largely replaced trilobites in shelf communities. Their success epitomizes the greatly increased diversity of carbonate shell-secreting organisms in the Ordovician compared to the Cambrian.[56] Ordovician geography had its effect on the diversity of fauna; Ordovician invertebrates displayed a very high degree of provincialism.[57] The widely separated continents of Laurentia and Baltica, then positioned close to the tropics and boasting many shallow seas rich in life, developed distinct trilobite faunas from the trilobite fauna of Gondwana,[58] and Gondwana developed distinct fauna in its tropical and temperature zones.[59] The Tien Shan terrane maintained a biogeographic affinity with Gondwana,[60] and the Alborz margin of Gondwana was linked biogeographically to South China.[61] Southeast Asia's fauna also maintained strong affinities to Gondwana's.[62] North China was biogeographically connected to Laurentia and the Argentinian margin of Gondwana.[63] A Celtic biogeographic province also existed, separate from the Laurentian and Baltican ones.[64] However, tropical articulate brachiopods had a more cosmopolitan distribution, with less diversity on different continents. During the Middle Ordovician, beta diversity began a significant decline as marine taxa began to disperse widely across space.[65] Faunas become less provincial later in the Ordovician, partly due to the narrowing of the Iapetus Ocean,[66] though they were still distinguishable into the late Ordovician.[67]\n Trilobites in particular were rich and diverse. Trilobites in the Ordovician were very different from their predecessors in the Cambrian. Many trilobites developed bizarre spines and nodules to defend against predators such as primitive eurypterids and nautiloids while other trilobites such as Aeglina prisca evolved to become swimming forms. Some trilobites even developed shovel-like snouts for ploughing through muddy sea bottoms. Another unusual clade of trilobites known as the trinucleids developed a broad pitted margin around their head shields.[68] Some trilobites such as Asaphus kowalewski evolved long eyestalks to assist in detecting predators whereas other trilobite eyes in contrast disappeared completely.[69] Molecular clock analyses suggest that early arachnids started living on land by the end of the Ordovician.[70] Although solitary corals date back to at least the Cambrian, reef-forming corals appeared in the early Ordovician, including the earliest known octocorals,[71][72] corresponding to an increase in the stability of carbonate and thus a new abundance of calcifying animals.[41] Brachiopods surged in diversity, adapting to almost every type of marine environment.[73][74][75] Even after GOBE, there is evidence suggesting that Ordovician brachiopods maintained elevated rates of speciation.[76] Molluscs, which appeared during the Cambrian or even the Ediacaran, became common and varied, especially bivalves, gastropods, and nautiloid cephalopods.[77][78] Cephalopods diversified from shallow marine tropical environments to dominate almost all marine environments.[79] Graptolites, which evolved in the preceding Cambrian period, thrived in the oceans.[80] This includes the distinctive Nemagraptus gracilis graptolite fauna, which was distributed widely during peak sea levels in the Sandbian.[81][23] Some new cystoids and crinoids appeared. It was long thought that the first true vertebrates (fish — Ostracoderms) appeared in the Ordovician, but recent discoveries in China reveal that they probably originated in the Early Cambrian.[82] The first gnathostome (jawed fish) may have appeared in the Late Ordovician epoch.[83] Chitinozoans, which first appeared late in the Wuliuan, exploded in diversity during the Tremadocian, quickly becoming globally widespread.[84][85] Several groups of endobiotic symbionts appeared in the Ordovician.[86][87]\n In the Early Ordovician, trilobites were joined by many new types of organisms, including tabulate corals, strophomenid, rhynchonellid, and many new orthid brachiopods, bryozoans, planktonic graptolites and conodonts, and many types of molluscs and echinoderms, including the ophiuroids (\"brittle stars\") and the first sea stars. Nevertheless, the arthropods remained abundant; all the Late Cambrian orders continued, and were joined by the new group Phacopida. The first evidence of land plants also appeared (see evolutionary history of life).\n In the Middle Ordovician, the trilobite-dominated Early Ordovician communities were replaced by generally more mixed ecosystems, in which brachiopods, bryozoans, molluscs, cornulitids, tentaculitids and echinoderms all flourished, tabulate corals diversified and the first rugose corals appeared. The planktonic graptolites remained diverse, with the Diplograptina making their appearance. One of the earliest known armoured agnathan (\"ostracoderm\") vertebrates, Arandaspis, dates from the Middle Ordovician.[88] During the Middle Ordovician there was a large increase in the intensity and diversity of bioeroding organisms. This is known as the Ordovician Bioerosion Revolution.[89] It is marked by a sudden abundance of hard substrate trace fossils such as Trypanites, Palaeosabella, Petroxestes and Osprioneides. Bioerosion became an important process, particularly in the thick calcitic skeletons of corals, bryozoans and brachiopods, and on the extensive carbonate hardgrounds that appear in abundance at this time. \n Green algae were common in the Late Cambrian (perhaps earlier) and in the Ordovician. Terrestrial plants probably evolved from green algae, first appearing as tiny non-vascular forms resembling liverworts, in the middle to late Ordovician.[91] Fossil spores found in Ordovician sedimentary rock are typical of bryophytes.[92]\n Among the first land fungi may have been arbuscular mycorrhiza fungi (Glomerales), playing a crucial role in facilitating the colonization of land by plants through mycorrhizal symbiosis, which makes mineral nutrients available to plant cells; such fossilized fungal hyphae and spores from the Ordovician of Wisconsin have been found with an age of about 460 million years ago, a time when the land flora most likely only consisted of plants similar to non-vascular bryophytes.[93]\n The Ordovician came to a close in a series of extinction events that, taken together, comprise the second largest of the five major extinction events in Earth's history in terms of percentage of genera that became extinct. The only larger one was the Permian–Triassic extinction event.\n The extinctions occurred approximately 447–444 million years ago and mark the boundary between the Ordovician and the following Silurian Period. At that time all complex multicellular organisms lived in the sea, and about 49% of genera of fauna disappeared forever; brachiopods and bryozoans were greatly reduced, along with many trilobite, conodont and graptolite families.\n The most commonly accepted theory is that these events were triggered by the onset of cold conditions in the late Katian, followed by an ice age, in the Hirnantian faunal stage, that ended the long, stable greenhouse conditions typical of the Ordovician.\n The ice age was possibly not long-lasting. Oxygen isotopes in fossil brachiopods show its duration may have been only 0.5 to 1.5 million years.[94] Other researchers (Page et al.) estimate more temperate conditions did not return until the late Silurian.\n The late Ordovician glaciation event was preceded by a fall in atmospheric carbon dioxide (from 7000 ppm to 4400 ppm).[95][96] The dip may have been caused by a burst of volcanic activity that deposited new silicate rocks, which draw CO2 out of the air as they erode.[96] Another possibility is that bryophytes and lichens, which colonized land in the middle to late Ordovician, may have increased weathering enough to draw down CO2 levels.[91] The drop in CO2 selectively affected the shallow seas where most organisms lived. As the southern supercontinent Gondwana drifted over the South Pole, ice caps formed on it, which have been detected in Upper Ordovician rock strata of North Africa and then-adjacent northeastern South America, which were south-polar locations at the time.\n As glaciers grew, the sea level dropped, and the vast shallow intra-continental Ordovician seas withdrew, which eliminated many ecological niches. When they returned, they carried diminished founder populations that lacked many whole families of organisms. They then withdrew again with the next pulse of glaciation, eliminating biological diversity with each change.[97] Species limited to a single epicontinental sea on a given landmass were severely affected.[40] Tropical lifeforms were hit particularly hard in the first wave of extinction, while cool-water species were hit worst in the second pulse.[40]\n Those species able to adapt to the changing conditions survived to fill the ecological niches left by the extinctions. For example, there is evidence the oceans became more deeply oxygenated during the glaciation, allowing unusual benthic organisms (Hirnantian fauna) to colonize the depths. These organisms were cosmopolitan in distribution and present at most latitudes.[67]\n At the end of the second event, melting glaciers caused the sea level to rise and stabilise once more. The rebound of life's diversity with the permanent re-flooding of continental shelves at the onset of the Silurian saw increased biodiversity within the surviving Orders. Recovery was characterized by an unusual number of \"Lazarus taxa\", disappearing during the extinction and reappearing well into the Silurian, which suggests that the taxa survived in small numbers in refugia.[98]\n An alternate extinction hypothesis suggested that a ten-second gamma-ray burst could have destroyed the ozone layer and exposed terrestrial and marine surface-dwelling life to deadly ultraviolet radiation and initiated global cooling.[99]\n Recent work considering the sequence stratigraphy of the Late Ordovician argues that the mass extinction was a single protracted episode lasting several hundred thousand years, with abrupt changes in water depth and sedimentation rate producing two pulses of last occurrences of species.[100]\n"}
{"key":"Elephant","link":"https:\/\/en.wikipedia.org\/wiki\/Elephant","headline":"Elephant - Wikipedia","content":"\n\n Elephants are the largest living land animals. Three living species are currently recognised: the African bush elephant (Loxodonta africana), the African forest elephant (L. cyclotis), and the Asian elephant (Elephas maximus). They are the only surviving members of the family Elephantidae and the order Proboscidea; extinct relatives include mammoths and mastodons. Distinctive features of elephants include a long proboscis called a trunk, tusks, large ear flaps, pillar-like legs, and tough but sensitive grey skin. The trunk is prehensile, bringing food and water to the mouth and grasping objects. Tusks, which are derived from the incisor teeth, serve both as weapons and as tools for moving objects and digging. The large ear flaps assist in maintaining a constant body temperature as well as in communication. African elephants have larger ears and concave backs, whereas Asian elephants have smaller ears and convex or level backs.\n Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia and are found in different habitats, including savannahs, forests, deserts, and marshes. They are herbivorous, and they stay near water when it is accessible. They are considered to be keystone species, due to their impact on their environments. Elephants have a fission–fusion society, in which multiple family groups come together to socialise. Females (cows) tend to live in family groups, which can consist of one female with her calves or several related females with offspring. The leader of a female group, usually the oldest cow, is known as the matriarch.\n Males (bulls) leave their family groups when they reach puberty and may live alone or with other males. Adult bulls mostly interact with family groups when looking for a mate. They enter a state of increased testosterone and aggression known as musth, which helps them gain dominance over other males as well as reproductive success. Calves are the centre of attention in their family groups and rely on their mothers for as long as three years. Elephants can live up to 70 years in the wild. They communicate by touch, sight, smell, and sound; elephants use infrasound and seismic communication over long distances. Elephant intelligence has been compared with that of primates and cetaceans. They appear to have self-awareness, and possibly show concern for dying and dead individuals of their kind.\n African bush elephants and Asian elephants are listed as endangered and African forest elephants as critically endangered by the International Union for Conservation of Nature (IUCN). One of the biggest threats to elephant populations is the ivory trade, as the animals are poached for their ivory tusks. Other threats to wild elephants include habitat destruction and conflicts with local people. Elephants are used as working animals in Asia. In the past, they were used in war; today, they are often controversially put on display in zoos, or employed for entertainment in circuses. Elephants have an iconic status in human culture, and have been widely featured in art, folklore, religion, literature, and popular culture.\n The word elephant is based on the Latin elephas (genitive elephantis) 'elephant', which is the Latinised form of the ancient Greek ἐλέφας (elephas) (genitive ἐλέφαντος (elephantos[1])), probably from a non-Indo-European language, likely Phoenician.[2] It is attested in Mycenaean Greek as e-re-pa (genitive e-re-pa-to) in Linear B syllabic script.[3][4] As in Mycenaean Greek, Homer used the Greek word to mean ivory, but after the time of Herodotus, it also referred to the animal.[1] The word elephant appears in Middle English as olyfaunt (c. 1300) and was borrowed from Old French oliphant (12th century).[2]\n Orycteropodidae \n Macroscelididae \n Chrysochloridae \n Tenrecidae \n Procaviidae \n Elephantidae \n Dugongidae \n Trichechidae \n Elephants belong to the family Elephantidae, the sole remaining family within the order Proboscidea. Their closest extant relatives are the sirenians (dugongs and manatees) and the hyraxes, with which they share the clade Paenungulata within the superorder Afrotheria.[6] Elephants and sirenians are further grouped in the clade Tethytheria.[7]\n Three species of living elephants are recognised; the African bush elephant (Loxodonta africana), forest elephant (Loxodonta cyclotis) and Asian elephant (Elephas maximus).[8] African elephants were traditionally considered a single species, Loxodonta africana, but molecular studies have affirmed their status as separate species.[9][10][11] Mammoths (Mammuthus) are nested within living elephants as they are more closely related to Asian elephants than to African elephants.[12] Another extinct genus of elephant, Palaeoloxodon, is also recognised, which appears to have close affinities with African elephants and to have hybridised with African forest elephants.[13]\n Over 180 extinct members of order Proboscidea have been described.[14] The earliest proboscideans, the African Eritherium and Phosphatherium are known from the late Paleocene.[15] The Eocene included Numidotherium, Moeritherium and Barytherium from Africa. These animals were relatively small and, some, like Moeritherium and Barytherium were probably amphibious.[16][17] Later on, genera such as Phiomia and Palaeomastodon arose; the latter likely inhabited more forested areas. Proboscidean diversification changed little during the Oligocene.[16] One notable species of this epoch was Eritreum melakeghebrekristosi of the Horn of Africa, which may have been an ancestor to several later species.[18]\n early proboscideans, e.g. Moeritherium \n Deinotheriidae \n Mammutidae \n Gomphotheriidae \n Stegodontidae \n Loxodonta \n Palaeoloxodon \n Mammuthus \n Elephas \n A major event in proboscidean evolution was the collision of Afro-Arabia with Eurasia, during the Early Miocene, around 18–19 million years ago, allowing proboscideans to disperse from their African homeland across Eurasia and later, around 16–15 million years ago into North America across the Bering Land Bridge. Proboscidean groups prominent during the Miocene include the deinotheres, along with the more advanced elephantimorphs, including mammutids (mastodons), gomphotheres, amebelodontids (which includes the \"shovel tuskers\" like Platybelodon), choerolophodontids and stegodontids.[21] Around 10 million years ago, the earliest members of the family Elephantidae emerged in Africa, having originated from gomphotheres.[22]\n Elephantids are distinguished from earlier proboscideans by a major shift in the molar morphology to parallel lophs rather than the cusps of earlier proboscideans, allowing them to become higher-crowned (hypsodont) and more efficient in consuming grass.[23] The Late Miocene saw major climactic changes, which resulted in the decline and extinction of many proboscidean groups.[21] The earliest members of the modern genera of Elephantidae appeared during the latest Miocene–early Pliocene around 5 million years ago. The elephantid genera Elephas (which includes the living Asian elephant) and Mammuthus (mammoths) migrated out of Africa during the late Pliocene, around 3.6 to 3.2 million years ago.[24]\n Over the course of the Early Pleistocene, all non-elephantid probobscidean genera outside of the Americas became extinct with the exception of Stegodon,[21] with gomphotheres dispersing into South America as part of the Great American interchange,[25] and mammoths migrating into North America around 1.5 million years ago.[26] At the end of the Early Pleistocene, around 800,000 years ago the elephantid genus Palaeoloxodon dispersed outside of Africa, becoming widely distributed in Eurasia.[27] Proboscideans were represented by around 23 species at the beginning of the Late Pleistocene. Proboscideans underwent a dramatic decline during the Late Pleistocene as part of the Late Pleistocene extinctions of most large mammals globally, with all remaining non-elephantid proboscideans (including Stegodon, mastodons, and the American gomphotheres Cuvieronius and Notiomastodon) and Palaeoloxodon becoming extinct, with mammoths only surviving in relict populations on islands around the Bering Strait into the Holocene, with their latest survival being on Wrangel Island, where they persisted until around 4,000 years ago.[21][28]\n Over the course of their evolution, probobscideans grew in size. With that came longer limbs and wider feet with a more digitigrade stance, along with a larger head and shorter neck. The trunk evolved and grew longer to provide reach. The number of premolars, incisors, and canines decreased, and the cheek teeth (molars and premolars) became longer and more specialised. The incisors developed into tusks of different shapes and sizes.[29] Several species of proboscideans became isolated on islands and experienced insular dwarfism,[30] some dramatically reducing in body size, such as the 1 metre (3.3 ft) tall dwarf elephant species Palaeoloxodon falconeri.[31]\n Elephants are the largest living terrestrial animals.[36] The skeleton is made up of 326–351 bones.[37] The vertebrae are connected by tight joints, which limit the backbone's flexibility. African elephants have 21 pairs of ribs, while Asian elephants have 19 or 20 pairs.[38] The skull contains air cavities (sinuses) that reduce the weight of the skull while maintaining overall strength. These cavities give the inside of the skull a honeycomb-like appearance. By contrast, the lower jaw is dense. The cranium is particularly large and provides enough room for the attachment of muscles to support the entire head.[37] The skull is built to withstand great stress, particularly when fighting or using the tusks. The brain is surrounded by arches in the skull, which serve as protection.[39] Because of the size of the head, the neck is relatively short to provide better support.[29]\nElephants are homeotherms and maintain their average body temperature at ~ 36 °C (97 °F), with a minimum of 35.2 °C (95.4 °F) during the cool season, and a maximum of 38.0 °C (100.4 °F) during the hot dry season.[40]\n Elephant ear flaps, or pinnae, are 1–2 mm (0.039–0.079 in) thick in the middle with a thinner tip and supported by a thicker base. They contain numerous blood vessels called capillaries. Warm blood flows into the capillaries, releasing excess heat into the environment. This effect is increased by flapping the ears back and forth. Larger ear surfaces contain more capillaries, and more heat can be released. Of all the elephants, African bush elephants live in the hottest climates and have the largest ear flaps.[37][41] The ossicles are adapted for hearing low frequencies, being most sensitive at 1 kHz.[42]\n Lacking a lacrimal apparatus (tear duct), the eye relies on the harderian gland in the orbit to keep it moist. A durable nictitating membrane shields the globe. The animal's field of vision is compromised by the location and limited mobility of the eyes.[43] Elephants are dichromats[44] and they can see well in dim light but not in bright light.[45]\n The elongated and prehensile trunk, or proboscis, consists of both the nose and upper lip, which fuse in early fetal development.[29] This versatile appendage contains up to 150,000 separate muscle fascicles, with no bone and little fat. These paired muscles consist of two major types: superficial (surface) and internal. The former are divided into dorsal, ventral, and lateral muscles, while the latter are divided into transverse and radiating muscles. The muscles of the trunk connect to a bony opening in the skull. The nasal septum consists of small elastic muscles between the nostrils, which are divided by cartilage at the base.[46] A unique proboscis nerve – a combination of the maxillary and facial nerves – lines each side of the appendage.[47]\n As a muscular hydrostat, the trunk moves through finely controlled muscle contractions, working both with and against each other.[47] Using three basic movements: bending, twisting, and longitudinal stretching or retracting, the trunk has near unlimited flexibility. Objects grasped by the end of the trunk can be moved to the mouth by curving the appendage inward. The trunk can also bend at different points by creating stiffened \"pseudo-joints\". The tip can be moved in a way similar to the human hand.[48] The skin is more elastic on the dorsal side of the elephant trunk than underneath; allowing the animal to stretch and coil while maintaining a strong grasp.[49] The African elephants have two finger-like extensions at the tip of the trunk that allow them to pluck small food. The Asian elephant has only one and relies more on wrapping around a food item.[33] Asian elephant trunks have better motor coordination.[46]\n The trunk's extreme flexibility allows it to forage and wrestle other elephants with it. It is powerful enough to lift up to 350 kg (770 lb), but it also has the precision to crack a peanut shell without breaking the seed. With its trunk, an elephant can reach items up to 7 m (23 ft) high and dig for water in the mud or sand below. It also uses it to clean itself.[50] Individuals may show lateral preference when grasping with their trunks: some prefer to twist them to the left, others to the right.[47] Elephant trunks are capable of powerful siphoning. They can expand their nostrils by 30%, leading to a 64% greater nasal volume, and can breathe in almost 30 times faster than a human sneeze, at over 150 m\/s (490 ft\/s).[51] They suck up water, which is squirted into the mouth or over the body.[29][51] The trunk of an adult Asian elephant is capable of retaining 8.5 L (2.2 US gal) of water.[46]  They will also sprinkle dust or grass on themselves.[29] When underwater, the elephant uses its trunk as a snorkel.[52]\n The trunk also acts as a sense organ. Its sense of smell may be four times greater than a bloodhound's nose.[53] The infraorbital nerve, which makes the trunk sensitive to touch, is thicker than both the optic and auditory nerves. Whiskers grow all along the trunk, and are particularly packed at the tip, where they contribute to its tactile sensitivity. Unlike those of many mammals, such as cats and rats, elephant whiskers do not move independently (\"whisk\") to sense the environment; the trunk itself must move to bring the whiskers into contact with nearby objects. Whiskers grow in rows along each side on the ventral surface of the trunk, which is thought to be essential in helping elephants balance objects there, whereas they are more evenly arranged on the dorsal surface. The number and patterns of whiskers are distinctly different between species.[54]\n Damaging the trunk would be detrimental to an elephant's survival,[29] although in rare cases, individuals have survived with shortened ones. One trunkless elephant has been observed to graze using its lips with its hind legs in the air and balancing on its front knees.[46] Floppy trunk syndrome is a condition of trunk paralysis recorded in African bush elephants and involves the degeneration of the peripheral nerves and muscles. The disorder has been linked to lead poisoning.[55]\n Elephants usually have 26 teeth: the incisors, known as the tusks; 12 deciduous premolars; and 12 molars. Unlike most mammals, teeth are not replaced by new ones emerging from the jaws vertically. Instead, new teeth start at the back of the mouth and push out the old ones. The first chewing tooth on each side of the jaw falls out when the elephant is two to three years old. This is followed by four more tooth replacements at the ages of four to six, 9–15, 18–28, and finally in their early 40s. The final (usually sixth) set must last the elephant the rest of its life. Elephant teeth have loop-shaped dental ridges, which are more diamond-shaped in African elephants.[56]\n The tusks of an elephant have modified second incisors in the upper jaw. They replace deciduous milk teeth at 6–12 months of age and keep growing at about 17 cm (7 in) a year. As the tusk develops, it is topped with smooth, cone-shaped enamel that eventually wanes. The dentine is known as ivory and has a cross-section of intersecting lines, known as \"engine turning\", which create diamond-shaped patterns. Being living tissue, tusks are fairly soft and about as dense as the mineral calcite. The tusk protrudes from a socket in the skull, and most of it is external. At least one-third of the tusk contains the pulp, and some have nerves that stretch even further. Thus, it would be difficult to remove it without harming the animal. When removed, ivory will dry up and crack if not kept cool and wet. Tusks function in digging, debarking, marking, moving objects, and fighting.[57]\n Elephants are usually right- or left-tusked, similar to humans, who are typically right- or left-handed. The dominant, or \"master\" tusk, is typically more worn down, as it is shorter and blunter. For African elephants, tusks are present in both males and females, and are around the same length in both sexes, reaching up to 300 cm (9 ft 10 in),[57] but those of males tend to be more massive.[58] In the Asian species, only the males have large tusks. Female Asians have very small tusks, or none at all.[57] Tuskless males exist and are particularly common among Sri Lankan elephants.[59] Asian males can have tusks as long as Africans', but they are usually slimmer and lighter; the largest recorded was 302 cm (9 ft 11 in) long and weighed 39 kg (86 lb). Hunting for elephant ivory in Africa[60] and Asia[61] has led to natural selection for shorter tusks[62][63] and tusklessness.[64][65]\n An elephant's skin is generally very tough, at 2.5 cm (1 in) thick on the back and parts of the head. The skin around the mouth, anus, and inside of the ear is considerably thinner. Elephants are typically grey, but African elephants look brown or reddish after rolling in coloured mud. Asian elephants have some patches of depigmentation, particularly on the head. Calves have brownish or reddish hair, with the head and back being particularly hairy. As elephants mature, their hair darkens and becomes sparser, but dense concentrations of hair and bristles remain on the tip of the tail and parts of the head and genitals. Normally, the skin of an Asian elephant is covered with more hair than its African counterpart.[66] Their hair is thought to help them lose heat in their hot environments.[67]\n Although tough, an elephant's skin is very sensitive and requires mud baths to maintain moisture and protection from burning and insect bites. After bathing, the elephant will usually use its trunk to blow dust onto its body, which dries into a protective crust. Elephants have difficulty releasing heat through the skin because of their low surface-area-to-volume ratio, which is many times smaller than that of a human. They have even been observed lifting up their legs to expose their soles to the air.[66] Elephants only have sweat glands between the toes,[68] but the skin allows water to disperse and evaporate, cooling the animal.[69][70] In addition, cracks in the skin may reduce dehydration and allow for increased thermal regulation in the long term.[71]\n To support the animal's weight, an elephant's limbs are positioned more vertically under the body than in most other mammals. The long bones of the limbs have cancellous bones in place of medullary cavities. This strengthens the bones while still allowing haematopoiesis (blood cell creation).[72] Both the front and hind limbs can support an elephant's weight, although 60% is borne by the front.[73] The position of the limbs and leg bones allows an elephant to stand still for extended periods of time without tiring. Elephants are incapable of turning their manus as the ulna and radius of the front legs are secured in pronation.[72] Elephants may also lack the pronator quadratus and pronator teres muscles or have very small ones.[74] The circular feet of an elephant have soft tissues, or \"cushion pads\" beneath the manus or pes, which allow them to bear the animal's great mass.[73] They appear to have a sesamoid, an extra \"toe\" similar in placement to a giant panda's extra \"thumb\", that also helps in weight distribution.[75] As many as five toenails can be found on both the front and hind feet.[33]\n Elephants can move both forward and backward, but are incapable of trotting, jumping, or galloping. They can move on land only by walking or ambling: a faster gait similar to running.[72][76] In walking, the legs act as pendulums, with the hips and shoulders moving up and down while the foot is planted on the ground. The fast gait does not meet all the criteria of running, since there is no point where all the feet are off the ground, although the elephant uses its legs much like other running animals, and can move faster by quickening its stride. Fast-moving elephants appear to 'run' with their front legs, but 'walk' with their hind legs and can reach a top speed of 25 km\/h (16 mph). At this speed, most other quadrupeds are well into a gallop, even accounting for leg length. Spring-like kinetics could explain the difference between the motion of elephants and other animals.[76][77] The cushion pads expand and contract, and reduce both the pain and noise that would come from a very heavy animal moving.[73] Elephants are capable swimmers: they can swim for up to six hours while completely waterborne, moving at 2.1 km\/h (1 mph) and traversing up to 48 km (30 mi) continuously.[78]\n The brain of an elephant weighs 4.5–5.5 kg (10–12 lb) compared to 1.6 kg (4 lb) for a human brain.[79] It is the largest of all terrestrial mammals.[80] While the elephant brain is larger overall, it is proportionally smaller than the human brain. At birth, an elephant's brain already weighs 30–40% of its adult weight. The cerebrum and cerebellum are well developed, and the temporal lobes are so large that they bulge out laterally.[79] Their temporal lobes are proportionally larger than those of other animals, including humans.[80] The throat of an elephant appears to contain a pouch where it can store water for later use.[29] The larynx of the elephant is the largest known among mammals. The vocal folds are anchored close to the epiglottis base. When comparing an elephant's vocal folds to those of a human, an elephant's are proportionally longer, thicker, with a greater cross-sectional area. In addition, they are located further up the vocal tract with an acute slope.[81]\n The heart of an elephant weighs 12–21 kg (26–46 lb). Its apex has two pointed ends, an unusual trait among mammals.[79] In addition, the ventricles of the heart split towards the top, a trait also found in sirenians.[82] When upright, the elephant's heart beats around 28 beats per minute and actually speeds up to 35 beats when it lies down.[79] The blood vessels are thick and wide and can hold up under high blood pressure.[82] The lungs are attached to the diaphragm, and breathing relies less on the expanding of the ribcage.[79] Connective tissue exists in place of the pleural cavity. This may allow the animal to deal with the pressure differences when its body is underwater and its trunk is breaking the surface for air.[52] Elephants breathe mostly with the trunk but also with the mouth. They have a hindgut fermentation system, and their large and small intestines together reach 35 m (115 ft) in length. Less than half of an elephant's food intake gets digested, despite the process lasting a day.[79] An elephant's kidneys can produce more than 50 litres of urine per day.[83]\n A male elephant's testes, like other Afrotheria,[84] are internally located near the kidneys.[85] The penis can be as long as 100 cm (39 in) with a 16 cm (6 in) wide base. It curves to an 'S' when fully erect and has an orifice shaped like a Y. The female's clitoris may be 40 cm (16 in). The vulva is found lower than in other herbivores, between the hind legs instead of under the tail. Determining pregnancy status can be difficult due to the animal's large belly. The female's mammary glands occupy the space between the front legs, which puts the suckling calf within reach of the female's trunk.[79] Elephants have a unique organ, the temporal gland, located on both sides of the head. This organ is associated with sexual behaviour, and males secrete a fluid from it when in musth.[86] Females have also been observed with these secretions.[53]\n Elephants are herbivorous and will eat leaves, twigs, fruit, bark, grass, and roots. African elephants mostly browse, while Asian elephants mainly graze.[34] They can eat as much as 300 kg (660 lb) of food and drink 40 L (11 US gal) of water in a day. Elephants tend to stay near water sources.[34][87] They have morning, afternoon, and nighttime feeding sessions. At midday, elephants rest under trees and may doze off while standing. Sleeping occurs at night while the animal is lying down.[87] Elephants average 3–4 hours of sleep per day.[88] Both males and family groups typically move no more than 20 km (12 mi) a day, but distances as far as 180 km (112 mi) have been recorded in the Etosha region of Namibia.[89] Elephants go on seasonal migrations in response to changes in environmental conditions.[90] In northern Botswana, they travel 325 km (202 mi) to the Chobe River after the local waterholes dry up in late August.[91]\n Because of their large size, elephants have a huge impact on their environments and are considered keystone species. Their habit of uprooting trees and undergrowth can transform savannah into grasslands;[92] smaller herbivores can access trees mowed down by elephants.[87] When they dig for water during droughts, they create waterholes that can be used by other animals. When they use waterholes, they end up making them bigger.[92] At Mount Elgon, elephants dig through caves and pave the way for ungulates, hyraxes, bats, birds and insects.[92] Elephants are important seed dispersers; African forest elephants consume and deposit many seeds over great distances, with either no effect or a positive effect on germination.[93] In Asian forests, large seeds require giant herbivores like elephants and rhinoceros for transport and dispersal. This ecological niche cannot be filled by the smaller Malayan tapir.[94] Because most of the food elephants eat goes undigested, their dung can provide food for other animals, such as dung beetles and monkeys.[92] Elephants can have a negative impact on ecosystems. At Murchison Falls National Park in Uganda, elephant numbers have threatened several species of small birds that depend on woodlands. Their weight causes the soil to compress, leading to runoff and erosion.[87]\n Elephants typically coexist peacefully with other herbivores, which will usually stay out of their way. Some aggressive interactions between elephants and rhinoceros have been recorded.[87] The size of adult elephants makes them nearly invulnerable to predators.[35] Calves may be preyed on by lions, spotted hyenas, and wild dogs in Africa[95] and tigers in Asia.[35] The lions of Savuti, Botswana, have adapted to hunting elephants, mostly calves, juveniles or even sub-adults.[96][97] There are rare reports of adult Asian elephants falling prey to tigers.[98] Elephants tend to have high numbers of parasites, particularly nematodes, compared to many other mammals. This is due to them being largely immune to predators, which would otherwise kill off many of the individuals with significant parasite loads.[99]\n Elephants are generally gregarious animals. African bush elephants in particular have a complex, stratified social structure.[100] Female elephants spend their entire lives in tight-knit matrilineal family groups.[101] They are led by the matriarch, who is often the eldest female.[102] She remains leader of the group until death[95] or if she no longer has the energy for the role;[103] a study on zoo elephants found that the death of the matriarch led to greater stress in the surviving elephants.[104] When her tenure is over, the matriarch's eldest daughter takes her place instead of her sister (if present).[95] One study found that younger matriarchs take potential threats less seriously.[105] Large family groups may split if they cannot be supported by local resources.[106]\n At Amboseli National Park, Kenya, female groups may consist of around ten members, including four adults and their dependent offspring. Here, a cow's life involves interaction with those outside her group. Two separate families may associate and bond with each other, forming what are known as bond groups. During the dry season, elephant families may aggregate into clans. These may number around nine groups, in which clans do not form strong bonds but defend their dry-season ranges against other clans. The Amboseli elephant population is further divided into the \"central\" and \"peripheral\" subpopulations.[101]\n Female Asian elephants tend to have more fluid social associations.[100] In Sri Lanka, there appear to be stable family units or \"herds\" and larger, looser \"groups\". They have been observed to have \"nursing units\" and \"juvenile-care units\". In southern India, elephant populations may contain family groups, bond groups and possibly clans. Family groups tend to be small, with only one or two adult females and their offspring. A group containing more than two cows and their offspring is known as a \"joint family\". Malay elephant populations have even smaller family units and do not reach levels above a bond group. Groups of African forest elephants typically consist of one cow with one to three offspring. These groups appear to interact with each other, especially at forest clearings.[101]\n Adult males live separate lives. As he matures, a bull associates more with outside males or even other families. At Amboseli, young males may be away from their families 80% of the time by 14–15 years of age. When males permanently leave, they either live alone or with other males. The former is typical of bulls in dense forests. A dominance hierarchy exists among males, whether they are social or solitary. Dominance depends on age, size, and sexual condition.[107] Male elephants can be quite sociable when not competing for mates and form vast and fluid social networks.[108][109] Older bulls act as the leaders of these groups.[110] The presence of older males appears to subdue the aggression and \"deviant\" behaviour of younger ones.[111] The largest all-male groups can reach close to 150 individuals. Adult males and females come together to breed. Bulls will accompany family groups if a cow is in oestrous.[107]\n Adult males enter a state of increased testosterone known as musth. In a population in southern India, males first enter musth at 15 years old, but it is not very intense until they are older than 25. At Amboseli, no bulls under 24 were found to be in musth, while half of those aged 25–35 and all those over 35 were. In some areas, there may be seasonal influences on the timing of musths. The main characteristic of a bull's musth is a fluid discharged from the temporal gland that runs down the side of his face. Behaviours associated with musth include walking with a high and swinging head, nonsynchronous ear flapping, picking at the ground with the tusks, marking, rumbling, and urinating in the sheath. The length of this varies between males of different ages and conditions, lasting from days to months.[112]\n Males become extremely aggressive during musth. Size is the determining factor in agonistic encounters when the individuals have the same condition. In contests between musth and non-musth individuals, musth bulls win the majority of the time, even when the non-musth bull is larger. A male may stop showing signs of musth when he encounters a musth male of higher rank. Those of equal rank tend to avoid each other. Agonistic encounters typically consist of threat displays, chases, and minor sparring. Rarely do they full-on fight.[112]\n Elephants are polygynous breeders,[113] and most copulations occur during rainfall.[114] An oestrous cow uses pheromones in her urine and vaginal secretions to signal her readiness to mate. A bull will follow a potential mate and assess her condition with the flehmen response, which requires him to collect a chemical sample with his trunk and taste it with the vomeronasal organ at the roof of the mouth.[115] The oestrous cycle of a cow lasts 14–16 weeks, with the follicular phase lasting 4–6 weeks and the luteal phase lasting 8–10 weeks. While most mammals have one surge of luteinizing hormone during the follicular phase, elephants have two. The first (or anovulatory) surge, appears to change the female's scent, signaling to males that she is in heat, but ovulation does not occur until the second (or ovulatory) surge.[116] Cows over 45–50 years of age are less fertile.[103]\n Bulls engage in a behaviour known as mate-guarding, where they follow oestrous females and defend them from other males.[117] Most mate-guarding is done by musth males, and females seek them out, particularly older ones.[118] Musth appears to signal to females the condition of the male, as weak or injured males do not have normal musths.[119] For young females, the approach of an older bull can be intimidating, so her relatives stay nearby for comfort.[120] During copulation, the male rests his trunk on the female.[121] The penis is mobile enough to move without the pelvis.[83] Before mounting, it curves forward and upward. Copulation lasts about 45 seconds and does not involve pelvic thrusting or an ejaculatory pause.[122]\n Homosexual behaviour is frequent in both sexes. As in heterosexual interactions, this involves mounting. Male elephants sometimes stimulate each other by playfighting, and \"championships\" may form between old bulls and younger males. Female same-sex behaviours have been documented only in captivity, where they engage in mutual masturbation with their trunks.[123]\n Gestation in elephants typically lasts between one and a half and two years and the female will not give birth again for at least four years.[124] The relatively long pregnancy is supported by several corpus luteums and gives the foetus more time to develop, particularly the brain and trunk.[125] Births tend to take place during the wet season.[114] Typically, only a single young is born, but twins sometimes occur.[125] Calves are born roughly 85 cm (33 in) tall and with a weight of around 120 kg (260 lb).[120] They are precocial and quickly stand and walk to follow their mother and family herd.[126] A newborn calf will attract the attention of all the herd members. Adults and most of the other young will gather around the newborn, touching and caressing it with their trunks. For the first few days, the mother limits access to her young. Alloparenting – where a calf is cared for by someone other than its mother – takes place in some family groups. Allomothers are typically aged two to twelve years.[120]\n For the first few days, the newborn is unsteady on its feet and needs its mother's help. It relies on touch, smell, and hearing, as its eyesight is less developed. With little coordination in its trunk, it can only flop it around which may cause it to trip. When it reaches its second week, the calf can walk with more balance and has more control over its trunk. After its first month, the trunk can grab and hold objects, but still lacks sucking abilities, and the calf must bend down to drink. It continues to stay near its mother as it is still reliant on her. For its first three months, a calf relies entirely on its mother's milk, after which it begins to forage for vegetation and can use its trunk to collect water. At the same time, there is progress in lip and leg movements. By nine months, mouth, trunk and foot coordination are mastered. Suckling bouts tend to last 2–4 min\/hr for a calf younger than a year. After a year, a calf is fully capable of grooming, drinking, and feeding itself. It still needs its mother's milk and protection until it is at least two years old. Suckling after two years may improve growth, health and fertility.[126]\n Play behaviour in calves differs between the sexes; females run or chase each other while males play-fight. The former are sexually mature by the age of nine years[120] while the latter become mature around 14–15 years.[107] Adulthood starts at about 18 years of age in both sexes.[127][128] Elephants have long lifespans, reaching 60–70 years of age.[56] Lin Wang, a captive male Asian elephant, lived for 86 years.[129]\n Elephants communicate in various ways. Individuals greet one another by touching each other on the mouth, temporal glands and genitals. This allows them to pick up chemical cues. Older elephants use trunk-slaps, kicks, and shoves to control younger ones. Touching is especially important for mother–calf communication. When moving, elephant mothers will touch their calves with their trunks or feet when side-by-side or with their tails if the calf is behind them. A calf will press against its mother's front legs to signal it wants to rest and will touch her breast or leg when it wants to suckle.[130]\n Visual displays mostly occur in agonistic situations. Elephants will try to appear more threatening by raising their heads and spreading their ears. They may add to the display by shaking their heads and snapping their ears, as well as tossing around dust and vegetation. They are usually bluffing when performing these actions. Excited elephants also raise their heads and spread their ears but additionally may raise their trunks. Submissive elephants will lower their heads and trunks, as well as flatten their ears against their necks, while those that are ready to fight will bend their ears in a V shape.[131]\n Elephants produce several vocalisations—some of which pass though the trunk[132]—for both short and long range communication. This includes trumpeting, bellowing, roaring, growling, barking, snorting, and rumbling.[132][133] Elephants can produce infrasonic rumbles.[134] For Asian elephants, these calls have a frequency of 14–24 Hz, with sound pressure levels of 85–90 dB and last 10–15 seconds.[135] For African elephants, calls range from 15 to 35 Hz with sound pressure levels as high as 117 dB, allowing communication for many kilometres, possibly over 10 km (6 mi).[136] Elephants are known to communicate with seismics, vibrations produced by impacts on the earth's surface or acoustical waves that travel through it. An individual foot stomping or mock charging can create seismic signals that can be heard at travel distances of up to 32 km (20 mi). Seismic waveforms produced by rumbles travel 16 km (10 mi).[137][138]\n Elephants are among the most intelligent animals. They exhibit mirror self-recognition, an indication of self-awareness and cognition that has also been demonstrated in some apes and dolphins.[139] One study of a captive female Asian elephant suggested the animal was capable of learning and distinguishing between several visual and some acoustic discrimination pairs. This individual was even able to score a high accuracy rating when re-tested with the same visual pairs a year later.[140] Elephants are among the species known to use tools. An Asian elephant has been observed fine-tuning branches for use as flyswatters.[141] Tool modification by these animals is not as advanced as that of chimpanzees. Elephants are popularly thought of as having an excellent memory. This could have a factual basis; they possibly have cognitive maps which give them long lasting memories of their environment on a wide scale. Individuals may be able to remember where their family members are located.[45]\n Scientists debate the extent to which elephants feel emotion. They are attracted to the bones of their own kind, regardless of whether they are related.[142] As with chimpanzees and dolphins, a dying or dead elephant may elicit attention and aid from others, including those from other groups. This has been interpreted as expressing \"concern\";[143] however, the Oxford Companion to Animal Behaviour (1987) said that \"one is well advised to study the behaviour rather than attempting to get at any underlying emotion\".[144]\n African bush elephants were listed as Endangered by the International Union for Conservation of Nature (IUCN) in 2021,[145] and African forest elephants were listed as Critically Endangered in the same year.[146] In 1979, Africa had an estimated population of at least 1.3 million elephants, possibly as high as 3.0 million. A decade later, the population was estimated to be 609,000; with 277,000 in Central Africa, 110,000 in Eastern Africa, 204,000 in Southern Africa, and 19,000 in Western Africa. The population of rainforest elephants was lower than anticipated, at around 214,000 individuals. Between 1977 and 1989, elephant populations declined by 74% in East Africa. After 1987, losses in elephant numbers hastened, and savannah populations from Cameroon to Somalia experienced a decline of 80%. African forest elephants had a total loss of 43%. Population trends in southern Africa were various, with unconfirmed losses in Zambia, Mozambique and Angola while populations grew in Botswana and Zimbabwe and were stable in South Africa.[147] The IUCN estimated that total population in Africa is estimated at to 415,000 individuals for both species combined as of 2016.[148]\n African elephants receive at least some legal protection in every country where they are found. Successful conservation efforts in certain areas have led to high population densities while failures have led to declines as high as 70% or more of the course of ten years. As of 2008, local numbers were controlled by contraception or translocation. Large-scale cullings stopped in the late 1980s and early 1990s. In 1989, the African elephant was listed under Appendix I by the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES), making trade illegal. Appendix II status (which allows restricted trade) was given to elephants in Botswana, Namibia, and Zimbabwe in 1997 and South Africa in 2000. In some countries, sport hunting of the animals is legal; Botswana, Cameroon, Gabon, Mozambique, Namibia, South Africa, Tanzania, Zambia, and Zimbabwe have CITES export quotas for elephant trophies.[145]\n In 2020, the IUCN listed the Asian elephant as endangered due to the population declining by half over \"the last three generations\".[149] Asian elephants once ranged from Western to East Asia and south to Sumatra.[150] and Java. It is now extinct in these areas,[149] and the current range of Asian elephants is highly fragmented.[150] The total population of Asian elephants is estimated to be around 40,000–50,000, although this may be a loose estimate. Around 60% of the population is in India. Although Asian elephants are declining in numbers overall, particularly in Southeast Asia, the population in Sri Lanka appears to have risen and elephant numbers in the Western Ghats may have stabilised.[149]\n The poaching of elephants for their ivory, meat and hides has been one of the major threats to their existence.[149] Historically, numerous cultures made ornaments and other works of art from elephant ivory, and its use was comparable to that of gold.[151] The ivory trade contributed to the fall of the African elephant population in the late 20th century.[145] This prompted international bans on ivory imports, starting with the United States in June 1989, and followed by bans in other North American countries, western European countries, and Japan.[151] Around the same time, Kenya destroyed all its ivory stocks.[152] Ivory was banned internationally by CITES in 1990. Following the bans, unemployment rose in India and China, where the ivory industry was important economically. By contrast, Japan and Hong Kong, which were also part of the industry, were able to adapt and were not as badly affected.[151] Zimbabwe, Botswana, Namibia, Zambia, and Malawi wanted to continue the ivory trade and were allowed to, since their local populations were healthy, but only if their supplies were from culled individuals or those that died of natural causes.[152]\n The ban allowed the elephant to recover in parts of Africa.[151] In February 2012, 650 elephants in Bouba Njida National Park, Cameroon, were slaughtered by Chadian raiders.[153] This has been called \"one of the worst concentrated killings\" since the ivory ban.[152] Asian elephants are potentially less vulnerable to the ivory trade, as females usually lack tusks. Still, members of the species have been killed for their ivory in some areas, such as Periyar National Park in India.[149] China was the biggest market for poached ivory but announced they would phase out the legal domestic manufacture and sale of ivory products in May 2015, and in September 2015, China and the United States said \"they would enact a nearly complete ban on the import and export of ivory\" due to causes of extinction.[154]\n Other threats to elephants include habitat destruction and fragmentation. The Asian elephant lives in areas with some of the highest human populations and may be confined to small islands of forest among human-dominated landscapes. Elephants commonly trample and consume crops, which contributes to conflicts with humans, and both elephants and humans have died by the hundreds as a result. Mitigating these conflicts is important for conservation. One proposed solution is the protection of wildlife corridors which give populations greater interconnectivity and space.[149] Chili pepper products as well as guarding with defense tools have been found to be effective in preventing crop-raiding by elephants. Less effective tactics include beehive and electric fences.[155]\n Elephants have been working animals since at least the Indus Valley civilization over 4,000 years ago[156] and continue to be used in modern times. There were 13,000–16,500 working elephants employed in Asia in 2000. These animals are typically captured from the wild when they are 10–20 years old when they are both more trainable and can work for more years.[157] They were traditionally captured with traps and lassos, but since 1950, tranquillisers have been used.[158] Individuals of the Asian species have been often trained as working animals. Asian elephants are used to carry and pull both objects and people in and out of areas as well as lead people in religious celebrations. They are valued over mechanised tools as they can perform the same tasks but in more difficult terrain, with strength, memory, and delicacy. Elephants can learn over 30 commands.[157] Musth bulls are difficult and dangerous to work with and so are chained up until their condition passes.[159]\n In India, many working elephants are alleged to have been subject to abuse. They and other captive elephants are thus protected under The Prevention of Cruelty to Animals Act of 1960.[160] In both Myanmar and Thailand, deforestation and other economic factors have resulted in sizable populations of unemployed elephants resulting in health problems for the elephants themselves as well as economic and safety problems for the people amongst whom they live.[161][162]\n The practice of working elephants has also been attempted in Africa. The taming of African elephants in the Belgian Congo began by decree of Leopold II of Belgium during the 19th century and continues to the present with the Api Elephant Domestication Centre.[163]\n Historically, elephants were considered formidable instruments of war. They were described in Sanskrit texts as far back as 1500 BC. From South Asia, the use of elephants in warfare spread west to Persia[164] and east to Southeast Asia.[165] The Persians used them during the Achaemenid Empire (between the 6th and 4th centuries BC)[164] while Southeast Asian states first used war elephants possibly as early as the 5th century BC and continued to the 20th century.[165] War elephants were also employed in the Mediterranean and North Africa throughout the classical period since the reign of Ptolemy II in Egypt. The Carthaginian general Hannibal famously took African elephants across the Alps during his war with the Romans and reached the Po Valley in 218 BC with all of them alive, but died of disease and combat a year later.[164]\n An elephant's head and sides were equipped with armour, the trunk may have had a sword tied to it and tusks were sometimes covered with sharpened iron or brass. Trained elephants would attack both humans and horses with their tusks. They might have grasped an enemy soldier with the trunk and tossed him to their mahout, or pinned the soldier to the ground and speared him. Some shortcomings of war elephants included their great visibility, which made them easy to target, and limited maneuverability compared to horses. Alexander the Great achieved victory over armies with war elephants by having his soldiers injure the trunks and legs of the animals which caused them to panic and become uncontrollable.[164]\n Elephants have traditionally been a major part of zoos and circuses around the world. In circuses, they are trained to perform tricks. The most famous circus elephant was probably Jumbo (1861 – 15 September 1885), who was a major attraction in the Barnum & Bailey Circus.[166][167] These animals do not reproduce well in captivity due to the difficulty of handling musth bulls and limited understanding of female oestrous cycles. Asian elephants were always more common than their African counterparts in modern zoos and circuses. After CITES listed the Asian elephant under Appendix I in 1975, imports of the species almost stopped by the end of the 1980s. Subsequently, the US received many captive African elephants from Zimbabwe, which had an overabundance of the animals.[167]\n Keeping elephants in zoos has met with some controversy. Proponents of zoos argue that they allow easy access to the animals and provide fund and knowledge for preserving their natural habitats, as well as safekeeping for the species. Opponents claim that animals in zoos are under physical and mental stress.[168] Elephants have been recorded displaying stereotypical behaviours in the form of wobbling the body or head and pacing the same route both forwards and backwards. This has been observed in 54% of individuals in UK zoos.[169] Elephants in European zoos appear to have shorter lifespans than their wild counterparts at only 17 years, although other studies suggest that zoo elephants live just as long.[170]\n The use of elephants in circuses has also been controversial; the Humane Society of the United States has accused circuses of mistreating and distressing their animals.[171] In testimony to a US federal court in 2009, Barnum & Bailey Circus CEO Kenneth Feld acknowledged that circus elephants are struck behind their ears, under their chins and on their legs with metal-tipped prods, called bull hooks or ankus. Feld stated that these practices are necessary to protect circus workers and acknowledged that an elephant trainer was rebuked for using an electric prod on an elephant. Despite this, he denied that any of these practices hurt the animals.[172] Some trainers have tried to train elephants without the use of physical punishment. Ralph Helfer is known to have relied on positive reinforcement when training his animals.[173] Barnum and Bailey circus retired its touring elephants in May 2016.[174]\n Elephants can exhibit bouts of aggressive behaviour and engage in destructive actions against humans.[175] In Africa, groups of adolescent elephants damaged homes in villages after cullings in the 1970s and 1980s. Because of the timing, these attacks have been interpreted as vindictive.[176][177] In parts of India, male elephants have entered villages at night, destroying homes and killing people. From 2000 to 2004, 300 people died in Jharkhand, and in Assam, 239 people were reportedly killed between 2001 and 2006.[175]\nThroughout the country, 1,500 people were killed by elephants between 2019 and 2022, which led to 300 elephants being killed in kind.[178] Local people have reported their belief that some elephants were drunk during their attacks, though officials have disputed this.[179][180] Purportedly drunk elephants attacked an Indian village in December 2002, killing six people, which led to the retaliatory slaughter of about 200 elephants by locals.[181]\n Elephants have a universal presence in global culture. They have been represented in art since Paleolithic times. Africa, in particular, contains many examples of elephant rock art, especially in the Sahara and southern Africa.[182] In Asia, the animals are depicted as motifs in Hindu and Buddhist shrines and temples.[183] Elephants were often difficult to portray by people with no first-hand experience of them.[184] The ancient Romans, who kept the animals in captivity, depicted elephants more accurately than medieval Europeans who portrayed them more like fantasy creatures, with horse, bovine and boar-like traits, and trumpet-like trunks. As Europeans gained more access to captive elephants during the 15th century, depictions of them became more accurate, including one made by Leonardo da Vinci.[185]\n Elephants have been the subject of religious beliefs. The Mbuti people of central Africa believe that the souls of their dead ancestors resided in elephants.[183] Similar ideas existed among other African societies, who believed that their chiefs would be reincarnated as elephants. During the 10th century AD, the people of Igbo-Ukwu, in modern-day Nigeria, placed elephant tusks underneath their death leader's feet in the grave.[186] The animals' importance is only totemic in Africa but is much more significant in Asia.[187] In Sumatra, elephants have been associated with lightning. Likewise in Hinduism, they are linked with thunderstorms as Airavata, the father of all elephants, represents both lightning and rainbows.[183] One of the most important Hindu deities, the elephant-headed Ganesha, is ranked equal with the supreme gods Shiva, Vishnu, and Brahma in some traditions.[188] Ganesha is associated with writers and merchants and it is believed that he can give people success as well as grant them their desires, but could also take these things away.[183] In Buddhism, Buddha is said to have been a white elephant reincarnated as a human.[189]\n In Western popular culture, elephants symbolise the exotic, especially since – as with the giraffe, hippopotamus and rhinoceros – there are no similar animals familiar to Western audiences. As characters, elephants are most common in children's stories, where they are portrayed positively. They are typically surrogates for humans with ideal human values. Many stories tell of isolated young elephants returning to or finding a family, such as \"The Elephant's Child\" from Rudyard Kipling's Just So Stories, Disney's Dumbo, and Kathryn and Byron Jackson's The Saggy Baggy Elephant. Other elephant heroes given human qualities include Jean de Brunhoff's Babar, David McKee's Elmer, and Dr. Seuss's Horton.[190]\n Several cultural references emphasise the elephant's size and strangeness. For instance, a \"white elephant\" is a byword for something that is weird, unwanted, and has no value.[190] The expression \"elephant in the room\" refers to something that is being ignored but ultimately must be addressed.[191] The story of the blind men and an elephant involves blind men touching different parts of an elephant and trying to figure out what it is.[192]\n \n"}
{"key":"Elephant","link":"https:\/\/en.wikipedia.org\/wiki\/Elephant","headline":"Elephant - Wikipedia","content":"\n\n Elephants are the largest living land animals. Three living species are currently recognised: the African bush elephant (Loxodonta africana), the African forest elephant (L. cyclotis), and the Asian elephant (Elephas maximus). They are the only surviving members of the family Elephantidae and the order Proboscidea; extinct relatives include mammoths and mastodons. Distinctive features of elephants include a long proboscis called a trunk, tusks, large ear flaps, pillar-like legs, and tough but sensitive grey skin. The trunk is prehensile, bringing food and water to the mouth and grasping objects. Tusks, which are derived from the incisor teeth, serve both as weapons and as tools for moving objects and digging. The large ear flaps assist in maintaining a constant body temperature as well as in communication. African elephants have larger ears and concave backs, whereas Asian elephants have smaller ears and convex or level backs.\n Elephants are scattered throughout sub-Saharan Africa, South Asia, and Southeast Asia and are found in different habitats, including savannahs, forests, deserts, and marshes. They are herbivorous, and they stay near water when it is accessible. They are considered to be keystone species, due to their impact on their environments. Elephants have a fission–fusion society, in which multiple family groups come together to socialise. Females (cows) tend to live in family groups, which can consist of one female with her calves or several related females with offspring. The leader of a female group, usually the oldest cow, is known as the matriarch.\n Males (bulls) leave their family groups when they reach puberty and may live alone or with other males. Adult bulls mostly interact with family groups when looking for a mate. They enter a state of increased testosterone and aggression known as musth, which helps them gain dominance over other males as well as reproductive success. Calves are the centre of attention in their family groups and rely on their mothers for as long as three years. Elephants can live up to 70 years in the wild. They communicate by touch, sight, smell, and sound; elephants use infrasound and seismic communication over long distances. Elephant intelligence has been compared with that of primates and cetaceans. They appear to have self-awareness, and possibly show concern for dying and dead individuals of their kind.\n African bush elephants and Asian elephants are listed as endangered and African forest elephants as critically endangered by the International Union for Conservation of Nature (IUCN). One of the biggest threats to elephant populations is the ivory trade, as the animals are poached for their ivory tusks. Other threats to wild elephants include habitat destruction and conflicts with local people. Elephants are used as working animals in Asia. In the past, they were used in war; today, they are often controversially put on display in zoos, or employed for entertainment in circuses. Elephants have an iconic status in human culture, and have been widely featured in art, folklore, religion, literature, and popular culture.\n The word elephant is based on the Latin elephas (genitive elephantis) 'elephant', which is the Latinised form of the ancient Greek ἐλέφας (elephas) (genitive ἐλέφαντος (elephantos[1])), probably from a non-Indo-European language, likely Phoenician.[2] It is attested in Mycenaean Greek as e-re-pa (genitive e-re-pa-to) in Linear B syllabic script.[3][4] As in Mycenaean Greek, Homer used the Greek word to mean ivory, but after the time of Herodotus, it also referred to the animal.[1] The word elephant appears in Middle English as olyfaunt (c. 1300) and was borrowed from Old French oliphant (12th century).[2]\n Orycteropodidae \n Macroscelididae \n Chrysochloridae \n Tenrecidae \n Procaviidae \n Elephantidae \n Dugongidae \n Trichechidae \n Elephants belong to the family Elephantidae, the sole remaining family within the order Proboscidea. Their closest extant relatives are the sirenians (dugongs and manatees) and the hyraxes, with which they share the clade Paenungulata within the superorder Afrotheria.[6] Elephants and sirenians are further grouped in the clade Tethytheria.[7]\n Three species of living elephants are recognised; the African bush elephant (Loxodonta africana), forest elephant (Loxodonta cyclotis) and Asian elephant (Elephas maximus).[8] African elephants were traditionally considered a single species, Loxodonta africana, but molecular studies have affirmed their status as separate species.[9][10][11] Mammoths (Mammuthus) are nested within living elephants as they are more closely related to Asian elephants than to African elephants.[12] Another extinct genus of elephant, Palaeoloxodon, is also recognised, which appears to have close affinities with African elephants and to have hybridised with African forest elephants.[13]\n Over 180 extinct members of order Proboscidea have been described.[14] The earliest proboscideans, the African Eritherium and Phosphatherium are known from the late Paleocene.[15] The Eocene included Numidotherium, Moeritherium and Barytherium from Africa. These animals were relatively small and, some, like Moeritherium and Barytherium were probably amphibious.[16][17] Later on, genera such as Phiomia and Palaeomastodon arose; the latter likely inhabited more forested areas. Proboscidean diversification changed little during the Oligocene.[16] One notable species of this epoch was Eritreum melakeghebrekristosi of the Horn of Africa, which may have been an ancestor to several later species.[18]\n early proboscideans, e.g. Moeritherium \n Deinotheriidae \n Mammutidae \n Gomphotheriidae \n Stegodontidae \n Loxodonta \n Palaeoloxodon \n Mammuthus \n Elephas \n A major event in proboscidean evolution was the collision of Afro-Arabia with Eurasia, during the Early Miocene, around 18–19 million years ago, allowing proboscideans to disperse from their African homeland across Eurasia and later, around 16–15 million years ago into North America across the Bering Land Bridge. Proboscidean groups prominent during the Miocene include the deinotheres, along with the more advanced elephantimorphs, including mammutids (mastodons), gomphotheres, amebelodontids (which includes the \"shovel tuskers\" like Platybelodon), choerolophodontids and stegodontids.[21] Around 10 million years ago, the earliest members of the family Elephantidae emerged in Africa, having originated from gomphotheres.[22]\n Elephantids are distinguished from earlier proboscideans by a major shift in the molar morphology to parallel lophs rather than the cusps of earlier proboscideans, allowing them to become higher-crowned (hypsodont) and more efficient in consuming grass.[23] The Late Miocene saw major climactic changes, which resulted in the decline and extinction of many proboscidean groups.[21] The earliest members of the modern genera of Elephantidae appeared during the latest Miocene–early Pliocene around 5 million years ago. The elephantid genera Elephas (which includes the living Asian elephant) and Mammuthus (mammoths) migrated out of Africa during the late Pliocene, around 3.6 to 3.2 million years ago.[24]\n Over the course of the Early Pleistocene, all non-elephantid probobscidean genera outside of the Americas became extinct with the exception of Stegodon,[21] with gomphotheres dispersing into South America as part of the Great American interchange,[25] and mammoths migrating into North America around 1.5 million years ago.[26] At the end of the Early Pleistocene, around 800,000 years ago the elephantid genus Palaeoloxodon dispersed outside of Africa, becoming widely distributed in Eurasia.[27] Proboscideans were represented by around 23 species at the beginning of the Late Pleistocene. Proboscideans underwent a dramatic decline during the Late Pleistocene as part of the Late Pleistocene extinctions of most large mammals globally, with all remaining non-elephantid proboscideans (including Stegodon, mastodons, and the American gomphotheres Cuvieronius and Notiomastodon) and Palaeoloxodon becoming extinct, with mammoths only surviving in relict populations on islands around the Bering Strait into the Holocene, with their latest survival being on Wrangel Island, where they persisted until around 4,000 years ago.[21][28]\n Over the course of their evolution, probobscideans grew in size. With that came longer limbs and wider feet with a more digitigrade stance, along with a larger head and shorter neck. The trunk evolved and grew longer to provide reach. The number of premolars, incisors, and canines decreased, and the cheek teeth (molars and premolars) became longer and more specialised. The incisors developed into tusks of different shapes and sizes.[29] Several species of proboscideans became isolated on islands and experienced insular dwarfism,[30] some dramatically reducing in body size, such as the 1 metre (3.3 ft) tall dwarf elephant species Palaeoloxodon falconeri.[31]\n Elephants are the largest living terrestrial animals.[36] The skeleton is made up of 326–351 bones.[37] The vertebrae are connected by tight joints, which limit the backbone's flexibility. African elephants have 21 pairs of ribs, while Asian elephants have 19 or 20 pairs.[38] The skull contains air cavities (sinuses) that reduce the weight of the skull while maintaining overall strength. These cavities give the inside of the skull a honeycomb-like appearance. By contrast, the lower jaw is dense. The cranium is particularly large and provides enough room for the attachment of muscles to support the entire head.[37] The skull is built to withstand great stress, particularly when fighting or using the tusks. The brain is surrounded by arches in the skull, which serve as protection.[39] Because of the size of the head, the neck is relatively short to provide better support.[29]\nElephants are homeotherms and maintain their average body temperature at ~ 36 °C (97 °F), with a minimum of 35.2 °C (95.4 °F) during the cool season, and a maximum of 38.0 °C (100.4 °F) during the hot dry season.[40]\n Elephant ear flaps, or pinnae, are 1–2 mm (0.039–0.079 in) thick in the middle with a thinner tip and supported by a thicker base. They contain numerous blood vessels called capillaries. Warm blood flows into the capillaries, releasing excess heat into the environment. This effect is increased by flapping the ears back and forth. Larger ear surfaces contain more capillaries, and more heat can be released. Of all the elephants, African bush elephants live in the hottest climates and have the largest ear flaps.[37][41] The ossicles are adapted for hearing low frequencies, being most sensitive at 1 kHz.[42]\n Lacking a lacrimal apparatus (tear duct), the eye relies on the harderian gland in the orbit to keep it moist. A durable nictitating membrane shields the globe. The animal's field of vision is compromised by the location and limited mobility of the eyes.[43] Elephants are dichromats[44] and they can see well in dim light but not in bright light.[45]\n The elongated and prehensile trunk, or proboscis, consists of both the nose and upper lip, which fuse in early fetal development.[29] This versatile appendage contains up to 150,000 separate muscle fascicles, with no bone and little fat. These paired muscles consist of two major types: superficial (surface) and internal. The former are divided into dorsal, ventral, and lateral muscles, while the latter are divided into transverse and radiating muscles. The muscles of the trunk connect to a bony opening in the skull. The nasal septum consists of small elastic muscles between the nostrils, which are divided by cartilage at the base.[46] A unique proboscis nerve – a combination of the maxillary and facial nerves – lines each side of the appendage.[47]\n As a muscular hydrostat, the trunk moves through finely controlled muscle contractions, working both with and against each other.[47] Using three basic movements: bending, twisting, and longitudinal stretching or retracting, the trunk has near unlimited flexibility. Objects grasped by the end of the trunk can be moved to the mouth by curving the appendage inward. The trunk can also bend at different points by creating stiffened \"pseudo-joints\". The tip can be moved in a way similar to the human hand.[48] The skin is more elastic on the dorsal side of the elephant trunk than underneath; allowing the animal to stretch and coil while maintaining a strong grasp.[49] The African elephants have two finger-like extensions at the tip of the trunk that allow them to pluck small food. The Asian elephant has only one and relies more on wrapping around a food item.[33] Asian elephant trunks have better motor coordination.[46]\n The trunk's extreme flexibility allows it to forage and wrestle other elephants with it. It is powerful enough to lift up to 350 kg (770 lb), but it also has the precision to crack a peanut shell without breaking the seed. With its trunk, an elephant can reach items up to 7 m (23 ft) high and dig for water in the mud or sand below. It also uses it to clean itself.[50] Individuals may show lateral preference when grasping with their trunks: some prefer to twist them to the left, others to the right.[47] Elephant trunks are capable of powerful siphoning. They can expand their nostrils by 30%, leading to a 64% greater nasal volume, and can breathe in almost 30 times faster than a human sneeze, at over 150 m\/s (490 ft\/s).[51] They suck up water, which is squirted into the mouth or over the body.[29][51] The trunk of an adult Asian elephant is capable of retaining 8.5 L (2.2 US gal) of water.[46]  They will also sprinkle dust or grass on themselves.[29] When underwater, the elephant uses its trunk as a snorkel.[52]\n The trunk also acts as a sense organ. Its sense of smell may be four times greater than a bloodhound's nose.[53] The infraorbital nerve, which makes the trunk sensitive to touch, is thicker than both the optic and auditory nerves. Whiskers grow all along the trunk, and are particularly packed at the tip, where they contribute to its tactile sensitivity. Unlike those of many mammals, such as cats and rats, elephant whiskers do not move independently (\"whisk\") to sense the environment; the trunk itself must move to bring the whiskers into contact with nearby objects. Whiskers grow in rows along each side on the ventral surface of the trunk, which is thought to be essential in helping elephants balance objects there, whereas they are more evenly arranged on the dorsal surface. The number and patterns of whiskers are distinctly different between species.[54]\n Damaging the trunk would be detrimental to an elephant's survival,[29] although in rare cases, individuals have survived with shortened ones. One trunkless elephant has been observed to graze using its lips with its hind legs in the air and balancing on its front knees.[46] Floppy trunk syndrome is a condition of trunk paralysis recorded in African bush elephants and involves the degeneration of the peripheral nerves and muscles. The disorder has been linked to lead poisoning.[55]\n Elephants usually have 26 teeth: the incisors, known as the tusks; 12 deciduous premolars; and 12 molars. Unlike most mammals, teeth are not replaced by new ones emerging from the jaws vertically. Instead, new teeth start at the back of the mouth and push out the old ones. The first chewing tooth on each side of the jaw falls out when the elephant is two to three years old. This is followed by four more tooth replacements at the ages of four to six, 9–15, 18–28, and finally in their early 40s. The final (usually sixth) set must last the elephant the rest of its life. Elephant teeth have loop-shaped dental ridges, which are more diamond-shaped in African elephants.[56]\n The tusks of an elephant have modified second incisors in the upper jaw. They replace deciduous milk teeth at 6–12 months of age and keep growing at about 17 cm (7 in) a year. As the tusk develops, it is topped with smooth, cone-shaped enamel that eventually wanes. The dentine is known as ivory and has a cross-section of intersecting lines, known as \"engine turning\", which create diamond-shaped patterns. Being living tissue, tusks are fairly soft and about as dense as the mineral calcite. The tusk protrudes from a socket in the skull, and most of it is external. At least one-third of the tusk contains the pulp, and some have nerves that stretch even further. Thus, it would be difficult to remove it without harming the animal. When removed, ivory will dry up and crack if not kept cool and wet. Tusks function in digging, debarking, marking, moving objects, and fighting.[57]\n Elephants are usually right- or left-tusked, similar to humans, who are typically right- or left-handed. The dominant, or \"master\" tusk, is typically more worn down, as it is shorter and blunter. For African elephants, tusks are present in both males and females, and are around the same length in both sexes, reaching up to 300 cm (9 ft 10 in),[57] but those of males tend to be more massive.[58] In the Asian species, only the males have large tusks. Female Asians have very small tusks, or none at all.[57] Tuskless males exist and are particularly common among Sri Lankan elephants.[59] Asian males can have tusks as long as Africans', but they are usually slimmer and lighter; the largest recorded was 302 cm (9 ft 11 in) long and weighed 39 kg (86 lb). Hunting for elephant ivory in Africa[60] and Asia[61] has led to natural selection for shorter tusks[62][63] and tusklessness.[64][65]\n An elephant's skin is generally very tough, at 2.5 cm (1 in) thick on the back and parts of the head. The skin around the mouth, anus, and inside of the ear is considerably thinner. Elephants are typically grey, but African elephants look brown or reddish after rolling in coloured mud. Asian elephants have some patches of depigmentation, particularly on the head. Calves have brownish or reddish hair, with the head and back being particularly hairy. As elephants mature, their hair darkens and becomes sparser, but dense concentrations of hair and bristles remain on the tip of the tail and parts of the head and genitals. Normally, the skin of an Asian elephant is covered with more hair than its African counterpart.[66] Their hair is thought to help them lose heat in their hot environments.[67]\n Although tough, an elephant's skin is very sensitive and requires mud baths to maintain moisture and protection from burning and insect bites. After bathing, the elephant will usually use its trunk to blow dust onto its body, which dries into a protective crust. Elephants have difficulty releasing heat through the skin because of their low surface-area-to-volume ratio, which is many times smaller than that of a human. They have even been observed lifting up their legs to expose their soles to the air.[66] Elephants only have sweat glands between the toes,[68] but the skin allows water to disperse and evaporate, cooling the animal.[69][70] In addition, cracks in the skin may reduce dehydration and allow for increased thermal regulation in the long term.[71]\n To support the animal's weight, an elephant's limbs are positioned more vertically under the body than in most other mammals. The long bones of the limbs have cancellous bones in place of medullary cavities. This strengthens the bones while still allowing haematopoiesis (blood cell creation).[72] Both the front and hind limbs can support an elephant's weight, although 60% is borne by the front.[73] The position of the limbs and leg bones allows an elephant to stand still for extended periods of time without tiring. Elephants are incapable of turning their manus as the ulna and radius of the front legs are secured in pronation.[72] Elephants may also lack the pronator quadratus and pronator teres muscles or have very small ones.[74] The circular feet of an elephant have soft tissues, or \"cushion pads\" beneath the manus or pes, which allow them to bear the animal's great mass.[73] They appear to have a sesamoid, an extra \"toe\" similar in placement to a giant panda's extra \"thumb\", that also helps in weight distribution.[75] As many as five toenails can be found on both the front and hind feet.[33]\n Elephants can move both forward and backward, but are incapable of trotting, jumping, or galloping. They can move on land only by walking or ambling: a faster gait similar to running.[72][76] In walking, the legs act as pendulums, with the hips and shoulders moving up and down while the foot is planted on the ground. The fast gait does not meet all the criteria of running, since there is no point where all the feet are off the ground, although the elephant uses its legs much like other running animals, and can move faster by quickening its stride. Fast-moving elephants appear to 'run' with their front legs, but 'walk' with their hind legs and can reach a top speed of 25 km\/h (16 mph). At this speed, most other quadrupeds are well into a gallop, even accounting for leg length. Spring-like kinetics could explain the difference between the motion of elephants and other animals.[76][77] The cushion pads expand and contract, and reduce both the pain and noise that would come from a very heavy animal moving.[73] Elephants are capable swimmers: they can swim for up to six hours while completely waterborne, moving at 2.1 km\/h (1 mph) and traversing up to 48 km (30 mi) continuously.[78]\n The brain of an elephant weighs 4.5–5.5 kg (10–12 lb) compared to 1.6 kg (4 lb) for a human brain.[79] It is the largest of all terrestrial mammals.[80] While the elephant brain is larger overall, it is proportionally smaller than the human brain. At birth, an elephant's brain already weighs 30–40% of its adult weight. The cerebrum and cerebellum are well developed, and the temporal lobes are so large that they bulge out laterally.[79] Their temporal lobes are proportionally larger than those of other animals, including humans.[80] The throat of an elephant appears to contain a pouch where it can store water for later use.[29] The larynx of the elephant is the largest known among mammals. The vocal folds are anchored close to the epiglottis base. When comparing an elephant's vocal folds to those of a human, an elephant's are proportionally longer, thicker, with a greater cross-sectional area. In addition, they are located further up the vocal tract with an acute slope.[81]\n The heart of an elephant weighs 12–21 kg (26–46 lb). Its apex has two pointed ends, an unusual trait among mammals.[79] In addition, the ventricles of the heart split towards the top, a trait also found in sirenians.[82] When upright, the elephant's heart beats around 28 beats per minute and actually speeds up to 35 beats when it lies down.[79] The blood vessels are thick and wide and can hold up under high blood pressure.[82] The lungs are attached to the diaphragm, and breathing relies less on the expanding of the ribcage.[79] Connective tissue exists in place of the pleural cavity. This may allow the animal to deal with the pressure differences when its body is underwater and its trunk is breaking the surface for air.[52] Elephants breathe mostly with the trunk but also with the mouth. They have a hindgut fermentation system, and their large and small intestines together reach 35 m (115 ft) in length. Less than half of an elephant's food intake gets digested, despite the process lasting a day.[79] An elephant's kidneys can produce more than 50 litres of urine per day.[83]\n A male elephant's testes, like other Afrotheria,[84] are internally located near the kidneys.[85] The penis can be as long as 100 cm (39 in) with a 16 cm (6 in) wide base. It curves to an 'S' when fully erect and has an orifice shaped like a Y. The female's clitoris may be 40 cm (16 in). The vulva is found lower than in other herbivores, between the hind legs instead of under the tail. Determining pregnancy status can be difficult due to the animal's large belly. The female's mammary glands occupy the space between the front legs, which puts the suckling calf within reach of the female's trunk.[79] Elephants have a unique organ, the temporal gland, located on both sides of the head. This organ is associated with sexual behaviour, and males secrete a fluid from it when in musth.[86] Females have also been observed with these secretions.[53]\n Elephants are herbivorous and will eat leaves, twigs, fruit, bark, grass, and roots. African elephants mostly browse, while Asian elephants mainly graze.[34] They can eat as much as 300 kg (660 lb) of food and drink 40 L (11 US gal) of water in a day. Elephants tend to stay near water sources.[34][87] They have morning, afternoon, and nighttime feeding sessions. At midday, elephants rest under trees and may doze off while standing. Sleeping occurs at night while the animal is lying down.[87] Elephants average 3–4 hours of sleep per day.[88] Both males and family groups typically move no more than 20 km (12 mi) a day, but distances as far as 180 km (112 mi) have been recorded in the Etosha region of Namibia.[89] Elephants go on seasonal migrations in response to changes in environmental conditions.[90] In northern Botswana, they travel 325 km (202 mi) to the Chobe River after the local waterholes dry up in late August.[91]\n Because of their large size, elephants have a huge impact on their environments and are considered keystone species. Their habit of uprooting trees and undergrowth can transform savannah into grasslands;[92] smaller herbivores can access trees mowed down by elephants.[87] When they dig for water during droughts, they create waterholes that can be used by other animals. When they use waterholes, they end up making them bigger.[92] At Mount Elgon, elephants dig through caves and pave the way for ungulates, hyraxes, bats, birds and insects.[92] Elephants are important seed dispersers; African forest elephants consume and deposit many seeds over great distances, with either no effect or a positive effect on germination.[93] In Asian forests, large seeds require giant herbivores like elephants and rhinoceros for transport and dispersal. This ecological niche cannot be filled by the smaller Malayan tapir.[94] Because most of the food elephants eat goes undigested, their dung can provide food for other animals, such as dung beetles and monkeys.[92] Elephants can have a negative impact on ecosystems. At Murchison Falls National Park in Uganda, elephant numbers have threatened several species of small birds that depend on woodlands. Their weight causes the soil to compress, leading to runoff and erosion.[87]\n Elephants typically coexist peacefully with other herbivores, which will usually stay out of their way. Some aggressive interactions between elephants and rhinoceros have been recorded.[87] The size of adult elephants makes them nearly invulnerable to predators.[35] Calves may be preyed on by lions, spotted hyenas, and wild dogs in Africa[95] and tigers in Asia.[35] The lions of Savuti, Botswana, have adapted to hunting elephants, mostly calves, juveniles or even sub-adults.[96][97] There are rare reports of adult Asian elephants falling prey to tigers.[98] Elephants tend to have high numbers of parasites, particularly nematodes, compared to many other mammals. This is due to them being largely immune to predators, which would otherwise kill off many of the individuals with significant parasite loads.[99]\n Elephants are generally gregarious animals. African bush elephants in particular have a complex, stratified social structure.[100] Female elephants spend their entire lives in tight-knit matrilineal family groups.[101] They are led by the matriarch, who is often the eldest female.[102] She remains leader of the group until death[95] or if she no longer has the energy for the role;[103] a study on zoo elephants found that the death of the matriarch led to greater stress in the surviving elephants.[104] When her tenure is over, the matriarch's eldest daughter takes her place instead of her sister (if present).[95] One study found that younger matriarchs take potential threats less seriously.[105] Large family groups may split if they cannot be supported by local resources.[106]\n At Amboseli National Park, Kenya, female groups may consist of around ten members, including four adults and their dependent offspring. Here, a cow's life involves interaction with those outside her group. Two separate families may associate and bond with each other, forming what are known as bond groups. During the dry season, elephant families may aggregate into clans. These may number around nine groups, in which clans do not form strong bonds but defend their dry-season ranges against other clans. The Amboseli elephant population is further divided into the \"central\" and \"peripheral\" subpopulations.[101]\n Female Asian elephants tend to have more fluid social associations.[100] In Sri Lanka, there appear to be stable family units or \"herds\" and larger, looser \"groups\". They have been observed to have \"nursing units\" and \"juvenile-care units\". In southern India, elephant populations may contain family groups, bond groups and possibly clans. Family groups tend to be small, with only one or two adult females and their offspring. A group containing more than two cows and their offspring is known as a \"joint family\". Malay elephant populations have even smaller family units and do not reach levels above a bond group. Groups of African forest elephants typically consist of one cow with one to three offspring. These groups appear to interact with each other, especially at forest clearings.[101]\n Adult males live separate lives. As he matures, a bull associates more with outside males or even other families. At Amboseli, young males may be away from their families 80% of the time by 14–15 years of age. When males permanently leave, they either live alone or with other males. The former is typical of bulls in dense forests. A dominance hierarchy exists among males, whether they are social or solitary. Dominance depends on age, size, and sexual condition.[107] Male elephants can be quite sociable when not competing for mates and form vast and fluid social networks.[108][109] Older bulls act as the leaders of these groups.[110] The presence of older males appears to subdue the aggression and \"deviant\" behaviour of younger ones.[111] The largest all-male groups can reach close to 150 individuals. Adult males and females come together to breed. Bulls will accompany family groups if a cow is in oestrous.[107]\n Adult males enter a state of increased testosterone known as musth. In a population in southern India, males first enter musth at 15 years old, but it is not very intense until they are older than 25. At Amboseli, no bulls under 24 were found to be in musth, while half of those aged 25–35 and all those over 35 were. In some areas, there may be seasonal influences on the timing of musths. The main characteristic of a bull's musth is a fluid discharged from the temporal gland that runs down the side of his face. Behaviours associated with musth include walking with a high and swinging head, nonsynchronous ear flapping, picking at the ground with the tusks, marking, rumbling, and urinating in the sheath. The length of this varies between males of different ages and conditions, lasting from days to months.[112]\n Males become extremely aggressive during musth. Size is the determining factor in agonistic encounters when the individuals have the same condition. In contests between musth and non-musth individuals, musth bulls win the majority of the time, even when the non-musth bull is larger. A male may stop showing signs of musth when he encounters a musth male of higher rank. Those of equal rank tend to avoid each other. Agonistic encounters typically consist of threat displays, chases, and minor sparring. Rarely do they full-on fight.[112]\n Elephants are polygynous breeders,[113] and most copulations occur during rainfall.[114] An oestrous cow uses pheromones in her urine and vaginal secretions to signal her readiness to mate. A bull will follow a potential mate and assess her condition with the flehmen response, which requires him to collect a chemical sample with his trunk and taste it with the vomeronasal organ at the roof of the mouth.[115] The oestrous cycle of a cow lasts 14–16 weeks, with the follicular phase lasting 4–6 weeks and the luteal phase lasting 8–10 weeks. While most mammals have one surge of luteinizing hormone during the follicular phase, elephants have two. The first (or anovulatory) surge, appears to change the female's scent, signaling to males that she is in heat, but ovulation does not occur until the second (or ovulatory) surge.[116] Cows over 45–50 years of age are less fertile.[103]\n Bulls engage in a behaviour known as mate-guarding, where they follow oestrous females and defend them from other males.[117] Most mate-guarding is done by musth males, and females seek them out, particularly older ones.[118] Musth appears to signal to females the condition of the male, as weak or injured males do not have normal musths.[119] For young females, the approach of an older bull can be intimidating, so her relatives stay nearby for comfort.[120] During copulation, the male rests his trunk on the female.[121] The penis is mobile enough to move without the pelvis.[83] Before mounting, it curves forward and upward. Copulation lasts about 45 seconds and does not involve pelvic thrusting or an ejaculatory pause.[122]\n Homosexual behaviour is frequent in both sexes. As in heterosexual interactions, this involves mounting. Male elephants sometimes stimulate each other by playfighting, and \"championships\" may form between old bulls and younger males. Female same-sex behaviours have been documented only in captivity, where they engage in mutual masturbation with their trunks.[123]\n Gestation in elephants typically lasts between one and a half and two years and the female will not give birth again for at least four years.[124] The relatively long pregnancy is supported by several corpus luteums and gives the foetus more time to develop, particularly the brain and trunk.[125] Births tend to take place during the wet season.[114] Typically, only a single young is born, but twins sometimes occur.[125] Calves are born roughly 85 cm (33 in) tall and with a weight of around 120 kg (260 lb).[120] They are precocial and quickly stand and walk to follow their mother and family herd.[126] A newborn calf will attract the attention of all the herd members. Adults and most of the other young will gather around the newborn, touching and caressing it with their trunks. For the first few days, the mother limits access to her young. Alloparenting – where a calf is cared for by someone other than its mother – takes place in some family groups. Allomothers are typically aged two to twelve years.[120]\n For the first few days, the newborn is unsteady on its feet and needs its mother's help. It relies on touch, smell, and hearing, as its eyesight is less developed. With little coordination in its trunk, it can only flop it around which may cause it to trip. When it reaches its second week, the calf can walk with more balance and has more control over its trunk. After its first month, the trunk can grab and hold objects, but still lacks sucking abilities, and the calf must bend down to drink. It continues to stay near its mother as it is still reliant on her. For its first three months, a calf relies entirely on its mother's milk, after which it begins to forage for vegetation and can use its trunk to collect water. At the same time, there is progress in lip and leg movements. By nine months, mouth, trunk and foot coordination are mastered. Suckling bouts tend to last 2–4 min\/hr for a calf younger than a year. After a year, a calf is fully capable of grooming, drinking, and feeding itself. It still needs its mother's milk and protection until it is at least two years old. Suckling after two years may improve growth, health and fertility.[126]\n Play behaviour in calves differs between the sexes; females run or chase each other while males play-fight. The former are sexually mature by the age of nine years[120] while the latter become mature around 14–15 years.[107] Adulthood starts at about 18 years of age in both sexes.[127][128] Elephants have long lifespans, reaching 60–70 years of age.[56] Lin Wang, a captive male Asian elephant, lived for 86 years.[129]\n Elephants communicate in various ways. Individuals greet one another by touching each other on the mouth, temporal glands and genitals. This allows them to pick up chemical cues. Older elephants use trunk-slaps, kicks, and shoves to control younger ones. Touching is especially important for mother–calf communication. When moving, elephant mothers will touch their calves with their trunks or feet when side-by-side or with their tails if the calf is behind them. A calf will press against its mother's front legs to signal it wants to rest and will touch her breast or leg when it wants to suckle.[130]\n Visual displays mostly occur in agonistic situations. Elephants will try to appear more threatening by raising their heads and spreading their ears. They may add to the display by shaking their heads and snapping their ears, as well as tossing around dust and vegetation. They are usually bluffing when performing these actions. Excited elephants also raise their heads and spread their ears but additionally may raise their trunks. Submissive elephants will lower their heads and trunks, as well as flatten their ears against their necks, while those that are ready to fight will bend their ears in a V shape.[131]\n Elephants produce several vocalisations—some of which pass though the trunk[132]—for both short and long range communication. This includes trumpeting, bellowing, roaring, growling, barking, snorting, and rumbling.[132][133] Elephants can produce infrasonic rumbles.[134] For Asian elephants, these calls have a frequency of 14–24 Hz, with sound pressure levels of 85–90 dB and last 10–15 seconds.[135] For African elephants, calls range from 15 to 35 Hz with sound pressure levels as high as 117 dB, allowing communication for many kilometres, possibly over 10 km (6 mi).[136] Elephants are known to communicate with seismics, vibrations produced by impacts on the earth's surface or acoustical waves that travel through it. An individual foot stomping or mock charging can create seismic signals that can be heard at travel distances of up to 32 km (20 mi). Seismic waveforms produced by rumbles travel 16 km (10 mi).[137][138]\n Elephants are among the most intelligent animals. They exhibit mirror self-recognition, an indication of self-awareness and cognition that has also been demonstrated in some apes and dolphins.[139] One study of a captive female Asian elephant suggested the animal was capable of learning and distinguishing between several visual and some acoustic discrimination pairs. This individual was even able to score a high accuracy rating when re-tested with the same visual pairs a year later.[140] Elephants are among the species known to use tools. An Asian elephant has been observed fine-tuning branches for use as flyswatters.[141] Tool modification by these animals is not as advanced as that of chimpanzees. Elephants are popularly thought of as having an excellent memory. This could have a factual basis; they possibly have cognitive maps which give them long lasting memories of their environment on a wide scale. Individuals may be able to remember where their family members are located.[45]\n Scientists debate the extent to which elephants feel emotion. They are attracted to the bones of their own kind, regardless of whether they are related.[142] As with chimpanzees and dolphins, a dying or dead elephant may elicit attention and aid from others, including those from other groups. This has been interpreted as expressing \"concern\";[143] however, the Oxford Companion to Animal Behaviour (1987) said that \"one is well advised to study the behaviour rather than attempting to get at any underlying emotion\".[144]\n African bush elephants were listed as Endangered by the International Union for Conservation of Nature (IUCN) in 2021,[145] and African forest elephants were listed as Critically Endangered in the same year.[146] In 1979, Africa had an estimated population of at least 1.3 million elephants, possibly as high as 3.0 million. A decade later, the population was estimated to be 609,000; with 277,000 in Central Africa, 110,000 in Eastern Africa, 204,000 in Southern Africa, and 19,000 in Western Africa. The population of rainforest elephants was lower than anticipated, at around 214,000 individuals. Between 1977 and 1989, elephant populations declined by 74% in East Africa. After 1987, losses in elephant numbers hastened, and savannah populations from Cameroon to Somalia experienced a decline of 80%. African forest elephants had a total loss of 43%. Population trends in southern Africa were various, with unconfirmed losses in Zambia, Mozambique and Angola while populations grew in Botswana and Zimbabwe and were stable in South Africa.[147] The IUCN estimated that total population in Africa is estimated at to 415,000 individuals for both species combined as of 2016.[148]\n African elephants receive at least some legal protection in every country where they are found. Successful conservation efforts in certain areas have led to high population densities while failures have led to declines as high as 70% or more of the course of ten years. As of 2008, local numbers were controlled by contraception or translocation. Large-scale cullings stopped in the late 1980s and early 1990s. In 1989, the African elephant was listed under Appendix I by the Convention on International Trade in Endangered Species of Wild Fauna and Flora (CITES), making trade illegal. Appendix II status (which allows restricted trade) was given to elephants in Botswana, Namibia, and Zimbabwe in 1997 and South Africa in 2000. In some countries, sport hunting of the animals is legal; Botswana, Cameroon, Gabon, Mozambique, Namibia, South Africa, Tanzania, Zambia, and Zimbabwe have CITES export quotas for elephant trophies.[145]\n In 2020, the IUCN listed the Asian elephant as endangered due to the population declining by half over \"the last three generations\".[149] Asian elephants once ranged from Western to East Asia and south to Sumatra.[150] and Java. It is now extinct in these areas,[149] and the current range of Asian elephants is highly fragmented.[150] The total population of Asian elephants is estimated to be around 40,000–50,000, although this may be a loose estimate. Around 60% of the population is in India. Although Asian elephants are declining in numbers overall, particularly in Southeast Asia, the population in Sri Lanka appears to have risen and elephant numbers in the Western Ghats may have stabilised.[149]\n The poaching of elephants for their ivory, meat and hides has been one of the major threats to their existence.[149] Historically, numerous cultures made ornaments and other works of art from elephant ivory, and its use was comparable to that of gold.[151] The ivory trade contributed to the fall of the African elephant population in the late 20th century.[145] This prompted international bans on ivory imports, starting with the United States in June 1989, and followed by bans in other North American countries, western European countries, and Japan.[151] Around the same time, Kenya destroyed all its ivory stocks.[152] Ivory was banned internationally by CITES in 1990. Following the bans, unemployment rose in India and China, where the ivory industry was important economically. By contrast, Japan and Hong Kong, which were also part of the industry, were able to adapt and were not as badly affected.[151] Zimbabwe, Botswana, Namibia, Zambia, and Malawi wanted to continue the ivory trade and were allowed to, since their local populations were healthy, but only if their supplies were from culled individuals or those that died of natural causes.[152]\n The ban allowed the elephant to recover in parts of Africa.[151] In February 2012, 650 elephants in Bouba Njida National Park, Cameroon, were slaughtered by Chadian raiders.[153] This has been called \"one of the worst concentrated killings\" since the ivory ban.[152] Asian elephants are potentially less vulnerable to the ivory trade, as females usually lack tusks. Still, members of the species have been killed for their ivory in some areas, such as Periyar National Park in India.[149] China was the biggest market for poached ivory but announced they would phase out the legal domestic manufacture and sale of ivory products in May 2015, and in September 2015, China and the United States said \"they would enact a nearly complete ban on the import and export of ivory\" due to causes of extinction.[154]\n Other threats to elephants include habitat destruction and fragmentation. The Asian elephant lives in areas with some of the highest human populations and may be confined to small islands of forest among human-dominated landscapes. Elephants commonly trample and consume crops, which contributes to conflicts with humans, and both elephants and humans have died by the hundreds as a result. Mitigating these conflicts is important for conservation. One proposed solution is the protection of wildlife corridors which give populations greater interconnectivity and space.[149] Chili pepper products as well as guarding with defense tools have been found to be effective in preventing crop-raiding by elephants. Less effective tactics include beehive and electric fences.[155]\n Elephants have been working animals since at least the Indus Valley civilization over 4,000 years ago[156] and continue to be used in modern times. There were 13,000–16,500 working elephants employed in Asia in 2000. These animals are typically captured from the wild when they are 10–20 years old when they are both more trainable and can work for more years.[157] They were traditionally captured with traps and lassos, but since 1950, tranquillisers have been used.[158] Individuals of the Asian species have been often trained as working animals. Asian elephants are used to carry and pull both objects and people in and out of areas as well as lead people in religious celebrations. They are valued over mechanised tools as they can perform the same tasks but in more difficult terrain, with strength, memory, and delicacy. Elephants can learn over 30 commands.[157] Musth bulls are difficult and dangerous to work with and so are chained up until their condition passes.[159]\n In India, many working elephants are alleged to have been subject to abuse. They and other captive elephants are thus protected under The Prevention of Cruelty to Animals Act of 1960.[160] In both Myanmar and Thailand, deforestation and other economic factors have resulted in sizable populations of unemployed elephants resulting in health problems for the elephants themselves as well as economic and safety problems for the people amongst whom they live.[161][162]\n The practice of working elephants has also been attempted in Africa. The taming of African elephants in the Belgian Congo began by decree of Leopold II of Belgium during the 19th century and continues to the present with the Api Elephant Domestication Centre.[163]\n Historically, elephants were considered formidable instruments of war. They were described in Sanskrit texts as far back as 1500 BC. From South Asia, the use of elephants in warfare spread west to Persia[164] and east to Southeast Asia.[165] The Persians used them during the Achaemenid Empire (between the 6th and 4th centuries BC)[164] while Southeast Asian states first used war elephants possibly as early as the 5th century BC and continued to the 20th century.[165] War elephants were also employed in the Mediterranean and North Africa throughout the classical period since the reign of Ptolemy II in Egypt. The Carthaginian general Hannibal famously took African elephants across the Alps during his war with the Romans and reached the Po Valley in 218 BC with all of them alive, but died of disease and combat a year later.[164]\n An elephant's head and sides were equipped with armour, the trunk may have had a sword tied to it and tusks were sometimes covered with sharpened iron or brass. Trained elephants would attack both humans and horses with their tusks. They might have grasped an enemy soldier with the trunk and tossed him to their mahout, or pinned the soldier to the ground and speared him. Some shortcomings of war elephants included their great visibility, which made them easy to target, and limited maneuverability compared to horses. Alexander the Great achieved victory over armies with war elephants by having his soldiers injure the trunks and legs of the animals which caused them to panic and become uncontrollable.[164]\n Elephants have traditionally been a major part of zoos and circuses around the world. In circuses, they are trained to perform tricks. The most famous circus elephant was probably Jumbo (1861 – 15 September 1885), who was a major attraction in the Barnum & Bailey Circus.[166][167] These animals do not reproduce well in captivity due to the difficulty of handling musth bulls and limited understanding of female oestrous cycles. Asian elephants were always more common than their African counterparts in modern zoos and circuses. After CITES listed the Asian elephant under Appendix I in 1975, imports of the species almost stopped by the end of the 1980s. Subsequently, the US received many captive African elephants from Zimbabwe, which had an overabundance of the animals.[167]\n Keeping elephants in zoos has met with some controversy. Proponents of zoos argue that they allow easy access to the animals and provide fund and knowledge for preserving their natural habitats, as well as safekeeping for the species. Opponents claim that animals in zoos are under physical and mental stress.[168] Elephants have been recorded displaying stereotypical behaviours in the form of wobbling the body or head and pacing the same route both forwards and backwards. This has been observed in 54% of individuals in UK zoos.[169] Elephants in European zoos appear to have shorter lifespans than their wild counterparts at only 17 years, although other studies suggest that zoo elephants live just as long.[170]\n The use of elephants in circuses has also been controversial; the Humane Society of the United States has accused circuses of mistreating and distressing their animals.[171] In testimony to a US federal court in 2009, Barnum & Bailey Circus CEO Kenneth Feld acknowledged that circus elephants are struck behind their ears, under their chins and on their legs with metal-tipped prods, called bull hooks or ankus. Feld stated that these practices are necessary to protect circus workers and acknowledged that an elephant trainer was rebuked for using an electric prod on an elephant. Despite this, he denied that any of these practices hurt the animals.[172] Some trainers have tried to train elephants without the use of physical punishment. Ralph Helfer is known to have relied on positive reinforcement when training his animals.[173] Barnum and Bailey circus retired its touring elephants in May 2016.[174]\n Elephants can exhibit bouts of aggressive behaviour and engage in destructive actions against humans.[175] In Africa, groups of adolescent elephants damaged homes in villages after cullings in the 1970s and 1980s. Because of the timing, these attacks have been interpreted as vindictive.[176][177] In parts of India, male elephants have entered villages at night, destroying homes and killing people. From 2000 to 2004, 300 people died in Jharkhand, and in Assam, 239 people were reportedly killed between 2001 and 2006.[175]\nThroughout the country, 1,500 people were killed by elephants between 2019 and 2022, which led to 300 elephants being killed in kind.[178] Local people have reported their belief that some elephants were drunk during their attacks, though officials have disputed this.[179][180] Purportedly drunk elephants attacked an Indian village in December 2002, killing six people, which led to the retaliatory slaughter of about 200 elephants by locals.[181]\n Elephants have a universal presence in global culture. They have been represented in art since Paleolithic times. Africa, in particular, contains many examples of elephant rock art, especially in the Sahara and southern Africa.[182] In Asia, the animals are depicted as motifs in Hindu and Buddhist shrines and temples.[183] Elephants were often difficult to portray by people with no first-hand experience of them.[184] The ancient Romans, who kept the animals in captivity, depicted elephants more accurately than medieval Europeans who portrayed them more like fantasy creatures, with horse, bovine and boar-like traits, and trumpet-like trunks. As Europeans gained more access to captive elephants during the 15th century, depictions of them became more accurate, including one made by Leonardo da Vinci.[185]\n Elephants have been the subject of religious beliefs. The Mbuti people of central Africa believe that the souls of their dead ancestors resided in elephants.[183] Similar ideas existed among other African societies, who believed that their chiefs would be reincarnated as elephants. During the 10th century AD, the people of Igbo-Ukwu, in modern-day Nigeria, placed elephant tusks underneath their death leader's feet in the grave.[186] The animals' importance is only totemic in Africa but is much more significant in Asia.[187] In Sumatra, elephants have been associated with lightning. Likewise in Hinduism, they are linked with thunderstorms as Airavata, the father of all elephants, represents both lightning and rainbows.[183] One of the most important Hindu deities, the elephant-headed Ganesha, is ranked equal with the supreme gods Shiva, Vishnu, and Brahma in some traditions.[188] Ganesha is associated with writers and merchants and it is believed that he can give people success as well as grant them their desires, but could also take these things away.[183] In Buddhism, Buddha is said to have been a white elephant reincarnated as a human.[189]\n In Western popular culture, elephants symbolise the exotic, especially since – as with the giraffe, hippopotamus and rhinoceros – there are no similar animals familiar to Western audiences. As characters, elephants are most common in children's stories, where they are portrayed positively. They are typically surrogates for humans with ideal human values. Many stories tell of isolated young elephants returning to or finding a family, such as \"The Elephant's Child\" from Rudyard Kipling's Just So Stories, Disney's Dumbo, and Kathryn and Byron Jackson's The Saggy Baggy Elephant. Other elephant heroes given human qualities include Jean de Brunhoff's Babar, David McKee's Elmer, and Dr. Seuss's Horton.[190]\n Several cultural references emphasise the elephant's size and strangeness. For instance, a \"white elephant\" is a byword for something that is weird, unwanted, and has no value.[190] The expression \"elephant in the room\" refers to something that is being ignored but ultimately must be addressed.[191] The story of the blind men and an elephant involves blind men touching different parts of an elephant and trying to figure out what it is.[192]\n \n"}
{"key":"Elephant","link":"https:\/\/en.wikipedia.org\/wiki\/Elephantidae","headline":"Elephantidae - Wikipedia","content":"\n Elephantidae is a family of large, herbivorous proboscidean mammals collectively called elephants and mammoths. These are large terrestrial mammals with a snout modified into a trunk and teeth modified into tusks. Most genera and species in the family are extinct. Only two genera, Loxodonta (African elephants) and Elephas (Asian elephants), are living.\n The family was first described by John Edward Gray in 1821,[5] and later assigned to taxonomic ranks within the order Proboscidea. Elephantidae has been revised by various authors to include or exclude other extinct proboscidean genera.\n Elephantids are distinguished from more primitive proboscideans like gomphotheres by their teeth, which have parallel lophs, formed from the merger of the cusps found in the teeth of more primitive proboscideans, which are bound by cement.[6] In later elephantids, these lophs became narrow lamellae,[7] with the number of lophs\/lamellae per tooth, as well as the tooth crown height (hypsodonty) increasing over time.[8] Elephantids chew using a proal jaw movement involving a forward stroke of the lower jaws, different from the oblique movement using side to side motion of the jaws in more primitive proboscideans.[9] The most primitive elephantid Stegotetrabelodon had a long lower jaw with lower tusks and retained permanent premolars similar to many gomphotheres, while modern elephantids lack permanent premolars, with the lower jaw being shortened (brevirostrine) and lower tusks being absent.[8]\n Some authors have suggested to classify the family into two subfamilies, Stegotetrabelodontinae, which is monotypic, only containing Stegotetrabelodon, and Elephantinae, containing all other elephantids.[8] Recent genetic research has indicated that Elephas and Mammuthus are more closely related to each other than to Loxodonta, with Palaeoloxodon closely related to Loxodonta. Palaeoloxodon also appears to have received extensive hybridisation with the African forest elephant, and to a lesser extent with mammoths.[10]\n Elephantids are thought to have evolved from gomphotheres, with some authors proposing the most likely ancestors to be African species of the \"tetralophodont gomphothere\" Tetralophodon.[11] The earliest members of the family, are known from the Late Miocene, around 9–10 million years ago.[12] The modern genera of elephants and mammoths had diverged from each other by the end of the Miocene, around 5 million years ago. Elephantids began to migrate out of Africa during the Pliocene, with mammoths and Elephas arriving in Eurasia around 3–3.8 million years ago.[13] Around 1.5 million years ago, mammoths migrated into North America.[14] At the end of the Early Pleistocene, around 0.8 million years ago, Palaeoloxodon migrated out of Africa, becoming widespread across Eurasia, from Western Europe to Japan.[15] Palaeoloxodon and Mammuthus became extinct during the Late Pleistocene-Holocene, with the last population of mammoths persisting on Wrangel Island until around 4,000 years ago.[16]\n \n"}
{"key":"Horse","link":"https:\/\/en.wikipedia.org\/wiki\/Horse","headline":"Horse - Wikipedia","content":"\n at least 48 published\n The horse (Equus ferus caballus)[2][3] is a domesticated, one-toed, hoofed mammal. It belongs to the taxonomic family Equidae and is one of two extant subspecies of Equus ferus. The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, close to Eohippus, into the large, single-toed animal of today. Humans began domesticating horses around 4000 BCE, and their domestication is believed to have been widespread by 3000 BCE. Horses in the subspecies caballus are domesticated, although some domesticated populations live in the wild as feral horses. These feral populations are not true wild horses, which are horses that never have been domesticated and historically linked to the megafauna category of species. There is an extensive, specialized vocabulary used to describe equine-related concepts, covering everything from anatomy to life stages, size, colors, markings, breeds, locomotion, and behavior.\n Horses are adapted to run, allowing them to quickly escape predators, and possess an excellent sense of balance and a strong fight-or-flight response. Related to this need to flee from predators in the wild is an unusual trait: horses are able to sleep both standing up and lying down, with younger horses tending to sleep significantly more than adults.[4] Female horses, called mares, carry their young for approximately 11 months and a young horse, called a foal, can stand and run shortly following birth. Most domesticated horses begin training under a saddle or in a harness between the ages of two and four. They reach full adult development by age five, and have an average lifespan of between 25 and 30 years.\n Horse breeds are loosely divided into three categories based on general temperament: spirited \"hot bloods\" with speed and endurance; \"cold bloods\", such as draft horses and some ponies, suitable for slow, heavy work; and \"warmbloods\", developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. There are more than 300 breeds of horse in the world today, developed for many different uses.\n Horses and humans interact in a wide variety of sport competitions and non-competitive recreational pursuits as well as in working activities such as police work, agriculture, entertainment, and therapy. Horses were historically used in warfare, from which a wide variety of riding and driving techniques developed, using many different styles of equipment and methods of control. Many products are derived from horses, including meat, milk, hide, hair, bone, and pharmaceuticals extracted from the urine of pregnant mares. Humans provide domesticated horses with food, water, and shelter, as well as attention from specialists such as veterinarians and farriers.\n Specific terms and specialized language are used to describe equine anatomy, different life stages, colors, and breeds.\n Depending on breed, management and environment, the modern domestic horse has a life expectancy of 25 to 30 years.[7] Uncommonly, a few animals live into their 40s and, occasionally, beyond.[8] The oldest verifiable record was \"Old Billy\", a 19th-century horse that lived to the age of 62.[7] In modern times, Sugar Puff, who had been listed in Guinness World Records as the world's oldest living pony, died in 2007 at age 56.[9]\n Regardless of a horse or pony's actual birth date, for most competition purposes a year is added to its age each January 1 of each year in the Northern Hemisphere[7][10] and each August 1 in the Southern Hemisphere.[11] The exception is in endurance riding, where the minimum age to compete is based on the animal's actual calendar age.[12]\n The following terminology is used to describe horses of various ages:\n In horse racing, these definitions may differ: For example, in the British Isles, Thoroughbred horse racing defines colts and fillies as less than five years old.[21] However, Australian Thoroughbred racing defines colts and fillies as less than four years old.[22]\n The height of horses is measured at the highest point of the withers, where the neck meets the back.[23] This point is used because it is a stable point of the anatomy, unlike the head or neck, which move up and down in relation to the body of the horse.\n In English-speaking countries, the height of horses is often stated in units of hands and inches: one hand is equal to 4 inches (101.6 mm). The height is expressed as the number of full hands, followed by a point, then the number of additional inches, and ending with the abbreviation \"h\" or \"hh\" (for \"hands high\"). Thus, a horse described as \"15.2 h\" is 15 hands plus 2 inches, for a total of 62 inches (157.5 cm) in height.[24]\n The size of horses varies by breed, but also is influenced by nutrition. Light-riding horses usually range in height from 14 to 16 hands (56 to 64 inches, 142 to 163 cm) and can weigh from 380 to 550 kilograms (840 to 1,210 lb).[25] Larger-riding horses usually start at about 15.2 hands (62 inches, 157 cm) and often are as tall as 17 hands (68 inches, 173 cm), weighing from 500 to 600 kilograms (1,100 to 1,320 lb).[26] Heavy or draft horses are usually at least 16 hands (64 inches, 163 cm) high and can be as tall as 18 hands (72 inches, 183 cm) high. They can weigh from about 700 to 1,000 kilograms (1,540 to 2,200 lb).[27]\n The largest horse in recorded history was probably a Shire horse named Mammoth, who was born in 1848. He stood 21.2 1⁄4 hands (86.25 inches, 219 cm) high and his peak weight was estimated at 1,524 kilograms (3,360 lb).[28] The record holder for the smallest horse ever is Thumbelina, a fully mature miniature horse affected by dwarfism. She was 43 centimetres; 4.1 hands (17 in) tall and weighed 26 kg (57 lb).[29][30]\n Ponies are taxonomically the same animals as horses. The distinction between a horse and pony is commonly drawn on the basis of height, especially for competition purposes. However, height alone is not dispositive; the difference between horses and ponies may also include aspects of phenotype, including conformation and temperament.\n The traditional standard for height of a horse or a pony at maturity is 14.2 hands (58 inches, 147 cm). An animal 14.2 hands (58 inches, 147 cm) or over is usually considered to be a horse and one less than 14.2 hands (58 inches, 147 cm) a pony,[31] but there are many exceptions to the traditional standard. In Australia, ponies are considered to be those under 14 hands (56 inches, 142 cm).[32] For competition in the Western division of the United States Equestrian Federation, the cutoff is 14.1 hands (57 inches, 145 cm).[33] The International Federation for Equestrian Sports, the world governing body for horse sport, uses metric measurements and defines a pony as being any horse measuring less than 148 centimetres (58.27 in) at the withers without shoes, which is just over 14.2 hands (58 inches, 147 cm), and 149 centimetres (58.66 in; 14.2+1⁄2 hands), with shoes.[34]\n Height is not the sole criterion for distinguishing horses from ponies. Breed registries for horses that typically produce individuals both under and over 14.2 hands (58 inches, 147 cm) consider all animals of that breed to be horses regardless of their height.[35] Conversely, some pony breeds may have features in common with horses, and individual animals may occasionally mature at over 14.2 hands (58 inches, 147 cm), but are still considered to be ponies.[36]\n Ponies often exhibit thicker manes, tails, and overall coat. They also have proportionally shorter legs, wider barrels, heavier bone, shorter and thicker necks, and short heads with broad foreheads. They may have calmer temperaments than horses and also a high level of intelligence that may or may not be used to cooperate with human handlers.[31] Small size, by itself, is not an exclusive determinant. For example, the Shetland pony which averages 10 hands (40 inches, 102 cm), is considered a pony.[31] Conversely, breeds such as the Falabella and other miniature horses, which can be no taller than 76 centimetres; 7.2 hands (30 in), are classified by their registries as very small horses, not ponies.[37]\n Horses have 64 chromosomes.[38] The horse genome was sequenced in 2007. It contains 2.7 billion DNA base pairs,[39] which is larger than the dog genome, but smaller than the human genome or the bovine genome.[40] The map is available to researchers.[41]\n Horses exhibit a diverse array of coat colors and distinctive markings, described by a specialized vocabulary. Often, a horse is classified first by its coat color, before breed or sex.[42] Horses of the same color may be distinguished from one another by white markings,[43] which, along with various spotting patterns, are inherited separately from coat color.[44]\n Many genes that create horse coat colors and patterns have been identified. Current genetic tests can identify at least 13 different alleles influencing coat color,[45] and research continues to discover new genes linked to specific traits. The basic coat colors of chestnut and black are determined by the gene controlled by the Melanocortin 1 receptor,[46] also known as the \"extension gene\" or \"red factor\",[45] as its recessive form is \"red\" (chestnut) and its dominant form is black.[47] Additional genes control suppression of black color to point coloration that results in a bay, spotting patterns such as pinto or leopard, dilution genes such as palomino or dun, as well as greying, and all the other factors that create the many possible coat colors found in horses.[45]\n Horses that have a white coat color are often mislabeled; a horse that looks \"white\" is usually a middle-aged or older gray. Grays are born a darker shade, get lighter as they age, but usually keep black skin underneath their white hair coat (with the exception of pink skin under white markings). The only horses properly called white are born with a predominantly white hair coat and pink skin, a fairly rare occurrence.[47] Different and unrelated genetic factors can produce white coat colors in horses, including several different alleles of dominant white and the sabino-1 gene.[48] However, there are no \"albino\" horses, defined as having both pink skin and red eyes.[49]\n Gestation lasts approximately 340 days, with an average range 320–370 days,[50][51] and usually results in one foal; twins are rare.[52] Horses are a precocial species, and foals are capable of standing and running within a short time following birth.[53] Foals are usually born in the spring. The estrous cycle of a mare occurs roughly every 19–22 days and occurs from early spring into autumn. Most mares enter an anestrus period during the winter and thus do not cycle in this period.[54] Foals are generally weaned from their mothers between four and six months of age.[55]\n Horses, particularly colts, are sometimes physically capable of reproduction at about 18 months, but domesticated horses are rarely allowed to breed before the age of three, especially females.[56] Horses four years old are considered mature, although the skeleton normally continues to develop until the age of six; maturation also depends on the horse's size, breed, sex, and quality of care. Larger horses have larger bones; therefore, not only do the bones take longer to form bone tissue, but the epiphyseal plates are larger and take longer to convert from cartilage to bone. These plates convert after the other parts of the bones, and are crucial to development.[57]\n Depending on maturity, breed, and work expected, horses are usually put under saddle and trained to be ridden between the ages of two and four.[58] Although Thoroughbred race horses are put on the track as young as the age of two in some countries,[59] horses specifically bred for sports such as dressage are generally not put under saddle until they are three or four years old, because their bones and muscles are not solidly developed.[60] For endurance riding competition, horses are not deemed mature enough to compete until they are a full 60 calendar months (five years) old.[12]\n The horse skeleton averages 205 bones.[61] A significant difference between the horse skeleton and that of a human is the lack of a collarbone—the horse's forelimbs are attached to the spinal column by a powerful set of muscles, tendons, and ligaments that attach the shoulder blade to the torso. The horse's four legs and hooves are also unique structures. Their leg bones are proportioned differently from those of a human. For example, the body part that is called a horse's \"knee\" is actually made up of the carpal bones that correspond to the human wrist. Similarly, the hock contains bones equivalent to those in the human ankle and heel. The lower leg bones of a horse correspond to the bones of the human hand or foot, and the fetlock (incorrectly called the \"ankle\") is actually the proximal sesamoid bones between the cannon bones (a single equivalent to the human metacarpal or metatarsal bones) and the proximal phalanges, located where one finds the \"knuckles\" of a human. A horse also has no muscles in its legs below the knees and hocks, only skin, hair, bone, tendons, ligaments, cartilage, and the assorted specialized tissues that make up the hoof.[62]\n The critical importance of the feet and legs is summed up by the traditional adage, \"no foot, no horse\".[63] The horse hoof begins with the distal phalanges, the equivalent of the human fingertip or tip of the toe, surrounded by cartilage and other specialized, blood-rich soft tissues such as the laminae. The exterior hoof wall and horn of the sole is made of keratin, the same material as a human fingernail.[64] The result is that a horse, weighing on average 500 kilograms (1,100 lb),[65] travels on the same bones as would a human on tiptoe.[66] For the protection of the hoof under certain conditions, some horses have horseshoes placed on their feet by a professional farrier. The hoof continually grows, and in most domesticated horses needs to be trimmed (and horseshoes reset, if used) every five to eight weeks,[67] though the hooves of horses in the wild wear down and regrow at a rate suitable for their terrain.\n Horses are adapted to grazing. In an adult horse, there are 12 incisors at the front of the mouth, adapted to biting off the grass or other vegetation. There are 24 teeth adapted for chewing, the premolars and molars, at the back of the mouth. Stallions and geldings have four additional teeth just behind the incisors, a type of canine teeth called \"tushes\". Some horses, both male and female, will also develop one to four very small vestigial teeth in front of the molars, known as \"wolf\" teeth, which are generally removed because they can interfere with the bit. There is an empty interdental space between the incisors and the molars where the bit rests directly on the gums, or \"bars\" of the horse's mouth when the horse is bridled.[68]\n An estimate of a horse's age can be made from looking at its teeth. The teeth continue to erupt throughout life and are worn down by grazing. Therefore, the incisors show changes as the horse ages; they develop a distinct wear pattern, changes in tooth shape, and changes in the angle at which the chewing surfaces meet. This allows a very rough estimate of a horse's age, although diet and veterinary care can also affect the rate of tooth wear.[7]\n Horses are herbivores with a digestive system adapted to a forage diet of grasses and other plant material, consumed steadily throughout the day. Therefore, compared to humans, they have a relatively small stomach but very long intestines to facilitate a steady flow of nutrients. A 450-kilogram (990 lb) horse will eat 7 to 11 kilograms (15 to 24 lb) of food per day and, under normal use, drink 38 to 45 litres (8.4 to 9.9 imp gal; 10 to 12 US gal) of water. Horses are not ruminants, they have only one stomach, like humans, but unlike humans, they can digest cellulose, a major component of grass. Horses are hindgut fermenters. Cellulose fermentation by symbiotic bacteria occurs in the cecum, or \"water gut\", which food goes through before reaching the large intestine. Horses cannot vomit, so digestion problems can quickly cause colic, a leading cause of death.[69] Horses do not have a gallbladder; however, they seem to tolerate high amounts of fat in their diet despite lack of a gallbladder.[70][71]\n The horses' senses are based on their status as prey animals, where they must be aware of their surroundings at all times.[72] They have the largest eyes of any land mammal,[73] and are lateral-eyed, meaning that their eyes are positioned on the sides of their heads.[74] This means that horses have a range of vision of more than 350°, with approximately 65° of this being binocular vision and the remaining 285° monocular vision.[73] Horses have excellent day and night vision, but they have two-color, or dichromatic vision; their color vision is somewhat like red-green color blindness in humans, where certain colors, especially red and related colors, appear as a shade of green.[75]\n Their sense of smell, while much better than that of humans, is not quite as good as that of a dog. It is believed to play a key role in the social interactions of horses as well as detecting other key scents in the environment. Horses have two olfactory centers. The first system is in the nostrils and nasal cavity, which analyze a wide range of odors. The second, located under the nasal cavity, are the vomeronasal organs, also called Jacobson's organs. These have a separate nerve pathway to the brain and appear to primarily analyze pheromones.[76]\n A horse's hearing is good,[72] and the pinna of each ear can rotate up to 180°, giving the potential for 360° hearing without having to move the head.[77] Noise impacts the behavior of horses and certain kinds of noise may contribute to stress: a 2013 study in the UK indicated that stabled horses were calmest in a quiet setting, or if listening to country or classical music, but displayed signs of nervousness when listening to jazz or rock music. This study also recommended keeping music under a volume of 21 decibels.[78] An Australian study found that stabled racehorses listening to talk radio had a higher rate of gastric ulcers than horses listening to music, and racehorses stabled where a radio was played had a higher overall rate of ulceration than horses stabled where there was no radio playing.[79]\n Horses have a great sense of balance, due partly to their ability to feel their footing and partly to highly developed proprioception—the unconscious sense of where the body and limbs are at all times.[80] A horse's sense of touch is well-developed. The most sensitive areas are around the eyes, ears, and nose.[81] Horses are able to sense contact as subtle as an insect landing anywhere on the body.[82]\n Horses have an advanced sense of taste, which allows them to sort through fodder and choose what they would most like to eat,[83] and their prehensile lips can easily sort even small grains. Horses generally will not eat poisonous plants, however, there are exceptions; horses will occasionally eat toxic amounts of poisonous plants even when there is adequate healthy food.[84]\n All horses move naturally with four basic gaits:[85]\n Besides these basic gaits, some horses perform a two-beat pace, instead of the trot.[88] There also are several four-beat 'ambling' gaits that are approximately the speed of a trot or pace, though smoother to ride. These include the lateral rack, running walk, and tölt as well as the diagonal fox trot.[89] Ambling gaits are often genetic in some breeds, known collectively as gaited horses.[90] These horses replace the trot with one of the ambling gaits.[91]\n Horses are prey animals with a strong fight-or-flight response. Their first reaction to a threat is to startle and usually flee, although they will stand their ground and defend themselves when flight is impossible or if their young are threatened.[92] They also tend to be curious; when startled, they will often hesitate an instant to ascertain the cause of their fright, and may not always flee from something that they perceive as non-threatening. Most light horse riding breeds were developed for speed, agility, alertness and endurance; natural qualities that extend from their wild ancestors. However, through selective breeding, some breeds of horses are quite docile, particularly certain draft horses.[93]\n Horses are herd animals, with a clear hierarchy of rank, led by a dominant individual, usually a mare. They are also social creatures that are able to form companionship attachments to their own species and to other animals, including humans. They communicate in various ways, including vocalizations such as nickering or whinnying, mutual grooming, and body language. Many horses will become difficult to manage if they are isolated, but with training, horses can learn to accept a human as a companion, and thus be comfortable away from other horses.[94] However, when confined with insufficient companionship, exercise, or stimulation, individuals may develop stable vices, an assortment of bad habits, mostly stereotypies of psychological origin, that include wood chewing, wall kicking, \"weaving\" (rocking back and forth), and other problems.[95]\n Studies have indicated that horses perform a number of cognitive tasks on a daily basis, meeting mental challenges that include food procurement and identification of individuals within a social system. They also have good spatial discrimination abilities.[96] They are naturally curious and apt to investigate things they have not seen before.[97] Studies have assessed equine intelligence in areas such as problem solving, speed of learning, and memory. Horses excel at simple learning, but also are able to use more advanced cognitive abilities that involve categorization and concept learning. They can learn using habituation, desensitization, classical conditioning, and operant conditioning, and positive and negative reinforcement.[96] One study has indicated that horses can differentiate between \"more or less\" if the quantity involved is less than four.[98]\n Domesticated horses may face greater mental challenges than wild horses, because they live in artificial environments that prevent instinctive behavior whilst also learning tasks that are not natural.[96] Horses are animals of habit that respond well to regimentation, and respond best when the same routines and techniques are used consistently. One trainer believes that \"intelligent\" horses are reflections of intelligent trainers who effectively use response conditioning techniques and positive reinforcement to train in the style that best fits with an individual animal's natural inclinations.[99]\n Horses are mammals. As such, they are warm-blooded, or endothermic creatures, as opposed to cold-blooded, or poikilothermic animals. However, these words have developed a separate meaning in the context of equine terminology, used to describe temperament, not body temperature. For example, the \"hot-bloods\", such as many race horses, exhibit more sensitivity and energy,[100] while the \"cold-bloods\", such as most draft breeds, are quieter and calmer.[101] Sometimes \"hot-bloods\" are classified as \"light horses\" or \"riding horses\",[102] with the \"cold-bloods\" classified as \"draft horses\" or \"work horses\".[103]\n \"Hot blooded\" breeds include \"oriental horses\" such as the Akhal-Teke, Arabian horse, Barb, and now-extinct Turkoman horse, as well as the Thoroughbred, a breed developed in England from the older oriental breeds.[100] Hot bloods tend to be spirited, bold, and learn quickly. They are bred for agility and speed.[104] They tend to be physically refined—thin-skinned, slim, and long-legged.[105] The original oriental breeds were brought to Europe from the Middle East and North Africa when European breeders wished to infuse these traits into racing and light cavalry horses.[106][107]\n Muscular, heavy draft horses are known as \"cold bloods\", as they are bred not only for strength, but also to have the calm, patient temperament needed to pull a plow or a heavy carriage full of people.[101] They are sometimes nicknamed \"gentle giants\".[108] Well-known draft breeds include the Belgian and the Clydesdale.[108] Some, like the Percheron, are lighter and livelier, developed to pull carriages or to plow large fields in drier climates.[109] Others, such as the Shire, are slower and more powerful, bred to plow fields with heavy, clay-based soils.[110] The cold-blooded group also includes some pony breeds.[111]\n \"Warmblood\" breeds, such as the Trakehner or Hanoverian, developed when European carriage and war horses were crossed with Arabians or Thoroughbreds, producing a riding horse with more refinement than a draft horse, but greater size and milder temperament than a lighter breed.[112] Certain pony breeds with warmblood characteristics have been developed for smaller riders.[113] Warmbloods are considered a \"light horse\" or \"riding horse\".[102]\n Today, the term \"Warmblood\" refers to a specific subset of sport horse breeds that are used for competition in dressage and show jumping.[114] Strictly speaking, the term \"warm blood\" refers to any cross between cold-blooded and hot-blooded breeds.[115] Examples include breeds such as the Irish Draught or the Cleveland Bay. The term was once used to refer to breeds of light riding horse other than Thoroughbreds or Arabians, such as the Morgan horse.[104]\n Horses are able to sleep both standing up and lying down. In an adaptation from life in the wild, horses are able to enter light sleep by using a \"stay apparatus\" in their legs, allowing them to doze without collapsing.[116] Horses sleep better when in groups because some animals will sleep while others stand guard to watch for predators. A horse kept alone will not sleep well because its instincts are to keep a constant eye out for danger.[117]\n Unlike humans, horses do not sleep in a solid, unbroken period of time, but take many short periods of rest. Horses spend four to fifteen hours a day in standing rest, and from a few minutes to several hours lying down. Total sleep time in a 24-hour period may range from several minutes to a couple of hours,[117] mostly in short intervals of about 15 minutes each.[118] The average sleep time of a domestic horse is said to be 2.9 hours per day.[119]\n Horses must lie down to reach REM sleep. They only have to lie down for an hour or two every few days to meet their minimum REM sleep requirements.[117] However, if a horse is never allowed to lie down, after several days it will become sleep-deprived, and in rare cases may suddenly collapse as it involuntarily slips into REM sleep while still standing.[120] This condition differs from narcolepsy, although horses may also suffer from that disorder.[121]\n The horse adapted to survive in areas of wide-open terrain with sparse vegetation, surviving in an ecosystem where other large grazing animals, especially ruminants, could not.[122] Horses and other equids are odd-toed ungulates of the order Perissodactyla, a group of mammals dominant during the Tertiary period. In the past, this order contained 14 families, but only three—Equidae (the horse and related species), Tapiridae (the tapir), and Rhinocerotidae (the rhinoceroses)—have survived to the present day.[123]\n The earliest known member of the family Equidae was the Hyracotherium, which lived between 45 and 55 million years ago, during the Eocene period. It had 4 toes on each front foot, and 3 toes on each back foot.[124] The extra toe on the front feet soon disappeared with the Mesohippus, which lived 32 to 37 million years ago.[125] Over time, the extra side toes shrank in size until they vanished. All that remains of them in modern horses is a set of small vestigial bones on the leg below the knee,[126] known informally as splint bones.[127] Their legs also lengthened as their toes disappeared until they were a hooved animal capable of running at great speed.[126] By about 5 million years ago, the modern Equus had evolved.[128] Equid teeth also evolved from browsing on soft, tropical plants to adapt to browsing of drier plant material, then to grazing of tougher plains grasses. Thus proto-horses changed from leaf-eating forest-dwellers to grass-eating inhabitants of semi-arid regions worldwide, including the steppes of Eurasia and the Great Plains of North America.\n By about 15,000 years ago, Equus ferus was a widespread holarctic species. Horse bones from this time period, the late Pleistocene, are found in Europe, Eurasia, Beringia, and North America.[129] Yet between 10,000 and 7,600 years ago, the horse became extinct in North America.[130][131][132] The reasons for this extinction are not fully known, but one theory notes that extinction in North America paralleled human arrival.[133] Another theory points to climate change, noting that approximately 12,500 years ago, the grasses characteristic of a steppe ecosystem gave way to shrub tundra, which was covered with unpalatable plants.[134]\n A truly wild horse is a species or subspecies with no ancestors that were ever successfully domesticated. Therefore, most \"wild\" horses today are actually feral horses, animals that escaped or were turned loose from domestic herds and the descendants of those animals.[135] Only two wild subspecies, the tarpan and the Przewalski's horse, survived into recorded history and only the latter survives today.\n The Przewalski's horse (Equus ferus przewalskii), named after the Russian explorer Nikolai Przhevalsky, is a rare Asian animal. It is also known as the Mongolian wild horse; Mongolian people know it as the taki, and the Kyrgyz people call it a kirtag. The subspecies was presumed extinct in the wild between 1969 and 1992, while a small breeding population survived in zoos around the world. In 1992, it was reestablished in the wild by the conservation efforts of numerous zoos.[136] Today, a small wild breeding population exists in Mongolia.[137][138] There are additional animals still maintained at zoos throughout the world.\n The question of whether the Przewalski's horse was ever domesticated was challenged in 2018 when DNA studies of horses found at Botai culture sites revealed captured animals with DNA markers of an ancestor to the Przewalski's horse. The study concluded that the Botai animals appear to have been an independent domestication attempt and apparently unsuccessful, as these genetic markers do not appear in modern domesticated horses.  However, the question of whether all Przewalski's horses descend from this population is also unresolved, as only one of seven modern Przewalski's horses in the study shared this ancestry.[139][140][141]\n The tarpan or European wild horse (Equus ferus ferus) was found in Europe and much of Asia. It survived into the historical era, but became extinct in 1909, when the last captive died in a Russian zoo.[142] Thus, the genetic line was lost. Attempts have been made to recreate the tarpan,[142][143][144] which resulted in horses with outward physical similarities, but nonetheless descended from domesticated ancestors and not true wild horses.\n Periodically, populations of horses in isolated areas are speculated to be relict populations of wild horses, but generally have been proven to be feral or domestic. For example, the Riwoche horse of Tibet was proposed as such,[138] but testing did not reveal genetic differences from domesticated horses.[145] Similarly, the Sorraia of Portugal was proposed as a direct descendant of the Tarpan on the basis of shared characteristics,[146][147] but genetic studies have shown that the Sorraia is more closely related to other horse breeds, and that the outward similarity is an unreliable measure of relatedness.[146][148]\n Besides the horse, there are six other species of genus Equus in the Equidae family. These are the ass or donkey, Equus asinus; the mountain zebra, Equus zebra; plains zebra, Equus quagga; Grévy's Zebra, Equus grevyi; the kiang, Equus kiang; and the onager, Equus hemionus.[149]\n Horses can crossbreed with other members of their genus. The most common hybrid is the mule, a cross between a \"jack\" (male donkey) and a mare. A related hybrid, a hinny, is a cross between a stallion and a \"jenny\" (female donkey).[150] Other hybrids include the zorse, a cross between a zebra and a horse.[151] With rare exceptions, most hybrids are sterile and cannot reproduce.[152]\n  Domestication of the horse most likely took place in central Asia prior to 3500 BCE. Two major sources of information are used to determine where and when the horse was first domesticated and how the domesticated horse spread around the world. The first source is based on palaeological and archaeological discoveries; the second source is a comparison of DNA obtained from modern horses to that from bones and teeth of ancient horse remains.\n The earliest archaeological evidence for the domestication of the horse comes from sites in Ukraine and Kazakhstan, dating to approximately 4000–3500 BCE.[153][154][155] By 3000 BCE, the horse was completely domesticated and by 2000 BCE there was a sharp increase in the number of horse bones found in human settlements in northwestern Europe, indicating the spread of domesticated horses throughout the continent.[156] The most recent, but most irrefutable evidence of domestication comes from sites where horse remains were interred with chariots in graves of the Sintashta and Petrovka cultures c. 2100 BCE.[157]\n A 2021 genetic study suggested that most modern domestic horses descend from the lower Volga-Don region. Ancient horse genomes indicate that these populations influenced almost all local populations as they expanded rapidly throughout Eurasia, beginning about 4,200 years ago. It also shows that certain adaptations were strongly selected due to riding, and that equestrian material culture, including Sintashta spoke-wheeled chariots spread with the horse itself.[158][159]\n Domestication is also studied by using the genetic material of present-day horses and comparing it with the genetic material present in the bones and teeth of horse remains found in archaeological and palaeological excavations. The variation in the genetic material shows that very few wild stallions contributed to the domestic horse,[160][161] while many mares were part of early domesticated herds.[148][162][163] This is reflected in the difference in genetic variation between the DNA that is passed on along the paternal, or sire line (Y-chromosome) versus that passed on along the maternal, or dam line (mitochondrial DNA). There are very low levels of Y-chromosome variability,[160][161] but a great deal of genetic variation in mitochondrial DNA.[148][162][163] There is also regional variation in mitochondrial DNA due to the inclusion of wild mares in domestic herds.[148][162][163][164] Another characteristic of domestication is an increase in coat color variation.[165] In horses, this increased dramatically between 5000 and 3000 BCE.[166]\n Before the availability of DNA techniques to resolve the questions related to the domestication of the horse, various hypotheses were proposed. One classification was based on body types and conformation, suggesting the presence of four basic prototypes that had adapted to their environment prior to domestication.[111] Another hypothesis held that the four prototypes originated from a single wild species and that all different body types were entirely a result of selective breeding after domestication.[167] However, the lack of a detectable substructure  in the horse has resulted in a rejection of both hypotheses.\n Feral horses are born and live in the wild, but are descended from domesticated animals.[135] Many populations of feral horses exist throughout the world.[168][169] Studies of feral herds have provided useful insights into the behavior of prehistoric horses,[170] as well as greater understanding of the instincts and behaviors that drive horses that live in domesticated conditions.[171]\n There are also semi-feral horses in many parts of the world, such as Dartmoor and the New Forest in the UK, where the animals are all privately owned but live for significant amounts of time in \"wild\" conditions on undeveloped, often public, lands. Owners of such animals often pay a fee for grazing rights.[172][173]\n The concept of purebred bloodstock and a controlled, written breed registry has come to be particularly significant and important in modern times. Sometimes purebred horses are incorrectly or inaccurately called \"thoroughbreds\". Thoroughbred is a specific breed of horse, while a \"purebred\" is a horse (or any other animal) with a defined pedigree recognized by a breed registry.[174] Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits result from a combination of natural crosses and artificial selection methods. Horses have been selectively bred since their domestication. An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines.[175] These pedigrees were originally transmitted via an oral tradition.[176] In the 14th century, Carthusian monks of southern Spain kept meticulous pedigrees of bloodstock lineages still found today in the Andalusian horse.[177]\n Breeds developed due to a need for \"form to function\", the necessity to develop certain characteristics in order to perform a particular type of work.[178] Thus, a powerful but refined breed such as the Andalusian developed as riding horses with an aptitude for dressage.[178] Heavy draft horses were developed out of a need to perform demanding farm work and pull heavy wagons.[179] Other horse breeds had been developed specifically for light agricultural work, carriage and road work, various sport disciplines, or simply as pets.[180] Some breeds developed through centuries of crossing other breeds, while others descended from a single foundation sire, or other limited or restricted foundation bloodstock. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the foundation bloodstock for the breed.[181] There are more than 300 horse breeds in the world today.[182]\n Worldwide, horses play a role within human cultures and have done so for millennia. Horses are used for leisure activities, sports, and working purposes. The Food and Agriculture Organization (FAO) estimates that in 2008, there were almost 59,000,000 horses in the world, with around 33,500,000 in the Americas, 13,800,000 in Asia and 6,300,000 in Europe and smaller portions in Africa and Oceania. There are estimated to be 9,500,000 horses in the United States alone.[183] The American Horse Council estimates that horse-related activities have a direct impact on the economy of the United States of over $39 billion, and when indirect spending is considered, the impact is over $102 billion.[184] In a 2004 \"poll\" conducted by Animal Planet, more than 50,000 viewers from 73 countries voted for the horse as the world's 4th favorite animal.[185]\n Communication between human and horse is paramount in any equestrian activity;[186] to aid this process horses are usually ridden with a saddle on their backs to assist the rider with balance and positioning, and a bridle or related headgear to assist the rider in maintaining control.[187] Sometimes horses are ridden without a saddle,[188] and occasionally, horses are trained to perform without a bridle or other headgear.[189] Many horses are also driven, which requires a harness, bridle, and some type of vehicle.[190]\n Historically, equestrians honed their skills through games and races. Equestrian sports provided entertainment for crowds and honed the excellent horsemanship that was needed in battle. Many sports, such as dressage, eventing, and show jumping, have origins in military training, which were focused on control and balance of both horse and rider. Other sports, such as rodeo, developed from practical skills such as those needed on working ranches and stations. Sport hunting from horseback evolved from earlier practical hunting techniques.[186] Horse racing of all types evolved from impromptu competitions between riders or drivers. All forms of competition, requiring demanding and specialized skills from both horse and rider, resulted in the systematic development of specialized breeds and equipment for each sport. The popularity of equestrian sports through the centuries has resulted in the preservation of skills that would otherwise have disappeared after horses stopped being used in combat.[186]\n Horses are trained to be ridden or driven in a variety of sporting competitions. Examples include show jumping, dressage, three-day eventing, competitive driving, endurance riding, gymkhana, rodeos, and fox hunting.[191] Horse shows, which have their origins in medieval European fairs, are held around the world. They host a huge range of classes, covering all of the mounted and harness disciplines, as well as \"In-hand\" classes where the horses are led, rather than ridden, to be evaluated on their conformation. The method of judging varies with the discipline, but winning usually depends on style and ability of both horse and rider.[192]\nSports such as polo do not judge the horse itself, but rather use the horse as a partner for human competitors as a necessary part of the game. Although the horse requires specialized training to participate, the details of its performance are not judged, only the result of the rider's actions—be it getting a ball through a goal or some other task.[193] Examples of these sports of partnership between human and horse include jousting, in which the main goal is for one rider to unseat the other,[194] and buzkashi, a team game played throughout Central Asia, the aim being to capture a goat carcass while on horseback.[193]\n Horse racing is an equestrian sport and major international industry, watched in almost every nation of the world. There are three types: \"flat\" racing; steeplechasing, i.e. racing over jumps; and harness racing, where horses trot or pace while pulling a driver in a small, light cart known as a sulky.[195] A major part of horse racing's economic importance lies in the gambling associated with it.[196]\n There are certain jobs that horses do very well, and no technology has yet developed to fully replace them. For example, mounted police horses are still effective for certain types of patrol duties and crowd control.[197] Cattle ranches still require riders on horseback to round up cattle that are scattered across remote, rugged terrain.[198] Search and rescue organizations in some countries depend upon mounted teams to locate people, particularly hikers and children, and to provide disaster relief assistance.[199] Horses can also be used in areas where it is necessary to avoid vehicular disruption to delicate soil, such as nature reserves. They may also be the only form of transport allowed in wilderness areas. Horses are quieter than motorized vehicles. Law enforcement officers such as park rangers or game wardens may use horses for patrols, and horses or mules may also be used for clearing trails or other work in areas of rough terrain where vehicles are less effective.[200]\n Although machinery has replaced horses in many parts of the world, an estimated 100 million horses, donkeys and mules are still used for agriculture and transportation in less developed areas. This number includes around 27 million working animals in Africa alone.[201] Some land management practices such as cultivating and logging can be efficiently performed with horses. In agriculture, less fossil fuel is used and increased environmental conservation occurs over time with the use of draft animals such as horses.[202][203] Logging with horses can result in reduced damage to soil structure and less damage to trees due to more selective logging.[204]\n Horses have been used in warfare for most of recorded history. The first archaeological evidence of horses used in warfare dates to between 4000 and 3000 BCE,[205] and the use of horses in warfare was widespread by the end of the Bronze Age.[206][207] Although mechanization has largely replaced the horse as a weapon of war, horses are still seen today in limited military uses, mostly for ceremonial purposes, or for reconnaissance and transport activities in areas of rough terrain where motorized vehicles are ineffective. Horses have been used in the 21st century by the Janjaweed militias in the War in Darfur.[208]\n Modern horses are often used to reenact many of their historical work purposes. Horses are used, complete with equipment that is authentic or a meticulously recreated replica, in various live action historical reenactments of specific periods of history, especially recreations of famous battles.[209] Horses are also used to preserve cultural traditions and for ceremonial purposes. Countries such as the United Kingdom still use horse-drawn carriages to convey royalty and other VIPs to and from certain culturally significant events.[210] Public exhibitions are another example, such as the Budweiser Clydesdales, seen in parades and other public settings, a team of draft horses that pull a beer wagon similar to that used before the invention of the modern motorized truck.[211]\n Horses are frequently used in television, films and literature. They are sometimes featured as a major character in films about particular animals, but also used as visual elements that assure the accuracy of historical stories.[212] Both live horses and iconic images of horses are used in advertising to promote a variety of products.[213] The horse frequently appears in coats of arms in heraldry, in a variety of poses and equipment.[214] The mythologies of many cultures, including Greco-Roman, Hindu, Islamic, and Germanic, include references to both normal horses and those with wings or additional limbs, and multiple myths also call upon the horse to draw the chariots of the Moon and Sun.[215] The horse also appears in the 12-year cycle of animals in the Chinese zodiac related to the Chinese calendar.[216]\n Horses serve as the inspiration for many modern automobile names and logos, including the Ford Pinto, Ford Bronco, Ford Mustang, Hyundai Equus, Hyundai Pony, Mitsubishi Starion, Subaru Brumby, Mitsubishi Colt\/Dodge Colt, Pinzgauer, Steyr-Puch Haflinger, Pegaso, Porsche, Rolls-Royce Camargue, Ferrari, Carlsson, Kamaz, Corre La Licorne, Iran Khodro, Eicher, and Baojun.[217][218][219] Indian TVS Motor Company also uses a horse on their motorcycles & scooters.\n People of all ages with physical and mental disabilities obtain beneficial results from an association with horses. Therapeutic riding is used to mentally and physically stimulate disabled persons and help them improve their lives through improved balance and coordination, increased self-confidence, and a greater feeling of freedom and independence.[220] The benefits of equestrian activity for people with disabilities has also been recognized with the addition of equestrian events to the Paralympic Games and recognition of para-equestrian events by the International Federation for Equestrian Sports (FEI).[221] Hippotherapy and therapeutic horseback riding are names for different physical, occupational, and speech therapy treatment strategies that use equine movement. In hippotherapy, a therapist uses the horse's movement to improve their patient's cognitive, coordination, balance, and fine motor skills, whereas therapeutic horseback riding uses specific riding skills.[222]\n Horses also provide psychological benefits to people whether they actually ride or not. \"Equine-assisted\" or \"equine-facilitated\" therapy is a form of experiential psychotherapy that uses horses as companion animals to assist people with mental illness, including anxiety disorders, psychotic disorders, mood disorders, behavioral difficulties, and those who are going through major life changes.[223] There are also experimental programs using horses in prison settings. Exposure to horses appears to improve the behavior of inmates and help reduce recidivism when they leave.[224]\n Horses are raw material for many products made by humans throughout history, including byproducts from the slaughter of horses as well as materials collected from living horses.\n Products collected from living horses include mare's milk, used by people with large horse herds, such as the Mongols, who let it ferment to produce kumis.[225] Horse blood was once used as food by the Mongols and other nomadic tribes, who found it a convenient source of nutrition when traveling. Drinking their own horses' blood allowed the Mongols to ride for extended periods of time without stopping to eat.[225] The drug Premarin is a mixture of estrogens extracted from the urine of pregnant mares (pregnant mares' urine), and was previously a widely used drug for hormone replacement therapy.[226] The tail hair of horses can be used for making bows for string instruments such as the violin, viola, cello, and double bass.[227]\n Horse meat has been used as food for humans and carnivorous animals throughout the ages. Approximately 5 million horses are slaughtered each year for meat worldwide.[228] It is eaten in many parts of the world, though consumption is taboo in some cultures,[229] and a subject of political controversy in others.[230] Horsehide leather has been used for boots, gloves, jackets,[231] baseballs,[232] and baseball gloves. Horse hooves can also be used to produce animal glue.[233] Horse bones can be used to make implements.[234] Specifically, in Italian cuisine, the horse tibia is sharpened into a probe called a spinto, which is used to test the readiness of a (pig) ham as it cures.[235] In Asia, the saba is a horsehide vessel used in the production of kumis.[236]\n Horses are grazing animals, and their major source of nutrients is good-quality forage from hay or pasture.[237] They can consume approximately 2% to 2.5% of their body weight in dry feed each day. Therefore, a 450-kilogram (990 lb) adult horse could eat up to 11 kilograms (24 lb) of food.[238] Sometimes, concentrated feed such as grain is fed in addition to pasture or hay, especially when the animal is very active.[239] When grain is fed, equine nutritionists recommend that 50% or more of the animal's diet by weight should still be forage.[240]\n Horses require a plentiful supply of clean water, a minimum of 38 to 45 litres (10 to 12 US gal) per day.[241] Although horses are adapted to live outside, they require shelter from the wind and precipitation, which can range from a simple shed or shelter to an elaborate stable.[242]\n Horses require routine hoof care from a farrier, as well as vaccinations to protect against various diseases, and dental examinations from a veterinarian or a specialized equine dentist.[243] If horses are kept inside in a barn, they require regular daily exercise for their physical health and mental well-being.[244] When turned outside, they require well-maintained, sturdy fences to be safely contained.[245] Regular grooming is also helpful to help the horse maintain good health of the hair coat and underlying skin.[246]\n As of 2019, there are around 17 million horses in the world. Healthy body temperature for adult horses is in the range between 37.5 and 38.5 °C (99.5 and 101.3 °F), which they can maintain while ambient temperatures are between 5 and 25 °C (41 and 77 °F). However, strenuous exercise increases core body temperature by 1 °C (1.8 °F)\/minute, as 80% of the energy used by equine muscles is released as heat. Along with bovines and primates, equines are the only animal group which use sweating as their primary method of thermoregulation: in fact, it can account for up to 70% of their heat loss, and horses sweat three times more than humans while undergoing comparably strenuous physical activity. Unlike humans, this sweat is created not by eccrine glands but by apocrine glands.[248] In hot conditions, horses during three hours of moderate-intersity exercise can loss 30 to 35 L of water and 100g of sodium, 198 g of choloride and 45 g of potassium.[248] In another difference from humans, their sweat is hypertonic, and contains a protein called latherin,[249] which enables it to spread across their body easier, and to foam, rather than to drip off. These adaptations are partly to compensate for their lower body surface-to-mass ratio, which makes it more difficult for horses to passively radiate heat. Yet, prolonged exposure to very hot and\/or humid conditions will lead to consequences such as anhidrosis, heat stroke, or brain damage, potentially culminating in death if not addressed with measures like cold water applications. Additionally, around 10% of incidents associated with horse transport have been attributed to heat stress. These issues are expected to worsen in the future.[247]\n"}
{"key":"Horse","link":"https:\/\/en.wikipedia.org\/wiki\/Horse","headline":"Horse - Wikipedia","content":"\n at least 48 published\n The horse (Equus ferus caballus)[2][3] is a domesticated, one-toed, hoofed mammal. It belongs to the taxonomic family Equidae and is one of two extant subspecies of Equus ferus. The horse has evolved over the past 45 to 55 million years from a small multi-toed creature, close to Eohippus, into the large, single-toed animal of today. Humans began domesticating horses around 4000 BCE, and their domestication is believed to have been widespread by 3000 BCE. Horses in the subspecies caballus are domesticated, although some domesticated populations live in the wild as feral horses. These feral populations are not true wild horses, which are horses that never have been domesticated and historically linked to the megafauna category of species. There is an extensive, specialized vocabulary used to describe equine-related concepts, covering everything from anatomy to life stages, size, colors, markings, breeds, locomotion, and behavior.\n Horses are adapted to run, allowing them to quickly escape predators, and possess an excellent sense of balance and a strong fight-or-flight response. Related to this need to flee from predators in the wild is an unusual trait: horses are able to sleep both standing up and lying down, with younger horses tending to sleep significantly more than adults.[4] Female horses, called mares, carry their young for approximately 11 months and a young horse, called a foal, can stand and run shortly following birth. Most domesticated horses begin training under a saddle or in a harness between the ages of two and four. They reach full adult development by age five, and have an average lifespan of between 25 and 30 years.\n Horse breeds are loosely divided into three categories based on general temperament: spirited \"hot bloods\" with speed and endurance; \"cold bloods\", such as draft horses and some ponies, suitable for slow, heavy work; and \"warmbloods\", developed from crosses between hot bloods and cold bloods, often focusing on creating breeds for specific riding purposes, particularly in Europe. There are more than 300 breeds of horse in the world today, developed for many different uses.\n Horses and humans interact in a wide variety of sport competitions and non-competitive recreational pursuits as well as in working activities such as police work, agriculture, entertainment, and therapy. Horses were historically used in warfare, from which a wide variety of riding and driving techniques developed, using many different styles of equipment and methods of control. Many products are derived from horses, including meat, milk, hide, hair, bone, and pharmaceuticals extracted from the urine of pregnant mares. Humans provide domesticated horses with food, water, and shelter, as well as attention from specialists such as veterinarians and farriers.\n Specific terms and specialized language are used to describe equine anatomy, different life stages, colors, and breeds.\n Depending on breed, management and environment, the modern domestic horse has a life expectancy of 25 to 30 years.[7] Uncommonly, a few animals live into their 40s and, occasionally, beyond.[8] The oldest verifiable record was \"Old Billy\", a 19th-century horse that lived to the age of 62.[7] In modern times, Sugar Puff, who had been listed in Guinness World Records as the world's oldest living pony, died in 2007 at age 56.[9]\n Regardless of a horse or pony's actual birth date, for most competition purposes a year is added to its age each January 1 of each year in the Northern Hemisphere[7][10] and each August 1 in the Southern Hemisphere.[11] The exception is in endurance riding, where the minimum age to compete is based on the animal's actual calendar age.[12]\n The following terminology is used to describe horses of various ages:\n In horse racing, these definitions may differ: For example, in the British Isles, Thoroughbred horse racing defines colts and fillies as less than five years old.[21] However, Australian Thoroughbred racing defines colts and fillies as less than four years old.[22]\n The height of horses is measured at the highest point of the withers, where the neck meets the back.[23] This point is used because it is a stable point of the anatomy, unlike the head or neck, which move up and down in relation to the body of the horse.\n In English-speaking countries, the height of horses is often stated in units of hands and inches: one hand is equal to 4 inches (101.6 mm). The height is expressed as the number of full hands, followed by a point, then the number of additional inches, and ending with the abbreviation \"h\" or \"hh\" (for \"hands high\"). Thus, a horse described as \"15.2 h\" is 15 hands plus 2 inches, for a total of 62 inches (157.5 cm) in height.[24]\n The size of horses varies by breed, but also is influenced by nutrition. Light-riding horses usually range in height from 14 to 16 hands (56 to 64 inches, 142 to 163 cm) and can weigh from 380 to 550 kilograms (840 to 1,210 lb).[25] Larger-riding horses usually start at about 15.2 hands (62 inches, 157 cm) and often are as tall as 17 hands (68 inches, 173 cm), weighing from 500 to 600 kilograms (1,100 to 1,320 lb).[26] Heavy or draft horses are usually at least 16 hands (64 inches, 163 cm) high and can be as tall as 18 hands (72 inches, 183 cm) high. They can weigh from about 700 to 1,000 kilograms (1,540 to 2,200 lb).[27]\n The largest horse in recorded history was probably a Shire horse named Mammoth, who was born in 1848. He stood 21.2 1⁄4 hands (86.25 inches, 219 cm) high and his peak weight was estimated at 1,524 kilograms (3,360 lb).[28] The record holder for the smallest horse ever is Thumbelina, a fully mature miniature horse affected by dwarfism. She was 43 centimetres; 4.1 hands (17 in) tall and weighed 26 kg (57 lb).[29][30]\n Ponies are taxonomically the same animals as horses. The distinction between a horse and pony is commonly drawn on the basis of height, especially for competition purposes. However, height alone is not dispositive; the difference between horses and ponies may also include aspects of phenotype, including conformation and temperament.\n The traditional standard for height of a horse or a pony at maturity is 14.2 hands (58 inches, 147 cm). An animal 14.2 hands (58 inches, 147 cm) or over is usually considered to be a horse and one less than 14.2 hands (58 inches, 147 cm) a pony,[31] but there are many exceptions to the traditional standard. In Australia, ponies are considered to be those under 14 hands (56 inches, 142 cm).[32] For competition in the Western division of the United States Equestrian Federation, the cutoff is 14.1 hands (57 inches, 145 cm).[33] The International Federation for Equestrian Sports, the world governing body for horse sport, uses metric measurements and defines a pony as being any horse measuring less than 148 centimetres (58.27 in) at the withers without shoes, which is just over 14.2 hands (58 inches, 147 cm), and 149 centimetres (58.66 in; 14.2+1⁄2 hands), with shoes.[34]\n Height is not the sole criterion for distinguishing horses from ponies. Breed registries for horses that typically produce individuals both under and over 14.2 hands (58 inches, 147 cm) consider all animals of that breed to be horses regardless of their height.[35] Conversely, some pony breeds may have features in common with horses, and individual animals may occasionally mature at over 14.2 hands (58 inches, 147 cm), but are still considered to be ponies.[36]\n Ponies often exhibit thicker manes, tails, and overall coat. They also have proportionally shorter legs, wider barrels, heavier bone, shorter and thicker necks, and short heads with broad foreheads. They may have calmer temperaments than horses and also a high level of intelligence that may or may not be used to cooperate with human handlers.[31] Small size, by itself, is not an exclusive determinant. For example, the Shetland pony which averages 10 hands (40 inches, 102 cm), is considered a pony.[31] Conversely, breeds such as the Falabella and other miniature horses, which can be no taller than 76 centimetres; 7.2 hands (30 in), are classified by their registries as very small horses, not ponies.[37]\n Horses have 64 chromosomes.[38] The horse genome was sequenced in 2007. It contains 2.7 billion DNA base pairs,[39] which is larger than the dog genome, but smaller than the human genome or the bovine genome.[40] The map is available to researchers.[41]\n Horses exhibit a diverse array of coat colors and distinctive markings, described by a specialized vocabulary. Often, a horse is classified first by its coat color, before breed or sex.[42] Horses of the same color may be distinguished from one another by white markings,[43] which, along with various spotting patterns, are inherited separately from coat color.[44]\n Many genes that create horse coat colors and patterns have been identified. Current genetic tests can identify at least 13 different alleles influencing coat color,[45] and research continues to discover new genes linked to specific traits. The basic coat colors of chestnut and black are determined by the gene controlled by the Melanocortin 1 receptor,[46] also known as the \"extension gene\" or \"red factor\",[45] as its recessive form is \"red\" (chestnut) and its dominant form is black.[47] Additional genes control suppression of black color to point coloration that results in a bay, spotting patterns such as pinto or leopard, dilution genes such as palomino or dun, as well as greying, and all the other factors that create the many possible coat colors found in horses.[45]\n Horses that have a white coat color are often mislabeled; a horse that looks \"white\" is usually a middle-aged or older gray. Grays are born a darker shade, get lighter as they age, but usually keep black skin underneath their white hair coat (with the exception of pink skin under white markings). The only horses properly called white are born with a predominantly white hair coat and pink skin, a fairly rare occurrence.[47] Different and unrelated genetic factors can produce white coat colors in horses, including several different alleles of dominant white and the sabino-1 gene.[48] However, there are no \"albino\" horses, defined as having both pink skin and red eyes.[49]\n Gestation lasts approximately 340 days, with an average range 320–370 days,[50][51] and usually results in one foal; twins are rare.[52] Horses are a precocial species, and foals are capable of standing and running within a short time following birth.[53] Foals are usually born in the spring. The estrous cycle of a mare occurs roughly every 19–22 days and occurs from early spring into autumn. Most mares enter an anestrus period during the winter and thus do not cycle in this period.[54] Foals are generally weaned from their mothers between four and six months of age.[55]\n Horses, particularly colts, are sometimes physically capable of reproduction at about 18 months, but domesticated horses are rarely allowed to breed before the age of three, especially females.[56] Horses four years old are considered mature, although the skeleton normally continues to develop until the age of six; maturation also depends on the horse's size, breed, sex, and quality of care. Larger horses have larger bones; therefore, not only do the bones take longer to form bone tissue, but the epiphyseal plates are larger and take longer to convert from cartilage to bone. These plates convert after the other parts of the bones, and are crucial to development.[57]\n Depending on maturity, breed, and work expected, horses are usually put under saddle and trained to be ridden between the ages of two and four.[58] Although Thoroughbred race horses are put on the track as young as the age of two in some countries,[59] horses specifically bred for sports such as dressage are generally not put under saddle until they are three or four years old, because their bones and muscles are not solidly developed.[60] For endurance riding competition, horses are not deemed mature enough to compete until they are a full 60 calendar months (five years) old.[12]\n The horse skeleton averages 205 bones.[61] A significant difference between the horse skeleton and that of a human is the lack of a collarbone—the horse's forelimbs are attached to the spinal column by a powerful set of muscles, tendons, and ligaments that attach the shoulder blade to the torso. The horse's four legs and hooves are also unique structures. Their leg bones are proportioned differently from those of a human. For example, the body part that is called a horse's \"knee\" is actually made up of the carpal bones that correspond to the human wrist. Similarly, the hock contains bones equivalent to those in the human ankle and heel. The lower leg bones of a horse correspond to the bones of the human hand or foot, and the fetlock (incorrectly called the \"ankle\") is actually the proximal sesamoid bones between the cannon bones (a single equivalent to the human metacarpal or metatarsal bones) and the proximal phalanges, located where one finds the \"knuckles\" of a human. A horse also has no muscles in its legs below the knees and hocks, only skin, hair, bone, tendons, ligaments, cartilage, and the assorted specialized tissues that make up the hoof.[62]\n The critical importance of the feet and legs is summed up by the traditional adage, \"no foot, no horse\".[63] The horse hoof begins with the distal phalanges, the equivalent of the human fingertip or tip of the toe, surrounded by cartilage and other specialized, blood-rich soft tissues such as the laminae. The exterior hoof wall and horn of the sole is made of keratin, the same material as a human fingernail.[64] The result is that a horse, weighing on average 500 kilograms (1,100 lb),[65] travels on the same bones as would a human on tiptoe.[66] For the protection of the hoof under certain conditions, some horses have horseshoes placed on their feet by a professional farrier. The hoof continually grows, and in most domesticated horses needs to be trimmed (and horseshoes reset, if used) every five to eight weeks,[67] though the hooves of horses in the wild wear down and regrow at a rate suitable for their terrain.\n Horses are adapted to grazing. In an adult horse, there are 12 incisors at the front of the mouth, adapted to biting off the grass or other vegetation. There are 24 teeth adapted for chewing, the premolars and molars, at the back of the mouth. Stallions and geldings have four additional teeth just behind the incisors, a type of canine teeth called \"tushes\". Some horses, both male and female, will also develop one to four very small vestigial teeth in front of the molars, known as \"wolf\" teeth, which are generally removed because they can interfere with the bit. There is an empty interdental space between the incisors and the molars where the bit rests directly on the gums, or \"bars\" of the horse's mouth when the horse is bridled.[68]\n An estimate of a horse's age can be made from looking at its teeth. The teeth continue to erupt throughout life and are worn down by grazing. Therefore, the incisors show changes as the horse ages; they develop a distinct wear pattern, changes in tooth shape, and changes in the angle at which the chewing surfaces meet. This allows a very rough estimate of a horse's age, although diet and veterinary care can also affect the rate of tooth wear.[7]\n Horses are herbivores with a digestive system adapted to a forage diet of grasses and other plant material, consumed steadily throughout the day. Therefore, compared to humans, they have a relatively small stomach but very long intestines to facilitate a steady flow of nutrients. A 450-kilogram (990 lb) horse will eat 7 to 11 kilograms (15 to 24 lb) of food per day and, under normal use, drink 38 to 45 litres (8.4 to 9.9 imp gal; 10 to 12 US gal) of water. Horses are not ruminants, they have only one stomach, like humans, but unlike humans, they can digest cellulose, a major component of grass. Horses are hindgut fermenters. Cellulose fermentation by symbiotic bacteria occurs in the cecum, or \"water gut\", which food goes through before reaching the large intestine. Horses cannot vomit, so digestion problems can quickly cause colic, a leading cause of death.[69] Horses do not have a gallbladder; however, they seem to tolerate high amounts of fat in their diet despite lack of a gallbladder.[70][71]\n The horses' senses are based on their status as prey animals, where they must be aware of their surroundings at all times.[72] They have the largest eyes of any land mammal,[73] and are lateral-eyed, meaning that their eyes are positioned on the sides of their heads.[74] This means that horses have a range of vision of more than 350°, with approximately 65° of this being binocular vision and the remaining 285° monocular vision.[73] Horses have excellent day and night vision, but they have two-color, or dichromatic vision; their color vision is somewhat like red-green color blindness in humans, where certain colors, especially red and related colors, appear as a shade of green.[75]\n Their sense of smell, while much better than that of humans, is not quite as good as that of a dog. It is believed to play a key role in the social interactions of horses as well as detecting other key scents in the environment. Horses have two olfactory centers. The first system is in the nostrils and nasal cavity, which analyze a wide range of odors. The second, located under the nasal cavity, are the vomeronasal organs, also called Jacobson's organs. These have a separate nerve pathway to the brain and appear to primarily analyze pheromones.[76]\n A horse's hearing is good,[72] and the pinna of each ear can rotate up to 180°, giving the potential for 360° hearing without having to move the head.[77] Noise impacts the behavior of horses and certain kinds of noise may contribute to stress: a 2013 study in the UK indicated that stabled horses were calmest in a quiet setting, or if listening to country or classical music, but displayed signs of nervousness when listening to jazz or rock music. This study also recommended keeping music under a volume of 21 decibels.[78] An Australian study found that stabled racehorses listening to talk radio had a higher rate of gastric ulcers than horses listening to music, and racehorses stabled where a radio was played had a higher overall rate of ulceration than horses stabled where there was no radio playing.[79]\n Horses have a great sense of balance, due partly to their ability to feel their footing and partly to highly developed proprioception—the unconscious sense of where the body and limbs are at all times.[80] A horse's sense of touch is well-developed. The most sensitive areas are around the eyes, ears, and nose.[81] Horses are able to sense contact as subtle as an insect landing anywhere on the body.[82]\n Horses have an advanced sense of taste, which allows them to sort through fodder and choose what they would most like to eat,[83] and their prehensile lips can easily sort even small grains. Horses generally will not eat poisonous plants, however, there are exceptions; horses will occasionally eat toxic amounts of poisonous plants even when there is adequate healthy food.[84]\n All horses move naturally with four basic gaits:[85]\n Besides these basic gaits, some horses perform a two-beat pace, instead of the trot.[88] There also are several four-beat 'ambling' gaits that are approximately the speed of a trot or pace, though smoother to ride. These include the lateral rack, running walk, and tölt as well as the diagonal fox trot.[89] Ambling gaits are often genetic in some breeds, known collectively as gaited horses.[90] These horses replace the trot with one of the ambling gaits.[91]\n Horses are prey animals with a strong fight-or-flight response. Their first reaction to a threat is to startle and usually flee, although they will stand their ground and defend themselves when flight is impossible or if their young are threatened.[92] They also tend to be curious; when startled, they will often hesitate an instant to ascertain the cause of their fright, and may not always flee from something that they perceive as non-threatening. Most light horse riding breeds were developed for speed, agility, alertness and endurance; natural qualities that extend from their wild ancestors. However, through selective breeding, some breeds of horses are quite docile, particularly certain draft horses.[93]\n Horses are herd animals, with a clear hierarchy of rank, led by a dominant individual, usually a mare. They are also social creatures that are able to form companionship attachments to their own species and to other animals, including humans. They communicate in various ways, including vocalizations such as nickering or whinnying, mutual grooming, and body language. Many horses will become difficult to manage if they are isolated, but with training, horses can learn to accept a human as a companion, and thus be comfortable away from other horses.[94] However, when confined with insufficient companionship, exercise, or stimulation, individuals may develop stable vices, an assortment of bad habits, mostly stereotypies of psychological origin, that include wood chewing, wall kicking, \"weaving\" (rocking back and forth), and other problems.[95]\n Studies have indicated that horses perform a number of cognitive tasks on a daily basis, meeting mental challenges that include food procurement and identification of individuals within a social system. They also have good spatial discrimination abilities.[96] They are naturally curious and apt to investigate things they have not seen before.[97] Studies have assessed equine intelligence in areas such as problem solving, speed of learning, and memory. Horses excel at simple learning, but also are able to use more advanced cognitive abilities that involve categorization and concept learning. They can learn using habituation, desensitization, classical conditioning, and operant conditioning, and positive and negative reinforcement.[96] One study has indicated that horses can differentiate between \"more or less\" if the quantity involved is less than four.[98]\n Domesticated horses may face greater mental challenges than wild horses, because they live in artificial environments that prevent instinctive behavior whilst also learning tasks that are not natural.[96] Horses are animals of habit that respond well to regimentation, and respond best when the same routines and techniques are used consistently. One trainer believes that \"intelligent\" horses are reflections of intelligent trainers who effectively use response conditioning techniques and positive reinforcement to train in the style that best fits with an individual animal's natural inclinations.[99]\n Horses are mammals. As such, they are warm-blooded, or endothermic creatures, as opposed to cold-blooded, or poikilothermic animals. However, these words have developed a separate meaning in the context of equine terminology, used to describe temperament, not body temperature. For example, the \"hot-bloods\", such as many race horses, exhibit more sensitivity and energy,[100] while the \"cold-bloods\", such as most draft breeds, are quieter and calmer.[101] Sometimes \"hot-bloods\" are classified as \"light horses\" or \"riding horses\",[102] with the \"cold-bloods\" classified as \"draft horses\" or \"work horses\".[103]\n \"Hot blooded\" breeds include \"oriental horses\" such as the Akhal-Teke, Arabian horse, Barb, and now-extinct Turkoman horse, as well as the Thoroughbred, a breed developed in England from the older oriental breeds.[100] Hot bloods tend to be spirited, bold, and learn quickly. They are bred for agility and speed.[104] They tend to be physically refined—thin-skinned, slim, and long-legged.[105] The original oriental breeds were brought to Europe from the Middle East and North Africa when European breeders wished to infuse these traits into racing and light cavalry horses.[106][107]\n Muscular, heavy draft horses are known as \"cold bloods\", as they are bred not only for strength, but also to have the calm, patient temperament needed to pull a plow or a heavy carriage full of people.[101] They are sometimes nicknamed \"gentle giants\".[108] Well-known draft breeds include the Belgian and the Clydesdale.[108] Some, like the Percheron, are lighter and livelier, developed to pull carriages or to plow large fields in drier climates.[109] Others, such as the Shire, are slower and more powerful, bred to plow fields with heavy, clay-based soils.[110] The cold-blooded group also includes some pony breeds.[111]\n \"Warmblood\" breeds, such as the Trakehner or Hanoverian, developed when European carriage and war horses were crossed with Arabians or Thoroughbreds, producing a riding horse with more refinement than a draft horse, but greater size and milder temperament than a lighter breed.[112] Certain pony breeds with warmblood characteristics have been developed for smaller riders.[113] Warmbloods are considered a \"light horse\" or \"riding horse\".[102]\n Today, the term \"Warmblood\" refers to a specific subset of sport horse breeds that are used for competition in dressage and show jumping.[114] Strictly speaking, the term \"warm blood\" refers to any cross between cold-blooded and hot-blooded breeds.[115] Examples include breeds such as the Irish Draught or the Cleveland Bay. The term was once used to refer to breeds of light riding horse other than Thoroughbreds or Arabians, such as the Morgan horse.[104]\n Horses are able to sleep both standing up and lying down. In an adaptation from life in the wild, horses are able to enter light sleep by using a \"stay apparatus\" in their legs, allowing them to doze without collapsing.[116] Horses sleep better when in groups because some animals will sleep while others stand guard to watch for predators. A horse kept alone will not sleep well because its instincts are to keep a constant eye out for danger.[117]\n Unlike humans, horses do not sleep in a solid, unbroken period of time, but take many short periods of rest. Horses spend four to fifteen hours a day in standing rest, and from a few minutes to several hours lying down. Total sleep time in a 24-hour period may range from several minutes to a couple of hours,[117] mostly in short intervals of about 15 minutes each.[118] The average sleep time of a domestic horse is said to be 2.9 hours per day.[119]\n Horses must lie down to reach REM sleep. They only have to lie down for an hour or two every few days to meet their minimum REM sleep requirements.[117] However, if a horse is never allowed to lie down, after several days it will become sleep-deprived, and in rare cases may suddenly collapse as it involuntarily slips into REM sleep while still standing.[120] This condition differs from narcolepsy, although horses may also suffer from that disorder.[121]\n The horse adapted to survive in areas of wide-open terrain with sparse vegetation, surviving in an ecosystem where other large grazing animals, especially ruminants, could not.[122] Horses and other equids are odd-toed ungulates of the order Perissodactyla, a group of mammals dominant during the Tertiary period. In the past, this order contained 14 families, but only three—Equidae (the horse and related species), Tapiridae (the tapir), and Rhinocerotidae (the rhinoceroses)—have survived to the present day.[123]\n The earliest known member of the family Equidae was the Hyracotherium, which lived between 45 and 55 million years ago, during the Eocene period. It had 4 toes on each front foot, and 3 toes on each back foot.[124] The extra toe on the front feet soon disappeared with the Mesohippus, which lived 32 to 37 million years ago.[125] Over time, the extra side toes shrank in size until they vanished. All that remains of them in modern horses is a set of small vestigial bones on the leg below the knee,[126] known informally as splint bones.[127] Their legs also lengthened as their toes disappeared until they were a hooved animal capable of running at great speed.[126] By about 5 million years ago, the modern Equus had evolved.[128] Equid teeth also evolved from browsing on soft, tropical plants to adapt to browsing of drier plant material, then to grazing of tougher plains grasses. Thus proto-horses changed from leaf-eating forest-dwellers to grass-eating inhabitants of semi-arid regions worldwide, including the steppes of Eurasia and the Great Plains of North America.\n By about 15,000 years ago, Equus ferus was a widespread holarctic species. Horse bones from this time period, the late Pleistocene, are found in Europe, Eurasia, Beringia, and North America.[129] Yet between 10,000 and 7,600 years ago, the horse became extinct in North America.[130][131][132] The reasons for this extinction are not fully known, but one theory notes that extinction in North America paralleled human arrival.[133] Another theory points to climate change, noting that approximately 12,500 years ago, the grasses characteristic of a steppe ecosystem gave way to shrub tundra, which was covered with unpalatable plants.[134]\n A truly wild horse is a species or subspecies with no ancestors that were ever successfully domesticated. Therefore, most \"wild\" horses today are actually feral horses, animals that escaped or were turned loose from domestic herds and the descendants of those animals.[135] Only two wild subspecies, the tarpan and the Przewalski's horse, survived into recorded history and only the latter survives today.\n The Przewalski's horse (Equus ferus przewalskii), named after the Russian explorer Nikolai Przhevalsky, is a rare Asian animal. It is also known as the Mongolian wild horse; Mongolian people know it as the taki, and the Kyrgyz people call it a kirtag. The subspecies was presumed extinct in the wild between 1969 and 1992, while a small breeding population survived in zoos around the world. In 1992, it was reestablished in the wild by the conservation efforts of numerous zoos.[136] Today, a small wild breeding population exists in Mongolia.[137][138] There are additional animals still maintained at zoos throughout the world.\n The question of whether the Przewalski's horse was ever domesticated was challenged in 2018 when DNA studies of horses found at Botai culture sites revealed captured animals with DNA markers of an ancestor to the Przewalski's horse. The study concluded that the Botai animals appear to have been an independent domestication attempt and apparently unsuccessful, as these genetic markers do not appear in modern domesticated horses.  However, the question of whether all Przewalski's horses descend from this population is also unresolved, as only one of seven modern Przewalski's horses in the study shared this ancestry.[139][140][141]\n The tarpan or European wild horse (Equus ferus ferus) was found in Europe and much of Asia. It survived into the historical era, but became extinct in 1909, when the last captive died in a Russian zoo.[142] Thus, the genetic line was lost. Attempts have been made to recreate the tarpan,[142][143][144] which resulted in horses with outward physical similarities, but nonetheless descended from domesticated ancestors and not true wild horses.\n Periodically, populations of horses in isolated areas are speculated to be relict populations of wild horses, but generally have been proven to be feral or domestic. For example, the Riwoche horse of Tibet was proposed as such,[138] but testing did not reveal genetic differences from domesticated horses.[145] Similarly, the Sorraia of Portugal was proposed as a direct descendant of the Tarpan on the basis of shared characteristics,[146][147] but genetic studies have shown that the Sorraia is more closely related to other horse breeds, and that the outward similarity is an unreliable measure of relatedness.[146][148]\n Besides the horse, there are six other species of genus Equus in the Equidae family. These are the ass or donkey, Equus asinus; the mountain zebra, Equus zebra; plains zebra, Equus quagga; Grévy's Zebra, Equus grevyi; the kiang, Equus kiang; and the onager, Equus hemionus.[149]\n Horses can crossbreed with other members of their genus. The most common hybrid is the mule, a cross between a \"jack\" (male donkey) and a mare. A related hybrid, a hinny, is a cross between a stallion and a \"jenny\" (female donkey).[150] Other hybrids include the zorse, a cross between a zebra and a horse.[151] With rare exceptions, most hybrids are sterile and cannot reproduce.[152]\n  Domestication of the horse most likely took place in central Asia prior to 3500 BCE. Two major sources of information are used to determine where and when the horse was first domesticated and how the domesticated horse spread around the world. The first source is based on palaeological and archaeological discoveries; the second source is a comparison of DNA obtained from modern horses to that from bones and teeth of ancient horse remains.\n The earliest archaeological evidence for the domestication of the horse comes from sites in Ukraine and Kazakhstan, dating to approximately 4000–3500 BCE.[153][154][155] By 3000 BCE, the horse was completely domesticated and by 2000 BCE there was a sharp increase in the number of horse bones found in human settlements in northwestern Europe, indicating the spread of domesticated horses throughout the continent.[156] The most recent, but most irrefutable evidence of domestication comes from sites where horse remains were interred with chariots in graves of the Sintashta and Petrovka cultures c. 2100 BCE.[157]\n A 2021 genetic study suggested that most modern domestic horses descend from the lower Volga-Don region. Ancient horse genomes indicate that these populations influenced almost all local populations as they expanded rapidly throughout Eurasia, beginning about 4,200 years ago. It also shows that certain adaptations were strongly selected due to riding, and that equestrian material culture, including Sintashta spoke-wheeled chariots spread with the horse itself.[158][159]\n Domestication is also studied by using the genetic material of present-day horses and comparing it with the genetic material present in the bones and teeth of horse remains found in archaeological and palaeological excavations. The variation in the genetic material shows that very few wild stallions contributed to the domestic horse,[160][161] while many mares were part of early domesticated herds.[148][162][163] This is reflected in the difference in genetic variation between the DNA that is passed on along the paternal, or sire line (Y-chromosome) versus that passed on along the maternal, or dam line (mitochondrial DNA). There are very low levels of Y-chromosome variability,[160][161] but a great deal of genetic variation in mitochondrial DNA.[148][162][163] There is also regional variation in mitochondrial DNA due to the inclusion of wild mares in domestic herds.[148][162][163][164] Another characteristic of domestication is an increase in coat color variation.[165] In horses, this increased dramatically between 5000 and 3000 BCE.[166]\n Before the availability of DNA techniques to resolve the questions related to the domestication of the horse, various hypotheses were proposed. One classification was based on body types and conformation, suggesting the presence of four basic prototypes that had adapted to their environment prior to domestication.[111] Another hypothesis held that the four prototypes originated from a single wild species and that all different body types were entirely a result of selective breeding after domestication.[167] However, the lack of a detectable substructure  in the horse has resulted in a rejection of both hypotheses.\n Feral horses are born and live in the wild, but are descended from domesticated animals.[135] Many populations of feral horses exist throughout the world.[168][169] Studies of feral herds have provided useful insights into the behavior of prehistoric horses,[170] as well as greater understanding of the instincts and behaviors that drive horses that live in domesticated conditions.[171]\n There are also semi-feral horses in many parts of the world, such as Dartmoor and the New Forest in the UK, where the animals are all privately owned but live for significant amounts of time in \"wild\" conditions on undeveloped, often public, lands. Owners of such animals often pay a fee for grazing rights.[172][173]\n The concept of purebred bloodstock and a controlled, written breed registry has come to be particularly significant and important in modern times. Sometimes purebred horses are incorrectly or inaccurately called \"thoroughbreds\". Thoroughbred is a specific breed of horse, while a \"purebred\" is a horse (or any other animal) with a defined pedigree recognized by a breed registry.[174] Horse breeds are groups of horses with distinctive characteristics that are transmitted consistently to their offspring, such as conformation, color, performance ability, or disposition. These inherited traits result from a combination of natural crosses and artificial selection methods. Horses have been selectively bred since their domestication. An early example of people who practiced selective horse breeding were the Bedouin, who had a reputation for careful practices, keeping extensive pedigrees of their Arabian horses and placing great value upon pure bloodlines.[175] These pedigrees were originally transmitted via an oral tradition.[176] In the 14th century, Carthusian monks of southern Spain kept meticulous pedigrees of bloodstock lineages still found today in the Andalusian horse.[177]\n Breeds developed due to a need for \"form to function\", the necessity to develop certain characteristics in order to perform a particular type of work.[178] Thus, a powerful but refined breed such as the Andalusian developed as riding horses with an aptitude for dressage.[178] Heavy draft horses were developed out of a need to perform demanding farm work and pull heavy wagons.[179] Other horse breeds had been developed specifically for light agricultural work, carriage and road work, various sport disciplines, or simply as pets.[180] Some breeds developed through centuries of crossing other breeds, while others descended from a single foundation sire, or other limited or restricted foundation bloodstock. One of the earliest formal registries was General Stud Book for Thoroughbreds, which began in 1791 and traced back to the foundation bloodstock for the breed.[181] There are more than 300 horse breeds in the world today.[182]\n Worldwide, horses play a role within human cultures and have done so for millennia. Horses are used for leisure activities, sports, and working purposes. The Food and Agriculture Organization (FAO) estimates that in 2008, there were almost 59,000,000 horses in the world, with around 33,500,000 in the Americas, 13,800,000 in Asia and 6,300,000 in Europe and smaller portions in Africa and Oceania. There are estimated to be 9,500,000 horses in the United States alone.[183] The American Horse Council estimates that horse-related activities have a direct impact on the economy of the United States of over $39 billion, and when indirect spending is considered, the impact is over $102 billion.[184] In a 2004 \"poll\" conducted by Animal Planet, more than 50,000 viewers from 73 countries voted for the horse as the world's 4th favorite animal.[185]\n Communication between human and horse is paramount in any equestrian activity;[186] to aid this process horses are usually ridden with a saddle on their backs to assist the rider with balance and positioning, and a bridle or related headgear to assist the rider in maintaining control.[187] Sometimes horses are ridden without a saddle,[188] and occasionally, horses are trained to perform without a bridle or other headgear.[189] Many horses are also driven, which requires a harness, bridle, and some type of vehicle.[190]\n Historically, equestrians honed their skills through games and races. Equestrian sports provided entertainment for crowds and honed the excellent horsemanship that was needed in battle. Many sports, such as dressage, eventing, and show jumping, have origins in military training, which were focused on control and balance of both horse and rider. Other sports, such as rodeo, developed from practical skills such as those needed on working ranches and stations. Sport hunting from horseback evolved from earlier practical hunting techniques.[186] Horse racing of all types evolved from impromptu competitions between riders or drivers. All forms of competition, requiring demanding and specialized skills from both horse and rider, resulted in the systematic development of specialized breeds and equipment for each sport. The popularity of equestrian sports through the centuries has resulted in the preservation of skills that would otherwise have disappeared after horses stopped being used in combat.[186]\n Horses are trained to be ridden or driven in a variety of sporting competitions. Examples include show jumping, dressage, three-day eventing, competitive driving, endurance riding, gymkhana, rodeos, and fox hunting.[191] Horse shows, which have their origins in medieval European fairs, are held around the world. They host a huge range of classes, covering all of the mounted and harness disciplines, as well as \"In-hand\" classes where the horses are led, rather than ridden, to be evaluated on their conformation. The method of judging varies with the discipline, but winning usually depends on style and ability of both horse and rider.[192]\nSports such as polo do not judge the horse itself, but rather use the horse as a partner for human competitors as a necessary part of the game. Although the horse requires specialized training to participate, the details of its performance are not judged, only the result of the rider's actions—be it getting a ball through a goal or some other task.[193] Examples of these sports of partnership between human and horse include jousting, in which the main goal is for one rider to unseat the other,[194] and buzkashi, a team game played throughout Central Asia, the aim being to capture a goat carcass while on horseback.[193]\n Horse racing is an equestrian sport and major international industry, watched in almost every nation of the world. There are three types: \"flat\" racing; steeplechasing, i.e. racing over jumps; and harness racing, where horses trot or pace while pulling a driver in a small, light cart known as a sulky.[195] A major part of horse racing's economic importance lies in the gambling associated with it.[196]\n There are certain jobs that horses do very well, and no technology has yet developed to fully replace them. For example, mounted police horses are still effective for certain types of patrol duties and crowd control.[197] Cattle ranches still require riders on horseback to round up cattle that are scattered across remote, rugged terrain.[198] Search and rescue organizations in some countries depend upon mounted teams to locate people, particularly hikers and children, and to provide disaster relief assistance.[199] Horses can also be used in areas where it is necessary to avoid vehicular disruption to delicate soil, such as nature reserves. They may also be the only form of transport allowed in wilderness areas. Horses are quieter than motorized vehicles. Law enforcement officers such as park rangers or game wardens may use horses for patrols, and horses or mules may also be used for clearing trails or other work in areas of rough terrain where vehicles are less effective.[200]\n Although machinery has replaced horses in many parts of the world, an estimated 100 million horses, donkeys and mules are still used for agriculture and transportation in less developed areas. This number includes around 27 million working animals in Africa alone.[201] Some land management practices such as cultivating and logging can be efficiently performed with horses. In agriculture, less fossil fuel is used and increased environmental conservation occurs over time with the use of draft animals such as horses.[202][203] Logging with horses can result in reduced damage to soil structure and less damage to trees due to more selective logging.[204]\n Horses have been used in warfare for most of recorded history. The first archaeological evidence of horses used in warfare dates to between 4000 and 3000 BCE,[205] and the use of horses in warfare was widespread by the end of the Bronze Age.[206][207] Although mechanization has largely replaced the horse as a weapon of war, horses are still seen today in limited military uses, mostly for ceremonial purposes, or for reconnaissance and transport activities in areas of rough terrain where motorized vehicles are ineffective. Horses have been used in the 21st century by the Janjaweed militias in the War in Darfur.[208]\n Modern horses are often used to reenact many of their historical work purposes. Horses are used, complete with equipment that is authentic or a meticulously recreated replica, in various live action historical reenactments of specific periods of history, especially recreations of famous battles.[209] Horses are also used to preserve cultural traditions and for ceremonial purposes. Countries such as the United Kingdom still use horse-drawn carriages to convey royalty and other VIPs to and from certain culturally significant events.[210] Public exhibitions are another example, such as the Budweiser Clydesdales, seen in parades and other public settings, a team of draft horses that pull a beer wagon similar to that used before the invention of the modern motorized truck.[211]\n Horses are frequently used in television, films and literature. They are sometimes featured as a major character in films about particular animals, but also used as visual elements that assure the accuracy of historical stories.[212] Both live horses and iconic images of horses are used in advertising to promote a variety of products.[213] The horse frequently appears in coats of arms in heraldry, in a variety of poses and equipment.[214] The mythologies of many cultures, including Greco-Roman, Hindu, Islamic, and Germanic, include references to both normal horses and those with wings or additional limbs, and multiple myths also call upon the horse to draw the chariots of the Moon and Sun.[215] The horse also appears in the 12-year cycle of animals in the Chinese zodiac related to the Chinese calendar.[216]\n Horses serve as the inspiration for many modern automobile names and logos, including the Ford Pinto, Ford Bronco, Ford Mustang, Hyundai Equus, Hyundai Pony, Mitsubishi Starion, Subaru Brumby, Mitsubishi Colt\/Dodge Colt, Pinzgauer, Steyr-Puch Haflinger, Pegaso, Porsche, Rolls-Royce Camargue, Ferrari, Carlsson, Kamaz, Corre La Licorne, Iran Khodro, Eicher, and Baojun.[217][218][219] Indian TVS Motor Company also uses a horse on their motorcycles & scooters.\n People of all ages with physical and mental disabilities obtain beneficial results from an association with horses. Therapeutic riding is used to mentally and physically stimulate disabled persons and help them improve their lives through improved balance and coordination, increased self-confidence, and a greater feeling of freedom and independence.[220] The benefits of equestrian activity for people with disabilities has also been recognized with the addition of equestrian events to the Paralympic Games and recognition of para-equestrian events by the International Federation for Equestrian Sports (FEI).[221] Hippotherapy and therapeutic horseback riding are names for different physical, occupational, and speech therapy treatment strategies that use equine movement. In hippotherapy, a therapist uses the horse's movement to improve their patient's cognitive, coordination, balance, and fine motor skills, whereas therapeutic horseback riding uses specific riding skills.[222]\n Horses also provide psychological benefits to people whether they actually ride or not. \"Equine-assisted\" or \"equine-facilitated\" therapy is a form of experiential psychotherapy that uses horses as companion animals to assist people with mental illness, including anxiety disorders, psychotic disorders, mood disorders, behavioral difficulties, and those who are going through major life changes.[223] There are also experimental programs using horses in prison settings. Exposure to horses appears to improve the behavior of inmates and help reduce recidivism when they leave.[224]\n Horses are raw material for many products made by humans throughout history, including byproducts from the slaughter of horses as well as materials collected from living horses.\n Products collected from living horses include mare's milk, used by people with large horse herds, such as the Mongols, who let it ferment to produce kumis.[225] Horse blood was once used as food by the Mongols and other nomadic tribes, who found it a convenient source of nutrition when traveling. Drinking their own horses' blood allowed the Mongols to ride for extended periods of time without stopping to eat.[225] The drug Premarin is a mixture of estrogens extracted from the urine of pregnant mares (pregnant mares' urine), and was previously a widely used drug for hormone replacement therapy.[226] The tail hair of horses can be used for making bows for string instruments such as the violin, viola, cello, and double bass.[227]\n Horse meat has been used as food for humans and carnivorous animals throughout the ages. Approximately 5 million horses are slaughtered each year for meat worldwide.[228] It is eaten in many parts of the world, though consumption is taboo in some cultures,[229] and a subject of political controversy in others.[230] Horsehide leather has been used for boots, gloves, jackets,[231] baseballs,[232] and baseball gloves. Horse hooves can also be used to produce animal glue.[233] Horse bones can be used to make implements.[234] Specifically, in Italian cuisine, the horse tibia is sharpened into a probe called a spinto, which is used to test the readiness of a (pig) ham as it cures.[235] In Asia, the saba is a horsehide vessel used in the production of kumis.[236]\n Horses are grazing animals, and their major source of nutrients is good-quality forage from hay or pasture.[237] They can consume approximately 2% to 2.5% of their body weight in dry feed each day. Therefore, a 450-kilogram (990 lb) adult horse could eat up to 11 kilograms (24 lb) of food.[238] Sometimes, concentrated feed such as grain is fed in addition to pasture or hay, especially when the animal is very active.[239] When grain is fed, equine nutritionists recommend that 50% or more of the animal's diet by weight should still be forage.[240]\n Horses require a plentiful supply of clean water, a minimum of 38 to 45 litres (10 to 12 US gal) per day.[241] Although horses are adapted to live outside, they require shelter from the wind and precipitation, which can range from a simple shed or shelter to an elaborate stable.[242]\n Horses require routine hoof care from a farrier, as well as vaccinations to protect against various diseases, and dental examinations from a veterinarian or a specialized equine dentist.[243] If horses are kept inside in a barn, they require regular daily exercise for their physical health and mental well-being.[244] When turned outside, they require well-maintained, sturdy fences to be safely contained.[245] Regular grooming is also helpful to help the horse maintain good health of the hair coat and underlying skin.[246]\n As of 2019, there are around 17 million horses in the world. Healthy body temperature for adult horses is in the range between 37.5 and 38.5 °C (99.5 and 101.3 °F), which they can maintain while ambient temperatures are between 5 and 25 °C (41 and 77 °F). However, strenuous exercise increases core body temperature by 1 °C (1.8 °F)\/minute, as 80% of the energy used by equine muscles is released as heat. Along with bovines and primates, equines are the only animal group which use sweating as their primary method of thermoregulation: in fact, it can account for up to 70% of their heat loss, and horses sweat three times more than humans while undergoing comparably strenuous physical activity. Unlike humans, this sweat is created not by eccrine glands but by apocrine glands.[248] In hot conditions, horses during three hours of moderate-intersity exercise can loss 30 to 35 L of water and 100g of sodium, 198 g of choloride and 45 g of potassium.[248] In another difference from humans, their sweat is hypertonic, and contains a protein called latherin,[249] which enables it to spread across their body easier, and to foam, rather than to drip off. These adaptations are partly to compensate for their lower body surface-to-mass ratio, which makes it more difficult for horses to passively radiate heat. Yet, prolonged exposure to very hot and\/or humid conditions will lead to consequences such as anhidrosis, heat stroke, or brain damage, potentially culminating in death if not addressed with measures like cold water applications. Additionally, around 10% of incidents associated with horse transport have been attributed to heat stress. These issues are expected to worsen in the future.[247]\n"}
{"key":"Horse","link":"https:\/\/en.wikipedia.org\/wiki\/Animal","headline":"Animal - Wikipedia","content":"\n Animals are multicellular, eukaryotic organisms in the biological kingdom Animalia. With few exceptions, animals consume organic material, breathe oxygen, have myocytes and are able to move, can reproduce sexually, and grow from a hollow sphere of cells, the blastula, during embryonic development. Animals form a single clade.\n Over 1.5 million living animal species have been described—of which around 1.05 million are insects, over 85,000 are molluscs, and around 65,000 are vertebrates. It has been estimated there are as many as 7.77 million animal species on Earth. Animal body lengths range from 8.5 μm (0.00033 in) to 33.6 m (110 ft). They have complex ecologies and interactions with each other and their environments, forming intricate food webs. The scientific study of animals is known as zoology, and the study of animal behaviors is known as ethology.\n Most living animal species belong to the infrakingdom Bilateria, a highly proliferative clade whose members have a bilaterally symmetric body plan. Extant bilaterians include the basal group Xenacoelomorpha, but the vast majority belong to two large superphyla: the protostomes, which include phyla such as arthropods, molluscs, flatworms, annelids and nematodes, etc.; and the deuterostomes, which include the three phyla echinoderms, hemichordates and chordates, the latter with the vertebrates being its most successful subphylum. Precambrian life forms interpreted as early complex animals were already present in the Ediacaran biota of the late Proterozoic, but fossils of primitive sponge and other speculative early animals have been dated to as early as the Tonian period. Nearly all modern animal phyla became clearly established in the fossil record as marine species during the Cambrian explosion, which began around 539 million years ago (Mya), and most classes during the Ordovician radiation 485.4 Mya. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 Mya during the Cryogenian period.\n Historically, Aristotle divided animals into those with blood and those without. Carl Linnaeus created the first hierarchical biological classification for animals in 1758 with his Systema Naturae, which Jean-Baptiste Lamarck expanded into 14 phyla by 1809. In 1874, Ernst Haeckel divided the animal kingdom into the multicellular Metazoa (now synonymous with Animalia) and the Protozoa, single-celled organisms no longer considered animals. In modern times, the biological classification of animals relies on advanced techniques, such as molecular phylogenetics, which are effective at demonstrating the evolutionary relationships between taxa.\n Humans make use of many other animal species for food (including meat, eggs and dairies), for materials (such as leather, fur and wool), as pets and as working animals for transportation, and services. Dogs, the first domesticated animal, have been used in hunting, in security and in warfare, as have horses, pigeons and birds of prey, while other terrestrial and aquatic animals are hunted for sports, trophies or profits. Non-human animals are also an important cultural element of human evolution, having appeared in cave arts and totems since the earliest times, and are frequently featured in mythology, religion, arts, literature, heraldry, politics and sports.\n The word animal comes from the Latin noun animal of the same meaning, which is itself derived from Latin animalis 'having breath or soul'.[4] The biological definition includes all members of the kingdom Animalia.[5] In colloquial usage, the term animal is often used to refer only to nonhuman animals.[6][7][8][9] The term metazoa is derived from Ancient Greek μετα (meta) 'after' (in biology, the prefix meta- stands for 'later') and ζῷᾰ (zōia) 'animals', plural of ζῷον zōion 'animal'.[10][11]\n Animals have several characteristics that set them apart from other living things. Animals are eukaryotic and multicellular.[12] Unlike plants and algae, which produce their own nutrients,[13] animals are heterotrophic,[14][15] feeding on organic material and digesting it internally.[16] With very few exceptions, animals respire aerobically.[a][18] All animals are motile[19] (able to spontaneously move their bodies) during at least part of their life cycle, but some animals, such as sponges, corals, mussels, and barnacles, later become sessile. The blastula is a stage in embryonic development that is unique to animals, allowing cells to be differentiated into specialised tissues and organs.[20]\n All animals are composed of cells, surrounded by a characteristic extracellular matrix composed of collagen and elastic glycoproteins.[21] During development, the animal extracellular matrix forms a relatively flexible framework upon which cells can move about and be reorganised, making the formation of complex structures possible. This may be calcified, forming structures such as shells, bones, and spicules.[22] In contrast, the cells of other multicellular organisms (primarily algae, plants, and fungi) are held in place by cell walls, and so develop by progressive growth.[23] Animal cells uniquely possess the cell junctions called tight junctions, gap junctions, and desmosomes.[24]\n With few exceptions—in particular, the sponges and placozoans—animal bodies are differentiated into tissues.[25] These include muscles, which enable locomotion, and nerve tissues, which transmit signals and coordinate the body. Typically, there is also an internal digestive chamber with either one opening (in Ctenophora, Cnidaria, and flatworms) or two openings (in most bilaterians).[26]\n Nearly all animals make use of some form of sexual reproduction.[27] They produce haploid gametes by meiosis; the smaller, motile gametes are spermatozoa and the larger, non-motile gametes are ova.[28] These fuse to form zygotes,[29] which develop via mitosis into a hollow sphere, called a blastula. In sponges, blastula larvae swim to a new location, attach to the seabed, and develop into a new sponge.[30] In most other groups, the blastula undergoes more complicated rearrangement.[31] It first invaginates to form a gastrula with a digestive chamber and two separate germ layers, an external ectoderm and an internal endoderm.[32] In most cases, a third germ layer, the mesoderm, also develops between them.[33] These germ layers then differentiate to form tissues and organs.[34]\n Repeated instances of mating with a close relative during sexual reproduction generally leads to inbreeding depression within a population due to the increased prevalence of harmful recessive traits.[35][36] Animals have evolved numerous mechanisms for avoiding close inbreeding.[37]\n Some animals are capable of asexual reproduction, which often results in a genetic clone of the parent. This may take place through fragmentation; budding, such as in Hydra and other cnidarians; or parthenogenesis, where fertile eggs are produced without mating, such as in aphids.[38][39]\n Animals are categorised into ecological groups depending on their trophic levels and how they consume organic material. Such groupings include carnivores (further divided into subcategories such as piscivores, insectivores, ovivores, etc.), herbivores (subcategorized into folivores, graminivores, frugivores, granivores, nectarivores, algivores, etc.), omnivores, fungivores, scavengers\/detritivores,[40] and parasites.[41] Interactions between animals of each biome form complex food webs within that ecosystem. In carnivorous or omnivorous species, predation is a consumer–resource interaction where the predator feeds on another organism, its prey,[42] who often evolves anti-predator adaptations to avoid being fed upon. Selective pressures imposed on one another lead to an evolutionary arms race between predator and prey, resulting in various antagonistic\/competitive coevolutions.[43][44] Almost all multicellular predators are animals.[45] Some consumers use multiple methods; for example, in parasitoid wasps, the larvae feed on the hosts' living tissues, killing them in the process,[46] but the adults primarily consume nectar from flowers.[47] Other animals may have very specific feeding behaviours, such as hawksbill sea turtles primarily eating sponges.[48]\n Most animals rely on biomass and bioenergy produced by plants and phytoplanktons (collectively called producers) through photosynthesis. Herbivores, as primary consumers, eat the plant material directly to digest and absorb the nutrients, while carnivores and other animals on higher trophic levels   indirectly acquire the nutrients by eating the herbivores or other animals that have eaten the herbivores. Animals oxidize carbohydrates, lipids, proteins and other biomolecules, which allows the animal to grow and to sustain basal metabolism and fuel other biological processes such as locomotion.[49][50][51] Some benthic animals living close to hydrothermal vents and cold seeps on the dark sea floor consume organic matter produced through chemosynthesis (via oxidizing inorganic compounds such as hydrogen sulfide) by archaea and bacteria.[52]\n Animals evolved in the sea. Lineages of arthropods colonised land around the same time as land plants, probably between 510 and 471 million years ago during the Late Cambrian or Early Ordovician.[53] Vertebrates such as the lobe-finned fish Tiktaalik started to move on to land in the late Devonian, about 375 million years ago.[54][55] Animals occupy virtually all of earth's habitats and microhabitats, with faunas adapted to salt water, hydrothermal vents, fresh water, hot springs, swamps, forests, pastures, deserts, air, and the interiors of other organisms.[56] Animals are however not particularly heat tolerant; very few of them can survive at constant temperatures above 50 °C (122 °F).[57] Only very few species of animals (mostly nematodes) inhabit the most extreme cold deserts of continental Antarctica.[58]\n The blue whale (Balaenoptera musculus) is the largest animal that has ever lived, weighing up to 190 tonnes and measuring up to 33.6 metres (110 ft) long.[59][60][61] The largest extant terrestrial animal is the African bush elephant (Loxodonta africana), weighing up to 12.25 tonnes[59] and measuring up to 10.67 metres (35.0 ft) long.[59] The largest terrestrial animals that ever lived were titanosaur sauropod dinosaurs such as Argentinosaurus, which may have weighed as much as 73 tonnes, and Supersaurus which may have reached 39 meters.[62][63] Several animals are microscopic; some Myxozoa (obligate parasites within the Cnidaria) never grow larger than 20 µm,[64] and one of the smallest species (Myxobolus shekel) is no more than 8.5 µm when fully grown.[65]\n The following table lists estimated numbers of described extant species for the major animal phyla,[66] along with their principal habitats (terrestrial, fresh water,[67] and marine),[68] and free-living or parasitic ways of life.[69] Species estimates shown here are based on numbers described scientifically; much larger estimates have been calculated based on various means of prediction, and these can vary wildly. For instance, around 25,000–27,000 species of nematodes have been described, while published estimates of the total number of nematode species include 10,000–20,000; 500,000; 10 million; and 100 million.[70] Using patterns within the taxonomic hierarchy, the total number of animal species—including those not yet described—was calculated to be about 7.77 million in 2011.[71][72][b]\n 3,000–6,500[81]\n 4,000–25,000[81]\n Animals are found as long ago as the Ediacaran biota, towards the end of the Precambrian, and possibly somewhat earlier. It had long been doubted whether these life-forms included animals,[87][88][89] but the discovery of the animal lipid cholesterol in fossils of Dickinsonia establishes their nature.[90] Animals are thought to have originated under low-oxygen conditions, suggesting that they were capable of living entirely by anaerobic respiration, but as they became specialized for aerobic metabolism they became fully dependent on oxygen in their environments.[91]\n Many animal phyla first appear in the fossil record during the Cambrian explosion, starting about 539 million years ago, in beds such as the Burgess shale.[92] Extant phyla in these rocks include molluscs, brachiopods, onychophorans, tardigrades, arthropods, echinoderms and hemichordates, along with numerous now-extinct forms such as the predatory Anomalocaris. The apparent suddenness of the event may however be an artefact of the fossil record, rather than showing that all these animals appeared simultaneously.[93][94][95][96] That view is supported by the discovery of Auroralumina attenboroughii, the earliest known Ediacaran crown-group cnidarian (557–562 mya, some 20 million years before the Cambrian explosion) from Charnwood Forest, England. It is thought to be one of the earliest predators, catching small prey with its nematocysts as modern cnidarians do.[97]\n Some palaeontologists have suggested that animals appeared much earlier than the Cambrian explosion, possibly as early as 1 billion years ago.[98] Early fossils that might represent animals appear for example in the 665-million-year-old rocks of the Trezona Formation of South Australia. These fossils are interpreted as most probably being early sponges.[99]\nTrace fossils such as tracks and burrows found in the Tonian period (from 1 gya) may indicate the presence of triploblastic worm-like animals, roughly as large (about 5 mm wide) and complex as earthworms.[100] However, similar tracks are produced by the giant single-celled protist Gromia sphaerica, so the Tonian trace fossils may not indicate early animal evolution.[101][102] Around the same time, the layered mats of microorganisms called stromatolites decreased in diversity, perhaps due to grazing by newly evolved animals.[103] Objects such as sediment-filled tubes that resemble trace fossils of the burrows of wormlike animals have been found in 1.2 gya rocks in North America, in 1.5 gya rocks in Australia and North America, and in 1.7 gya rocks in Australia. Their interpretation as having an animal origin is disputed, as they might be water-escape or other structures.[104][105]\n Animals are monophyletic, meaning they are derived from a common ancestor. Animals are sister to the Choanoflagellata, with which they form the Choanozoa.[106]  \nThe dates on the phylogenetic tree indicate approximately how many millions of years ago (mya) the lineages split.[107][108][109][110][111]\n Ros-Rocher and colleagues (2021) trace the origins of animals to unicellular ancestors, providing the external phylogeny shown in the cladogram. Uncertainty of relationships is indicated with dashed lines.[112]\n Holomycota (inc. fungi) \n Ichthyosporea \n Pluriformea \n Filasterea \n \n \n The most basal animals, the Porifera, Ctenophora, Cnidaria, and Placozoa, have body plans that lack bilateral symmetry. Their relationships are still disputed; the sister group to all other animals could be the Porifera or the Ctenophora,[113] both of which lack hox genes, important in body plan development.[114]\n These genes are found in the Placozoa[115][116] and the higher animals, the Bilateria.[117][118] 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago in the Precambrian. 25 of these are novel core gene groups, found only in animals; of those, 8 are for essential components of the Wnt and TGF-beta signalling pathways which may have enabled animals to become multicellular by providing a pattern for the body's system of axes (in three dimensions), and another 7 are for transcription factors including homeodomain proteins involved in the control of development.[119][120]\n Giribet and Edgecombe (2020) provide what they consider to be a consensus internal phylogeny of the animals, embodying uncertainty about the structure at the base of the tree (dashed lines).[121]\n Porifera \n Ctenophora \n Placozoa\n Cnidaria \n Xenacoelomorpha \n Ambulacraria \n Chordata \n Ecdysozoa \n Spiralia \n An alternative phylogeny, from Kapli and colleagues (2021), proposes a clade Xenambulacraria for the Xenacoelamorpha + Ambulacraria; this is either within Deuterostomia, as sister to Chordata, or the Deuterostomia are recovered as paraphyletic, and Xenambulacraria is sister to the proposed clade Centroneuralia, consisting of Chordata + Protostomia.[122]\n Several animal phyla lack bilateral symmetry. These are the Porifera (sea sponges), Placozoa, Cnidaria (which includes jellyfish, sea anemones, and corals), and Ctenophora (comb jellies).\n Sponges are physically very distinct from other animals, and were long thought to have diverged first, representing the oldest animal phylum and forming a sister clade to all other animals.[123] Despite their morphological dissimilarity with all other animals, genetic evidence suggests sponges may be more closely related to other animals than the comb jellies are.[124][125] Sponges lack the complex organization found in most other animal phyla;[126] their cells are differentiated, but in most cases not organised into distinct tissues, unlike all other animals.[127] They typically feed by drawing in water through pores, filtering out food and nutrients.[128]\n The comb jellies and Cnidaria are radially symmetric and have digestive chambers with a single opening, which serves as both mouth and anus.[129] Animals in both phyla have distinct tissues, but these are not organised into discrete organs.[130] They are diploblastic, having only two main germ layers, ectoderm and endoderm.[131]\n The tiny placozoans have no permanent digestive chamber and no symmetry; they superficially resemble amoebae.[132][133] Their phylogeny is poorly defined, and under active research.[124][134]\n The remaining animals, the great majority—comprising some 29 phyla and over a million species—form a clade, the Bilateria, which have a bilaterally symmetric body plan. The Bilateria are triploblastic, with three well-developed germ layers, and their tissues form distinct organs. The digestive chamber has two openings, a mouth and an anus, and there is an internal body cavity, a coelom or pseudocoelom. These animals have a head end (anterior) and a tail end (posterior), a back (dorsal) surface and a belly (ventral) surface, and a left and a right side.[135][136]\n Having a front end means that this part of the body encounters stimuli, such as food, favouring cephalisation, the development of a head with sense organs and a mouth. Many bilaterians have a combination of circular muscles that constrict the body, making it longer, and an opposing set of longitudinal muscles, that shorten the body;[136] these enable soft-bodied animals with a hydrostatic skeleton to move by peristalsis.[137] They also have a gut that extends through the basically cylindrical body from mouth to anus. Many bilaterian phyla have primary larvae which swim with cilia and have an apical organ containing sensory cells. However, over evolutionary time, descendant spaces have evolved which have lost one or more of each of these characteristics. For example, adult echinoderms are radially symmetric (unlike their larvae), while some parasitic worms have extremely simplified body structures.[135][136]\n Genetic studies have considerably changed zoologists' understanding of the relationships within the Bilateria. Most appear to belong to two major lineages, the protostomes and the deuterostomes.[138] It is often suggested that the basalmost bilaterians are the Xenacoelomorpha, with all other bilaterians belonging to the subclade Nephrozoa[139][140][141] However, this suggestion has been contested, with other studies finding that xenacoelomorphs are more closely related to Ambulacraria than to other bilaterians.[122]\n Protostomes and deuterostomes differ in several ways. Early in development, deuterostome embryos undergo radial cleavage during cell division, while many protostomes (the Spiralia) undergo spiral cleavage.[142]\nAnimals from both groups possess a complete digestive tract, but in protostomes the first opening of the embryonic gut develops into the mouth, and the anus forms secondarily. In deuterostomes, the anus forms first while the mouth develops secondarily.[143][144] Most protostomes have schizocoelous development, where cells simply fill in the interior of the gastrula to form the mesoderm. In deuterostomes, the mesoderm forms by enterocoelic pouching, through invagination of the endoderm.[145]\n The main deuterostome phyla are the Echinodermata and the Chordata.[146] Echinoderms are exclusively marine and include starfish, sea urchins, and sea cucumbers.[147] The chordates are dominated by the vertebrates (animals with backbones),[148] which consist of fishes, amphibians, reptiles, birds, and mammals.[149] The deuterostomes also include the Hemichordata (acorn worms).[150][151]\n The Ecdysozoa are protostomes, named after their shared trait of ecdysis, growth by moulting.[152] They include the largest animal phylum, the Arthropoda, which contains insects, spiders, crabs, and their kin. All of these have a body divided into repeating segments, typically with paired appendages. Two smaller phyla, the Onychophora and Tardigrada, are close relatives of the arthropods and share these traits. The ecdysozoans also include the Nematoda or roundworms, perhaps the second largest animal phylum. Roundworms are typically microscopic, and occur in nearly every environment where there is water;[153] some are important parasites.[154] Smaller phyla related to them are the Nematomorpha or horsehair worms, and the Kinorhyncha, Priapulida, and Loricifera. These groups have a reduced coelom, called a pseudocoelom.[155]\n The Spiralia are a large group of protostomes that develop by spiral cleavage in the early embryo.[156] The Spiralia's phylogeny has been disputed, but it contains a large clade, the superphylum Lophotrochozoa, and smaller groups of phyla such as the Rouphozoa which includes the gastrotrichs and the flatworms. All of these are grouped as the Platytrochozoa, which has a sister group, the Gnathifera, which includes the rotifers.[157][158]\n The Lophotrochozoa includes the molluscs, annelids, brachiopods, nemerteans, bryozoa and entoprocts.[157][159][160] The molluscs, the second-largest animal phylum by number of described species, includes snails, clams, and squids, while the annelids are the segmented worms, such as earthworms, lugworms, and leeches. These two groups have long been considered close relatives because they share trochophore larvae.[161][162]\n In the classical era, Aristotle divided animals,[e] based on his own observations, into those with blood (roughly, the vertebrates) and those without. The animals were then arranged on a scale from man (with blood, 2 legs, rational soul) down through the live-bearing tetrapods (with blood, 4 legs, sensitive soul) and other groups such as crustaceans (no blood, many legs, sensitive soul) down to spontaneously generating creatures like sponges (no blood, no legs, vegetable soul). Aristotle was uncertain whether sponges were animals, which in his system ought to have sensation, appetite, and locomotion, or plants, which did not: he knew that sponges could sense touch, and would contract if about to be pulled off their rocks, but that they were rooted like plants and never moved about.[164]\n In 1758, Carl Linnaeus created the first hierarchical classification in his Systema Naturae.[165] In his original scheme, the animals were one of three kingdoms, divided into the classes of Vermes, Insecta, Pisces, Amphibia, Aves, and Mammalia. Since then the last four have all been subsumed into a single phylum, the Chordata, while his Insecta (which included the crustaceans and arachnids) and Vermes have been renamed or broken up. The process was begun in 1793 by Jean-Baptiste de Lamarck, who called the Vermes une espèce de chaos (a chaotic mess)[f] and split the group into three new phyla: worms, echinoderms, and polyps (which contained corals and jellyfish). By 1809, in his Philosophie Zoologique, Lamarck had created 9 phyla apart from vertebrates (where he still had 4 phyla: mammals, birds, reptiles, and fish) and molluscs, namely cirripedes, annelids, crustaceans, arachnids, insects, worms, radiates, polyps, and infusorians.[163]\n In his 1817 Le Règne Animal, Georges Cuvier used comparative anatomy to group the animals into four embranchements (\"branches\" with different body plans, roughly corresponding to phyla), namely vertebrates, molluscs, articulated animals (arthropods and annelids), and zoophytes (radiata) (echinoderms, cnidaria and other forms).[167] This division into four was followed by the embryologist Karl Ernst von Baer in 1828, the zoologist Louis Agassiz in 1857, and the comparative anatomist Richard Owen in 1860.[168]\n In 1874, Ernst Haeckel divided the animal kingdom into two subkingdoms: Metazoa (multicellular animals, with five phyla: coelenterates, echinoderms, articulates, molluscs, and vertebrates) and Protozoa (single-celled animals), including a sixth animal phylum, sponges.[169][168] The protozoa were later moved to the former kingdom Protista, leaving only the Metazoa as a synonym of Animalia.[170]\n The human population exploits a large number of other animal species for food, both of domesticated livestock species in animal husbandry and, mainly at sea, by hunting wild species.[171][172] Marine fish of many species are caught commercially for food. A smaller number of species are farmed commercially.[171][173][174] Humans and their livestock make up more than 90% of the biomass of all terrestrial vertebrates, and almost as much as all insects combined.[175]\n Invertebrates including cephalopods, crustaceans, and bivalve or gastropod molluscs are hunted or farmed for food.[176] Chickens, cattle, sheep, pigs, and other animals are raised as livestock for meat across the world.[172][177][178] Animal fibres such as wool are used to make textiles, while animal sinews have been used as lashings and bindings, and leather is widely used to make shoes and other items. Animals have been hunted and farmed for their fur to make items such as coats and hats.[179] Dyestuffs including carmine (cochineal),[180][181] shellac,[182][183] and kermes[184][185] have been made from the bodies of insects. Working animals including cattle and horses have been used for work and transport from the first days of agriculture.[186]\n Animals such as the fruit fly Drosophila melanogaster serve a major role in science as experimental models.[187][188][189][190] Animals have been used to create vaccines since their discovery in the 18th century.[191] Some medicines such as the cancer drug trabectedin are based on toxins or other molecules of animal origin.[192]\n People have used hunting dogs to help chase down and retrieve animals,[193] and birds of prey to catch birds and mammals,[194] while tethered cormorants have been used to catch fish.[195] Poison dart frogs have been used to poison the tips of blowpipe darts.[196][197]\nA wide variety of animals are kept as pets, from invertebrates such as tarantulas and octopuses, insects including praying mantises,[198] reptiles such as snakes and chameleons,[199] and birds including canaries, parakeets, and parrots[200] all finding a place. However, the most kept pet species are mammals, namely dogs, cats, and rabbits.[201][202][203] There is a tension between the role of animals as companions to humans, and their existence as individuals with rights of their own.[204]\nA wide variety of terrestrial and aquatic animals are hunted for sport.[205]\n Animals have been the subjects of art from the earliest times, both historical, as in Ancient Egypt, and prehistoric, as in the cave paintings at Lascaux. Major animal paintings include Albrecht Dürer's 1515 The Rhinoceros, and George Stubbs's c. 1762 horse portrait Whistlejacket.[206] Insects, birds and mammals play roles in literature and film,[207] such as in giant bug movies.[208][209][210]\n Animals including insects[211] and mammals[212] feature in mythology and religion. In both Japan and Europe, a butterfly was seen as the personification of a person's soul,[211][213][214] while the scarab beetle was sacred in ancient Egypt.[215] Among the mammals, cattle,[216] deer,[212] horses,[217] lions,[218] bats,[219] bears,[220] and wolves[221] are the subjects of myths and worship. The signs of the Western and Chinese zodiacs are based on animals.[222][223]\n"}
{"key":"Cryptocurrency","link":"https:\/\/en.wikipedia.org\/wiki\/Cryptocurrency","headline":"Cryptocurrency - Wikipedia","content":"\n A cryptocurrency, crypto-currency, or crypto[a] is a digital currency designed to work as a medium of exchange through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it.[2]\n Individual coin ownership records are stored in a digital ledger, which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership.[3][4][5] Despite the term that has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies, cryptocurrencies are generally viewed as a distinct asset class in practice.[6][7][8] Some crypto schemes use validators to maintain the cryptocurrency. In a proof-of-stake model, owners put up their tokens as collateral. In return, they get authority over the token in proportion to the amount they stake. Generally, these token stakers get additional ownership in the token over time via network fees, newly minted tokens, or other such reward mechanisms.[9]\n Cryptocurrency does not exist in physical form (like paper money) and is typically not issued by a central authority. Cryptocurrencies typically use decentralized control as opposed to a central bank digital currency (CBDC).[10] When a cryptocurrency is minted, created prior to issuance, or issued by a single issuer, it is generally considered centralized. When implemented with decentralized control, each cryptocurrency works through distributed ledger technology, typically a blockchain, that serves as a public financial transaction database.[11]\n The first cryptocurrency was Bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion.[12] Throughout their existence, cryptocurrencies have been involved in criminal activities and multi-billion-dollar fraud schemes. Some economists and investors, such as Warren Buffett, considered cryptocurrencies to be a speculative bubble.\n In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash.[13][14] Later, in 1995, he implemented it through Digicash,[15] an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before it can be sent to a recipient. This allowed the digital currency to be untraceable by a third party.\n In 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list[16] and later in 1997 in The American Law Review.[17]\n In 1998, Wei Dai described \"b-money\", an anonymous, distributed electronic cash system.[18] Shortly thereafter, Nick Szabo described bit gold.[19] Like Bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system which required users to complete a proof of work function with solutions being cryptographically put together and published.\n In January 2009, Bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme.[20][21] In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin was released which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake.[22]\n Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013–2014\/15, 2017–2018 and 2021–2023.[23][24]\n On 6 August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies, and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered.[25] Its final report was published in 2018,[26] and it issued a consultation on cryptoassets and stablecoins in January 2021.[27]\n In June 2021, El Salvador became the first country to accept Bitcoin as legal tender, after the Legislative Assembly had voted 62–22 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such.[28]\n In August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as Bitcoin.[29]\n In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.[30]\n On 15 September 2022, the world's second largest cryptocurrency at that time, Ethereum transitioned its consensus mechanism from proof-of-work (PoW) to proof-of-stake (PoS) in an upgrade process known as \"the Merge\".  According to the Ethereum Founder, the upgrade can cut both Ethereum's energy use and carbon-dioxide emissions by 99.9%.[31]\n On 11 November 2022, FTX Trading Ltd., a cryptocurrency exchange, which also operated a crypto hedge fund, and had been valued at $18 billion,[32] filed for bankruptcy.[33] The financial impact of the collapse extended beyond the immediate FTX customer base, as reported,[34] while, at a Reuters conference, financial industry executives said that \"regulators must step in to protect crypto investors.\"[35] Technology analyst Avivah Litan commented on the cryptocurrency ecosystem that \"everything...needs to improve dramatically in terms of user experience, controls, safety, customer service.\"[36]\n According to Jan Lansky, a cryptocurrency is a system that meets six conditions:[37]\n In March 2018, the word cryptocurrency was added to the Merriam-Webster Dictionary.[38]\n After the early innovation of Bitcoin in 2008, and the early network effect gained by Bitcoin, tokens, cryptocurrencies, and other digital assets that were not Bitcoin became collectively known during the 2010s as alternative cryptocurrencies,[39][40][41] or \"altcoins.\"[42] \nSometimes the term \"alt coins\" was used,[43][44] or disparagingly, \"shitcoins\".[45] Paul Vigna of The Wall Street Journal described altcoins in 2020 as \"alternative versions of Bitcoin\"[46] given its role as the model protocol for cryptocurrency designers.  A Polytechnic University of Catalonia thesis in 2021 used a somewhat broader description. Not just as alternative versions of Bitcoin itself, but for any cryptocurrency other than bitcoin. \"As of early 2020, there were more than 5,000 cryptocurrencies. Altcoin is the combination of two words \"alt\" and \"coin\" and includes all alternatives to Bitcoin.\"[42]: 14 \n  Altcoins often have underlying differences when compared to Bitcoin. For example, Litecoin aims to process a block every 2.5 minutes, rather than Bitcoin's 10 minutes, which allows Litecoin to confirm transactions faster than Bitcoin.[47] Another example is Ethereum, which has smart contract functionality that allows decentralized applications to be run on its blockchain.[48] Ethereum was the most used blockchain in 2020, according to Bloomberg News.[49] In 2016, it had the largest \"following\" of any altcoin, according to the New York Times.[50]\n Significant market price rallies across multiple altcoin markets are often referred to as an \"altseason\".[51][52]\n Stablecoins are cryptocurrencies designed to maintain a stable level of purchasing power.[53] Notably, these designs are not foolproof, as a number of stablecoins have crashed or lost their peg. For example, on 11 May 2022, Terra's stablecoin UST fell from $1 to 26 cents.[54][55] The subsequent failure of Terraform Labs resulted in the loss of nearly $40B invested in the Terra and Luna coins.[56] In September 2022, South Korean prosecutors requested the issuance of an Interpol Red Notice against the company's founder, Do Kwon.[57] In Hong Kong, the expected regulatory framework for stablecoins in 2023\/24 is being shaped and includes a few considerations.[58]\n Cryptocurrency is produced by an entire cryptocurrency system collectively, at a rate which is defined when the system is created and which is publicly stated. In centralized banking and economic systems such as the US Federal Reserve System, corporate boards or governments control the supply of currency.[citation needed] In the case of cryptocurrency, companies or governments cannot produce new units, and have not so far provided backing for other firms, banks or corporate entities which hold asset value measured in it. The underlying technical system upon which cryptocurrencies are based was created by Satoshi Nakamoto.[59]\n Within a proof-of-work system such as Bitcoin, the safety, integrity and balance of ledgers is maintained by a community of mutually distrustful parties referred to as miners. Miners use their computers to help validate and timestamp transactions, adding them to the ledger in accordance with a particular timestamping scheme.[20] In a proof-of-stake blockchain, transactions are validated by holders of the associated cryptocurrency, sometimes grouped together in stake pools.\n Most cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation.[60] Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.[3]\n The validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography.[59][61] Each block typically contains a hash pointer as a link to a previous block,[61] a timestamp and transaction data.[62] By design, blockchains are inherently resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\".[63] For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority.\n Blockchains are secure by design and are an example of a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been achieved with a blockchain.[64]\n A node is a computer that connects to a cryptocurrency network. The node supports the cryptocurrency's network through either relaying transactions, validation, or hosting a copy of the blockchain. In terms of relaying transactions, each network computer (node) has a copy of the blockchain of the cryptocurrency it supports. When a transaction is made, the node creating the transaction broadcasts details of the transaction using encryption to other nodes throughout the node network so that the transaction (and every other transaction) is known.\n Node owners are either volunteers, those hosted by the organization or body responsible for developing the cryptocurrency blockchain network technology, or those who are enticed to host a node to receive rewards from hosting the node network.[65]\n Cryptocurrencies use various timestamping schemes to \"prove\" the validity of transactions added to the blockchain ledger without the need for a trusted third party.\n The first timestamping scheme invented was the proof-of-work scheme. The most widely used proof-of-work schemes are based on SHA-256 and scrypt.[22]\n Some other hashing algorithms that are used for proof-of-work include CryptoNote, Blake, SHA-3, and X11.\n Another method is called the proof-of-stake scheme. Proof-of-stake is a method of securing a cryptocurrency network and achieving distributed consensus through requesting users to show ownership of a certain amount of currency. It is different from proof-of-work systems that run difficult hashing algorithms to validate electronic transactions. The scheme is largely dependent on the coin, and there is currently no standard form of it. Some cryptocurrencies use a combined proof-of-work and proof-of-stake scheme.[22]\n On a blockchain, mining is the validation of transactions. For this effort, successful miners obtain new cryptocurrency as a reward. The reward decreases transaction fees by creating a complementary incentive to contribute to the processing power of the network. The rate of generating hashes, which validate any transaction, has been increased by the use of specialized machines such as FPGAs and ASICs running complex hashing algorithms like SHA-256 and scrypt.[66] This arms race for cheaper-yet-efficient machines has existed since Bitcoin was introduced in 2009.[66] Mining is measured by hash rate typically in TH\/s.[67]\n With more people entering the world of virtual currency, generating hashes for validation has become more complex over time, forcing miners to invest increasingly large sums of money to improve computing performance. Consequently, the reward for finding a hash has diminished and often does not justify the investment in equipment and cooling facilities (to mitigate the heat the equipment produces), and the electricity required to run them.[68] Popular regions for mining include those with inexpensive electricity, a cold climate, and jurisdictions with clear and conducive regulations. By July 2019, Bitcoin's electricity consumption was estimated to be approximately 7 gigawatts, around 0.2% of the global total, or equivalent to the energy consumed nationally by Switzerland.[69]\n Some miners pool resources, sharing their processing power over a network to split the reward equally, according to the amount of work they contributed to the probability of finding a block. A \"share\" is awarded to members of the mining pool who present a valid partial proof-of-work.\n As of February 2018[update], the Chinese Government has halted trading of virtual currency, banned initial coin offerings and shut down mining. Many Chinese miners have since relocated to Canada[70] and Texas.[71] One company is operating data centers for mining operations at Canadian oil and gas field sites, due to low gas prices.[72] In June 2018, Hydro Quebec proposed to the provincial government to allocate 500 megawatts of power to crypto companies for mining.[73] According to a February 2018 report from Fortune, Iceland has become a haven for cryptocurrency miners in part because of its cheap electricity.[74]\n In March 2018, the city of Plattsburgh, New York put an 18-month moratorium on all cryptocurrency mining in an effort to preserve natural resources and the \"character and direction\" of the city.[75] In 2021, Kazakhstan became the second-biggest crypto-currency mining country, producing 18.1% of the global exahash rate. The country built a compound containing 50,000 computers near Ekibastuz.[76]\n An increase in cryptocurrency mining increased the demand for graphics cards (GPU) in 2017.[77] The computing power of GPUs makes them well-suited to generating hashes. Popular favorites of cryptocurrency miners such as Nvidia's GTX 1060 and GTX 1070 graphics cards, as well as AMD's RX 570 and RX 580 GPUs, doubled or tripled in price – or were out of stock.[78] A GTX 1070 Ti which was released at a price of $450 sold for as much as $1,100. Another popular card, the GTX 1060 (6 GB model) was released at an MSRP of $250, and sold for almost $500. RX 570 and RX 580 cards from AMD were out of stock for almost a year. Miners regularly buy up the entire stock of new GPU's as soon as they are available.[79]\n Nvidia has asked retailers to do what they can when it comes to selling GPUs to gamers instead of miners. Boris Böhles, PR manager for Nvidia in the German region, said: \"Gamers come first for Nvidia.\"[80]\n Numerous companies developed dedicated crypto-mining accelerator chips, capable of price-performance far higher than that of CPU or GPU mining. At one point Intel marketed its own brand of crypto accelerator chip, named Blockscale.[81]\n A cryptocurrency wallet is a means of storing the public and private \"keys\" (address) or seed which can be used to receive or spend the cryptocurrency.[82] With the private key, it is possible to write in the public ledger, effectively spending the associated cryptocurrency. With the public key, it is possible for others to send currency to the wallet.\n There exist multiple methods of storing keys or seed in a wallet. These methods range from using paper wallets (which are public, private or seed keys written on paper), to using hardware wallets (which are hardware to store your wallet information), to a digital wallet (which is a computer with a software hosting your wallet information), to hosting your wallet using an exchange where cryptocurrency is traded, or by storing your wallet information on a digital medium such as plaintext.[83]\n Bitcoin is pseudonymous, rather than anonymous; the cryptocurrency in a wallet is not tied to a person, but rather to one or more specific keys (or \"addresses\").[84] Thereby, Bitcoin owners are not immediately identifiable, but all transactions are publicly available in the blockchain.[85] Still, cryptocurrency exchanges are often required by law to collect the personal information of their users.[86]\n Some cryptocurrencies, such as Monero, Zerocoin, Zerocash, and CryptoNote, implement additional measures to increase privacy, such as by using zero-knowledge proofs.[87][88]\n A recent 2020 study presented different attacks on privacy in cryptocurrencies. The attacks demonstrated how the anonymity techniques are not sufficient safeguards. In order to improve privacy, researchers suggested several different ideas including new cryptographic schemes and mechanisms for hiding the IP address of the source.[89]\n Cryptocurrencies are used primarily outside banking and governmental institutions and are exchanged over the Internet.\n Proof-of-work cryptocurrencies, such as Bitcoin, offer block rewards incentives for miners. There has been an implicit belief that whether miners are paid by block rewards or transaction fees does not affect the security of the blockchain, but a study suggests that this may not be the case under certain circumstances.[90]\n The rewards paid to miners increase the supply of the cryptocurrency. By making sure that verifying transactions is a costly business, the integrity of the network can be preserved as long as benevolent nodes control a majority of computing power. The verification algorithm requires a lot of processing power, and thus electricity in order to make verification costly enough to accurately validate public blockchain. Not only do miners have to factor in the costs associated with expensive equipment necessary to stand a chance of solving a hash problem, they must further consider the significant amount of electrical power in search of the solution. Generally, the block rewards outweigh electricity and equipment costs, but this may not always be the case.[91]\n The current value, not the long-term value, of the cryptocurrency supports the reward scheme to incentivize miners to engage in costly mining activities.[92] In 2018, Bitcoin's design caused a 1.4% welfare loss compared to an efficient cash system, while a cash system with 2% money growth has a minor 0.003% welfare cost. The main source for this inefficiency is the large mining cost, which is estimated to be US$360 million per year. This translates into users being willing to accept a cash system with an inflation rate of 230% before being better off using Bitcoin as a means of payment. However, the efficiency of the Bitcoin system can be significantly improved by optimizing the rate of coin creation and minimizing transaction fees. Another potential improvement is to eliminate inefficient mining activities by changing the consensus protocol altogether.[93]\n Transaction fees for cryptocurrency depend mainly on the supply of network capacity at the time, versus the demand from the currency holder for a faster transaction.[citation needed] The currency holder can choose a specific transaction fee, while network entities process transactions in order of highest offered fee to lowest.[citation needed] Cryptocurrency exchanges can simplify the process for currency holders by offering priority alternatives and thereby determine which fee will likely cause the transaction to be processed in the requested time.[citation needed]\n For Ethereum, transaction fees differ by computational complexity, bandwidth use, and storage needs, while Bitcoin transaction fees differ by transaction size and whether the transaction uses SegWit. In February 2023, the median transaction fee for Ether corresponded to $2.2845,[94] while for Bitcoin it corresponded to $0.659.[95]\n Some cryptocurrencies have no transaction fees, and instead rely on client-side proof-of-work as the transaction prioritization and anti-spam mechanism.[96][97][98]\n Cryptocurrency exchanges allow customers to trade cryptocurrencies[99] for other assets, such as conventional fiat money, or to trade between different digital currencies.\n Crypto marketplaces do not guarantee that an investor is completing a purchase or trade at the optimal price. As a result, as of 2020 it was possible to arbitrage to find the difference in price across several markets.[100]\n Atomic swaps are a mechanism where one cryptocurrency can be exchanged directly for another cryptocurrency, without the need for a trusted third party such as an exchange.[101]\n Jordan Kelley, founder of Robocoin, launched the first Bitcoin ATM in the United States on 20 February 2014. The kiosk installed in Austin, Texas, is similar to bank ATMs but has scanners to read government-issued identification such as a driver's license or a passport to confirm users' identities.[102]\n An initial coin offering (ICO) is a controversial means of raising funds for a new cryptocurrency venture. An ICO may be used by startups with the intention of avoiding regulation. However, securities regulators in many jurisdictions, including in the U.S., and Canada, have indicated that if a coin or token is an \"investment contract\" (e.g., under the Howey test, i.e., an investment of money with a reasonable expectation of profit based significantly on the entrepreneurial or managerial efforts of others), it is a security and is subject to securities regulation. In an ICO campaign, a percentage of the cryptocurrency (usually in the form of \"tokens\") is sold to early backers of the project in exchange for legal tender or other cryptocurrencies, often Bitcoin or Ether.[103][104][105]\n According to PricewaterhouseCoopers, four of the 10 biggest proposed initial coin offerings have used Switzerland as a base, where they are frequently registered as non-profit foundations. The Swiss regulatory agency FINMA stated that it would take a \"balanced approach\" to ICO projects and would allow \"legitimate innovators to navigate the regulatory landscape and so launch their projects in a way consistent with national laws protecting investors and the integrity of the financial system.\" In response to numerous requests by industry representatives, a legislative ICO working group began to issue legal guidelines in 2018, which are intended to remove uncertainty from cryptocurrency offerings and to establish sustainable business practices.[106]\n The market capitalization of a cryptocurrency is calculated by multiplying the price by the number of coins in circulation. The total cryptocurrency market cap has historically been dominated by Bitcoin accounting for at least 50% of the market cap value where altcoins have increased and decreased in market cap value in relation to Bitcoin. Bitcoin's value is largely determined by speculation among other technological limiting factors known as blockchain rewards coded into the architecture technology of Bitcoin itself. The cryptocurrency market cap follows a trend known as the \"halving\", which is when the block rewards received from Bitcoin are halved due to technological mandated limited factors instilled into Bitcoin which in turn limits the supply of Bitcoin. As the date reaches near of a halving (twice thus far historically) the cryptocurrency market cap increases, followed by a downtrend.[107]\n By June 2021, cryptocurrency had begun to be offered by some wealth managers in the US for 401(k)s.[108][109][110]\n Cryptocurrency prices are much more volatile than established financial assets such as stocks. For example, over one week in May 2022, Bitcoin lost 20% of its value and Ethereum lost 26%, while Solana and Cardano lost 41% and 35% respectively. The falls were attributed to warnings about inflation. By comparison, in the same week, the Nasdaq tech stock index fell 7.6 per cent and the FTSE 100 was 3.6 per cent down.[111]\n In the longer term, of the 10 leading cryptocurrencies identified by the total value of coins in circulation in January 2018, only four (Bitcoin, Ethereum, Cardano and Ripple (XRP)) were still in that position in early 2022.[112] The total value of all cryptocurrencies was  $2 trillion at the end of 2021, but had halved nine months later.[113][114] The Wall Street Journal has commented that the crypto sector has become \"intertwined\" with the rest of the capital markets and \"sensitive to the same forces that drive tech stocks and other risk assets\", such as inflation forecasts.[115]\n There are also centralized databases, outside of blockchains, that store crypto market data. Compared to the blockchain, databases perform fast as there is no verification process. Four of the most popular cryptocurrency market databases are CoinMarketCap, CoinGecko, BraveNewCoin, and Cryptocompare.[116]\n According to Alan Feuer of The New York Times, libertarians and anarcho-capitalists were attracted to the philosophical idea behind Bitcoin. Early Bitcoin supporter Roger Ver said: \"At first, almost everyone who got involved did so for philosophical reasons. We saw Bitcoin as a great idea, as a way to separate money from the state.\"[117] Economist Paul Krugman argues that cryptocurrencies like Bitcoin are \"something of a cult\" based in \"paranoid fantasies\" of government power.[118]\n David Golumbia says that the ideas influencing Bitcoin advocates emerge from right-wing extremist movements such as the Liberty Lobby and the John Birch Society and their anti-Central Bank rhetoric, or, more recently, Ron Paul and Tea Party-style libertarianism.[119] Steve Bannon, who owns a \"good stake\" in Bitcoin, sees cryptocurrency as a form of disruptive populism, taking control back from central authorities.[120]\n Bitcoin's founder, Satoshi Nakamoto, has supported the idea that cryptocurrencies go well with libertarianism. \"It's very attractive to the libertarian viewpoint if we can explain it properly,\" Nakamoto said in 2008.[121]\n According to the European Central Bank, the decentralization of money offered by Bitcoin has its theoretical roots in the Austrian school of economics, especially with Friedrich von Hayek in his book Denationalisation of Money: The Argument Refined,[122] in which Hayek advocates a complete free market in the production, distribution and management of money to end the monopoly of central banks.[123]\n The rise in the popularity of cryptocurrencies and their adoption by financial institutions has led some governments to assess whether regulation is needed to protect users. The Financial Action Task Force (FATF) has defined cryptocurrency-related services as \"virtual asset service providers\" (VASPs) and recommended that they be regulated with the same money laundering (AML) and know your customer (KYC) requirements as financial institutions.[124]\n In May 2020, the Joint Working Group on interVASP Messaging Standards published \"IVMS 101\", a universal common language for communication of required originator and beneficiary information between VASPs. The FATF and financial regulators were informed as the data model was developed.[125]\n In June 2020, FATF updated its guidance to include the \"Travel Rule\" for cryptocurrencies, a measure which mandates that VASPs obtain, hold, and exchange information about the originators and beneficiaries of virtual asset transfers.[126] Subsequent standardized protocol specifications recommended using JSON for relaying data between VASPs and identity services. As of December 2020, the IVMS 101 data model has yet to be finalized and ratified by the three global standard setting bodies that created it.[127]\n The European Commission published a digital finance strategy in September 2020. This included a draft regulation on Markets in Crypto-Assets (MiCA), which aimed to provide a comprehensive regulatory framework for digital assets in the EU.[128][129]\n On 10 June 2021, the Basel Committee on Banking Supervision proposed that banks that held cryptocurrency assets must set aside capital to cover all potential losses. For instance, if a bank were to hold Bitcoin worth $2 billion, it would be required to set aside enough capital to cover the entire $2 billion. This is a more extreme standard than banks are usually held to when it comes to other assets. However, this is a proposal and not a regulation.\n The IMF is seeking a coordinated, consistent and comprehensive approach to supervising cryptocurrencies. Tobias Adrian, the IMF's financial counsellor and head of its monetary and capital markets department said in a January 2022 interview that \"Agreeing global regulations is never quick. But if we start now, we can achieve the goal of maintaining financial stability while also enjoying the benefits which the underlying technological innovations bring,\"[130]\n In September 2017, China banned ICOs to cause abnormal return from cryptocurrency decreasing during announcement window. The liquidity changes by banning ICOs in China was temporarily negative while the liquidity effect became positive after news.[131]\n On 18 May 2021, China banned financial institutions and payment companies from being able to provide cryptocurrency transaction related services.[132] This led to a sharp fall in the price of the biggest proof of work cryptocurrencies. For instance, Bitcoin fell 31%, Ethereum fell 44%, Binance Coin fell 32% and Dogecoin fell 30%.[133] Proof of work mining was the next focus, with regulators in popular mining regions citing the use of electricity generated from highly polluting sources such as coal to create Bitcoin and Ethereum.[134]\n In September 2021, the Chinese government declared all cryptocurrency transactions of any kind illegal, completing its crackdown on cryptocurrency.[30]\n On 9 June 2021, El Salvador announced that it will adopt Bitcoin as legal tender, becoming the first country to do so.[135]\n At present, India neither prohibits nor allows investment in the cryptocurrency market. In 2020, the Supreme Court of India had lifted the ban on cryptocurrency, which was imposed by the Reserve Bank of India.[136][137][138][139] Since then, an investment in cryptocurrency is considered legitimate, though there is still ambiguity about the issues regarding the extent and payment of tax on the income accrued thereupon and also its regulatory regime. But it is being contemplated that the Indian Parliament will soon pass a specific law to either ban or regulate the cryptocurrency market in India.[140] Expressing his public policy opinion on the Indian cryptocurrency market to a well-known online publication, a leading public policy lawyer and Vice President of SAARCLAW (South Asian Association for Regional Co-operation in Law) Hemant Batra has said that the \"cryptocurrency market has now become very big with involvement of billions of dollars in the market hence, it is now unattainable and irreconcilable for the government to completely ban all sorts of cryptocurrency and its trading and investment\".[141] He mooted regulating the cryptocurrency market rather than completely banning it. He favoured following IMF and FATF guidelines in this regard.\n South Africa, which has seen a large number of scams related to cryptocurrency, is said to be putting a regulatory timeline in place that will produce a regulatory framework.[142] The largest scam occurred in April 2021, where the two founders of an African-based cryptocurrency exchange called Africrypt, Raees Cajee and Ameer Cajee, disappeared with $3.8 billion worth of Bitcoin.[143] Additionally, Mirror Trading International disappeared with $170 million worth of cryptocurrency in January 2021.[143]\n In March 2021, South Korea implemented new legislation to strengthen their oversight of digital assets. This legislation requires all digital asset managers, providers and exchanges to be registered with the Korea Financial Intelligence Unit in order to operate in South Korea.[144] Registering with this unit requires that all exchanges are certified by the Information Security Management System and that they ensure all customers have real name bank accounts. It also requires that the CEO and board members of the exchanges have not been convicted of any crimes and that the exchange holds sufficient levels of deposit insurance to cover losses arising from hacks.[144]\n Switzerland was one of the first countries to implement the FATF's Travel Rule. FINMA, the Swiss regulator, issued its own guidance to VASPs in 2019. The guidance followed the FATF's Recommendation 16, however with stricter requirements. According to FINMA's[145] requirements, VASPs need to verify the identity of the beneficiary of the transfer.\n On 30 April 2021, the Central Bank of the Republic of Turkey banned the use of cryptocurrencies and cryptoassets for making purchases on the grounds that the use of cryptocurrencies for such payments poses significant transaction risks.[146]\n In the United Kingdom, as of 10 January 2021, all cryptocurrency firms, such as exchanges, advisors and professionals that have either a presence, market product or provide services within the UK market must register with the Financial Conduct Authority. Additionally, on 27 June 2021, the financial watchdog demanded that Binance, the world's largest cryptocurrency exchange,[147] cease all regulated activities in the UK.[148]\n In 2021, 17 states passed laws and resolutions concerning cryptocurrency regulation.[149] The U.S. Securities and Exchange Commission (SEC) is considering what steps to take. On 8 July 2021, Senator Elizabeth Warren, part of the Senate Banking Committee, wrote to the chairman of the SEC and demanded answers on cryptocurrency regulation due to the increase in cryptocurrency exchange use and the danger this posed to consumers. On 5 August 2021, SEC Chairman Gary Gensler responded to Senator Elizabeth Warren's letter regarding cryptocurrency regulation and called for legislation focused on \"crypto trading, lending and DeFi platforms,\" because of how vulnerable the investors could be when they traded on crypto trading platforms without a broker. He also argued that many tokens in the crypto market may be unregistered securities without required disclosures or market oversight. Additionally, Gensler did not hold back in his criticism of stablecoins. These tokens, which are pegged to the value of fiat currencies, may allow individuals to bypass important public policy goals related to traditional banking and financial systems, such as anti-money laundering, tax compliance, and sanctions.[150]\n On 19 October 2021, the first bitcoin-linked exchange-traded fund (ETF) from ProShares started trading on the NYSE under the ticker \"BITO.\" ProShares CEO Michael L. Sapir said the ETF would expose Bitcoin to a wider range of investors without the hassle of setting up accounts with cryptocurrency providers. Ian Balina, the CEO of Token Metrics, stated that the approval of the \"BITO\" ETF by the SEC was a significant endorsement for the crypto industry because many regulators globally were not in favor of crypto as well as the hesitance to accept crypto from retail investors. This event would eventually open more opportunities for new capital and new people in this space.[151]\n The United States Department of the Treasury, on 20 May 2021, announced that it would require any transfer worth $10,000 or more to be reported to the Internal Revenue Service since cryptocurrency already posed a problem where illegal activity like tax evasion was facilitated broadly. This release from the IRS was a part of efforts to promote better compliance and consider more severe penalties for tax evaders.[152]\n On 17 February 2022, the Justice department named Eun Young Choi as the first director of a National Cryptocurrency Enforcement Team to aid in identification of and dealing with misuse of cryptocurrencies and other digital assets.[153]\n The Biden administration faced a dilemma as it tried to develop regulations for the cryptocurrency industry. On one hand, officials were hesitant to restrict the growing and profitable industry. On the other hand, they were committed to preventing illegal cryptocurrency transactions. To reconcile these conflicting goals, on 9 March 2022, President Biden issued an executive order.[154] Followed by the executive order, on 16 September 2022, the Comprehensive Framework for Responsible Development of Digital Assets document was released [155] to support development of cryptocurrencies and restrict their illegal use. The executive order included all digital assets, but cryptocurrencies posed both the greatest security risks and potential economic benefits. Though this might not address all of the challenges in crypto industry, it was a significant milestone in the U.S. cryptocurrency regulation history.[156]\n In February 2023, the Securities and Exchange Commission (SEC) ruled that cryptocurrency exchange Kraken's estimated $42 billion in staked assets globally operated as an illegal securities seller. The company agreed to a $30 million settlement with the SEC and to cease selling its staking service in the U.S. The case would impact other major crypto exchanges operating staking programs.[157]\n On 23 March 2023, the U.S. Securities and Exchange Commission (SEC) issued an alert to investors stating that firms offering crypto asset securities may not be complying with U.S. laws. The SEC stated that unregistered offerings of crypto asset securities may not include important information.[158]\n The legal status of cryptocurrencies varies substantially from country to country and is still undefined or changing in many of them. At least one study has shown that broad generalizations about the use of Bitcoin in illicit finance are significantly overstated and that blockchain analysis is an effective crime fighting and intelligence gathering tool.[159] While some countries have explicitly allowed their use and trade,[160] others have banned or restricted it. According to the Library of Congress in 2021,\nan \"absolute ban\" on trading or using cryptocurrencies applies in 9 countries:\nAlgeria, Bangladesh, Bolivia, China, Egypt, Iraq, Morocco, Nepal, and the United Arab Emirates. An \"implicit ban\" applies in another 39 countries or regions, which include: Bahrain, Benin, Burkina Faso, Burundi, Cameroon, Chad, Cote d’Ivoire, the Dominican Republic, Ecuador, Gabon, Georgia, Guyana, Indonesia, Iran, Jordan, Kazakhstan, Kuwait, Lebanon, Lesotho, Macau, Maldives, Mali, Moldova, Namibia, Niger, Nigeria, Oman, Pakistan, Palau, Republic of Congo, Saudi Arabia, Sengeal, Tajikistan, Tanzania, Togo, Turkey, Turkmenistan, Qatar and Vietnam.[161] In the United States and Canada, state and provincial securities regulators, coordinated through the North American Securities Administrators Association, are investigating \"Bitcoin scams\" and ICOs in 40 jurisdictions.[162]\n Various government agencies, departments, and courts have classified Bitcoin differently. China Central Bank banned the handling of Bitcoins by financial institutions in China in early 2014.\n In Russia, though owning cryptocurrency is legal, its residents are only allowed to purchase goods from other residents using the Russian ruble while nonresidents are allowed to use foreign currency.[163] Regulations and bans that apply to Bitcoin probably extend to similar cryptocurrency systems.[164]\n In August 2018, the Bank of Thailand announced its plans to create its own cryptocurrency, the Central Bank Digital Currency (CBDC).[165]\n Cryptocurrency advertisements have been banned on the following platforms:\n On 25 March 2014, the United States Internal Revenue Service (IRS) ruled that Bitcoin will be treated as property for tax purposes. Therefore, virtual currencies are considered commodities subject to capital gains tax.[173]\n As the popularity and demand for online currencies has increased since the inception of Bitcoin in 2009,[174] so have concerns that such an unregulated person to person global economy that cryptocurrencies offer may become a threat to society. Concerns abound that altcoins may become tools for anonymous web criminals.[175]\n Cryptocurrency networks display a lack of regulation that has been criticized as enabling criminals who seek to evade taxes and launder money. Money laundering issues are also present in regular bank transfers, however with bank-to-bank wire transfers for instance, the account holder must at least provide a proven identity.\n Transactions that occur through the use and exchange of these altcoins are independent from formal banking systems, and therefore can make tax evasion simpler for individuals. Since charting taxable income is based upon what a recipient reports to the revenue service, it becomes extremely difficult to account for transactions made using existing cryptocurrencies, a mode of exchange that is complex and difficult to track.[175]\n Systems of anonymity that most cryptocurrencies offer can also serve as a simpler means to launder money. Rather than laundering money through an intricate net of financial actors and offshore bank accounts, laundering money through altcoins can be achieved through anonymous transactions.[175]\n Cryptocurrency makes legal enforcement against extremist groups more complicated, which consequently strengthens them.[176] White supremacist Richard Spencer went as far as to declare Bitcoin the \"currency of the alt-right\".[177]\n In February 2014, the world's largest Bitcoin exchange, Mt. Gox, declared bankruptcy. Likely due to theft, the company claimed that it had lost nearly 750,000 Bitcoins belonging to their clients. This added up to approximately 7% of all Bitcoins in existence, worth a total of $473 million. Mt. Gox blamed hackers, who had exploited the transaction malleability problems in the network. The price of a Bitcoin fell from a high of about $1,160 in December to under $400 in February.[178]\n On 21 November 2017, Tether announced that it had been hacked, losing $31 million in USDT from its core treasury wallet.[179]\n On 7 December 2017, Slovenian cryptocurrency exchange Nicehash reported that hackers had stolen over $70M using a hijacked company computer.[180]\n On 19 December 2017, Yapian, the owner of South Korean exchange Youbit, filed for bankruptcy after suffering two hacks that year.[181][182] Customers were still granted access to 75% of their assets.\n In May 2018, Bitcoin Gold had its transactions hijacked and abused by unknown hackers.[183] Exchanges lost an estimated $18m and Bitcoin Gold was delisted from Bittrex after it refused to pay its share of the damages.\n On 13 September 2018, Homero Josh Garza was sentenced to 21 months of imprisonment, followed by three years of supervised release.[184] Garza had founded the cryptocurrency startups GAW Miners and ZenMiner in 2014, acknowledged in a plea agreement that the companies were part of a pyramid scheme, and pleaded guilty to wire fraud in 2015. The U.S. Securities and Exchange Commission separately brought a civil enforcement action against Garza, who was eventually ordered to pay a judgment of $9.1 million plus $700,000 in interest. The SEC's complaint stated that Garza, through his companies, had fraudulently sold \"investment contracts representing shares in the profits they claimed would be generated\" from mining.[185]\n In January 2018, Japanese exchange Coincheck reported that hackers had stolen $530M worth of cryptocurrencies.[186]\n In June 2018, South Korean exchange Coinrail was hacked, losing over $37M worth of cryptos.[187] The hack worsened an already ongoing cryptocurrency selloff by an additional $42 billion.[188]\n On 9 July 2018, the exchange Bancor, whose code and fundraising had been subjects of controversy, had $23.5 million in cryptocurrency stolen.[189]\n A 2020 EU report found that users had lost crypto-assets worth hundreds of millions of US dollars in security breaches at exchanges and storage providers. Between 2011 and 2019, reported breaches ranged from four to twelve a year. In 2019, more than a billion dollars worth of cryptoassets was reported stolen. Stolen assets \"typically find their way to illegal markets and are used to fund further criminal activity\".[190]\n According to a 2020 report produced by the United States Attorney General's Cyber-Digital Task Force, the following three categories make up the majority of illicit cryptocurrency uses: \"(1) financial transactions associated with the commission of crimes; (2) money laundering and the shielding of legitimate activity from tax, reporting, or other legal requirements; or (3) crimes, such as theft, directly implicating the cryptocurrency marketplace itself.\" The report concludes that \"for cryptocurrency to realize its truly transformative potential, it is imperative that these risks be addressed\" and that \"the government has legal and regulatory tools available at its disposal to confront the threats posed by cryptocurrency's illicit uses\".[191][192]\n According to the UK 2020 national risk assessment—a comprehensive assessment of money laundering and terrorist financing risk in the UK—the risk of using cryptoassets such as Bitcoin for money laundering and terrorism financing is assessed as \"medium\" (from \"low\" in the previous 2017 report).[193] Legal scholars suggested that the money laundering opportunities may be more perceived than real.[194] Blockchain analysis company Chainalysis concluded that illicit activities like cybercrime, money laundering and terrorism financing made up only 0.15% of all crypto transactions conducted in 2021, representing a total of $14 billion.[195][196][197]\n In December 2021, Monkey Kingdom, a NFT project based in Hong Kong, lost US$1.3 million worth of cryptocurrencies via a phishing link used by the hacker.[198]\n According to blockchain data company Chainalysis, criminals laundered US$8,600,000,000 worth of cryptocurrency in 2021, up by 30% from the previous year.[199] The data suggests that rather than managing numerous illicit havens, cybercriminals make use of a small group of purpose built centralized exchanges for sending and receiving illicit cryptocurrency. In 2021, those exchanges received 47% of funds sent by crime linked addresses.[200] Almost $2.2bn worth of cryptocurrencies was embezzled from DeFi protocols in 2021, which represents 72% of all cryptocurrency theft in 2021.\n According to Bloomberg and the New York Times, Federation Tower, a two skyscraper complex in the heart of Moscow City, is home to many cryptocurrency businesses under suspicion of facilitating extensive money laundering, including accepting illicit cryptocurrency funds obtained through scams, darknet markets, and ransomware.[201] Notable businesses include Garantex,[202] Eggchange, Cashbank, Buy-Bitcoin, Tetchange, Bitzlato, and Suex, which was sanctioned by the U.S. in 2021. Bitzlato founder and owner Anatoly Legkodymov was arrested following money-laundering charges by the United States Department of Justice.[203]\n Dark money has also been flowing into Russia through a dark web marketplace called Hydra, which is powered by cryptocurrency, and enjoyed more than $1 billion in sales in 2020, according to Chainalysis.[204] The platform demands that sellers liquidate cryptocurrency only through certain regional exchanges, which has made it difficult for investigators to trace the money.\n Almost 74% of ransomware revenue in 2021 — over $400 million worth of cryptocurrency — went to software strains likely affiliated with Russia, where oversight is notoriously limited.[201] However, Russians are also leaders in the benign adoption of cryptocurrencies, as the ruble is unreliable, and President Putin favours the idea of \"overcoming the excessive domination of the limited number of reserve currencies.\"[205]\n In 2022, RenBridge - an unregulated alternative to exchanges for transferring value between blockchains - was found to be responsible for the laundering of at least $540 million since 2020. It is especially popular with people attempting to launder money from theft. This includes a cyberattack on Japanese crypto exchange Liquid that has been linked to North Korea.[206]\n Properties of cryptocurrencies gave them popularity in applications such as a safe haven in banking crises and means of payment, which also led to the cryptocurrency use in controversial settings in the form of online black markets, such as Silk Road.[175] The original Silk Road was shut down in October 2013 and there have been two more versions in use since then. In the year following the initial shutdown of Silk Road, the number of prominent dark markets increased from four to twelve, while the amount of drug listings increased from 18,000 to 32,000.[175]\n Darknet markets present challenges in regard to legality. Cryptocurrency used in dark markets are not clearly or legally classified in almost all parts of the world. In the U.S., Bitcoins are labelled as \"virtual assets\".[citation needed] This type of ambiguous classification puts pressure on law enforcement agencies around the world to adapt to the shifting drug trade of dark markets.[207][unreliable source?]\n Various studies have found that crypto-trading is rife with wash trading. Wash trading is a process, illegal in some jurisdictions, involving buyers and sellers being the same person or group, and may be used to manipulate the price of a cryptocurrency or inflate volume artificially. Exchanges with higher volumes can demand higher premiums from token issuers.[208] A study from 2019 concluded that up to 80% of trades on unregulated cryptocurrency exchanges could be wash trades.[208] A 2019 report by Bitwise Asset Management claimed that 95% of all Bitcoin trading volume reported on major website CoinMarketCap had been artificially generated, and of 81 exchanges studied, only 10 provided legitimate volume figures.[209]\n In 2022, cryptocurrencies attracted attention when Western nations imposed severe economic sanctions on Russia in the aftermath of its invasion of Ukraine in February. However, American sources warned in March that some crypto-transactions could potentially be used to evade economic sanctions against Russia and Belarus.[210]\n In April 2022, the computer programmer Virgil Griffith received a five-year prison sentence in the US for attending a Pyongyang cryptocurrency conference, where he gave a presentation on blockchains which might be used for sanctions evasion.[211]\n The Bank for International Settlements summarized several criticisms of cryptocurrencies in Chapter V of their 2018 annual report. The criticisms include the lack of stability in their price, the high energy consumption, high and variable transactions costs, the poor security and fraud at cryptocurrency exchanges, vulnerability to debasement (from forking), and the influence of miners.[212][213][214]\n Cryptocurrencies have been compared to Ponzi schemes, pyramid schemes[215] and economic bubbles,[216] such as housing market bubbles.[217] Howard Marks of Oaktree Capital Management stated in 2017 that digital currencies were \"nothing but an unfounded fad (or perhaps even a pyramid scheme), based on a willingness to ascribe value to something that has little or none beyond what people will pay for it\", and compared them to the tulip mania (1637), South Sea Bubble (1720), and dot-com bubble (1999), which all experienced profound price booms and busts.[218]\n Regulators in several countries have warned against cryptocurrency and some have taken measures to dissuade users.[219] However, research in 2021 by the UK's financial regulator suggests such warnings either went unheard, or were ignored. Fewer than one in 10 potential cryptocurrency buyers were aware of consumer warnings on the FCA website, and 12% of crypto users were not aware that their holdings were not protected by statutory compensation.[220][221] Of 1,000 respondents between the ages of eighteen and forty, almost 70% falsely assumed cryptocurrencies were regulated, 75% of younger crypto investors claimed to be driven by competition with friends and family, 58% said that social media enticed them to make high risk investments.[222] The FCA recommends making use of its warning list, which flags unauthorized financial firms.[223]\n Many banks do not offer virtual currency services themselves and can refuse to do business with virtual currency companies.[224] In 2014, Gareth Murphy, a senior banking officer, suggested that the widespread adoption of cryptocurrencies may lead to too much money being obfuscated, blinding economists who would use such information to better steer the economy.[225] While traditional financial products have strong consumer protections in place, there is no intermediary with the power to limit consumer losses if Bitcoins are lost or stolen. One of the features cryptocurrency lacks in comparison to credit cards, for example, is consumer protection against fraud, such as chargebacks.\n The French regulator Autorité des marchés financiers (AMF) lists 16 websites of companies that solicit investment in cryptocurrency without being authorized to do so in France.[226]\n An October 2021 paper by the National Bureau of Economic Research found that Bitcoin suffers from systemic risk as the top 10,000 addresses control about one-third of all Bitcoin in circulation.[227] It is even worse for Bitcoin miners, with 0.01% controlling 50% of the capacity. According to researcher Flipside Crypto, less than 2% of anonymous accounts control 95% of all available Bitcoin supply.[228] This is considered risky as a great deal of the market is in the hands of a few entities.\n A paper by John Griffin, a finance professor at the University of Texas, and Amin Shams, a graduate student found that in 2017 the price of Bitcoin had been substantially inflated using another cryptocurrency, Tether.[229]\n Roger Lowenstein, author of \"Bank of America: The Epic Struggle to Create the Federal Reserve,\" says in a New York Times story that FTX will face over $8 billion in claims.[230]\n Non-fungible tokens (NFTs) are digital assets that represent art, collectibles, gaming, etc. Like crypto, their data is stored on the blockchain. NFTs are bought and traded using cryptocurrency. The Ethereum blockchain was the first place where NFTs were implemented, but now many other blockchains have created their own versions of NFTs.\n As the first big Wall Street bank to embrace cryptocurrencies, Morgan Stanley announced on 17 March 2021 that they will be offering access to Bitcoin funds for their wealthy clients through three funds which enable Bitcoin ownership for investors with an aggressive risk tolerance.[231] BNY Mellon on 11 February 2021 announced that it would begin offering cryptocurrency services to its clients.[232]\n On 20 April 2021,[233] Venmo added support to its platform to enable customers to buy, hold and sell cryptocurrencies.[234]\n In October 2021, financial services company Mastercard announced it is working with digital asset manager Bakkt on a platform that would allow any bank or merchant on the Mastercard network to offer cryptocurrency services.[235]\n Mining for proof-of-work cryptocurrencies requires enormous amounts of electricity and consequently comes with a large carbon footprint due to causing greenhouse gas emissions.[236] Proof-of-work blockchains such as Bitcoin, Ethereum, Litecoin, and Monero were estimated to have added between 3 million and 15 million tons of carbon dioxide (CO2) to the atmosphere in the period from 1 January 2016 to 30 June 2017.[237] By November 2018, Bitcoin was estimated to have an annual energy consumption of 45.8TWh, generating 22.0 to 22.9 million tons of CO2, rivalling nations like Jordan and Sri Lanka.[238] By the end of 2021, Bitcoin was estimated to produce 65.4 million tons of CO2, as much as Greece,[239] and consume between 91 and 177 terawatt-hours annually.[240][241]\n Critics have also identified a large electronic waste problem in disposing of mining rigs.[242] Mining hardware is improving at a fast rate, quickly resulting in older generations of hardware.[243]\n Bitcoin is the least energy-efficient cryptocurrency, using 707.6 kilowatt-hours of electricity per transaction.[244]\n Before June 2021, China was the primary location for Bitcoin mining. However, due to concerns over power usage and other factors, China forced out Bitcoin operations, at least temporarily. As a result, the United States promptly emerged as the top global leader in the industry. An example of a gross amount of electronic waste associated with Bitcoin mining operations in the US is a facility that located in Dalton, Georgia which is consuming nearly the same amount of electricity as the combined power usage of 97,000 households in its vicinity. Another example is that Riot Platforms operates a Bitcoin mining facility in Rockdale, Texas, which consumes approximately as much electricity as the nearby 300,000 households. This makes it the most energy-intensive Bitcoin mining operation in the United States.[245]\n The world's second-largest cryptocurrency, Ethereum, uses 62.56 kilowatt-hours of electricity per transaction.[246] XRP is the world's most energy efficient cryptocurrency, using 0.0079 kilowatt-hours of electricity per transaction.[247]\n Although the biggest PoW blockchains consume energy on the scale of medium-sized countries, the annual power demand from proof-of-stake (PoS) blockchains is on a scale equivalent to a housing estate. The Times identified six \"environmentally friendly\" cryptocurrencies: Chia, IOTA, Cardano, Nano, Solarcoin and Bitgreen.[248] Academics and researchers have used various methods for estimating the energy use and energy efficiency of blockchains. A study of the six largest proof-of-stake networks in May 2021 concluded:\n In terms of annual consumption (kWh\/yr), the figures were: Polkadot (70,237), Tezos (113,249), Avalanche (489,311), Algorand (512,671), Cardano (598,755) and Solana (1,967,930). This equates to Polkadot consuming 7 times the electricity of an average U.S. home, Cardano 57 homes and Solana 200 times as much. The research concluded that PoS networks consumed 0.001% the electricity of the Bitcoin network.[249] University College London researchers reached a similar conclusion.[250]\n Variable renewable energy power stations could invest in Bitcoin mining to reduce curtailment, hedge electricity price risk, stabilize the grid, increase the profitability of renewable energy power stations and therefore accelerate transition to sustainable energy.[251][252][253][254][255]\n There are also purely technical elements to consider. For example, technological advancement in cryptocurrencies such as Bitcoin result in high up-front costs to miners in the form of specialized hardware and software.[256] Cryptocurrency transactions are normally irreversible after a number of blocks confirm the transaction. Additionally, cryptocurrency private keys can be permanently lost from local storage due to malware, data loss or the destruction of the physical media. This precludes the cryptocurrency from being spent, resulting in its effective removal from the markets.[257]\n In September 2015, the establishment of the peer-reviewed academic journal Ledger (ISSN 2379-5980) was announced. It covers studies of cryptocurrencies and related technologies, and is published by the University of Pittsburgh.[258]\n The journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the Bitcoin blockchain. Authors are also asked to include a personal Bitcoin address in the first page of their papers.[259][260]\n A number of aid agencies have started accepting donations in cryptocurrencies, including UNICEF.[261] Christopher Fabian, principal adviser at UNICEF Innovation, said the children's fund would uphold donor protocols, meaning that people making donations online would have to pass checks before they were allowed to deposit funds.[262][263]\n However, in 2021, there was a backlash against donations in Bitcoin because of the environmental emissions it caused. Some agencies stopped accepting Bitcoin and others turned to \"greener\" cryptocurrencies.[264] The U.S. arm of Greenpeace stopped accepting bitcoin donations after seven years. It said: \"As the amount of energy needed to run Bitcoin became clearer, this policy became no longer tenable.\"[265]\n In 2022, the Ukrainian government raised over US$10,000,000 worth of aid through cryptocurrency following the 2022 Russian invasion of Ukraine.[266]\n Bitcoin has been characterized as a speculative bubble by eight winners of the Nobel Memorial Prize in Economic Sciences: Paul Krugman,[267] Robert J. Shiller,[268] Joseph Stiglitz,[269] Richard Thaler,[270] James Heckman,[271] Thomas Sargent,[271] Angus Deaton,[271] and Oliver Hart;[271] and by central bank officials including Alan Greenspan,[272] Agustín Carstens,[273] Vítor Constâncio,[274] and Nout Wellink.[275]\n Investors Warren Buffett and George Soros have respectively characterized it as a \"mirage\"[276] and a \"bubble\";[277] while business executives Jack Ma and JP Morgan Chase CEO Jamie Dimon have called it a \"bubble\"[278] and a \"fraud\",[279] respectively, although Jamie Dimon later said he regretted dubbing Bitcoin a fraud.[280] BlackRock CEO Laurence D. Fink called Bitcoin an \"index of money laundering\".[281]\n In June 2022, business magnate Bill Gates said that cryptocurrencies are \"100% based on greater fool theory\".[282]\n Legal scholars criticize the lack of regulation, which hinders conflict resolution when crypto assets are at the center of a legal dispute, for example a divorce or an inheritance. In Switzerland, jurists generally deny that cryptocurrencies are objects that fall under property law, as cryptocurrencies do not belong to any class of legally defined objects (Typenzwang, the legal numerus clausus). Therefore, it is debated whether anybody could even be sued for embezzlement of cryptocurrency if he\/she had access to someone's wallet. However, in the law of obligations and contract law, any kind of object would be legally valid, but the object would have to be tied to an identified counterparty. However, as the more popular cryptocurrencies can be freely and quickly exchanged into legal tender, they are financial assets and have to be taxed and accounted for as such.[283][284]\n In 2018, an increase in crypto-related suicides was noticed after the cryptocurrency market crashed in August. The situation was particularly critical in Korea as crypto traders were on \"suicide watch\". A cryptocurrency forum on Reddit even started providing suicide prevention support to affected investors.[285][286] The May 2022 collapse of the Luna currency operated by Terra also led to reports of suicidal investors in crypto-related subreddits.[287]\n"}
{"key":"Cryptocurrency","link":"https:\/\/en.wikipedia.org\/wiki\/Cryptocurrency","headline":"Cryptocurrency - Wikipedia","content":"\n A cryptocurrency, crypto-currency, or crypto[a] is a digital currency designed to work as a medium of exchange through a computer network that is not reliant on any central authority, such as a government or bank, to uphold or maintain it.[2]\n Individual coin ownership records are stored in a digital ledger, which is a computerized database using strong cryptography to secure transaction records, control the creation of additional coins, and verify the transfer of coin ownership.[3][4][5] Despite the term that has come to describe many of the fungible blockchain tokens that have been created, cryptocurrencies are not considered to be currencies in the traditional sense, and varying legal treatments have been applied to them in various jurisdicitons, including classification as commodities, securities, and currencies, cryptocurrencies are generally viewed as a distinct asset class in practice.[6][7][8] Some crypto schemes use validators to maintain the cryptocurrency. In a proof-of-stake model, owners put up their tokens as collateral. In return, they get authority over the token in proportion to the amount they stake. Generally, these token stakers get additional ownership in the token over time via network fees, newly minted tokens, or other such reward mechanisms.[9]\n Cryptocurrency does not exist in physical form (like paper money) and is typically not issued by a central authority. Cryptocurrencies typically use decentralized control as opposed to a central bank digital currency (CBDC).[10] When a cryptocurrency is minted, created prior to issuance, or issued by a single issuer, it is generally considered centralized. When implemented with decentralized control, each cryptocurrency works through distributed ledger technology, typically a blockchain, that serves as a public financial transaction database.[11]\n The first cryptocurrency was Bitcoin, which was first released as open-source software in 2009. As of June 2023, there were more than 25,000 other cryptocurrencies in the marketplace, of which more than 40 had a market capitalization exceeding $1 billion.[12] Throughout their existence, cryptocurrencies have been involved in criminal activities and multi-billion-dollar fraud schemes. Some economists and investors, such as Warren Buffett, considered cryptocurrencies to be a speculative bubble.\n In 1983, American cryptographer David Chaum conceived of a type of cryptographic electronic money called ecash.[13][14] Later, in 1995, he implemented it through Digicash,[15] an early form of cryptographic electronic payments. Digicash required user software in order to withdraw notes from a bank and designate specific encrypted keys before it can be sent to a recipient. This allowed the digital currency to be untraceable by a third party.\n In 1996, the National Security Agency published a paper entitled How to Make a Mint: The Cryptography of Anonymous Electronic Cash, describing a cryptocurrency system. The paper was first published in an MIT mailing list[16] and later in 1997 in The American Law Review.[17]\n In 1998, Wei Dai described \"b-money\", an anonymous, distributed electronic cash system.[18] Shortly thereafter, Nick Szabo described bit gold.[19] Like Bitcoin and other cryptocurrencies that would follow it, bit gold (not to be confused with the later gold-based exchange BitGold) was described as an electronic currency system which required users to complete a proof of work function with solutions being cryptographically put together and published.\n In January 2009, Bitcoin was created by pseudonymous developer Satoshi Nakamoto. It used SHA-256, a cryptographic hash function, in its proof-of-work scheme.[20][21] In April 2011, Namecoin was created as an attempt at forming a decentralized DNS. In October 2011, Litecoin was released which used scrypt as its hash function instead of SHA-256. Peercoin, created in August 2012, used a hybrid of proof-of-work and proof-of-stake.[22]\n Cryptocurrency has undergone several periods of growth and retraction, including several bubbles and market crashes, such as in 2011, 2013–2014\/15, 2017–2018 and 2021–2023.[23][24]\n On 6 August 2014, the UK announced its Treasury had commissioned a study of cryptocurrencies, and what role, if any, they could play in the UK economy. The study was also to report on whether regulation should be considered.[25] Its final report was published in 2018,[26] and it issued a consultation on cryptoassets and stablecoins in January 2021.[27]\n In June 2021, El Salvador became the first country to accept Bitcoin as legal tender, after the Legislative Assembly had voted 62–22 to pass a bill submitted by President Nayib Bukele classifying the cryptocurrency as such.[28]\n In August 2021, Cuba followed with Resolution 215 to recognize and regulate cryptocurrencies such as Bitcoin.[29]\n In September 2021, the government of China, the single largest market for cryptocurrency, declared all cryptocurrency transactions illegal. This completed a crackdown on cryptocurrency that had previously banned the operation of intermediaries and miners within China.[30]\n On 15 September 2022, the world's second largest cryptocurrency at that time, Ethereum transitioned its consensus mechanism from proof-of-work (PoW) to proof-of-stake (PoS) in an upgrade process known as \"the Merge\".  According to the Ethereum Founder, the upgrade can cut both Ethereum's energy use and carbon-dioxide emissions by 99.9%.[31]\n On 11 November 2022, FTX Trading Ltd., a cryptocurrency exchange, which also operated a crypto hedge fund, and had been valued at $18 billion,[32] filed for bankruptcy.[33] The financial impact of the collapse extended beyond the immediate FTX customer base, as reported,[34] while, at a Reuters conference, financial industry executives said that \"regulators must step in to protect crypto investors.\"[35] Technology analyst Avivah Litan commented on the cryptocurrency ecosystem that \"everything...needs to improve dramatically in terms of user experience, controls, safety, customer service.\"[36]\n According to Jan Lansky, a cryptocurrency is a system that meets six conditions:[37]\n In March 2018, the word cryptocurrency was added to the Merriam-Webster Dictionary.[38]\n After the early innovation of Bitcoin in 2008, and the early network effect gained by Bitcoin, tokens, cryptocurrencies, and other digital assets that were not Bitcoin became collectively known during the 2010s as alternative cryptocurrencies,[39][40][41] or \"altcoins.\"[42] \nSometimes the term \"alt coins\" was used,[43][44] or disparagingly, \"shitcoins\".[45] Paul Vigna of The Wall Street Journal described altcoins in 2020 as \"alternative versions of Bitcoin\"[46] given its role as the model protocol for cryptocurrency designers.  A Polytechnic University of Catalonia thesis in 2021 used a somewhat broader description. Not just as alternative versions of Bitcoin itself, but for any cryptocurrency other than bitcoin. \"As of early 2020, there were more than 5,000 cryptocurrencies. Altcoin is the combination of two words \"alt\" and \"coin\" and includes all alternatives to Bitcoin.\"[42]: 14 \n  Altcoins often have underlying differences when compared to Bitcoin. For example, Litecoin aims to process a block every 2.5 minutes, rather than Bitcoin's 10 minutes, which allows Litecoin to confirm transactions faster than Bitcoin.[47] Another example is Ethereum, which has smart contract functionality that allows decentralized applications to be run on its blockchain.[48] Ethereum was the most used blockchain in 2020, according to Bloomberg News.[49] In 2016, it had the largest \"following\" of any altcoin, according to the New York Times.[50]\n Significant market price rallies across multiple altcoin markets are often referred to as an \"altseason\".[51][52]\n Stablecoins are cryptocurrencies designed to maintain a stable level of purchasing power.[53] Notably, these designs are not foolproof, as a number of stablecoins have crashed or lost their peg. For example, on 11 May 2022, Terra's stablecoin UST fell from $1 to 26 cents.[54][55] The subsequent failure of Terraform Labs resulted in the loss of nearly $40B invested in the Terra and Luna coins.[56] In September 2022, South Korean prosecutors requested the issuance of an Interpol Red Notice against the company's founder, Do Kwon.[57] In Hong Kong, the expected regulatory framework for stablecoins in 2023\/24 is being shaped and includes a few considerations.[58]\n Cryptocurrency is produced by an entire cryptocurrency system collectively, at a rate which is defined when the system is created and which is publicly stated. In centralized banking and economic systems such as the US Federal Reserve System, corporate boards or governments control the supply of currency.[citation needed] In the case of cryptocurrency, companies or governments cannot produce new units, and have not so far provided backing for other firms, banks or corporate entities which hold asset value measured in it. The underlying technical system upon which cryptocurrencies are based was created by Satoshi Nakamoto.[59]\n Within a proof-of-work system such as Bitcoin, the safety, integrity and balance of ledgers is maintained by a community of mutually distrustful parties referred to as miners. Miners use their computers to help validate and timestamp transactions, adding them to the ledger in accordance with a particular timestamping scheme.[20] In a proof-of-stake blockchain, transactions are validated by holders of the associated cryptocurrency, sometimes grouped together in stake pools.\n Most cryptocurrencies are designed to gradually decrease the production of that currency, placing a cap on the total amount of that currency that will ever be in circulation.[60] Compared with ordinary currencies held by financial institutions or kept as cash on hand, cryptocurrencies can be more difficult for seizure by law enforcement.[3]\n The validity of each cryptocurrency's coins is provided by a blockchain. A blockchain is a continuously growing list of records, called blocks, which are linked and secured using cryptography.[59][61] Each block typically contains a hash pointer as a link to a previous block,[61] a timestamp and transaction data.[62] By design, blockchains are inherently resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\".[63] For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without the alteration of all subsequent blocks, which requires collusion of the network majority.\n Blockchains are secure by design and are an example of a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been achieved with a blockchain.[64]\n A node is a computer that connects to a cryptocurrency network. The node supports the cryptocurrency's network through either relaying transactions, validation, or hosting a copy of the blockchain. In terms of relaying transactions, each network computer (node) has a copy of the blockchain of the cryptocurrency it supports. When a transaction is made, the node creating the transaction broadcasts details of the transaction using encryption to other nodes throughout the node network so that the transaction (and every other transaction) is known.\n Node owners are either volunteers, those hosted by the organization or body responsible for developing the cryptocurrency blockchain network technology, or those who are enticed to host a node to receive rewards from hosting the node network.[65]\n Cryptocurrencies use various timestamping schemes to \"prove\" the validity of transactions added to the blockchain ledger without the need for a trusted third party.\n The first timestamping scheme invented was the proof-of-work scheme. The most widely used proof-of-work schemes are based on SHA-256 and scrypt.[22]\n Some other hashing algorithms that are used for proof-of-work include CryptoNote, Blake, SHA-3, and X11.\n Another method is called the proof-of-stake scheme. Proof-of-stake is a method of securing a cryptocurrency network and achieving distributed consensus through requesting users to show ownership of a certain amount of currency. It is different from proof-of-work systems that run difficult hashing algorithms to validate electronic transactions. The scheme is largely dependent on the coin, and there is currently no standard form of it. Some cryptocurrencies use a combined proof-of-work and proof-of-stake scheme.[22]\n On a blockchain, mining is the validation of transactions. For this effort, successful miners obtain new cryptocurrency as a reward. The reward decreases transaction fees by creating a complementary incentive to contribute to the processing power of the network. The rate of generating hashes, which validate any transaction, has been increased by the use of specialized machines such as FPGAs and ASICs running complex hashing algorithms like SHA-256 and scrypt.[66] This arms race for cheaper-yet-efficient machines has existed since Bitcoin was introduced in 2009.[66] Mining is measured by hash rate typically in TH\/s.[67]\n With more people entering the world of virtual currency, generating hashes for validation has become more complex over time, forcing miners to invest increasingly large sums of money to improve computing performance. Consequently, the reward for finding a hash has diminished and often does not justify the investment in equipment and cooling facilities (to mitigate the heat the equipment produces), and the electricity required to run them.[68] Popular regions for mining include those with inexpensive electricity, a cold climate, and jurisdictions with clear and conducive regulations. By July 2019, Bitcoin's electricity consumption was estimated to be approximately 7 gigawatts, around 0.2% of the global total, or equivalent to the energy consumed nationally by Switzerland.[69]\n Some miners pool resources, sharing their processing power over a network to split the reward equally, according to the amount of work they contributed to the probability of finding a block. A \"share\" is awarded to members of the mining pool who present a valid partial proof-of-work.\n As of February 2018[update], the Chinese Government has halted trading of virtual currency, banned initial coin offerings and shut down mining. Many Chinese miners have since relocated to Canada[70] and Texas.[71] One company is operating data centers for mining operations at Canadian oil and gas field sites, due to low gas prices.[72] In June 2018, Hydro Quebec proposed to the provincial government to allocate 500 megawatts of power to crypto companies for mining.[73] According to a February 2018 report from Fortune, Iceland has become a haven for cryptocurrency miners in part because of its cheap electricity.[74]\n In March 2018, the city of Plattsburgh, New York put an 18-month moratorium on all cryptocurrency mining in an effort to preserve natural resources and the \"character and direction\" of the city.[75] In 2021, Kazakhstan became the second-biggest crypto-currency mining country, producing 18.1% of the global exahash rate. The country built a compound containing 50,000 computers near Ekibastuz.[76]\n An increase in cryptocurrency mining increased the demand for graphics cards (GPU) in 2017.[77] The computing power of GPUs makes them well-suited to generating hashes. Popular favorites of cryptocurrency miners such as Nvidia's GTX 1060 and GTX 1070 graphics cards, as well as AMD's RX 570 and RX 580 GPUs, doubled or tripled in price – or were out of stock.[78] A GTX 1070 Ti which was released at a price of $450 sold for as much as $1,100. Another popular card, the GTX 1060 (6 GB model) was released at an MSRP of $250, and sold for almost $500. RX 570 and RX 580 cards from AMD were out of stock for almost a year. Miners regularly buy up the entire stock of new GPU's as soon as they are available.[79]\n Nvidia has asked retailers to do what they can when it comes to selling GPUs to gamers instead of miners. Boris Böhles, PR manager for Nvidia in the German region, said: \"Gamers come first for Nvidia.\"[80]\n Numerous companies developed dedicated crypto-mining accelerator chips, capable of price-performance far higher than that of CPU or GPU mining. At one point Intel marketed its own brand of crypto accelerator chip, named Blockscale.[81]\n A cryptocurrency wallet is a means of storing the public and private \"keys\" (address) or seed which can be used to receive or spend the cryptocurrency.[82] With the private key, it is possible to write in the public ledger, effectively spending the associated cryptocurrency. With the public key, it is possible for others to send currency to the wallet.\n There exist multiple methods of storing keys or seed in a wallet. These methods range from using paper wallets (which are public, private or seed keys written on paper), to using hardware wallets (which are hardware to store your wallet information), to a digital wallet (which is a computer with a software hosting your wallet information), to hosting your wallet using an exchange where cryptocurrency is traded, or by storing your wallet information on a digital medium such as plaintext.[83]\n Bitcoin is pseudonymous, rather than anonymous; the cryptocurrency in a wallet is not tied to a person, but rather to one or more specific keys (or \"addresses\").[84] Thereby, Bitcoin owners are not immediately identifiable, but all transactions are publicly available in the blockchain.[85] Still, cryptocurrency exchanges are often required by law to collect the personal information of their users.[86]\n Some cryptocurrencies, such as Monero, Zerocoin, Zerocash, and CryptoNote, implement additional measures to increase privacy, such as by using zero-knowledge proofs.[87][88]\n A recent 2020 study presented different attacks on privacy in cryptocurrencies. The attacks demonstrated how the anonymity techniques are not sufficient safeguards. In order to improve privacy, researchers suggested several different ideas including new cryptographic schemes and mechanisms for hiding the IP address of the source.[89]\n Cryptocurrencies are used primarily outside banking and governmental institutions and are exchanged over the Internet.\n Proof-of-work cryptocurrencies, such as Bitcoin, offer block rewards incentives for miners. There has been an implicit belief that whether miners are paid by block rewards or transaction fees does not affect the security of the blockchain, but a study suggests that this may not be the case under certain circumstances.[90]\n The rewards paid to miners increase the supply of the cryptocurrency. By making sure that verifying transactions is a costly business, the integrity of the network can be preserved as long as benevolent nodes control a majority of computing power. The verification algorithm requires a lot of processing power, and thus electricity in order to make verification costly enough to accurately validate public blockchain. Not only do miners have to factor in the costs associated with expensive equipment necessary to stand a chance of solving a hash problem, they must further consider the significant amount of electrical power in search of the solution. Generally, the block rewards outweigh electricity and equipment costs, but this may not always be the case.[91]\n The current value, not the long-term value, of the cryptocurrency supports the reward scheme to incentivize miners to engage in costly mining activities.[92] In 2018, Bitcoin's design caused a 1.4% welfare loss compared to an efficient cash system, while a cash system with 2% money growth has a minor 0.003% welfare cost. The main source for this inefficiency is the large mining cost, which is estimated to be US$360 million per year. This translates into users being willing to accept a cash system with an inflation rate of 230% before being better off using Bitcoin as a means of payment. However, the efficiency of the Bitcoin system can be significantly improved by optimizing the rate of coin creation and minimizing transaction fees. Another potential improvement is to eliminate inefficient mining activities by changing the consensus protocol altogether.[93]\n Transaction fees for cryptocurrency depend mainly on the supply of network capacity at the time, versus the demand from the currency holder for a faster transaction.[citation needed] The currency holder can choose a specific transaction fee, while network entities process transactions in order of highest offered fee to lowest.[citation needed] Cryptocurrency exchanges can simplify the process for currency holders by offering priority alternatives and thereby determine which fee will likely cause the transaction to be processed in the requested time.[citation needed]\n For Ethereum, transaction fees differ by computational complexity, bandwidth use, and storage needs, while Bitcoin transaction fees differ by transaction size and whether the transaction uses SegWit. In February 2023, the median transaction fee for Ether corresponded to $2.2845,[94] while for Bitcoin it corresponded to $0.659.[95]\n Some cryptocurrencies have no transaction fees, and instead rely on client-side proof-of-work as the transaction prioritization and anti-spam mechanism.[96][97][98]\n Cryptocurrency exchanges allow customers to trade cryptocurrencies[99] for other assets, such as conventional fiat money, or to trade between different digital currencies.\n Crypto marketplaces do not guarantee that an investor is completing a purchase or trade at the optimal price. As a result, as of 2020 it was possible to arbitrage to find the difference in price across several markets.[100]\n Atomic swaps are a mechanism where one cryptocurrency can be exchanged directly for another cryptocurrency, without the need for a trusted third party such as an exchange.[101]\n Jordan Kelley, founder of Robocoin, launched the first Bitcoin ATM in the United States on 20 February 2014. The kiosk installed in Austin, Texas, is similar to bank ATMs but has scanners to read government-issued identification such as a driver's license or a passport to confirm users' identities.[102]\n An initial coin offering (ICO) is a controversial means of raising funds for a new cryptocurrency venture. An ICO may be used by startups with the intention of avoiding regulation. However, securities regulators in many jurisdictions, including in the U.S., and Canada, have indicated that if a coin or token is an \"investment contract\" (e.g., under the Howey test, i.e., an investment of money with a reasonable expectation of profit based significantly on the entrepreneurial or managerial efforts of others), it is a security and is subject to securities regulation. In an ICO campaign, a percentage of the cryptocurrency (usually in the form of \"tokens\") is sold to early backers of the project in exchange for legal tender or other cryptocurrencies, often Bitcoin or Ether.[103][104][105]\n According to PricewaterhouseCoopers, four of the 10 biggest proposed initial coin offerings have used Switzerland as a base, where they are frequently registered as non-profit foundations. The Swiss regulatory agency FINMA stated that it would take a \"balanced approach\" to ICO projects and would allow \"legitimate innovators to navigate the regulatory landscape and so launch their projects in a way consistent with national laws protecting investors and the integrity of the financial system.\" In response to numerous requests by industry representatives, a legislative ICO working group began to issue legal guidelines in 2018, which are intended to remove uncertainty from cryptocurrency offerings and to establish sustainable business practices.[106]\n The market capitalization of a cryptocurrency is calculated by multiplying the price by the number of coins in circulation. The total cryptocurrency market cap has historically been dominated by Bitcoin accounting for at least 50% of the market cap value where altcoins have increased and decreased in market cap value in relation to Bitcoin. Bitcoin's value is largely determined by speculation among other technological limiting factors known as blockchain rewards coded into the architecture technology of Bitcoin itself. The cryptocurrency market cap follows a trend known as the \"halving\", which is when the block rewards received from Bitcoin are halved due to technological mandated limited factors instilled into Bitcoin which in turn limits the supply of Bitcoin. As the date reaches near of a halving (twice thus far historically) the cryptocurrency market cap increases, followed by a downtrend.[107]\n By June 2021, cryptocurrency had begun to be offered by some wealth managers in the US for 401(k)s.[108][109][110]\n Cryptocurrency prices are much more volatile than established financial assets such as stocks. For example, over one week in May 2022, Bitcoin lost 20% of its value and Ethereum lost 26%, while Solana and Cardano lost 41% and 35% respectively. The falls were attributed to warnings about inflation. By comparison, in the same week, the Nasdaq tech stock index fell 7.6 per cent and the FTSE 100 was 3.6 per cent down.[111]\n In the longer term, of the 10 leading cryptocurrencies identified by the total value of coins in circulation in January 2018, only four (Bitcoin, Ethereum, Cardano and Ripple (XRP)) were still in that position in early 2022.[112] The total value of all cryptocurrencies was  $2 trillion at the end of 2021, but had halved nine months later.[113][114] The Wall Street Journal has commented that the crypto sector has become \"intertwined\" with the rest of the capital markets and \"sensitive to the same forces that drive tech stocks and other risk assets\", such as inflation forecasts.[115]\n There are also centralized databases, outside of blockchains, that store crypto market data. Compared to the blockchain, databases perform fast as there is no verification process. Four of the most popular cryptocurrency market databases are CoinMarketCap, CoinGecko, BraveNewCoin, and Cryptocompare.[116]\n According to Alan Feuer of The New York Times, libertarians and anarcho-capitalists were attracted to the philosophical idea behind Bitcoin. Early Bitcoin supporter Roger Ver said: \"At first, almost everyone who got involved did so for philosophical reasons. We saw Bitcoin as a great idea, as a way to separate money from the state.\"[117] Economist Paul Krugman argues that cryptocurrencies like Bitcoin are \"something of a cult\" based in \"paranoid fantasies\" of government power.[118]\n David Golumbia says that the ideas influencing Bitcoin advocates emerge from right-wing extremist movements such as the Liberty Lobby and the John Birch Society and their anti-Central Bank rhetoric, or, more recently, Ron Paul and Tea Party-style libertarianism.[119] Steve Bannon, who owns a \"good stake\" in Bitcoin, sees cryptocurrency as a form of disruptive populism, taking control back from central authorities.[120]\n Bitcoin's founder, Satoshi Nakamoto, has supported the idea that cryptocurrencies go well with libertarianism. \"It's very attractive to the libertarian viewpoint if we can explain it properly,\" Nakamoto said in 2008.[121]\n According to the European Central Bank, the decentralization of money offered by Bitcoin has its theoretical roots in the Austrian school of economics, especially with Friedrich von Hayek in his book Denationalisation of Money: The Argument Refined,[122] in which Hayek advocates a complete free market in the production, distribution and management of money to end the monopoly of central banks.[123]\n The rise in the popularity of cryptocurrencies and their adoption by financial institutions has led some governments to assess whether regulation is needed to protect users. The Financial Action Task Force (FATF) has defined cryptocurrency-related services as \"virtual asset service providers\" (VASPs) and recommended that they be regulated with the same money laundering (AML) and know your customer (KYC) requirements as financial institutions.[124]\n In May 2020, the Joint Working Group on interVASP Messaging Standards published \"IVMS 101\", a universal common language for communication of required originator and beneficiary information between VASPs. The FATF and financial regulators were informed as the data model was developed.[125]\n In June 2020, FATF updated its guidance to include the \"Travel Rule\" for cryptocurrencies, a measure which mandates that VASPs obtain, hold, and exchange information about the originators and beneficiaries of virtual asset transfers.[126] Subsequent standardized protocol specifications recommended using JSON for relaying data between VASPs and identity services. As of December 2020, the IVMS 101 data model has yet to be finalized and ratified by the three global standard setting bodies that created it.[127]\n The European Commission published a digital finance strategy in September 2020. This included a draft regulation on Markets in Crypto-Assets (MiCA), which aimed to provide a comprehensive regulatory framework for digital assets in the EU.[128][129]\n On 10 June 2021, the Basel Committee on Banking Supervision proposed that banks that held cryptocurrency assets must set aside capital to cover all potential losses. For instance, if a bank were to hold Bitcoin worth $2 billion, it would be required to set aside enough capital to cover the entire $2 billion. This is a more extreme standard than banks are usually held to when it comes to other assets. However, this is a proposal and not a regulation.\n The IMF is seeking a coordinated, consistent and comprehensive approach to supervising cryptocurrencies. Tobias Adrian, the IMF's financial counsellor and head of its monetary and capital markets department said in a January 2022 interview that \"Agreeing global regulations is never quick. But if we start now, we can achieve the goal of maintaining financial stability while also enjoying the benefits which the underlying technological innovations bring,\"[130]\n In September 2017, China banned ICOs to cause abnormal return from cryptocurrency decreasing during announcement window. The liquidity changes by banning ICOs in China was temporarily negative while the liquidity effect became positive after news.[131]\n On 18 May 2021, China banned financial institutions and payment companies from being able to provide cryptocurrency transaction related services.[132] This led to a sharp fall in the price of the biggest proof of work cryptocurrencies. For instance, Bitcoin fell 31%, Ethereum fell 44%, Binance Coin fell 32% and Dogecoin fell 30%.[133] Proof of work mining was the next focus, with regulators in popular mining regions citing the use of electricity generated from highly polluting sources such as coal to create Bitcoin and Ethereum.[134]\n In September 2021, the Chinese government declared all cryptocurrency transactions of any kind illegal, completing its crackdown on cryptocurrency.[30]\n On 9 June 2021, El Salvador announced that it will adopt Bitcoin as legal tender, becoming the first country to do so.[135]\n At present, India neither prohibits nor allows investment in the cryptocurrency market. In 2020, the Supreme Court of India had lifted the ban on cryptocurrency, which was imposed by the Reserve Bank of India.[136][137][138][139] Since then, an investment in cryptocurrency is considered legitimate, though there is still ambiguity about the issues regarding the extent and payment of tax on the income accrued thereupon and also its regulatory regime. But it is being contemplated that the Indian Parliament will soon pass a specific law to either ban or regulate the cryptocurrency market in India.[140] Expressing his public policy opinion on the Indian cryptocurrency market to a well-known online publication, a leading public policy lawyer and Vice President of SAARCLAW (South Asian Association for Regional Co-operation in Law) Hemant Batra has said that the \"cryptocurrency market has now become very big with involvement of billions of dollars in the market hence, it is now unattainable and irreconcilable for the government to completely ban all sorts of cryptocurrency and its trading and investment\".[141] He mooted regulating the cryptocurrency market rather than completely banning it. He favoured following IMF and FATF guidelines in this regard.\n South Africa, which has seen a large number of scams related to cryptocurrency, is said to be putting a regulatory timeline in place that will produce a regulatory framework.[142] The largest scam occurred in April 2021, where the two founders of an African-based cryptocurrency exchange called Africrypt, Raees Cajee and Ameer Cajee, disappeared with $3.8 billion worth of Bitcoin.[143] Additionally, Mirror Trading International disappeared with $170 million worth of cryptocurrency in January 2021.[143]\n In March 2021, South Korea implemented new legislation to strengthen their oversight of digital assets. This legislation requires all digital asset managers, providers and exchanges to be registered with the Korea Financial Intelligence Unit in order to operate in South Korea.[144] Registering with this unit requires that all exchanges are certified by the Information Security Management System and that they ensure all customers have real name bank accounts. It also requires that the CEO and board members of the exchanges have not been convicted of any crimes and that the exchange holds sufficient levels of deposit insurance to cover losses arising from hacks.[144]\n Switzerland was one of the first countries to implement the FATF's Travel Rule. FINMA, the Swiss regulator, issued its own guidance to VASPs in 2019. The guidance followed the FATF's Recommendation 16, however with stricter requirements. According to FINMA's[145] requirements, VASPs need to verify the identity of the beneficiary of the transfer.\n On 30 April 2021, the Central Bank of the Republic of Turkey banned the use of cryptocurrencies and cryptoassets for making purchases on the grounds that the use of cryptocurrencies for such payments poses significant transaction risks.[146]\n In the United Kingdom, as of 10 January 2021, all cryptocurrency firms, such as exchanges, advisors and professionals that have either a presence, market product or provide services within the UK market must register with the Financial Conduct Authority. Additionally, on 27 June 2021, the financial watchdog demanded that Binance, the world's largest cryptocurrency exchange,[147] cease all regulated activities in the UK.[148]\n In 2021, 17 states passed laws and resolutions concerning cryptocurrency regulation.[149] The U.S. Securities and Exchange Commission (SEC) is considering what steps to take. On 8 July 2021, Senator Elizabeth Warren, part of the Senate Banking Committee, wrote to the chairman of the SEC and demanded answers on cryptocurrency regulation due to the increase in cryptocurrency exchange use and the danger this posed to consumers. On 5 August 2021, SEC Chairman Gary Gensler responded to Senator Elizabeth Warren's letter regarding cryptocurrency regulation and called for legislation focused on \"crypto trading, lending and DeFi platforms,\" because of how vulnerable the investors could be when they traded on crypto trading platforms without a broker. He also argued that many tokens in the crypto market may be unregistered securities without required disclosures or market oversight. Additionally, Gensler did not hold back in his criticism of stablecoins. These tokens, which are pegged to the value of fiat currencies, may allow individuals to bypass important public policy goals related to traditional banking and financial systems, such as anti-money laundering, tax compliance, and sanctions.[150]\n On 19 October 2021, the first bitcoin-linked exchange-traded fund (ETF) from ProShares started trading on the NYSE under the ticker \"BITO.\" ProShares CEO Michael L. Sapir said the ETF would expose Bitcoin to a wider range of investors without the hassle of setting up accounts with cryptocurrency providers. Ian Balina, the CEO of Token Metrics, stated that the approval of the \"BITO\" ETF by the SEC was a significant endorsement for the crypto industry because many regulators globally were not in favor of crypto as well as the hesitance to accept crypto from retail investors. This event would eventually open more opportunities for new capital and new people in this space.[151]\n The United States Department of the Treasury, on 20 May 2021, announced that it would require any transfer worth $10,000 or more to be reported to the Internal Revenue Service since cryptocurrency already posed a problem where illegal activity like tax evasion was facilitated broadly. This release from the IRS was a part of efforts to promote better compliance and consider more severe penalties for tax evaders.[152]\n On 17 February 2022, the Justice department named Eun Young Choi as the first director of a National Cryptocurrency Enforcement Team to aid in identification of and dealing with misuse of cryptocurrencies and other digital assets.[153]\n The Biden administration faced a dilemma as it tried to develop regulations for the cryptocurrency industry. On one hand, officials were hesitant to restrict the growing and profitable industry. On the other hand, they were committed to preventing illegal cryptocurrency transactions. To reconcile these conflicting goals, on 9 March 2022, President Biden issued an executive order.[154] Followed by the executive order, on 16 September 2022, the Comprehensive Framework for Responsible Development of Digital Assets document was released [155] to support development of cryptocurrencies and restrict their illegal use. The executive order included all digital assets, but cryptocurrencies posed both the greatest security risks and potential economic benefits. Though this might not address all of the challenges in crypto industry, it was a significant milestone in the U.S. cryptocurrency regulation history.[156]\n In February 2023, the Securities and Exchange Commission (SEC) ruled that cryptocurrency exchange Kraken's estimated $42 billion in staked assets globally operated as an illegal securities seller. The company agreed to a $30 million settlement with the SEC and to cease selling its staking service in the U.S. The case would impact other major crypto exchanges operating staking programs.[157]\n On 23 March 2023, the U.S. Securities and Exchange Commission (SEC) issued an alert to investors stating that firms offering crypto asset securities may not be complying with U.S. laws. The SEC stated that unregistered offerings of crypto asset securities may not include important information.[158]\n The legal status of cryptocurrencies varies substantially from country to country and is still undefined or changing in many of them. At least one study has shown that broad generalizations about the use of Bitcoin in illicit finance are significantly overstated and that blockchain analysis is an effective crime fighting and intelligence gathering tool.[159] While some countries have explicitly allowed their use and trade,[160] others have banned or restricted it. According to the Library of Congress in 2021,\nan \"absolute ban\" on trading or using cryptocurrencies applies in 9 countries:\nAlgeria, Bangladesh, Bolivia, China, Egypt, Iraq, Morocco, Nepal, and the United Arab Emirates. An \"implicit ban\" applies in another 39 countries or regions, which include: Bahrain, Benin, Burkina Faso, Burundi, Cameroon, Chad, Cote d’Ivoire, the Dominican Republic, Ecuador, Gabon, Georgia, Guyana, Indonesia, Iran, Jordan, Kazakhstan, Kuwait, Lebanon, Lesotho, Macau, Maldives, Mali, Moldova, Namibia, Niger, Nigeria, Oman, Pakistan, Palau, Republic of Congo, Saudi Arabia, Sengeal, Tajikistan, Tanzania, Togo, Turkey, Turkmenistan, Qatar and Vietnam.[161] In the United States and Canada, state and provincial securities regulators, coordinated through the North American Securities Administrators Association, are investigating \"Bitcoin scams\" and ICOs in 40 jurisdictions.[162]\n Various government agencies, departments, and courts have classified Bitcoin differently. China Central Bank banned the handling of Bitcoins by financial institutions in China in early 2014.\n In Russia, though owning cryptocurrency is legal, its residents are only allowed to purchase goods from other residents using the Russian ruble while nonresidents are allowed to use foreign currency.[163] Regulations and bans that apply to Bitcoin probably extend to similar cryptocurrency systems.[164]\n In August 2018, the Bank of Thailand announced its plans to create its own cryptocurrency, the Central Bank Digital Currency (CBDC).[165]\n Cryptocurrency advertisements have been banned on the following platforms:\n On 25 March 2014, the United States Internal Revenue Service (IRS) ruled that Bitcoin will be treated as property for tax purposes. Therefore, virtual currencies are considered commodities subject to capital gains tax.[173]\n As the popularity and demand for online currencies has increased since the inception of Bitcoin in 2009,[174] so have concerns that such an unregulated person to person global economy that cryptocurrencies offer may become a threat to society. Concerns abound that altcoins may become tools for anonymous web criminals.[175]\n Cryptocurrency networks display a lack of regulation that has been criticized as enabling criminals who seek to evade taxes and launder money. Money laundering issues are also present in regular bank transfers, however with bank-to-bank wire transfers for instance, the account holder must at least provide a proven identity.\n Transactions that occur through the use and exchange of these altcoins are independent from formal banking systems, and therefore can make tax evasion simpler for individuals. Since charting taxable income is based upon what a recipient reports to the revenue service, it becomes extremely difficult to account for transactions made using existing cryptocurrencies, a mode of exchange that is complex and difficult to track.[175]\n Systems of anonymity that most cryptocurrencies offer can also serve as a simpler means to launder money. Rather than laundering money through an intricate net of financial actors and offshore bank accounts, laundering money through altcoins can be achieved through anonymous transactions.[175]\n Cryptocurrency makes legal enforcement against extremist groups more complicated, which consequently strengthens them.[176] White supremacist Richard Spencer went as far as to declare Bitcoin the \"currency of the alt-right\".[177]\n In February 2014, the world's largest Bitcoin exchange, Mt. Gox, declared bankruptcy. Likely due to theft, the company claimed that it had lost nearly 750,000 Bitcoins belonging to their clients. This added up to approximately 7% of all Bitcoins in existence, worth a total of $473 million. Mt. Gox blamed hackers, who had exploited the transaction malleability problems in the network. The price of a Bitcoin fell from a high of about $1,160 in December to under $400 in February.[178]\n On 21 November 2017, Tether announced that it had been hacked, losing $31 million in USDT from its core treasury wallet.[179]\n On 7 December 2017, Slovenian cryptocurrency exchange Nicehash reported that hackers had stolen over $70M using a hijacked company computer.[180]\n On 19 December 2017, Yapian, the owner of South Korean exchange Youbit, filed for bankruptcy after suffering two hacks that year.[181][182] Customers were still granted access to 75% of their assets.\n In May 2018, Bitcoin Gold had its transactions hijacked and abused by unknown hackers.[183] Exchanges lost an estimated $18m and Bitcoin Gold was delisted from Bittrex after it refused to pay its share of the damages.\n On 13 September 2018, Homero Josh Garza was sentenced to 21 months of imprisonment, followed by three years of supervised release.[184] Garza had founded the cryptocurrency startups GAW Miners and ZenMiner in 2014, acknowledged in a plea agreement that the companies were part of a pyramid scheme, and pleaded guilty to wire fraud in 2015. The U.S. Securities and Exchange Commission separately brought a civil enforcement action against Garza, who was eventually ordered to pay a judgment of $9.1 million plus $700,000 in interest. The SEC's complaint stated that Garza, through his companies, had fraudulently sold \"investment contracts representing shares in the profits they claimed would be generated\" from mining.[185]\n In January 2018, Japanese exchange Coincheck reported that hackers had stolen $530M worth of cryptocurrencies.[186]\n In June 2018, South Korean exchange Coinrail was hacked, losing over $37M worth of cryptos.[187] The hack worsened an already ongoing cryptocurrency selloff by an additional $42 billion.[188]\n On 9 July 2018, the exchange Bancor, whose code and fundraising had been subjects of controversy, had $23.5 million in cryptocurrency stolen.[189]\n A 2020 EU report found that users had lost crypto-assets worth hundreds of millions of US dollars in security breaches at exchanges and storage providers. Between 2011 and 2019, reported breaches ranged from four to twelve a year. In 2019, more than a billion dollars worth of cryptoassets was reported stolen. Stolen assets \"typically find their way to illegal markets and are used to fund further criminal activity\".[190]\n According to a 2020 report produced by the United States Attorney General's Cyber-Digital Task Force, the following three categories make up the majority of illicit cryptocurrency uses: \"(1) financial transactions associated with the commission of crimes; (2) money laundering and the shielding of legitimate activity from tax, reporting, or other legal requirements; or (3) crimes, such as theft, directly implicating the cryptocurrency marketplace itself.\" The report concludes that \"for cryptocurrency to realize its truly transformative potential, it is imperative that these risks be addressed\" and that \"the government has legal and regulatory tools available at its disposal to confront the threats posed by cryptocurrency's illicit uses\".[191][192]\n According to the UK 2020 national risk assessment—a comprehensive assessment of money laundering and terrorist financing risk in the UK—the risk of using cryptoassets such as Bitcoin for money laundering and terrorism financing is assessed as \"medium\" (from \"low\" in the previous 2017 report).[193] Legal scholars suggested that the money laundering opportunities may be more perceived than real.[194] Blockchain analysis company Chainalysis concluded that illicit activities like cybercrime, money laundering and terrorism financing made up only 0.15% of all crypto transactions conducted in 2021, representing a total of $14 billion.[195][196][197]\n In December 2021, Monkey Kingdom, a NFT project based in Hong Kong, lost US$1.3 million worth of cryptocurrencies via a phishing link used by the hacker.[198]\n According to blockchain data company Chainalysis, criminals laundered US$8,600,000,000 worth of cryptocurrency in 2021, up by 30% from the previous year.[199] The data suggests that rather than managing numerous illicit havens, cybercriminals make use of a small group of purpose built centralized exchanges for sending and receiving illicit cryptocurrency. In 2021, those exchanges received 47% of funds sent by crime linked addresses.[200] Almost $2.2bn worth of cryptocurrencies was embezzled from DeFi protocols in 2021, which represents 72% of all cryptocurrency theft in 2021.\n According to Bloomberg and the New York Times, Federation Tower, a two skyscraper complex in the heart of Moscow City, is home to many cryptocurrency businesses under suspicion of facilitating extensive money laundering, including accepting illicit cryptocurrency funds obtained through scams, darknet markets, and ransomware.[201] Notable businesses include Garantex,[202] Eggchange, Cashbank, Buy-Bitcoin, Tetchange, Bitzlato, and Suex, which was sanctioned by the U.S. in 2021. Bitzlato founder and owner Anatoly Legkodymov was arrested following money-laundering charges by the United States Department of Justice.[203]\n Dark money has also been flowing into Russia through a dark web marketplace called Hydra, which is powered by cryptocurrency, and enjoyed more than $1 billion in sales in 2020, according to Chainalysis.[204] The platform demands that sellers liquidate cryptocurrency only through certain regional exchanges, which has made it difficult for investigators to trace the money.\n Almost 74% of ransomware revenue in 2021 — over $400 million worth of cryptocurrency — went to software strains likely affiliated with Russia, where oversight is notoriously limited.[201] However, Russians are also leaders in the benign adoption of cryptocurrencies, as the ruble is unreliable, and President Putin favours the idea of \"overcoming the excessive domination of the limited number of reserve currencies.\"[205]\n In 2022, RenBridge - an unregulated alternative to exchanges for transferring value between blockchains - was found to be responsible for the laundering of at least $540 million since 2020. It is especially popular with people attempting to launder money from theft. This includes a cyberattack on Japanese crypto exchange Liquid that has been linked to North Korea.[206]\n Properties of cryptocurrencies gave them popularity in applications such as a safe haven in banking crises and means of payment, which also led to the cryptocurrency use in controversial settings in the form of online black markets, such as Silk Road.[175] The original Silk Road was shut down in October 2013 and there have been two more versions in use since then. In the year following the initial shutdown of Silk Road, the number of prominent dark markets increased from four to twelve, while the amount of drug listings increased from 18,000 to 32,000.[175]\n Darknet markets present challenges in regard to legality. Cryptocurrency used in dark markets are not clearly or legally classified in almost all parts of the world. In the U.S., Bitcoins are labelled as \"virtual assets\".[citation needed] This type of ambiguous classification puts pressure on law enforcement agencies around the world to adapt to the shifting drug trade of dark markets.[207][unreliable source?]\n Various studies have found that crypto-trading is rife with wash trading. Wash trading is a process, illegal in some jurisdictions, involving buyers and sellers being the same person or group, and may be used to manipulate the price of a cryptocurrency or inflate volume artificially. Exchanges with higher volumes can demand higher premiums from token issuers.[208] A study from 2019 concluded that up to 80% of trades on unregulated cryptocurrency exchanges could be wash trades.[208] A 2019 report by Bitwise Asset Management claimed that 95% of all Bitcoin trading volume reported on major website CoinMarketCap had been artificially generated, and of 81 exchanges studied, only 10 provided legitimate volume figures.[209]\n In 2022, cryptocurrencies attracted attention when Western nations imposed severe economic sanctions on Russia in the aftermath of its invasion of Ukraine in February. However, American sources warned in March that some crypto-transactions could potentially be used to evade economic sanctions against Russia and Belarus.[210]\n In April 2022, the computer programmer Virgil Griffith received a five-year prison sentence in the US for attending a Pyongyang cryptocurrency conference, where he gave a presentation on blockchains which might be used for sanctions evasion.[211]\n The Bank for International Settlements summarized several criticisms of cryptocurrencies in Chapter V of their 2018 annual report. The criticisms include the lack of stability in their price, the high energy consumption, high and variable transactions costs, the poor security and fraud at cryptocurrency exchanges, vulnerability to debasement (from forking), and the influence of miners.[212][213][214]\n Cryptocurrencies have been compared to Ponzi schemes, pyramid schemes[215] and economic bubbles,[216] such as housing market bubbles.[217] Howard Marks of Oaktree Capital Management stated in 2017 that digital currencies were \"nothing but an unfounded fad (or perhaps even a pyramid scheme), based on a willingness to ascribe value to something that has little or none beyond what people will pay for it\", and compared them to the tulip mania (1637), South Sea Bubble (1720), and dot-com bubble (1999), which all experienced profound price booms and busts.[218]\n Regulators in several countries have warned against cryptocurrency and some have taken measures to dissuade users.[219] However, research in 2021 by the UK's financial regulator suggests such warnings either went unheard, or were ignored. Fewer than one in 10 potential cryptocurrency buyers were aware of consumer warnings on the FCA website, and 12% of crypto users were not aware that their holdings were not protected by statutory compensation.[220][221] Of 1,000 respondents between the ages of eighteen and forty, almost 70% falsely assumed cryptocurrencies were regulated, 75% of younger crypto investors claimed to be driven by competition with friends and family, 58% said that social media enticed them to make high risk investments.[222] The FCA recommends making use of its warning list, which flags unauthorized financial firms.[223]\n Many banks do not offer virtual currency services themselves and can refuse to do business with virtual currency companies.[224] In 2014, Gareth Murphy, a senior banking officer, suggested that the widespread adoption of cryptocurrencies may lead to too much money being obfuscated, blinding economists who would use such information to better steer the economy.[225] While traditional financial products have strong consumer protections in place, there is no intermediary with the power to limit consumer losses if Bitcoins are lost or stolen. One of the features cryptocurrency lacks in comparison to credit cards, for example, is consumer protection against fraud, such as chargebacks.\n The French regulator Autorité des marchés financiers (AMF) lists 16 websites of companies that solicit investment in cryptocurrency without being authorized to do so in France.[226]\n An October 2021 paper by the National Bureau of Economic Research found that Bitcoin suffers from systemic risk as the top 10,000 addresses control about one-third of all Bitcoin in circulation.[227] It is even worse for Bitcoin miners, with 0.01% controlling 50% of the capacity. According to researcher Flipside Crypto, less than 2% of anonymous accounts control 95% of all available Bitcoin supply.[228] This is considered risky as a great deal of the market is in the hands of a few entities.\n A paper by John Griffin, a finance professor at the University of Texas, and Amin Shams, a graduate student found that in 2017 the price of Bitcoin had been substantially inflated using another cryptocurrency, Tether.[229]\n Roger Lowenstein, author of \"Bank of America: The Epic Struggle to Create the Federal Reserve,\" says in a New York Times story that FTX will face over $8 billion in claims.[230]\n Non-fungible tokens (NFTs) are digital assets that represent art, collectibles, gaming, etc. Like crypto, their data is stored on the blockchain. NFTs are bought and traded using cryptocurrency. The Ethereum blockchain was the first place where NFTs were implemented, but now many other blockchains have created their own versions of NFTs.\n As the first big Wall Street bank to embrace cryptocurrencies, Morgan Stanley announced on 17 March 2021 that they will be offering access to Bitcoin funds for their wealthy clients through three funds which enable Bitcoin ownership for investors with an aggressive risk tolerance.[231] BNY Mellon on 11 February 2021 announced that it would begin offering cryptocurrency services to its clients.[232]\n On 20 April 2021,[233] Venmo added support to its platform to enable customers to buy, hold and sell cryptocurrencies.[234]\n In October 2021, financial services company Mastercard announced it is working with digital asset manager Bakkt on a platform that would allow any bank or merchant on the Mastercard network to offer cryptocurrency services.[235]\n Mining for proof-of-work cryptocurrencies requires enormous amounts of electricity and consequently comes with a large carbon footprint due to causing greenhouse gas emissions.[236] Proof-of-work blockchains such as Bitcoin, Ethereum, Litecoin, and Monero were estimated to have added between 3 million and 15 million tons of carbon dioxide (CO2) to the atmosphere in the period from 1 January 2016 to 30 June 2017.[237] By November 2018, Bitcoin was estimated to have an annual energy consumption of 45.8TWh, generating 22.0 to 22.9 million tons of CO2, rivalling nations like Jordan and Sri Lanka.[238] By the end of 2021, Bitcoin was estimated to produce 65.4 million tons of CO2, as much as Greece,[239] and consume between 91 and 177 terawatt-hours annually.[240][241]\n Critics have also identified a large electronic waste problem in disposing of mining rigs.[242] Mining hardware is improving at a fast rate, quickly resulting in older generations of hardware.[243]\n Bitcoin is the least energy-efficient cryptocurrency, using 707.6 kilowatt-hours of electricity per transaction.[244]\n Before June 2021, China was the primary location for Bitcoin mining. However, due to concerns over power usage and other factors, China forced out Bitcoin operations, at least temporarily. As a result, the United States promptly emerged as the top global leader in the industry. An example of a gross amount of electronic waste associated with Bitcoin mining operations in the US is a facility that located in Dalton, Georgia which is consuming nearly the same amount of electricity as the combined power usage of 97,000 households in its vicinity. Another example is that Riot Platforms operates a Bitcoin mining facility in Rockdale, Texas, which consumes approximately as much electricity as the nearby 300,000 households. This makes it the most energy-intensive Bitcoin mining operation in the United States.[245]\n The world's second-largest cryptocurrency, Ethereum, uses 62.56 kilowatt-hours of electricity per transaction.[246] XRP is the world's most energy efficient cryptocurrency, using 0.0079 kilowatt-hours of electricity per transaction.[247]\n Although the biggest PoW blockchains consume energy on the scale of medium-sized countries, the annual power demand from proof-of-stake (PoS) blockchains is on a scale equivalent to a housing estate. The Times identified six \"environmentally friendly\" cryptocurrencies: Chia, IOTA, Cardano, Nano, Solarcoin and Bitgreen.[248] Academics and researchers have used various methods for estimating the energy use and energy efficiency of blockchains. A study of the six largest proof-of-stake networks in May 2021 concluded:\n In terms of annual consumption (kWh\/yr), the figures were: Polkadot (70,237), Tezos (113,249), Avalanche (489,311), Algorand (512,671), Cardano (598,755) and Solana (1,967,930). This equates to Polkadot consuming 7 times the electricity of an average U.S. home, Cardano 57 homes and Solana 200 times as much. The research concluded that PoS networks consumed 0.001% the electricity of the Bitcoin network.[249] University College London researchers reached a similar conclusion.[250]\n Variable renewable energy power stations could invest in Bitcoin mining to reduce curtailment, hedge electricity price risk, stabilize the grid, increase the profitability of renewable energy power stations and therefore accelerate transition to sustainable energy.[251][252][253][254][255]\n There are also purely technical elements to consider. For example, technological advancement in cryptocurrencies such as Bitcoin result in high up-front costs to miners in the form of specialized hardware and software.[256] Cryptocurrency transactions are normally irreversible after a number of blocks confirm the transaction. Additionally, cryptocurrency private keys can be permanently lost from local storage due to malware, data loss or the destruction of the physical media. This precludes the cryptocurrency from being spent, resulting in its effective removal from the markets.[257]\n In September 2015, the establishment of the peer-reviewed academic journal Ledger (ISSN 2379-5980) was announced. It covers studies of cryptocurrencies and related technologies, and is published by the University of Pittsburgh.[258]\n The journal encourages authors to digitally sign a file hash of submitted papers, which will then be timestamped into the Bitcoin blockchain. Authors are also asked to include a personal Bitcoin address in the first page of their papers.[259][260]\n A number of aid agencies have started accepting donations in cryptocurrencies, including UNICEF.[261] Christopher Fabian, principal adviser at UNICEF Innovation, said the children's fund would uphold donor protocols, meaning that people making donations online would have to pass checks before they were allowed to deposit funds.[262][263]\n However, in 2021, there was a backlash against donations in Bitcoin because of the environmental emissions it caused. Some agencies stopped accepting Bitcoin and others turned to \"greener\" cryptocurrencies.[264] The U.S. arm of Greenpeace stopped accepting bitcoin donations after seven years. It said: \"As the amount of energy needed to run Bitcoin became clearer, this policy became no longer tenable.\"[265]\n In 2022, the Ukrainian government raised over US$10,000,000 worth of aid through cryptocurrency following the 2022 Russian invasion of Ukraine.[266]\n Bitcoin has been characterized as a speculative bubble by eight winners of the Nobel Memorial Prize in Economic Sciences: Paul Krugman,[267] Robert J. Shiller,[268] Joseph Stiglitz,[269] Richard Thaler,[270] James Heckman,[271] Thomas Sargent,[271] Angus Deaton,[271] and Oliver Hart;[271] and by central bank officials including Alan Greenspan,[272] Agustín Carstens,[273] Vítor Constâncio,[274] and Nout Wellink.[275]\n Investors Warren Buffett and George Soros have respectively characterized it as a \"mirage\"[276] and a \"bubble\";[277] while business executives Jack Ma and JP Morgan Chase CEO Jamie Dimon have called it a \"bubble\"[278] and a \"fraud\",[279] respectively, although Jamie Dimon later said he regretted dubbing Bitcoin a fraud.[280] BlackRock CEO Laurence D. Fink called Bitcoin an \"index of money laundering\".[281]\n In June 2022, business magnate Bill Gates said that cryptocurrencies are \"100% based on greater fool theory\".[282]\n Legal scholars criticize the lack of regulation, which hinders conflict resolution when crypto assets are at the center of a legal dispute, for example a divorce or an inheritance. In Switzerland, jurists generally deny that cryptocurrencies are objects that fall under property law, as cryptocurrencies do not belong to any class of legally defined objects (Typenzwang, the legal numerus clausus). Therefore, it is debated whether anybody could even be sued for embezzlement of cryptocurrency if he\/she had access to someone's wallet. However, in the law of obligations and contract law, any kind of object would be legally valid, but the object would have to be tied to an identified counterparty. However, as the more popular cryptocurrencies can be freely and quickly exchanged into legal tender, they are financial assets and have to be taxed and accounted for as such.[283][284]\n In 2018, an increase in crypto-related suicides was noticed after the cryptocurrency market crashed in August. The situation was particularly critical in Korea as crypto traders were on \"suicide watch\". A cryptocurrency forum on Reddit even started providing suicide prevention support to affected investors.[285][286] The May 2022 collapse of the Luna currency operated by Terra also led to reports of suicidal investors in crypto-related subreddits.[287]\n"}
{"key":"Cryptocurrency","link":"https:\/\/en.wikipedia.org\/wiki\/Virtual_currency","headline":"Virtual currency - Wikipedia","content":"Virtual currency, or virtual money, is a digital currency that is largely unregulated, issued and usually controlled by its developers, and used and accepted electronically among the members of a specific virtual community.[1] In 2014, the European Banking Authority defined virtual currency as \"a digital representation of value that is neither issued by a central bank or a public authority, nor necessarily attached to a fiat currency but is accepted by natural or legal persons as a means of payment and can be transferred, stored or traded electronically.\"[2] A digital currency issued by a central bank is referred to as a central bank digital currency.\n In 2012, the European Central Bank (ECB) defined virtual currency as \"a type of unregulated, digital money, which is issued and usually controlled by its developers, and used and accepted among the members of a specific virtual community\".[1]: 13 \n In 2013, the Financial Crimes Enforcement Network (FinCEN), a bureau of the US Treasury, in contrast to its regulations defining currency as \"the coin and paper money of the United States or of any other country that [i] is designated as legal tender and that [ii] circulates and [iii] is customarily used and accepted as a medium of exchange in the country of issuance\", also called \"real currency\" by FinCEN, defined virtual currency as \"a medium of exchange that operates like a currency in some environments, but does not have all the attributes of real currency\". In particular, virtual currency does not have legal tender status in any jurisdiction.[3]\n In 2014, the European Banking Authority defined virtual currency as \"a digital representation of value that is neither issued by a central bank or a public authority, nor necessarily attached to a fiat currency, but is accepted by natural or legal persons as a means of payment and can be transferred, stored or traded electronically\".[2]\n In 2018, Directive (EU) 2018\/843 of the European Parliament and of the Council entered into force. The Directive defines the term \"virtual currencies\" to mean \"a digital representation of value that is not issued or guaranteed by a central bank or a public authority, is not necessarily attached to a legally established currency and does not possess a legal status of currency or money, but is accepted by natural or legal persons as a means of exchange and which can be transferred, stored and traded electronically\".[4]\n In a 2013 congressional hearing on virtual currencies, Ben Bernanke said they \"have been viewed as a form of 'electronic money' or area of payment system technology that has been evolving over the past 20 years\", referencing a 1995 congressional hearing on the Future of Money before the Committee on Banking and Financial Services.[5]\nThe Internet currency Flooz was created in 1999.[6] The term \"virtual currency\" appears to have been coined around 2009, paralleling the development of digital currencies and social gaming.[7]\n Although the correct classification is \"digital currency\", the US government prefers and has uniformly adopted the term \"virtual currency\". The FinCEN was first, followed by the FBI in 2012,[8] the General Accounting Office in 2013,[9] as well as the government agencies testifying at the November 2013 US Senate hearing on bitcoin, including the Department of Homeland Security, the US Securities and Exchange Commission, the Office of the Attorney General.[10]\n Attributes of a real currency, as defined in 2011 in the Code of Federal Regulations, such as real paper money and real coins are simply that they act as legal tender and circulate \"customarily\".[11]\n The IRS decided in March 2014, to treat bitcoin and other virtual currencies as property for tax purposes, not as currency.[12][13] Some have suggested that this makes bitcoins not fungible—that is one bitcoin is not identical to another bitcoin, unlike one gallon of crude oil being identical to another gallon of crude oil—making bitcoin unworkable as a currency.[14] Others have stated that a measure like accounting on average cost basis would restore fungibility to the currency.[14]\n Virtual currencies have been called \"closed\" or \"fictional currency\" when they have no official connection to the real economy, for example, currencies in massively multiplayer online role-playing games such as World of Warcraft. While there may be a grey market for exchanging such currencies or other virtual assets for real-world assets, this is usually forbidden by the games' terms of service.\n This type of currency, units of which may also be circulated as (printed) coupons, stamps or reward points, has been known for a long time in the form of customer incentive programs or loyalty programs. A coupon loses its face value when redeemed for an eligible asset or service (hence: flow in one direction), may be valid for only a limited time and subject to other restrictions set by the issuer. The business issuing the coupon functions as a central authority.[15][dubious  – discuss] Coupons remained unchanged for 100 years until new technology enabling credit cards became more common in the 1980s, and credit card rewards were invented. The latest incarnation drives the increase of internet commerce, online services, development of online communities and games. Here virtual or game currency can be bought, but not exchanged back into real money. The virtual currency is akin to a coupon. Examples are frequent flyer programs by various airlines, Microsoft Points, Nintendo Points, Facebook Credits and Amazon Coin.\n A virtual currency that can be bought with and sold back is called a convertible currency. A virtual currency can be decentralized, for example bitcoin, a cryptocurrency. Transacting or even holding convertible virtual currency may be illegal in particular jurisdictions and to particular national citizens at particular times and the transactor\/recipient\/facilitator liable for prosecution by the State.[16]\n FinCEN defined centralized virtual currencies in 2013 as virtual currencies that have a \"centralized repository\", similar to a central bank, and a \"central administrator\".\n A decentralized currency was defined by the US Department of Treasury as a \"currency (1) that has no central repository and no single administrator, and (2) that persons may obtain by their own computing or manufacturing effort\".[3] Rather than relying on confidence in a central authority, it depends instead on a distributed system of trust.[17]\n Digital currency is a particular form of currency which is electronically transferred and stored, i.e., distinct from physical currency, such as coins or banknotes. According to the European Central Bank, virtual currencies are \"generally digital\", although their enduring precursor, the coupon, for example, is physical.[1]\n A cryptocurrency is a digital currency using cryptography to secure transactions and to control the creation of new currency units.[18] Since not all virtual currencies use cryptography, not all virtual currencies are cryptocurrencies.\n Cryptocurrencies are not always legal tender, but some countries have moved to regulate cryptocurrency-related services as they would financial institutions. \nEcuador is the first country attempting a government run a cryptography-free digital currency; during the introductory phase from Christmas Eve 2014 until mid February 2015 people can open accounts and change passwords. At the end of February 2015 transactions of electronic money will be possible.[19][20]\n Estonia has been exploring various possibilities for blockchain technology, such as Estcoin and the use of crypto tokens within its e-residency program, which gives both Estonians and foreigners a digital form of identification.[21]\n Ripple, Stellar[22]\n Virtual currencies pose challenges for central banks, financial regulators, departments or ministries of finance, as well as fiscal authorities and statistical authorities.\nGareth Murphy, Central Bank of Ireland, described the regulatory challenges posed by virtual currencies as relating to:[citation needed]\n The US Commodity Futures Trading Commission (CFTC) has determined virtual currencies are properly defined as commodities in 2015.[23] The CFTC warned investors against pump and dump schemes that use virtual currencies.[24]\n The US Internal Revenue Service (IRS) ruling Notice 2014-21[25] defines any virtual currency, cryptocurrency and digital currency as property; gains and losses are taxable within standard property policies.\n On 20 March 2013, the Financial Crimes Enforcement Network issued a guidance to clarify how the US Bank Secrecy Act applied to persons creating, exchanging and transmitting virtual currencies.[3]\n In May 2014 the US Securities and Exchange Commission (SEC) \"warned about the hazards of bitcoin and other virtual currencies\".[citation needed]\n In July 2014, the New York State Department of Financial Services proposed the most comprehensive regulation of virtual currencies to date commonly referred to as a BitLicense. Unlike the US federal regulators it has gathered input from bitcoin supporters and the financial industry through public hearings and a comment period until October 21, 2014, to customize the rules. The proposal, per NY DFS press release \"sought to strike an appropriate balance that helps protect consumers and root out illegal activity\".[26] It has been criticized by smaller companies to favor established institutions, and Chinese bitcoin exchanges have complained that the rules are \"overly broad in its application outside the United States\".[27]\n In February 2015 the ECB concluded \"Virtual currency schemes, such as Bitcoin, are not full forms of money as usually defined in economic literature, nor are virtual currencies money or currency from a legal perspective. Nevertheless, Virtual currency may substitute [for] banknotes and coins, scriptural money and e-money in certain payment situations\".[28] In a May 2019 report ECB expressed concerns that \"crypto assets provide opportunity for anonymous participation in illegal activities of all sorts\".[29]\n In the European Union, a legal definition of cryptocurrency was introduced to be broadly be regarded as \"a digital representation of value that can be digitally transferred, stored or traded and is accepted… as a medium of exchange\" in the 5th Anti Money Laundering Directive. This also means that within the European Union cryptocurrencies and cryptocurrency exchanges are considered \"obliged entities\" subject to the European Union's Anti-Money Laundering Directives, and face the same CFT\/AML regulations. As of July 20, 2021, the European Commission has proposed replacing the previous Directive 2015\/849\/EU with provisions from the 6th Anti-Money Laundering Directive.[30]\n Virtual currencies are defined as \"a digital representation of value that is not issued or guaranteed by a central bank or a public authority, is not necessarily attached to a legally established currency and does not possess a legal status of currency or money, but is accepted by natural or legal persons as a means of exchange and which can be transferred, stored and traded electronically\".\n The fact that European Union lawmakers regard Bitcoin as the archetypal example of virtual currencies and that Bitcoin therefore fulfills all elements of the legal definition can serve as an anchor point for interpretation. Basically, the definition consists of six elements:[31][32]\n The authors of the legal definition under EU law primarily had blockchain technology in mind – and Bitcoin as an archetypal form.[33] Against this background, it is remarkable that the definition does not contain any elements tailored to the use of a particular technology. On the contrary, the legal definition is strikingly technology neutral.\n In a CNN interview, the financial crime expert Veit Buetterlin explained that the raise of the cryptocurrency market opened creative channels for terror groups to finance themselves.[34]\n"}
{"key":"Cryptocurrency","link":"https:\/\/en.wikipedia.org\/wiki\/Bitcoin","headline":"Bitcoin - Wikipedia","content":"\n Bitcoin (abbreviation: BTC[a]; sign: ₿) is the first decentralized cryptocurrency. Nodes in the peer-to-peer bitcoin network verify transactions through cryptography and record them in a public distributed ledger, called a blockchain, without central oversight. Consensus between nodes is achieved using a computationally intensive process based on proof of work, called mining, that requires increasing quantities of electricity and guarantees the security of the bitcoin blockchain.[5]\n Based on a free market ideology, bitcoin was invented in 2008 by Satoshi Nakamoto, an unknown person.[6] Use of bitcoin as a currency began in 2009,[7] with the release of its open-source implementation.[8]: ch. 1  In 2021, El Salvador adopted it as legal tender.[4] Bitcoin is currently used more as a store of value and less as a medium of exchange or unit of account. It is mostly seen as an investment and has been described by many scholars as an economic bubble.[9] As bitcoin is pseudonymous, its use by criminals has attracted the attention of regulators, leading to its ban by several countries as of 2021[update].[10]\n Before bitcoin, several digital cash technologies were released, starting with David Chaum's ecash in the 1980s.[11] The idea that solutions to computational puzzles could have some value was first proposed by cryptographers Cynthia Dwork and Moni Naor in 1992.[11] The concept was independently rediscovered by Adam Back who developed Hashcash, a proof-of-work scheme for spam control in 1997.[11] The first proposals for distributed digital scarcity-based cryptocurrencies came from cypherpunks Wei Dai (b-money) and Nick Szabo (bit gold) in 1998.[12] In 2004, Hal Finney developed the first currency based on reusable proof of work.[13] These various attempts were not successful:[11] Chaum's concept required centralized control and no banks wanted to sign on, Hashcash had no protection against double-spending, while b-money and bit gold were not resistant to Sybil attacks.[11]\n The domain name bitcoin.org was registered on 18 August 2008.[14] On 31 October 2008, a link to a white paper authored by Satoshi Nakamoto titled Bitcoin: A Peer-to-Peer Electronic Cash System was posted to a cryptography mailing list.[15] Nakamoto implemented the bitcoin software as open-source code and released it in January 2009.[7] Nakamoto's identity remains unknown.[6] All individual components of bitcoin originated in earlier academic literature.[11] Nakamoto's innovation was their complex interplay resulting in the first decentralized, Sybil resistant, Byzantine fault tolerant digital cash system, that would eventually be referred to as the first blockchain.[11][16] Nakamoto's paper was not peer reviewed and was initially ignored by academics, who argued that it could not work, based on theoretical models, even though it was working in practice.[11]\n On 3 January 2009, the bitcoin network was created when Nakamoto mined the starting block of the chain, known as the genesis block.[17] Embedded in this block was the text \"The Times 03\/Jan\/2009 Chancellor on brink of second bailout for banks\", which is the date and headline of an issue of The Times newspaper.[7] Nine days later, Hal Finney received the first bitcoin transaction: ten bitcoins from Nakamoto.[18] Wei Dai and Nick Szabo were also early supporters.[17] In 2010, the first known commercial transaction using bitcoin occurred when programmer Laszlo Hanyecz bought two Papa John's pizzas for ₿10,000.[19]\n Blockchain analysts estimate that Nakamoto had mined about one million bitcoins[20] before disappearing in 2010 when he handed the network alert key and control of the code repository over to Gavin Andresen. Andresen later became lead developer at the Bitcoin Foundation,[21][22] an organization founded in September 2012 to promote bitcoin.[23]\n After early \"proof-of-concept\" transactions, the first major users of bitcoin were black markets, such as the dark web Silk Road. During its 30 months of existence, beginning in February 2011, Silk Road exclusively accepted bitcoins as payment, transacting ₿9.9 million, worth about $214 million.[24]: 222 \n In March 2013, the US Financial Crimes Enforcement Network (FinCEN) established regulatory guidelines for \"decentralized virtual currencies\" such as bitcoin, classifying American bitcoin miners who sell their generated bitcoins as money services businesses, subject to registration and other legal obligations.[25] In May 2013, US authorities seized the unregistered exchange Mt. Gox.[26] In June 2013, the US Drug Enforcement Administration seized ₿11.02 from a man attempting to use them to buy illegal substances. This marked the first time a government agency had seized bitcoins.[27] The FBI seized about ₿30,000 in October 2013 from Silk Road, following the arrest of its founder Ross Ulbricht.[28]\n In December 2013, the People's Bank of China prohibited Chinese financial institutions from using bitcoin.[29] After the announcement, the value of bitcoin dropped,[30] and Baidu no longer accepted bitcoins for certain services.[31] Buying real-world goods with any virtual currency had been illegal in China since at least 2009.[32]\n Research produced by the University of Cambridge estimated that in 2017, there were 2.9 to 5.8 million unique users using a cryptocurrency wallet, most of them using bitcoin.[33] In August 2017, the SegWit software upgrade was activated. Segwit was intended to support the Lightning Network as well as improve scalability.[34] SegWit opponents, who supported larger blocks as a scalability solution, forked to create Bitcoin Cash, one of many forks of bitcoin.[35]\n In December 2017, the first futures on bitcoin was introduced by the Chicago Mercantile Exchange (CME).[36]\n In February 2018, the price crashed after China imposed a complete ban on Bitcoin trading.[37] The percentage of bitcoin trading in the Chinese renminbi fell from over 90% in September 2017 to less than 1% in June 2018.[38] During the same year, Bitcoin prices were negatively affected by several hacks or thefts from cryptocurrency exchanges.[39]\n In 2020, some major companies and institutions started to acquire bitcoin: MicroStrategy invested $250 million in bitcoin as a treasury reserve asset,[40] Square, Inc., $50 million,[41] and MassMutual, $100 million.[42] In November 2020, PayPal added support for bitcoin in the US.[43]\n In February 2021, Bitcoin's market capitalization reached $1 trillion for the first time.[44] In November 2021, the Taproot soft-fork upgrade was activated, adding support for Schnorr signatures, improved functionality of smart contracts and Lightning Network.[45] Before, Bitcoin only used a custom elliptic curve with the ECDSA algorithm to produce signatures.[46]: 101  In September 2021, Bitcoin became legal tender in El Salvador, alongside the US dollar.[4] In October 2021, the first bitcoin futures Exchange-traded fund (ETF), called BITO, from ProShares was approved by the SEC and listed on the CME.[47]\n In May and June 2022, the bitcoin price fell following the collapses of TerraUSD, a stablecoin,[48] and the Celsius Network, a cryptocurrency loan company.[49][50]\n In 2023, ordinals—non-fungible tokens (NFTs)—on Bitcoin, went live.[51] In January 2024, the first 11 US spot bitcoin ETFs began trading, offering direct exposure to bitcoin for the first time on American stock exchanges.[52][53]\n The unit of account of the bitcoin system is the bitcoin. It is most commonly represented with the currency code BTC[a] as well as the symbol ₿.[1] XBT, a code that conforms to ISO 4217 though not officially part of it, is used by Bloomberg L.P.[54]\n No uniform capitalization convention exists; some sources use Bitcoin, capitalized, to refer to the technology and network, and bitcoin, lowercase, for the unit of account.[55] The Oxford English Dictionary advocates the use of lowercase bitcoin in all cases.[56]\n One bitcoin is divisible to eight decimal places.[8]: ch. 5  Units for smaller amounts of bitcoin are the millibitcoin (mBTC), equal to 1⁄1000 bitcoin, and the satoshi[b] (sat), representing 1⁄100000000 (one hundred millionth) bitcoin, the smallest amount possible.[2] 100,000 satoshis are one mBTC.[57]\n As a decentralized system, bitcoin operates without a central authority or single administrator,[58] so that anyone can create a new bitcoin address and transact without needing any approval.[8]: ch. 1  This is accomplished through a specialized distributed ledger called a blockchain that records bitcoin transactions.[59]\n The blockchain is implemented as an ordered list of blocks. Each block contains a SHA-256 hash of the previous block,[59] \"chaining\" them in chronological order.[8]: ch. 7 [59] The blockchain is maintained by a peer-to-peer network.[24]: 215–219  Individual blocks, public addresses, and transactions within blocks are public information, and can be examined using a blockchain explorer.[60]\n Nodes validate and broadcast transactions, each maintaining a copy of the blockchain for ownership verification.[61] A new block is created every 10 minutes on average, updating the blockchain across all nodes without central oversight. This process tracks bitcoin spending, ensuring each bitcoin is spent only once. Unlike a traditional ledger that tracks physical currency, bitcoins exist digitally as unspent outputs of transactions.[8]: ch. 5 \n In the blockchain, bitcoins are linked to specific addresses that are hashes of a public key. Creating an address involves generating a random private key and then computing the corresponding address. This process is almost instant, but the reverse (finding the private key for a given address) is nearly impossible.[8]: ch. 4  Publishing a bitcoin address does not risk its private key, and it is extremely unlikely to accidentally generate a used key with funds. To use bitcoins, owners need their private key to digitally sign transactions, which are verified by the network using the public key, keeping the private key secret.[8]: ch. 5 \n Bitcoin transactions use a Forth-like scripting language,[8]: ch. 5  involving one or more inputs and outputs. When sending bitcoins, a user specifies the recipients' addresses and the amount for each output. This allows sending bitcoins to several recipients in a single transaction. To prevent double-spending, each input must refer to a previous unspent output in the blockchain.[62] Using multiple inputs is similar to using multiple coins in a cash transaction. As in a cash transaction, the sum of inputs can exceed the intended sum of payments. In such a case, an additional output can return the change back to the payer.[62] Unallocated input satoshis in the transaction become the transaction fee.[62]\n Losing a private key means losing access to the bitcoins, with no other proof of ownership accepted by the protocol.[24] For instance, in 2013, a user lost ₿7,500, valued at US$7.5 million, by accidentally discarding a hard drive with the private key.[63] It is estimated that around 20% of all bitcoins are lost.[64] The private key must also be kept secret as its exposure, such as through a data breach, can lead to theft of the associated bitcoins.[8]: ch. 10 [65] As of December 2017[update], approximately ₿980,000 had been stolen from cryptocurrency exchanges.[66]\n The mining process in Bitcoin involves maintaining the blockchain through computer processing power. Miners group and broadcast new transactions into blocks, which are then verified by the network.[59] Each block must contain a proof of work (PoW) to be accepted,[59] involving finding a nonce number that, combined with the block content, produces a hash numerically smaller than the network's difficulty target.[8]: ch. 8  This PoW is simple to verify but hard to generate, requiring many attempts.[8]: ch. 8  PoW forms the basis of Bitcoin's consensus mechanism.[67]\n The difficulty of generating a block is deterministically adjusted based on the mining power on the network by changing the difficulty target, which is recalibrated every 2,016 blocks (approximately two weeks) to maintain an average time of ten minutes between new blocks. The process requires significant computational power and specialized hardware.[8]: ch. 8 [68]\n Miners who successfully find a new block can collect transaction fees from the included transactions and a set reward in bitcoins.[69] To claim this reward, a special transaction called a coinbase is included in the block, with the miner as the payee. All bitcoins in existence have been created through this type of transaction.[8]: ch. 8  This reward is halved every 210,000 blocks until ₿21 million,[c] with new bitcoin issuance slated to end around 2140. Afterward, miners will only earn from transaction fees. These fees are determined by the transaction's size and the amount of data stored, measured in satoshis per byte.[70][62][8]: ch. 8 \n The proof of work system and the chaining of blocks make blockchain modifications very difficult, as altering one block requires changing all subsequent blocks. As more blocks are added, modifying older blocks becomes increasingly challenging.[71][59] In case of disagreement, nodes trust the longest chain, which required the greatest amount of effort to produce.[67] To tamper or censor the ledger, one needs to control the majority of the global hashrate.[67] The high cost required to reach this level of computational power guarantees the security of the bitcoin blockchain.[67]\n Bitcoin mining's environmental impact is controversial and has attracted the attention of regulators, leading to restrictions or incentives in various jurisdictions.[72] As of 2022[update], a non-peer-reviewed study by the Cambridge Centre for Alternative Finance (CCAF) estimated that bitcoin mining represented 0.4% of global electricity consumption.[73] Another 2022 non-peer-reviewed commentary published in Joule estimated that bitcoin mining was responsible for 0.2% of world greenhouse gas emissions.[74] About half of the electricity used is generated through fossil fuels.[75] Moreover, mining hardware's short lifespan results in electronic waste.[76] The amount of electrical energy consumed, and the e-waste generated, is comparable to that of Greece and the Netherlands, respectively.[76][74]\n Bitcoin is pseudonymous, with funds linked to addresses, not real-world identities. While the owners of these addresses are not directly identified, all transactions are public on the blockchain. Patterns of use, like spending coins from multiple inputs, can hint at a common owner. Public data can sometimes be matched with known address owners.[77] Bitcoin exchanges might also need to collect personal data as per legal requirements.[78] For enhanced privacy, users can generate a new address for each transaction.[79]\n In the Bitcoin network, each bitcoin is treated equally, ensuring basic fungibility. However, users and applications can choose to differentiate between bitcoins. While wallets and software treat all bitcoins the same, each bitcoin's transaction history is recorded on the blockchain. This public record allows for chain analysis, where users can identify and potentially reject bitcoins from controversial sources.[80] For example, in 2012, Mt. Gox froze accounts containing bitcoins identified as stolen.[81]\n Bitcoin wallets were the first cryptocurrency wallets, enabling users to store the information necessary to transact bitcoins.[82][8]: ch. 1, glossary  The first wallet program, simply named Bitcoin, and sometimes referred to as the Satoshi client, was released in 2009 by Nakamoto as open-source software.[7] Bitcoin Core is among the best known clients. Forks of Bitcoin Core exist such as Bitcoin Unlimited.[83] Wallets can be full clients, with a full copy of the blockchain to check the validity of mined blocks,[8]: ch. 1  or lightweight clients, just to send and receive transactions without a local copy of the entire blockchain.[84] Third-party internet services called online wallets store users' credentials on their servers, making them susceptible of hacks.[85] \"Cold storage\" protects bitcoins from such hacks by keeping private keys offline, either through specialized hardware wallets or paper printouts.[86][8]: ch. 4 \n Nakamoto limited the block size to one megabyte.[87] The limited block size and frequency can lead to delayed processing of transactions, increased fees and a Bitcoin scalability problem.[88] The Lightning Network, second-layer routing network, is a potential scaling solution.[8]: ch. 8 \n Research shows a trend towards centralization in bitcoin as miners join pools for stable income.[24]: 215, 219–222 [89]: 3  If a single miner or pool controls more than 50% of the hashing power, it would allow them to censor transactions and double-spend coins.[58] In 2014, mining pool Ghash.io reached 51% mining power, causing safety concerns, but later voluntarily capped its power at 39.99% for the benefit of the whole network.[90] A few entities also dominate other parts of the ecosystem such as the client software, online wallets, and simplified payment verification (SPV) clients.[58]\n According to the European Central Bank, the decentralization of money offered by bitcoin has its theoretical roots in the Austrian school of economics, especially with Friedrich von Hayek's book The Denationalization of Money, in which he advocates a complete free market in the production, distribution and management of money to end the monopoly of central banks.[91]: 22  Sociologist Nigel Dodd, citing the crypto-anarchist Declaration of Bitcoin's Independence, argues that the essence of the bitcoin ideology is to remove money from social, as well as governmental, control.[92] The Economist describes bitcoin as \"a techno-anarchist project to create an online version of cash, a way for people to transact without the possibility of interference from malicious governments or banks\".[93] These philosophical ideas initially attracted libertarians and anarchists.[94] Economist Paul Krugman argues that cryptocurrencies like bitcoin are only used by bank skeptics and criminals.[95]\n \nMoney serves three purposes: a store of value, a medium of exchange, and a unit of account.[96] According to The Economist in 2014, bitcoin functions best as a medium of exchange.[96] In 2015, The Economist noted that bitcoins had three qualities useful in a currency: they are \"hard to earn, limited in supply and easy to verify\".[97] However, a 2018 assessment by The Economist stated that cryptocurrencies met none of these three criteria.[93] Per some researchers, as of 2015[update], bitcoin functions more as a payment system than as a currency.[24] In 2014, economist Robert J. Shiller wrote that bitcoin has potential as a unit of account for measuring the relative value of goods, as with Chile's Unidad de Fomento, but that \"Bitcoin in its present form ... doesn't really solve any sensible economic problem\".[98] François R. Velde, Senior Economist at the Chicago Fed, described bitcoin as \"an elegant solution to the problem of creating a digital currency\".[99] David Andolfatto, Vice President at the Federal Reserve Bank of St. Louis, stated that bitcoin is a threat to the establishment, which he argues is a good thing for the Federal Reserve System and other central banks, because it prompts these institutions to operate sound policies.[100]\n The legal status of bitcoin varies substantially from one jurisdiction to another. Because of its decentralized nature and its global presence, regulating bitcoin is difficult. However, the use of bitcoin can be criminalized, and shutting down exchanges and the peer-to-peer economy in a given country would constitute a de facto ban.[101] The use of bitcoin by criminals has attracted the attention of financial regulators, legislative bodies, and law enforcement.[102] Nobel-prize winning economist Joseph Stiglitz says that bitcoin's anonymity encourages money laundering and other crimes.[103] This is the main justification behind bitcoin bans.[10] As of November 2021[update], nine countries applied an absolute ban (Algeria, Bangladesh, China, Egypt, Iraq, Morocco, Nepal, Qatar, and Tunisia) while another 42 countries had an implicit ban.[104][needs update] Bitcoin is only legal tender in El Salvador.[4]\n As of 2018[update], Bitcoin is rarely used in transactions with merchants,[105] but it is popular to purchase illegal goods online.[106][107] Prices are not usually quoted in bitcoin and trades involve conversions into fiat currencies.[24] Commonly cited reasons for not using Bitcoin include high costs, the inability to process chargebacks, high price volatility, long transaction times, transaction fees (especially for small purchases).[105][108] Bloomberg reported that bitcoin was being used for large-item purchases on the site Overstock.com and for cross-border payments to freelancers.[109] As of 2015[update], there was little sign of bitcoin use in international remittances despite high fees charged by banks and Western Union who compete in this market.[24][110]\n In September 2021, the Bitcoin Law made bitcoin legal tender in El Salvador, alongside the US dollar.[4] The adoption has been criticized both internationally and within El Salvador.[4][111] In particular, in 2022, the International Monetary Fund (IMF) urged El Salvador to reverse its decision.[112] As of 2022[update], the use of Bitcoin in El Salvador remains low: 80% of businesses refused to accept it despite being legally required to.[113] In April 2022, the Central African Republic (CAR) adopted Bitcoin as legal tender alongside the CFA franc,[114] but repealed the reform one year later.[115]\n Bitcoin is also used by some governments. For instance, the Iranian government initially opposed cryptocurrencies, but later saw them as an opportunity to circumvent sanctions.[116] Since 2020, Iran has required local bitcoin miners to sell bitcoin to the Central Bank of Iran, allowing the central bank to use it for imports.[117] Some constituent states also accept tax payments in bitcoin, including Colorado (US)[118] and Zug (Switzerland).[119] As of 2023, the US government owned more than $5 billion worth of seized bitcoin.[120][121]\n As of 2018[update], the overwhelming majority of bitcoin transactions took place on cryptocurrency exchanges.[105] Since 2014, regulated bitcoin funds also allow exposure to the asset or to futures as an investment.[122][123] Individuals and companies such as the Winklevoss twins[124] and Elon Musk's companies SpaceX and Tesla have massively invested in Bitcoin.[125][126] Bitcoin wealth is highly concentrated, with 0.01% holding 27% of in-circulation currency, as of 2021.[127] As of September 2023[update], El Salvador had $76.5 million worth of bitcoin in its international reserves.[128]\n In 2018, research published in the Journal of Monetary Economics concluded that price manipulation occurred during the Mt. Gox bitcoin theft and that the market remained vulnerable to manipulation.[129] Research published in The Journal of Finance also suggested that trading associated with increases in the amount of the Tether cryptocurrency and associated trading at the Bitfinex exchange accounted for about half of the price increase in bitcoin in late 2017.[130][131]\n Bitcoin, along with other cryptocurrencies, has been described as an economic bubble by several economists, including Nobel Prize in Economics laureates, such as Joseph Stiglitz,[132] James Heckman,[9] and Paul Krugman.[95] Another recipient of the prize, Robert Shiller, argues that bitcoin is rather a fad that may become an asset class. He describes its price growth as an \"epidemic\", driven by contagious narratives.[133]\n According to research published in the International Review of Financial Analysis in 2018, Bitcoin as an asset is highly volatile and does not behave like any other conventional asset.[134] According to one 2022 analysis published in The Journal of Alternative Investments, bitcoin was less volatile than oil, silver, US Treasuries, and 190 stocks in the S&P 500 during and after the 2020 stock market crash.[135] The term \"hodl\" was created in December 2013 for holding Bitcoin rather than selling it during periods of volatility.[136][137]\n Economists, investors, and the central bank of Estonia have described bitcoin as a potential Ponzi scheme.[138][139][140] Legal scholar Eric Posner disagrees, however, as \"a real Ponzi scheme takes fraud; bitcoin, by contrast, seems more like a collective delusion.\"[141] A 2014 World Bank report also concluded that bitcoin was not a deliberate Ponzi scheme.[142]\n"}
{"key":"Cryptocurrency","link":"https:\/\/en.wikipedia.org\/wiki\/Decentralized","headline":"Decentralization - Wikipedia","content":"\n Decentralization or decentralisation is the process by which the activities of an organization, particularly those regarding planning and decision-making, are distributed or delegated away from a central, authoritative location or group and given to smaller factions within it.[1]\n Concepts of decentralization have been applied to group dynamics and management science in private businesses and organizations, political science, law and public administration, technology, economics and money.\n The word \"centralisation\" came into use in France in 1794 as the post-Revolution French Directory leadership created a new government structure. The word \"décentralisation\" came into usage in the 1820s.[2] \"Centralization\" entered written English in the first third of the 1800s;[3] mentions of decentralization also first appear during those years. In the mid-1800s Tocqueville would write that the French Revolution began with \"a push towards decentralization ... [but became,] in the end, an extension of centralization.\"[4] In 1863, retired French bureaucrat Maurice Block wrote an article called \"Decentralization\" for a French journal that reviewed the dynamics of government and bureaucratic centralization and recent French efforts at decentralization of government functions.[5]\n Ideas of liberty and decentralization were carried to their logical conclusions during the 19th and 20th centuries by anti-state political activists calling themselves \"anarchists\", \"libertarians\", and even decentralists. Tocqueville was an advocate, writing: \"Decentralization has, not only an administrative value but also a civic dimension since it increases the opportunities for citizens to take interest in public affairs; it makes them get accustomed to using freedom. And from the accumulation of these local, active, persnickety freedoms, is born the most efficient counterweight against the claims of the central government, even if it were supported by an impersonal, collective will.\"[6] Pierre-Joseph Proudhon (1809–1865), influential anarchist theorist[7][8] wrote: \"All my economic ideas as developed over twenty-five years can be summed up in the words: agricultural-industrial federation. All my political ideas boil down to a similar formula: political federation or decentralization.\"[9]\n In the early 20th century, America's response to the centralization of economic wealth and political power was a decentralist movement. It blamed large-scale industrial production for destroying middle-class shop keepers and small manufacturers and promoted increased property ownership and a return to small scale living. The decentralist movement attracted Southern Agrarians like Robert Penn Warren, as well as journalist Herbert Agar.[10] New Left and libertarian individuals who identified with social, economic, and often political decentralism through the ensuing years included Ralph Borsodi, Wendell Berry, Paul Goodman, Carl Oglesby, Karl Hess, Donald Livingston, Kirkpatrick Sale (author of Human Scale),[11] Murray Bookchin,[12] Dorothy Day,[13] Senator Mark O. Hatfield,[14] Mildred J. Loomis[15] and Bill Kauffman.[16]\n Leopold Kohr, author of the 1957 book The Breakdown of Nations – known for its statement \"Whenever something is wrong, something is too big\" – was a major influence on E. F. Schumacher, author of the 1973 bestseller Small Is Beautiful: A Study of Economics As If People Mattered.[17][18] In the next few years a number of best-selling books promoted decentralization.\n Daniel Bell's The Coming of Post-Industrial Society[4] discussed the need for decentralization and a \"comprehensive overhaul of government structure to find the appropriate size and scope of units\", as well as the need to detach functions from current state boundaries, creating regions based on functions like water, transport, education and economics which might have \"different 'overlays' on the map.\"[19][20] Alvin Toffler published Future Shock (1970) and The Third Wave (1980). Discussing the books in a later interview, Toffler said that industrial-style, centralized, top-down bureaucratic planning would be replaced by a more open, democratic, decentralized style which he called \"anticipatory democracy\".[21] Futurist John Naisbitt's 1982 book \"Megatrends\" was on The New York Times Best Seller list for more than two years and sold 14 million copies.[22] Naisbitt's book outlines 10 \"megatrends\", the fifth of which is from centralization to decentralization.[23] In 1996 David Osborne and Ted Gaebler had a best selling book Reinventing Government proposing decentralist public administration theories which became labeled the \"New Public Management\".[24]\n Stephen Cummings wrote that decentralization became a \"revolutionary megatrend\" in the 1980s.[25] In 1983 Diana Conyers asked if decentralization was the \"latest fashion\" in development administration.[26] Cornell University's project on Restructuring Local Government states that decentralization refers to the \"global trend\" of devolving responsibilities to regional or local governments.[27] Robert J. Bennett's Decentralization, Intergovernmental Relations and Markets: Towards a Post-Welfare Agenda describes how after World War II governments pursued a centralized \"welfarist\" policy of entitlements which now has become a \"post-welfare\" policy of intergovernmental and market-based decentralization.[27]\n In 1983, \"Decentralization\" was identified as one of the \"Ten Key Values\" of the Green Movement in the United States.\n A 1999 United Nations Development Programme report stated:\n \"A large number of developing and transitional countries have embarked on some form of decentralization programmes. This trend is coupled with a growing interest in the role of civil society and the private sector as partners to governments in seeking new ways of service delivery ... Decentralization of governance and the strengthening of local governing capacity is in part also a function of broader societal trends. These include, for example, the growing distrust of government generally, the spectacular demise of some of the most centralized regimes in the world (especially the Soviet Union) and the emerging separatist demands that seem to routinely pop up in one or another part of the world. The movement toward local accountability and greater control over one's destiny is, however, not solely the result of the negative attitude towards central government. Rather, these developments, as we have already noted, are principally being driven by a strong desire for greater participation of citizens and private sector organizations in governance.\"[28] Those studying the goals and processes of implementing decentralization often use a systems theory approach, which according to the United Nations Development Programme report applies to the topic of decentralization \"a whole systems perspective, including levels, spheres, sectors and functions and seeing the community level as the entry point at which holistic definitions of development goals are\nfrom the people themselves and where it is most practical to support them. It involves seeing multi-level frameworks and continuous, synergistic processes of interaction and iteration of cycles as critical for achieving wholeness in a decentralized system and for sustaining its development.\"[29]\n However, it has been seen as part of a systems approach. Norman Johnson of Los Alamos National Laboratory wrote in a 1999 paper: \"A decentralized system is where some decisions by the agents are made without centralized control or processing. An important property of agent systems is the degree of connectivity or connectedness between the agents, a measure global flow of information or influence. If each agent is connected (exchange states or influence) to all other agents, then the system is highly connected.\"[30]\n University of California, Irvine's Institute for Software Research's \"PACE\" project is creating an \"architectural style for trust management in decentralized applications.\" It adopted Rohit Khare's definition of decentralization: \"A decentralized system is one which requires multiple parties to make their own independent decisions\" and applies it to Peer-to-peer software creation, writing:\n In such a decentralized system, there is no single centralized authority that makes decisions on behalf of all the parties. Instead each party, also called a peer, makes local autonomous decisions towards its individual goals which may possibly conflict with those of other peers. Peers directly interact with each other and share information or provide service to other peers. An open decentralized system is one in which the entry of peers is not regulated. Any peer can enter or leave the system at any time ...[31]\n Decentralization in any area is a response to the problems of centralized systems. Decentralization in government, the topic most studied, has been seen as a solution to problems like economic decline, government inability to fund services and their general decline in performance of overloaded services, the demands of minorities for a greater say in local governance, the general weakening legitimacy of the public sector and global and international pressure on countries with inefficient, undemocratic, overly centralized systems.[32] The following four goals or objectives are frequently stated in various analyses of decentralization.\n In decentralization, the principle of subsidiarity is often invoked. It holds that the lowest or least centralized authority that is capable of addressing an issue effectively should do so. According to one definition: \"Decentralization, or decentralizing governance, refers to the restructuring or reorganization of authority so that there is a system of co-responsibility between institutions of governance at the central, regional and local levels according to the principle of subsidiarity, thus increasing the overall quality and effectiveness of the system of governance while increasing the authority and capacities of sub-national levels.\"[33]\n Decentralization is often linked to concepts of participation in decision-making, democracy, equality and liberty from a higher authority.[34][35] Decentralization enhances the democratic voice.[27] Theorists believe that local representative authorities with actual discretionary powers are the basis of decentralization that can lead to local efficiency, equity and development.\"[36] Columbia University's Earth Institute identified one of three major trends relating to decentralization: \"increased involvement of local jurisdictions and civil society in the management of their affairs, with new forms of participation, consultation, and partnerships.\"[6]\n Decentralization has been described as a \"counterpoint to globalization [which] removes decisions from the local and national stage to the global sphere of multi-national or non-national interests. Decentralization brings decision-making back to the sub-national levels\".  Decentralization strategies must account for the interrelations of global, regional, national, sub-national, and local levels.[37]\n Norman L. Johnson writes that diversity plays an important role in decentralized systems like ecosystems, social groups, large organizations, political systems. \"Diversity is defined to be unique properties of entities, agents, or individuals that are not shared by the larger group, population, structure. Decentralized is defined as a property of a system where the agents have some ability to operate \"locally.\"  Both decentralization and diversity are necessary attributes to achieve the self-organizing properties of interest.\"[30]\n Advocates of political decentralization hold that greater participation by better informed diverse interests in society will lead to more relevant decisions than those made only by authorities on the national level.[38] Decentralization has been described as a response to demands for diversity.[6][39]\n In business, decentralization leads to a management by results philosophy which focuses on definite objectives to be achieved by unit results.[40] Decentralization of government programs is said to increase efficiency – and effectiveness – due to reduction of congestion in communications, quicker reaction to unanticipated problems, improved ability to deliver services, improved information about local conditions, and more support from beneficiaries of programs.[41]\n Firms may prefer decentralization because it ensures efficiency by making sure that managers closest to the local information make decisions and in a more timely fashion; that their taking responsibility frees upper management for long term strategics rather than day-to-day decision-making; that managers have hands on training to prepare them to move up the management hierarchy; that managers are motivated by having the freedom to exercise their own initiative and creativity; that managers and divisions are encouraged to prove that they are profitable, instead of allowing their failures to be masked by the overall profitability of the company.[42]\n The same principles can be applied to the government. Decentralization promises to enhance efficiency through both inter-governmental competitions with market features and fiscal discipline which assigns tax and expenditure authority to the lowest level of government possible.  It works best where members of the subnational government have strong traditions of democracy, accountability, and professionalism.[27]\n Economic and\/or political decentralization can help prevent or reduce conflict because they reduce actual or perceived inequities between various regions or between a region and the central government.[43] Dawn Brancati finds that political decentralization reduces intrastate conflict unless politicians create political parties that mobilize minority and even extremist groups to demand more resources and power within national governments. However, the likelihood this will be done depends on factors like how democratic transitions happen and features like a regional party's proportion of legislative seats, a country's number of regional legislatures, elector procedures, and the order in which national and regional elections occur. Brancati holds that decentralization can promote peace if it encourages statewide parties to incorporate regional demands and limit the power of regional parties.[44]\n The processes by which entities move from a more to a less centralized state vary. They can be initiated from the centers of authority (\"top-down\") or from individuals, localities or regions (\"bottom-up\"),[45] or from a \"mutually desired\" combination of authorities and localities working together.[46] Bottom-up decentralization usually stresses political values like local responsiveness and increased participation and tends to increase political stability. Top-down decentralization may be motivated by the desire to \"shift deficits downwards\" and find more resources to pay for services or pay off government debt.[45]  Some hold that decentralization should not be imposed, but done in a respectful manner.[47]\n Gauging the appropriate size or scale of decentralized units has been studied in relation to the size of sub-units of hospitals[48] and schools,[32] road networks,[49] administrative units in business[50] and public administration, and especially town and city governmental areas and decision-making bodies.[51][52]\n In creating planned communities (\"new towns\"), it is important to determine the appropriate population and geographical size. While in earlier years small towns were considered appropriate, by the 1960s, 60,000 inhabitants was considered the size necessary to support a diversified job market and an adequate shopping center and array of services and entertainment. Appropriate size of governmental units for revenue raising also is a consideration.[53]\n Even in bioregionalism, which seeks to reorder many functions and even the boundaries of governments according to physical and environmental features, including watershed boundaries and soil and terrain characteristics, appropriate size must be considered. The unit may be larger than many decentralist-bioregionalists prefer.[54]\n Decentralization ideally happens as a careful, rational, and orderly process, but it often takes place during times of economic and political crisis, the fall of a regime and the resultant power struggles. Even when it happens slowly, there is a need for experimentation, testing, adjusting, and replicating successful experiments in other contexts. There is no one blueprint for decentralization since it depends on the initial state of a country and the power and views of political interests and whether they support or oppose decentralization.[55]\n Decentralization usually is a conscious process based on explicit policies. However, it may occur as \"silent decentralization\" in the absence of reforms as changes in networks, policy emphasize and resource availability lead inevitably to a more decentralized system.[56]\n Decentralization may be uneven and \"asymmetric\" given any one country's population, political, ethnic and other forms of diversity. In many countries, political, economic and administrative responsibilities may be decentralized to the larger urban areas, while rural areas are administered by the central government. Decentralization of responsibilities to provinces may be limited only to those provinces or states which want or are capable of handling responsibility. Some privatization may be more appropriate to an urban than a rural area; some types of privatization may be more appropriate for some states and provinces but not others.[57]\n The academic literature frequently mentions the following factors as determinants of decentralization:[58]\n Historians have described the history of governments and empires in terms of centralization and decentralization. In his 1910 The History of Nations Henry Cabot Lodge wrote that Persian king Darius I (550–486 BC) was a master of organization and \"for the first time in history centralization becomes a political fact.\" He also noted that this contrasted with the decentralization of Ancient Greece.[59] Since the 1980s a number of scholars have written about cycles of centralization and decentralization. Stephen K. Sanderson wrote that over the last 4000 years chiefdoms and actual states have gone through sequences of centralization and decentralization of economic, political and social power.[60] Yildiz Atasoy writes this process has been going on \"since the Stone Age\" through not just chiefdoms and states, but empires and today's \"hegemonic core states\".[61] Christopher K. Chase-Dunn and Thomas D. Hall review other works that detail these cycles, including works which analyze the concept of core elites which compete with state accumulation of wealth and how their \"intra-ruling-class competition accounts for the rise and fall of states\" and their phases of centralization and decentralization.[62]\n Rising government expenditures, poor economic performance and the rise of free market-influenced ideas have convinced governments to decentralize their operations, to induce competition within their services, to contract out to private firms operating in the market, and to privatize some functions and services entirely.[63]\n Government decentralization has both political and administrative aspects.  Its decentralization may be territorial, moving power from a central city to other localities, and it may be functional, moving decision-making from the top administrator of any branch of government to lower level officials, or divesting of the function entirely through privatization.[64] It has been called the \"new public management\" which has been described as decentralization, management by objectives, contracting out, competition within government and consumer orientation.[65]\n Political decentralization signifies a reduction in the authority of national governments over policy-making. This process is accomplished by the institution of reforms that either delegate a certain degree of meaningful decision-making autonomy to sub-national tiers of government,[66] or grant citizens the right to elect lower-level officials, like local or regional representatives.[67] Depending on the country, this may require constitutional or statutory reforms, the development of new political parties, increased power for legislatures, the creation of local political units, and encouragement of advocacy groups.[38]\n A national government may decide to decentralize its authority and responsibilities for a variety of reasons. Decentralization reforms may occur for administrative reasons, when government officials decide that certain responsibilities and decisions would be handled best at the regional or local level. In democracies, traditionally conservative parties include political decentralization as a directive in their platforms because rightist parties tend to advocate for a decrease in the role of central government. There is also strong evidence to support the idea that government stability increases the probability of political decentralization, since instability brought on by gridlock between opposing parties in legislatures often impedes a government's overall ability to enact sweeping reforms.[66]\n The rise of regional ethnic parties in the national politics of parliamentary democracies is also heavily associated with the implementation of decentralization reforms.[66] Ethnic parties may endeavor to transfer more autonomy to their respective regions, and as a partisan strategy, ruling parties within the central government may cooperate by establishing regional assemblies in order to curb the rise of ethnic parties in national elections.[66] This phenomenon famously occurred in 1999, when the United Kingdom's Labour Party appealed to Scottish constituents by creating a semi-autonomous Scottish Parliament in order to neutralize the threat from the increasingly popular Scottish National Party at the national level.[66]\n In addition to increasing the administrative efficacy of government and endowing citizens with more power, there are many projected advantages to political decentralization. Individuals who take advantage of their right to elect local and regional authorities have been shown to have more positive attitudes toward politics,[68] and increased opportunities for civic decision-making through participatory democracy mechanisms like public consultations and participatory budgeting are believed to help legitimize government institutions in the eyes of marginalized groups.[69] Moreover, political decentralization is perceived as a valid means of protecting marginalized communities at a local level from the detrimental aspects of development and globalization driven by the state, like the degradation of local customs, codes, and beliefs.[70] In his 2013 book, Democracy and Political Ignorance, George Mason University law professor Ilya Somin argued that political decentralization in a federal democracy confronts the widespread issue of political ignorance by allowing citizens to engage in foot voting, or moving to other jurisdictions with more favorable laws.[71] He cites the mass migration of over one million southern-born African Americans to the North or the West to evade discriminatory Jim Crow laws in the late 19th century and early 20th century.[71]\n The European Union follows the principle of subsidiarity, which holds that decision-making should be made by the most local competent authority. The EU should decide only on enumerated issues that a local or member state authority cannot address themselves. Furthermore, enforcement is exclusively the domain of member states. In Finland, the Centre Party explicitly supports decentralization. For example, government departments have been moved from the capital Helsinki to the provinces. The centre supports substantial subsidies that limit potential economic and political centralization to Helsinki.[72]\n Political decentralization does not come without its drawbacks. A study by Fan concludes that there is an increase in corruption and rent-seeking when there are more vertical tiers in the government, as well as when there are higher levels of subnational government employment.[73] Other studies warn of high-level politicians that may intentionally deprive regional and local authorities of power and resources when conflicts arise.[70] In order to combat these negative forces, experts believe that political decentralization should be supplemented with other conflict management mechanisms like power-sharing, particularly in regions with ethnic tensions.[69]\n Four major forms of administrative decentralization have been described.[74][75]\n Fiscal decentralization means decentralizing revenue raising and\/or expenditure of moneys to a lower level of government while maintaining financial responsibility.[74]  While this process usually is called fiscal federalism, it may be relevant to unitary, federal, or confederal governments. Fiscal federalism also concerns the \"vertical imbalances\" where the central government gives too much or too little money to the lower levels.  It actually can be a way of increasing central government control of lower levels of government, if it is not linked to other kinds of responsibilities and authority.[79][80][81]\n Fiscal decentralization can be achieved through user fees, user participation through monetary or labor contributions, expansion of local property or sales taxes, intergovernmental transfers of central government tax monies to local governments through transfer payments or grants, and authorization of municipal borrowing with national government loan guarantees. Transfers of money may be given conditionally with instructions or unconditionally without them.[74][82]\n Market decentralization can be done through privatization of public owned functions and businesses, as described briefly above. But it also is done through deregulation, the abolition of restrictions on businesses competing with government services, for example, postal services, schools, garbage collection. Even as private companies and corporations have worked to have such services contracted out to or privatized by them, others have worked to have these turned over to non-profit organizations or associations.[74]\n From the 1970s to the 1990s, there was deregulation of some industries, like banking, trucking, airlines and telecommunications, which resulted generally in more competition and lower prices.[83] According to the Cato Institute, an American libertarian think-tank, in some cases deregulation in some aspects of an industry were offset by increased regulation in other aspects, the electricity industry being a prime example.[84] For example, in banking, Cato Institute believes some deregulation allowed banks to compete across state lines, increasing consumer choice, while an actual increase in regulators and regulations forced banks to make loans to individuals incapable of repaying them, leading eventually to the financial crisis of 2007–2008.[85][unreliable source?]\n One example of economic decentralization, which is based on a libertarian socialist model, is decentralized economic planning. Decentralized planning is a type of economic system in which decision-making is distributed amongst various economic agents or localized within production agents. An example of this method in practice is in Kerala, India which experimented in 1996 with the People's Plan campaign.[86]\n Emmanuelle Auriol and Michel Benaim write about the \"comparative benefits\" of decentralization versus government regulation in the setting of standards. They find that while there may be a need for public regulation if public safety is at stake, private creation of standards usually is better because \"regulators or 'experts' might misrepresent consumers' tastes and needs.\" As long as companies are averse to incompatible standards, standards will be created that satisfy needs of a modern economy.[87]\n Central governments themselves may own large tracts of land and control the forest, water, mineral, wildlife and other resources they contain. They may manage them through government operations or leasing them to private businesses; or they may neglect them to be exploited by individuals or groups who defy non-enforced laws against exploitation. It also may control most private land through land-use, zoning, environmental and other regulations.[88] Selling off or leasing lands can be profitable for governments willing to relinquish control, but such programs can face public scrutiny because of fear of a loss of heritage or of environmental damage. Devolution of control to regional or local governments has been found to be an effective way of dealing with these concerns.[89][90] Such decentralization has happened in India[91] and other developing nations.[92]\n Libertarian socialism is a political philosophy that promotes a non-hierarchical, non-bureaucratic society without private property in the means of production.  Libertarian socialists believe in converting present-day private productive property into common or public goods.[94] Libertarian socialism is opposed to coercive forms of social organization. It promotes free association in place of government and opposes the various social relations of capitalism, such as wage slavery.[95] The term libertarian socialism is used by some socialists to differentiate their philosophy from state socialism,[96][97] and by some as a synonym for left anarchism.[98][99][100]\n Accordingly, libertarian socialists believe that \"the exercise of power in any institutionalized form – whether economic, political, religious, or sexual – brutalizes both the wielder of power and the one over whom it is exercised\".[101] Libertarian socialists generally place their hopes in decentralized means of direct democracy such as libertarian municipalism, citizens' assemblies, or workers' councils.[102] Libertarian socialists are strongly critical of coercive institutions, which often leads them to reject the legitimacy of the state in favor of anarchism.[103] Adherents propose achieving this through decentralization of political and economic power, usually involving the socialization of most large-scale private property and enterprise (while retaining respect for personal property). Libertarian socialism tends to deny the legitimacy of most forms of economically significant private property, viewing capitalist property relations as forms of domination that are antagonistic to individual freedom.[104][105]\n Political philosophies commonly described as libertarian socialist include most varieties of anarchism (especially anarcho-communism, anarchist collectivism, anarcho-syndicalism,[106] social anarchism and mutualism)[107] as well as autonomism, communalism, participism, libertarian Marxist philosophies such as council communism and Luxemburgism,[108] and some versions of utopian socialism[109] and individualist anarchism.[110][111][112] For Murray Bookchin \"In the modern world, anarchism first appeared as a movement of the peasantry and yeomanry against declining feudal institutions. In Germany its foremost spokesman during the Peasant Wars was Thomas Muenzer; in England, Gerrard Winstanley, a leading participant in the Digger movement. The concepts held by Muenzer and Winstanley were superbly attuned to the needs of their time  –  a historical period when the majority of the population lived in the countryside and when the most militant revolutionary forces came from an agrarian world. It would be painfully academic to argue whether Muenzer and Winstanley could have achieved their ideals. What is of real importance is that they spoke to their time; their anarchist concepts followed naturally from the rural society that furnished the bands of the peasant armies in Germany and the New Model in England.\"[113] The term \"anarchist\" first entered the English language in 1642, during the English Civil War, as a term of abuse, used by Royalists against their Roundhead opponents.[114] By the time of the French Revolution some, such as the Enragés, began to use the term positively,[115] in opposition to Jacobin centralisation of power, seeing \"revolutionary government\" as oxymoronic.[114] By the turn of the 19th century, the English word \"anarchism\" had lost its initial negative connotation.[114]\n For Pierre-Joseph Proudhon, mutualism involved creating \"industrial democracy\", a system where workplaces would be \"handed over to democratically organised workers' associations . . . We want these associations to be models for agriculture, industry and trade, the pioneering core of that vast federation of companies and societies woven into the common cloth of the democratic social Republic.\"[116] He urged \"workers to form themselves into democratic societies, with equal conditions for all members, on pain of a relapse into feudalism.\" This would result in \"Capitalistic and proprietary exploitation, stopped everywhere, the wage system abolished, equal and just exchange guaranteed.\"[117] Workers would no longer sell their labour to a capitalist but rather work for themselves in co-operatives. Anarcho-communism calls for a confederal form in relationships of mutual aid and free association between communes as an alternative to the centralism of the nation-state. Peter Kropotkin thus suggested that \"Representative government has accomplished its historical mission; it has given a mortal blow to court-rule; and by its debates it has awakened public interest in public questions. But to see in it the government of the future socialist society is to commit a gross error. Each economic phase of life implies its own political phase; and it is impossible to touch the very basis of the present economic life-private property – without a corresponding change in the very basis of the political organization. Life already shows in which direction the change will be made. Not in increasing the powers of the State, but in resorting to free organization and free federation in all those branches which are now considered as attributes of the State.\"[118] When the First Spanish Republic was established in 1873 after the abdication of King Amadeo, the first president, Estanislao Figueras, named Francesc Pi i Margall Minister of the Interior. His acquaintance with Proudhon enabled Pi to warm relations between the Republicans and the socialists in Spain. Pi i Margall became the principal translator of Proudhon's works into Spanish[119] and later briefly became president of Spain in 1873 while being the leader of the Democratic Republican Federal Party. According to George Woodcock \"These translations were to have a profound and lasting effect on the development of Spanish anarchism after 1870, but before that time Proudhonian ideas, as interpreted by Pi, already provided much of the inspiration for the federalist movement which sprang up in the early 1860s.\"[120] According to the Encyclopædia Britannica \"During the Spanish revolution of 1873, Pi y Margall attempted to establish a decentralized, cantonalist political system on Proudhonian lines.\"[121]\n To date, the best-known examples of an anarchist communist society (i.e., established around the ideas as they exist today and achieving worldwide attention and knowledge in the historical canon), are the anarchist territories during the Spanish Revolution[122] and the Makhnovshchina during the Russian Revolution. Through the efforts and influence of the Spanish anarchists during the Spanish Revolution within the Spanish Civil War, starting in 1936 anarchist communism existed in most of Aragon, parts of the Levante and Andalusia, as well as in the stronghold of Anarchist Catalonia before being crushed by the combined forces of the regime that won the war, Hitler, Mussolini, Spanish Communist Party repression (backed by the USSR) as well as economic and armaments blockades from the capitalist countries and the Second Spanish Republic itself.[123] During the Russian Revolution, anarchists such as Nestor Makhno worked to create and defend – through the Revolutionary Insurgent Army of Ukraine – anarchist communism in Ukraine from 1919 before being conquered by the Bolsheviks in 1921. Several libertarian socialists, notably Noam Chomsky among others, believe that anarchism shares much in common with certain variants of Marxism (see libertarian Marxism) such as the council communism of Marxist Anton Pannekoek. In Chomsky's Notes on Anarchism,[124] he suggests the possibility \"that some form of council communism is the natural form of revolutionary socialism in an industrial society. It reflects the belief that democracy is severely limited when the industrial system is controlled by any form of autocratic elite, whether of owners, managers, and technocrats, a 'vanguard' party, or a State bureaucracy.\"[124]\n Free market ideas popular in the 19th century such as those of Adam Smith returned to prominence in the 1970s and 1980s. Austrian School economist Friedrich von Hayek argued that free markets themselves are decentralized systems where outcomes are produced without explicit agreement or coordination by individuals who use prices as their guide.[125] Eleanor Doyle writes that \"[e]conomic decision-making in free markets is decentralized across all the individuals dispersed in each market and is synchronized or coordinated by the price system,\" and holds that an individual right to property is part of this decentralized system.[126] Criticizing central government control, Hayek wrote in The Road to Serfdom:\n There would be no difficulty about efficient control or planning were conditions so simple that a single person or board could effectively survey all the relevant facts. It is only as the factors which have to be taken into account become so numerous that it is impossible to gain a synoptic view of them that decentralization becomes imperative.[127] According to Bruce M. Owen, this does not mean that all firms themselves have to be equally decentralized. He writes: \"markets allocate resources through arms-length transactions among decentralized actors. Much of the time, markets work very efficiently, but there is a variety of conditions under which firms do better. Hence, goods and services are produced and sold by firms with various degrees of horizontal and vertical integration.\" Additionally, he writes that the \"economic incentive to expand horizontally or vertically is usually, but not always, compatible with the social interest in maximizing long-run consumer welfare.\"[128]\n It is often claimed that free markets and private property generate centralized monopolies and other ills; free market advocates counter with the argument that government is the source of monopoly.[129] Historian Gabriel Kolko in his book The Triumph of Conservatism argued that in the first decade of the 20th century businesses were highly decentralized and competitive, with new businesses constantly entering existing industries. In his view, there was no trend towards concentration and monopolization. While there were a wave of mergers of companies trying to corner markets, they found there was too much competition to do so. According to Kolko, this was also true in banking and finance, which saw decentralization as leading to instability as state and local banks competed with the big New York City firms. He argues that, as a result, the largest firms turned to the power of the state and worked with leaders like United States Presidents Theodore Roosevelt, William H. Taft and Woodrow Wilson to pass as \"progressive reforms\" centralizing laws like The Federal Reserve Act of 1913 that gave control of the monetary system to the wealthiest bankers; the formation of monopoly \"public utilities\" that made competition with those monopolies illegal; federal inspection of meat packers biased against small companies; extending Interstate Commerce Commission to regulating telephone companies and keeping rates high to benefit AT&T; and using the Sherman Antitrust Act against companies which might combine to threaten larger or monopoly companies.[130][131] D. T. Armentano, writing for the Cato Institute, argues that when government licensing, franchises, and other legal restrictions create monopoly and protect companies from open competition, deregulation is the solution.[132]\n Author and activist Jane Jacobs's influential 1961 book The Death and Life of American Cities criticized large-scale redevelopment projects which were part of government-planned decentralization of population and businesses to suburbs. She believed it destroyed cities' economies and impoverished remaining residents.[133] Her 1980 book The Question of Separatism: Quebec and the Struggle over Sovereignty supported secession of Quebec from Canada.[134] Her 1984 book Cities and the Wealth of Nations proposed a solution to the problems faced by cities whose economies were being ruined by centralized national governments: decentralization through the \"multiplication of sovereignties\", meaning an acceptance of the right of cities to secede from the larger nation states that were greatly limiting their ability to produce wealth.[135][136]\n In managerial economics, the principal-agent problem is a challenge faced by every firm.[137] In response to these incentive and information conflicts, a firm can either centralize their organizational structure by concentrating decision-making to upper management, or decentralize their organizational structure by delegating authority throughout the organization.[138] The delegation of authority comes with a basic trade-off: while it can increase efficiency and information flow, the central authority consequentially suffers a loss of control.[139] However, through creating an environment of trust and allocating authority formally in the firm, coupled with a stronger rule of law in the geographical location of the firm, the negative consequences of the trade-off can be minimized.[140]\n In having a decentralized organizational structure, a firm can remain agile to external shocks and competing trends. Decision-making in a centralized organization can face information flow inefficiencies and barriers to effective communication which decreases the speed and accuracy in which decisions are made. A decentralized firm is said to hold greater flexibility given the efficiency in which it can analyze information and implement relevant outcomes.[141] Additionally, having decision-making power spread across different areas allows for local knowledge to inform decisions, increasing their relevancy and implementational effectiveness.[142] In the process of developing new products or services, the decentralization enable the firm gain advantages of closely meet particular division's needs.[143]\n Decentralization also impacts human resource management. The high level of individual agency that workers experience within a decentralized firm can create job enrichment. Studies have shown this enhances the development of new ideas and innovations given the sense of involvement that comes from responsibility.[144] The impacts of decentralization on innovation are furthered by the ease of information flow that comes from this organizational structure. With increased knowledge sharing, workers are more able to use relevant information to inform decision-making.[145] These benefits are enhanced in firms with skill-intensive environments. Skilled workers are more able to analyze information, they pose less risk of information duplication given increased communication abilities, and the productivity cost of multi-tasking is lower. These outcomes of decentralizion make it a particularly effective organizational structure for entrepreneurial and competitive firm environments, such as start-up companies. The flexibility, efficiency of information flow and higher worker autonomy complement the rapid growth and innovation seen in successful start up companies.[146]\n Technological decentralization can be defined as a shift from concentrated to distributed modes of production and consumption of goods and services.[147] Generally, such shifts are accompanied by transformations in technology and different technologies are applied for either system. Technology includes tools, materials, skills, techniques and processes by which goals are accomplished in the public and private spheres. Concepts of decentralization of technology are used throughout all types of technology, including especially information technology and appropriate technology.\n Technologies often mentioned as best implemented in a decentralized manner, include: water purification, delivery and waste water disposal,[148][149] agricultural technology[150] and energy technology.[151][152] Advancing technology may allow decentralized, privatized and free market solutions for what have been public services, such utilities producing and\/or delivering power, water, mail, telecommunications and services like consumer product safety, money and banking, medical licensing and detection and metering technologies for highways, parking, and auto emissions.[153][clarification needed] However, in terms of technology, a clear distinction between fully centralized or decentralized technical solutions is often not possible and therefore finding an optimal degree of centralization difficult from an infrastructure planning perspective.[154]\n Information technology encompasses computers and computer networks, as well as information distribution technologies such as television and telephones. The whole computer industry of computer hardware, software, electronics, internet, telecommunications equipment, e-commerce and computer services are included.[155]\n Executives and managers face a constant tension between centralizing and decentralizing information technology for their organizations. They must find the right balance of centralizing which lowers costs and allows more control by upper management, and decentralizing which allows sub-units and users more control. This will depend on analysis of the specific situation. Decentralization is particularly applicable to business or management units which have a high level of independence, complicated products and customers, and technology less relevant to other units.[156]\n Information technology applied to government communications with citizens, often called e-Government, is supposed to support decentralization and democratization. Various forms have been instituted in most nations worldwide.[157]\n The internet is an example of an extremely decentralized network, having no owners at all (although some have argued that this is less the case in recent years[158]). \"No one is in charge of internet, and everyone is.\" As long as they follow a certain minimal number of rules, anyone can be a service provider or a user. Voluntary boards establish protocols, but cannot stop anyone from developing new ones.[159] Other examples of open source or decentralized movements are Wikis which allow users to add, modify, or delete content via the internet.[160] Wikipedia has been described as decentralized (although it is a centralized web site, with a single entity operating the servers).[161] Smartphones have been described as being an important part of the decentralizing effects of smaller and cheaper computers worldwide.[162]\n Decentralization continues throughout the industry, for example as the decentralized architecture of wireless routers installed in homes and offices supplement and even replace phone companies' relatively centralized long-range cell towers.[163]\n Inspired by system and cybernetics theorists like Norbert Wiener, Marshall McLuhan and Buckminster Fuller, in the 1960s Stewart Brand started the Whole Earth Catalog and later computer networking efforts to bring Silicon Valley computer technologists and entrepreneurs together with countercultural ideas. This resulted in ideas like personal computing, virtual communities and the vision of an \"electronic frontier\" which would be a more decentralized, egalitarian and free-market libertarian society. Related ideas coming out of Silicon Valley included the free software and creative commons movements which produced visions of a \"networked information economy\".[164]\n Because human interactions in cyberspace transcend physical geography, there is a necessity for new theories in legal and other rule-making systems to deal with decentralized decision-making processes in such systems. For example, what rules should apply to conduct on the global digital network and who should set them? The laws of which nations govern issues of internet transactions (like seller disclosure requirements or definitions of \"fraud\"), copyright and trademark?[165]\n Decentralized computing is the allocation of resources, both hardware and software, to each individual workstation, or office location. In contrast, centralized computing exists when the majority of functions are carried out, or obtained from a remote centralized location. Decentralized computing is a trend in modern-day business environments. This is the opposite of centralized computing, which was prevalent during the early days of computers. \nA decentralized computer system has many benefits over a conventional centralized network.[166] Desktop computers have advanced so rapidly, that their potential performance far exceeds the requirements of most business applications. This results in most desktop computers remaining idle (in relation to their full potential). A decentralized system can use the potential of these systems to maximize efficiency. However, it is debatable whether these networks increase overall effectiveness.\n All computers have to be updated individually with new software, unlike a centralized computer system. Decentralized systems still enable file sharing and all computers can share peripherals such as printers and scanners as well as modems, allowing all the computers in the network to connect to the internet.\n The New Yorker reports that although the Internet was originally decentralized, by 2013 it had become less so: \"a staggering percentage of communications flow through a small set of corporations – and thus, under the profound influence of those companies and other institutions [...] One solution, espoused by some programmers, is to make the Internet more like it used to be – less centralized and more distributed.\"[158]\n Examples of projects that attempt to contribute to the re-decentralization of the Internet include ArkOS, Diaspora, FreedomBox, IndieWeb, Namecoin, SAFE Network, twtxt and ZeroNet as well as advocacy group Redecentralize.org, which provides support for projects that aim to make the Web less centralized.[158]\n In an interview with BBC Radio 5 Live one of the co-founders of Redecentralize.org explained that:\n \"As we've gone on there's been more and more internet traffic focused through particular nodes such as Google or Facebook. [...] Centralised services that hold all the user data and host it themselves have become increasingly popular because that business model has worked. As the Internet has become more mass market, people are not necessarily willing or knowledgable to host it themselves, so where that hosting is outsourced it's become the default, which allows a centralization of power and a centralization of data that I think is worrying.\"[167] In blockchain, decentralization refers to the transfer of control and decision-making from a centralized entity (individual, organization, or group thereof) to a distributed network. Decentralized networks strive to reduce the level of trust that participants must place in one another, and deter their ability to exert authority or control over one another in ways that degrade the functionality of the network.[168] This is seen as having the potential to create new ways of doing business in the finance industry.[169]\n Cryptocurrencies use cryptographic proofs such as proof of work (e.g. Bitcoin) or proof of stake (e.g. Cardano) as a means of establishing decentralized consensus. Academic researchers are examining ways of measuring the level of decentralization in blockchain systems, by taking into account hardware, software, the network, consensus mechanism, economics (tokenomics), application interfaces, how the blockchain is managed over time (governance) and geographical distribution. [170]\n Decentralized protocols, applications, and ledgers (used in Web3[171][172]) could be more difficult for governments to regulate, similar to difficulties regulating BitTorrent (which is not a blockchain technology).[173][174]\n \"Appropriate technology\", originally described as \"intermediate technology\" by economist E. F. Schumacher in  Small Is Beautiful: A Study of Economics As If People Mattered, is generally recognized as encompassing technologies that are small-scale, decentralized, labor-intensive, energy-efficient, environmentally sound, and locally controlled.[175][better source needed]\n Factors hindering decentralization include weak local administrative or technical capacity, which may result in inefficient or ineffective services; inadequate financial resources available to perform new local responsibilities, especially in the start-up phase when they are most needed; or inequitable distribution of resources.[176] Decentralization can make national policy coordination too complex; it may allow local elites to capture functions; local cooperation may be undermined by any distrust between private and public sectors; decentralization may result in higher enforcement costs and conflict for resources if there is no higher level of authority.[177] Additionally, decentralization may not be as efficient for standardized, routine, network-based services, as opposed to those that need more complicated inputs. If there is a loss of economies of scale in procurement of labor or resources, the expense of decentralization can rise, even as central governments lose control over financial resources.[74]\n Other challenges, and even dangers, include the possibility that corrupt local elites can capture regional or local power centers, while constituents lose representation; patronage politics will become rampant and civil servants feel compromised; further necessary decentralization can be stymied; incomplete information and hidden decision-making can occur up and down the hierarchies; centralized power centers can find reasons to frustrate decentralization and bring power back to themselves.[citation needed]\n It has been noted that while decentralization may increase \"productive efficiency\" it may undermine \"allocative efficiency\" by making redistribution of wealth more difficult. Decentralization will cause greater disparities between rich and poor regions, especially during times of crisis when the national government may not be able to help regions needing it.[178]\n"}
{"key":"Biotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/Biotechnology","headline":"Biotechnology - Wikipedia","content":"\n Biotechnology is a multidisciplinary field that involves the integration of natural sciences and engineering sciences in order to achieve the application of organisms, cells, parts thereof and molecular analogues for products and services.[1]\n The term biotechnology was first used by Károly Ereky in 1919[2] to refer to the production of products from raw materials with the aid of living organisms. The core principle of biotechnology involves harnessing biological systems and organisms, such as bacteria, yeast, and plants, to perform specific tasks or produce valuable substances.\n Biotechnology  had a significant impact on many areas of society, from medicine to agriculture to environmental science. One of the key techniques used in biotechnology is genetic engineering, which allows scientists to modify the genetic makeup of organisms to achieve desired outcomes. This can involve inserting genes from one organism into another, and consequently, create new traits or modifying existing ones.[3]\n Other important techniques used in biotechnology include tissue culture, which allows researchers to grow cells and tissues in the lab for research and medical purposes, and fermentation, which is used to produce a wide range of products such as beer, wine, and cheese.\n The applications of biotechnology are diverse and have led to the development of essential products like life-saving drugs, biofuels, genetically modified crops, and innovative materials.[4] It has also been used to address environmental challenges, such as developing biodegradable plastics and using microorganisms to clean up contaminated sites.\n Biotechnology is a rapidly evolving field with significant potential to address pressing global challenges and improve the quality of life for people around the world; however, despite its numerous benefits, it also poses ethical and societal challenges, such as questions around genetic modification and intellectual property rights. As a result, there is ongoing debate and regulation surrounding the use and application of biotechnology in various industries and fields.[5]\n The concept of biotechnology encompasses a wide range of procedures for modifying living organisms for human purposes, going back to domestication of animals, cultivation of the plants, and \"improvements\" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering, as well as cell and tissue culture technologies. The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms, such as pharmaceuticals, crops, and livestock.[6] As per the European Federation of Biotechnology, biotechnology is the integration of natural science and organisms, cells, parts thereof, and molecular analogues for products and services.[7] Biotechnology is based on the basic biological sciences (e.g., molecular biology, biochemistry, cell biology, embryology, genetics, microbiology) and conversely provides methods to support and perform basic research in biology.\n Biotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation, and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured, and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products).[8][9][10] The utilization of biological processes, organisms or systems to produce products that are anticipated to improve human lives is termed biotechnology.[11]\n By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials directly) for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells, and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals.[12] Relatedly, biomedical engineering is an overlapping field that often draws upon and applies biotechnology (by various definitions), especially in certain sub-fields of biomedical or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.\n Although not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of \"utilizing a biotechnological system to make products\". Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.\n Agriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution. Through early biotechnology, the earliest farmers selected and bred the best-suited crops (e.g., those with the highest yields) to produce enough food to support a growing population. As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants — one of the first forms of biotechnology.[clarification needed]\n These processes also were included in early fermentation of beer.[13] These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods.  In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains broke down into alcohols, such as ethanol. Later, other cultures produced the process of lactic acid fermentation, which produced other preserved foods, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.\n Before the time of Charles Darwin's work and life, animal and plant scientists had already used selective breeding. Darwin added to that body of work with his scientific observations about the ability of science to change species. These accounts contributed to Darwin's theory of natural selection.[14]\n For thousands of years, humans have used selective breeding to improve the production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.[15]\n In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using Clostridium acetobutylicum, to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.[16]\n Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold Penicillium. His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley – to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.[15]\n The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success. Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced. The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of Diamond v. Chakrabarty.[17] Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the genus Pseudomonas) capable of breaking down crude oil, which he proposed to use in treating oil spills. (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the Pseudomonas bacterium).\n The MOSFET (metal–oxide–semiconductor field-effect transistor) was invented by Mohamed M. Atalla and Dawon Kahng in 1959.[18] Two years later, Leland C. Clark and Champ Lyons invented the first biosensor in 1962.[19][20] Biosensor MOSFETs were later developed, and they have since been widely used to measure physical, chemical, biological and environmental parameters.[21] The first BioFET was the ion-sensitive field-effect transistor (ISFET), invented by Piet Bergveld in 1970.[22][23] It is a special type of MOSFET,[21] where the metal gate is replaced by an ion-sensitive membrane, electrolyte solution and reference electrode.[24] The ISFET is widely used in biomedical applications, such as the detection of DNA hybridization, biomarker detection from blood, antibody detection, glucose measurement, pH sensing, and genetic technology.[24]\n By the mid-1980s, other BioFETs had been developed, including the gas sensor FET (GASFET), pressure sensor FET (PRESSFET), chemical field-effect transistor (ChemFET), reference ISFET (REFET), enzyme-modified FET (ENFET) and immunologically modified FET (IMFET).[21] By the early 2000s, BioFETs such as the DNA field-effect transistor (DNAFET), gene-modified FET (GenFET) and cell-potential BioFET (CPFET) had been developed.[24]\n A factor influencing the biotechnology sector's success is improved intellectual property rights legislation—and enforcement—worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.[25]\n Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans—the main inputs into biofuels—by developing genetically modified seeds that resist pests and drought. By increasing farm productivity, biotechnology boosts biofuel production.[26]\n Biotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non-food (industrial) uses of crops and other products (e.g., biodegradable plastics, vegetable oil, biofuels), and environmental uses.\n For example, one application of biotechnology is the directed use of microorganisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, clean up sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.\n A series of derived terms have been coined to identify several branches of biotechnology, for example:\n In medicine, modern biotechnology has many applications in areas such as pharmaceutical drug discoveries and production, pharmacogenomics, and genetic testing (or genetic screening). In 2021, nearly 40% of the total company value of pharmaceutical biotech companies worldwide were active in Oncology with Neurology and Rare Diseases being the other two big applications.[36]\n Pharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs.[37] Researchers in the field investigate the influence of genetic variation on drug responses in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity.[38] The purpose of pharmacogenomics is to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects.[39] Such approaches promise the advent of \"personalized medicine\"; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.[40][41]\n Biotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology – biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium Escherichia coli. Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle or pigs). The genetically engineered bacteria are able to produce large quantities of synthetic human insulin at relatively low cost.[42][43] Biotechnology has also enabled emerging therapeutics like gene therapy. The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well.[43]\n Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders. Genetic testing identifies changes in chromosomes, genes, or proteins.[44] Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use.[45][46] Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.\n Genetically modified crops (\"GM crops\", or \"biotech crops\") are plants used in agriculture, the DNA of which has been modified with genetic engineering techniques. In most cases, the main aim is to introduce a new trait that does not occur naturally in the species. Biotechnology firms can contribute to future food security by improving the nutrition and viability of urban agriculture. Furthermore, the protection of intellectual property rights encourages private sector investment in agrobiotechnology.\n Examples in food crops include resistance to certain pests,[47] diseases,[48] stressful environmental conditions,[49] resistance to chemical treatments (e.g. resistance to a herbicide[50]), reduction of spoilage,[51] or improving the nutrient profile of the crop.[52] Examples in non-food crops include production of pharmaceutical agents,[53] biofuels,[54] and other industrially useful goods,[55] as well as for bioremediation.[56][57]\n Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from 17,000 square kilometers (4,200,000 acres) to 1,600,000 km2 (395 million acres).[58] 10% of the world's crop lands were planted with GM crops in 2010.[58] As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the US, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.[58]\n Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA with the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding.[59] Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato.[60] To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed; in November 2013 none were available on the market,[61] but in 2015 the FDA approved the first GM salmon for commercial production and consumption.[62]\n There is a scientific consensus[63][64][65][66] that currently available food derived from GM crops poses no greater risk to human health than conventional food,[67][68][69][70][71] but that each GM food needs to be tested on a case-by-case basis before introduction.[72][73][74] Nonetheless, members of the public are much less likely than scientists to perceive GM foods as safe.[75][76][77][78] The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.[79][80][81][82]\n GM crops also provide a number of ecological benefits, if not used in excess.[83] Insect-resistant crops have proven to lower pesticide usage, therefore reducing the environmental impact of pesticides as a whole.[84] However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.\n Biotechnology has several applications in the realm of food security. Crops like Golden rice are engineered to have higher nutritional content, and there is potential for food products with longer shelf lives.[85] Though not a form of agricultural biotechnology, vaccines can help prevent diseases found in animal agriculture. Additionally, agricultural biotechnology can expedite breeding processes in order to yield faster results and provide greater quantities of food.[86] Transgenic biofortification in cereals has been considered as a promising method to combat malnutrition in India and other countries.[87]\n Industrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as microorganisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels.[88] In the current decades, significant progress has been done in creating genetically modified organisms (GMOs) that enhance the diversity of applications and economical viability of industrial biotechnology. By using renewable raw materials to produce a variety of chemicals and fuels, industrial biotechnology is actively advancing towards lowering greenhouse gas emissions and moving away from a petrochemical-based economy.[89]\n Synthetic biology is considered one of the essential cornerstones in industrial biotechnology due to its financial and sustainable contribution to the manufacturing sector. Jointly biotechnology and synthetic biology play a crucial role in generating cost-effective products with nature-friendly features by using bio-based production instead of fossil-based.[90] Synthetic biology can be used to engineer model microorganisms, such as Escherichia coli, by genome editing tools to enhance their ability to produce bio-based products, such as bioproduction of medicines and biofuels.[91] For instance, E. coli and Saccharomyces cerevisiae in a consortium could be used as industrial microbes to produce precursors of the chemotherapeutic agent paclitaxel by applying the metabolic engineering in a co-culture approach to exploit the benefits from the two microbes.[92]\n Another example of synthetic biology applications in industrial biotechnology is the re-engineering of the metabolic pathways of E. coli by CRISPR and CRISPRi systems toward the production of a chemical known as 1,4-butanediol, which is used in fiber manufacturing. In order to produce 1,4-butanediol, the authors alter the metabolic regulation of the Escherichia coli by CRISPR to induce point mutation in the gltA gene, knockout of the sad gene, and knock-in six genes (cat1, sucD, 4hbd, cat2, bld, and bdh). Whereas CRISPRi system used to knockdown the three competing genes (gabD, ybgC, and tesB) that affect the biosynthesis pathway of 1,4-butanediol. Consequently, the yield of 1,4-butanediol significantly increased from 0.9 to 1.8 g\/L.[93]\n Environmental biotechnology includes various disciplines that play an essential role in reducing environmental waste and providing environmentally safe processes, such as biofiltration and biodegradation.[94][95] The environment can be affected by biotechnologies, both positively and adversely. Vallero and others have argued that the difference between beneficial biotechnology (e.g., bioremediation is to clean up an oil spill or hazard chemical leak) versus the adverse effects stemming from biotechnological enterprises (e.g., flow of genetic material from transgenic organisms into wild strains) can be seen as applications and implications, respectively.[96] Cleaning up environmental wastes is an example of an application of environmental biotechnology; whereas loss of biodiversity or loss of containment of a harmful microbe are examples of environmental implications of biotechnology.\n Many cities have installed CityTrees, which use biotechnology to filter pollutants from urban atmospheres.[97]\n The regulation of genetic engineering concerns approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology, and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the US and Europe.[98] Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety.[99] The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing.[100] The cultivation of GMOs has triggered a debate about the coexistence of GM and non-GM crops. Depending on the coexistence regulations, incentives for the cultivation of GM crops differ.[101]\n The EUginius (European GMO Initiative for a Unified Database System) database is intended to help companies, interested private users and competent authorities to find precise information on the presence, detection and identification of GMOs used in the European Union. The information is provided in English.\n In 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support are provided for two or three years during the course of their PhD thesis work. Nineteen institutions offer NIGMS supported BTPs.[102] Biotechnology training is also offered at the undergraduate level and in community colleges.\n But see also: Domingo, José L.; Bordonaba, Jordi Giné (2011). \"A literature review on the safety assessment of genetically modified plants\" (PDF). Environment International. 37 (4): 734–742. doi:10.1016\/j.envint.2011.01.003. PMID 21296423. Archived (PDF) from the original on October 9, 2022. In spite of this, the number of studies specifically focused on safety assessment of GM plants is still limited. However, it is important to remark that for the first time, a certain equilibrium in the number of research groups suggesting, on the basis of their studies, that a number of varieties of GM products (mainly maize and soybeans) are as safe and nutritious as the respective conventional non-GM plant, and those raising still serious concerns, was observed. Moreover, it is worth mentioning that most of the studies demonstrating that GM foods are as nutritional and safe as those obtained by conventional breeding, have been performed by biotechnology companies or associates, which are also responsible of commercializing these GM plants. Anyhow, this represents a notable advance in comparison with the lack of studies published in recent years in scientific journals by those companies. Krimsky, Sheldon (2015). \"An Illusory Consensus behind GMO Health Assessment\". Science, Technology, & Human Values. 40 (6): 883–914. doi:10.1177\/0162243915598381. S2CID 40855100. I began this article with the testimonials from respected scientists that there is literally no scientific controversy over the health effects of GMOs. My investigation into the scientific literature tells another story. And contrast: Panchin, Alexander Y.; Tuzhikov, Alexander I. (January 14, 2016). \"Published GMO studies find no evidence of harm when corrected for multiple comparisons\". Critical Reviews in Biotechnology. 37 (2): 213–217. doi:10.3109\/07388551.2015.1130684. ISSN 0738-8551. PMID 26767435. S2CID 11786594. Here, we show that a number of articles some of which have strongly and negatively influenced the public opinion on GM crops and even provoked political actions, such as GMO embargo, share common flaws in the statistical evaluation of the data. Having accounted for these flaws, we conclude that the data presented in these articles does not provide any substantial evidence of GMO harm.  The presented articles suggesting possible harm of GMOs received high public attention. However, despite their claims, they actually weaken the evidence for the harm and lack of substantial equivalency of studied GMOs. We emphasize that with over 1783 published articles on GMOs over the last 10 years it is expected that some of them should have reported undesired differences between GMOs and conventional crops even if no such differences exist in reality. and"}
{"key":"Biotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/Biotechnology","headline":"Biotechnology - Wikipedia","content":"\n Biotechnology is a multidisciplinary field that involves the integration of natural sciences and engineering sciences in order to achieve the application of organisms, cells, parts thereof and molecular analogues for products and services.[1]\n The term biotechnology was first used by Károly Ereky in 1919[2] to refer to the production of products from raw materials with the aid of living organisms. The core principle of biotechnology involves harnessing biological systems and organisms, such as bacteria, yeast, and plants, to perform specific tasks or produce valuable substances.\n Biotechnology  had a significant impact on many areas of society, from medicine to agriculture to environmental science. One of the key techniques used in biotechnology is genetic engineering, which allows scientists to modify the genetic makeup of organisms to achieve desired outcomes. This can involve inserting genes from one organism into another, and consequently, create new traits or modifying existing ones.[3]\n Other important techniques used in biotechnology include tissue culture, which allows researchers to grow cells and tissues in the lab for research and medical purposes, and fermentation, which is used to produce a wide range of products such as beer, wine, and cheese.\n The applications of biotechnology are diverse and have led to the development of essential products like life-saving drugs, biofuels, genetically modified crops, and innovative materials.[4] It has also been used to address environmental challenges, such as developing biodegradable plastics and using microorganisms to clean up contaminated sites.\n Biotechnology is a rapidly evolving field with significant potential to address pressing global challenges and improve the quality of life for people around the world; however, despite its numerous benefits, it also poses ethical and societal challenges, such as questions around genetic modification and intellectual property rights. As a result, there is ongoing debate and regulation surrounding the use and application of biotechnology in various industries and fields.[5]\n The concept of biotechnology encompasses a wide range of procedures for modifying living organisms for human purposes, going back to domestication of animals, cultivation of the plants, and \"improvements\" to these through breeding programs that employ artificial selection and hybridization. Modern usage also includes genetic engineering, as well as cell and tissue culture technologies. The American Chemical Society defines biotechnology as the application of biological organisms, systems, or processes by various industries to learning about the science of life and the improvement of the value of materials and organisms, such as pharmaceuticals, crops, and livestock.[6] As per the European Federation of Biotechnology, biotechnology is the integration of natural science and organisms, cells, parts thereof, and molecular analogues for products and services.[7] Biotechnology is based on the basic biological sciences (e.g., molecular biology, biochemistry, cell biology, embryology, genetics, microbiology) and conversely provides methods to support and perform basic research in biology.\n Biotechnology is the research and development in the laboratory using bioinformatics for exploration, extraction, exploitation, and production from any living organisms and any source of biomass by means of biochemical engineering where high value-added products could be planned (reproduced by biosynthesis, for example), forecasted, formulated, developed, manufactured, and marketed for the purpose of sustainable operations (for the return from bottomless initial investment on R & D) and gaining durable patents rights (for exclusives rights for sales, and prior to this to receive national and international approval from the results on animal experiment and human experiment, especially on the pharmaceutical branch of biotechnology to prevent any undetected side-effects or safety concerns by using the products).[8][9][10] The utilization of biological processes, organisms or systems to produce products that are anticipated to improve human lives is termed biotechnology.[11]\n By contrast, bioengineering is generally thought of as a related field that more heavily emphasizes higher systems approaches (not necessarily the altering or using of biological materials directly) for interfacing with and utilizing living things. Bioengineering is the application of the principles of engineering and natural sciences to tissues, cells, and molecules. This can be considered as the use of knowledge from working with and manipulating biology to achieve a result that can improve functions in plants and animals.[12] Relatedly, biomedical engineering is an overlapping field that often draws upon and applies biotechnology (by various definitions), especially in certain sub-fields of biomedical or chemical engineering such as tissue engineering, biopharmaceutical engineering, and genetic engineering.\n Although not normally what first comes to mind, many forms of human-derived agriculture clearly fit the broad definition of \"utilizing a biotechnological system to make products\". Indeed, the cultivation of plants may be viewed as the earliest biotechnological enterprise.\n Agriculture has been theorized to have become the dominant way of producing food since the Neolithic Revolution. Through early biotechnology, the earliest farmers selected and bred the best-suited crops (e.g., those with the highest yields) to produce enough food to support a growing population. As crops and fields became increasingly large and difficult to maintain, it was discovered that specific organisms and their by-products could effectively fertilize, restore nitrogen, and control pests. Throughout the history of agriculture, farmers have inadvertently altered the genetics of their crops through introducing them to new environments and breeding them with other plants — one of the first forms of biotechnology.[clarification needed]\n These processes also were included in early fermentation of beer.[13] These processes were introduced in early Mesopotamia, Egypt, China and India, and still use the same basic biological methods.  In brewing, malted grains (containing enzymes) convert starch from grains into sugar and then adding specific yeasts to produce beer. In this process, carbohydrates in the grains broke down into alcohols, such as ethanol. Later, other cultures produced the process of lactic acid fermentation, which produced other preserved foods, such as soy sauce. Fermentation was also used in this time period to produce leavened bread. Although the process of fermentation was not fully understood until Louis Pasteur's work in 1857, it is still the first use of biotechnology to convert a food source into another form.\n Before the time of Charles Darwin's work and life, animal and plant scientists had already used selective breeding. Darwin added to that body of work with his scientific observations about the ability of science to change species. These accounts contributed to Darwin's theory of natural selection.[14]\n For thousands of years, humans have used selective breeding to improve the production of crops and livestock to use them for food. In selective breeding, organisms with desirable characteristics are mated to produce offspring with the same characteristics. For example, this technique was used with corn to produce the largest and sweetest crops.[15]\n In the early twentieth century scientists gained a greater understanding of microbiology and explored ways of manufacturing specific products. In 1917, Chaim Weizmann first used a pure microbiological culture in an industrial process, that of manufacturing corn starch using Clostridium acetobutylicum, to produce acetone, which the United Kingdom desperately needed to manufacture explosives during World War I.[16]\n Biotechnology has also led to the development of antibiotics. In 1928, Alexander Fleming discovered the mold Penicillium. His work led to the purification of the antibiotic compound formed by the mold by Howard Florey, Ernst Boris Chain and Norman Heatley – to form what we today know as penicillin. In 1940, penicillin became available for medicinal use to treat bacterial infections in humans.[15]\n The field of modern biotechnology is generally thought of as having been born in 1971 when Paul Berg's (Stanford) experiments in gene splicing had early success. Herbert W. Boyer (Univ. Calif. at San Francisco) and Stanley N. Cohen (Stanford) significantly advanced the new technology in 1972 by transferring genetic material into a bacterium, such that the imported material would be reproduced. The commercial viability of a biotechnology industry was significantly expanded on June 16, 1980, when the United States Supreme Court ruled that a genetically modified microorganism could be patented in the case of Diamond v. Chakrabarty.[17] Indian-born Ananda Chakrabarty, working for General Electric, had modified a bacterium (of the genus Pseudomonas) capable of breaking down crude oil, which he proposed to use in treating oil spills. (Chakrabarty's work did not involve gene manipulation but rather the transfer of entire organelles between strains of the Pseudomonas bacterium).\n The MOSFET (metal–oxide–semiconductor field-effect transistor) was invented by Mohamed M. Atalla and Dawon Kahng in 1959.[18] Two years later, Leland C. Clark and Champ Lyons invented the first biosensor in 1962.[19][20] Biosensor MOSFETs were later developed, and they have since been widely used to measure physical, chemical, biological and environmental parameters.[21] The first BioFET was the ion-sensitive field-effect transistor (ISFET), invented by Piet Bergveld in 1970.[22][23] It is a special type of MOSFET,[21] where the metal gate is replaced by an ion-sensitive membrane, electrolyte solution and reference electrode.[24] The ISFET is widely used in biomedical applications, such as the detection of DNA hybridization, biomarker detection from blood, antibody detection, glucose measurement, pH sensing, and genetic technology.[24]\n By the mid-1980s, other BioFETs had been developed, including the gas sensor FET (GASFET), pressure sensor FET (PRESSFET), chemical field-effect transistor (ChemFET), reference ISFET (REFET), enzyme-modified FET (ENFET) and immunologically modified FET (IMFET).[21] By the early 2000s, BioFETs such as the DNA field-effect transistor (DNAFET), gene-modified FET (GenFET) and cell-potential BioFET (CPFET) had been developed.[24]\n A factor influencing the biotechnology sector's success is improved intellectual property rights legislation—and enforcement—worldwide, as well as strengthened demand for medical and pharmaceutical products to cope with an ageing, and ailing, U.S. population.[25]\n Rising demand for biofuels is expected to be good news for the biotechnology sector, with the Department of Energy estimating ethanol usage could reduce U.S. petroleum-derived fuel consumption by up to 30% by 2030. The biotechnology sector has allowed the U.S. farming industry to rapidly increase its supply of corn and soybeans—the main inputs into biofuels—by developing genetically modified seeds that resist pests and drought. By increasing farm productivity, biotechnology boosts biofuel production.[26]\n Biotechnology has applications in four major industrial areas, including health care (medical), crop production and agriculture, non-food (industrial) uses of crops and other products (e.g., biodegradable plastics, vegetable oil, biofuels), and environmental uses.\n For example, one application of biotechnology is the directed use of microorganisms for the manufacture of organic products (examples include beer and milk products). Another example is using naturally present bacteria by the mining industry in bioleaching. Biotechnology is also used to recycle, treat waste, clean up sites contaminated by industrial activities (bioremediation), and also to produce biological weapons.\n A series of derived terms have been coined to identify several branches of biotechnology, for example:\n In medicine, modern biotechnology has many applications in areas such as pharmaceutical drug discoveries and production, pharmacogenomics, and genetic testing (or genetic screening). In 2021, nearly 40% of the total company value of pharmaceutical biotech companies worldwide were active in Oncology with Neurology and Rare Diseases being the other two big applications.[36]\n Pharmacogenomics (a combination of pharmacology and genomics) is the technology that analyses how genetic makeup affects an individual's response to drugs.[37] Researchers in the field investigate the influence of genetic variation on drug responses in patients by correlating gene expression or single-nucleotide polymorphisms with a drug's efficacy or toxicity.[38] The purpose of pharmacogenomics is to develop rational means to optimize drug therapy, with respect to the patients' genotype, to ensure maximum efficacy with minimal adverse effects.[39] Such approaches promise the advent of \"personalized medicine\"; in which drugs and drug combinations are optimized for each individual's unique genetic makeup.[40][41]\n Biotechnology has contributed to the discovery and manufacturing of traditional small molecule pharmaceutical drugs as well as drugs that are the product of biotechnology – biopharmaceutics. Modern biotechnology can be used to manufacture existing medicines relatively easily and cheaply. The first genetically engineered products were medicines designed to treat human diseases. To cite one example, in 1978 Genentech developed synthetic humanized insulin by joining its gene with a plasmid vector inserted into the bacterium Escherichia coli. Insulin, widely used for the treatment of diabetes, was previously extracted from the pancreas of abattoir animals (cattle or pigs). The genetically engineered bacteria are able to produce large quantities of synthetic human insulin at relatively low cost.[42][43] Biotechnology has also enabled emerging therapeutics like gene therapy. The application of biotechnology to basic science (for example through the Human Genome Project) has also dramatically improved our understanding of biology and as our scientific knowledge of normal and disease biology has increased, our ability to develop new medicines to treat previously untreatable diseases has increased as well.[43]\n Genetic testing allows the genetic diagnosis of vulnerabilities to inherited diseases, and can also be used to determine a child's parentage (genetic mother and father) or in general a person's ancestry. In addition to studying chromosomes to the level of individual genes, genetic testing in a broader sense includes biochemical tests for the possible presence of genetic diseases, or mutant forms of genes associated with increased risk of developing genetic disorders. Genetic testing identifies changes in chromosomes, genes, or proteins.[44] Most of the time, testing is used to find changes that are associated with inherited disorders. The results of a genetic test can confirm or rule out a suspected genetic condition or help determine a person's chance of developing or passing on a genetic disorder. As of 2011 several hundred genetic tests were in use.[45][46] Since genetic testing may open up ethical or psychological problems, genetic testing is often accompanied by genetic counseling.\n Genetically modified crops (\"GM crops\", or \"biotech crops\") are plants used in agriculture, the DNA of which has been modified with genetic engineering techniques. In most cases, the main aim is to introduce a new trait that does not occur naturally in the species. Biotechnology firms can contribute to future food security by improving the nutrition and viability of urban agriculture. Furthermore, the protection of intellectual property rights encourages private sector investment in agrobiotechnology.\n Examples in food crops include resistance to certain pests,[47] diseases,[48] stressful environmental conditions,[49] resistance to chemical treatments (e.g. resistance to a herbicide[50]), reduction of spoilage,[51] or improving the nutrient profile of the crop.[52] Examples in non-food crops include production of pharmaceutical agents,[53] biofuels,[54] and other industrially useful goods,[55] as well as for bioremediation.[56][57]\n Farmers have widely adopted GM technology. Between 1996 and 2011, the total surface area of land cultivated with GM crops had increased by a factor of 94, from 17,000 square kilometers (4,200,000 acres) to 1,600,000 km2 (395 million acres).[58] 10% of the world's crop lands were planted with GM crops in 2010.[58] As of 2011, 11 different transgenic crops were grown commercially on 395 million acres (160 million hectares) in 29 countries such as the US, Brazil, Argentina, India, Canada, China, Paraguay, Pakistan, South Africa, Uruguay, Bolivia, Australia, Philippines, Myanmar, Burkina Faso, Mexico and Spain.[58]\n Genetically modified foods are foods produced from organisms that have had specific changes introduced into their DNA with the methods of genetic engineering. These techniques have allowed for the introduction of new crop traits as well as a far greater control over a food's genetic structure than previously afforded by methods such as selective breeding and mutation breeding.[59] Commercial sale of genetically modified foods began in 1994, when Calgene first marketed its Flavr Savr delayed ripening tomato.[60] To date most genetic modification of foods have primarily focused on cash crops in high demand by farmers such as soybean, corn, canola, and cotton seed oil. These have been engineered for resistance to pathogens and herbicides and better nutrient profiles. GM livestock have also been experimentally developed; in November 2013 none were available on the market,[61] but in 2015 the FDA approved the first GM salmon for commercial production and consumption.[62]\n There is a scientific consensus[63][64][65][66] that currently available food derived from GM crops poses no greater risk to human health than conventional food,[67][68][69][70][71] but that each GM food needs to be tested on a case-by-case basis before introduction.[72][73][74] Nonetheless, members of the public are much less likely than scientists to perceive GM foods as safe.[75][76][77][78] The legal and regulatory status of GM foods varies by country, with some nations banning or restricting them, and others permitting them with widely differing degrees of regulation.[79][80][81][82]\n GM crops also provide a number of ecological benefits, if not used in excess.[83] Insect-resistant crops have proven to lower pesticide usage, therefore reducing the environmental impact of pesticides as a whole.[84] However, opponents have objected to GM crops per se on several grounds, including environmental concerns, whether food produced from GM crops is safe, whether GM crops are needed to address the world's food needs, and economic concerns raised by the fact these organisms are subject to intellectual property law.\n Biotechnology has several applications in the realm of food security. Crops like Golden rice are engineered to have higher nutritional content, and there is potential for food products with longer shelf lives.[85] Though not a form of agricultural biotechnology, vaccines can help prevent diseases found in animal agriculture. Additionally, agricultural biotechnology can expedite breeding processes in order to yield faster results and provide greater quantities of food.[86] Transgenic biofortification in cereals has been considered as a promising method to combat malnutrition in India and other countries.[87]\n Industrial biotechnology (known mainly in Europe as white biotechnology) is the application of biotechnology for industrial purposes, including industrial fermentation. It includes the practice of using cells such as microorganisms, or components of cells like enzymes, to generate industrially useful products in sectors such as chemicals, food and feed, detergents, paper and pulp, textiles and biofuels.[88] In the current decades, significant progress has been done in creating genetically modified organisms (GMOs) that enhance the diversity of applications and economical viability of industrial biotechnology. By using renewable raw materials to produce a variety of chemicals and fuels, industrial biotechnology is actively advancing towards lowering greenhouse gas emissions and moving away from a petrochemical-based economy.[89]\n Synthetic biology is considered one of the essential cornerstones in industrial biotechnology due to its financial and sustainable contribution to the manufacturing sector. Jointly biotechnology and synthetic biology play a crucial role in generating cost-effective products with nature-friendly features by using bio-based production instead of fossil-based.[90] Synthetic biology can be used to engineer model microorganisms, such as Escherichia coli, by genome editing tools to enhance their ability to produce bio-based products, such as bioproduction of medicines and biofuels.[91] For instance, E. coli and Saccharomyces cerevisiae in a consortium could be used as industrial microbes to produce precursors of the chemotherapeutic agent paclitaxel by applying the metabolic engineering in a co-culture approach to exploit the benefits from the two microbes.[92]\n Another example of synthetic biology applications in industrial biotechnology is the re-engineering of the metabolic pathways of E. coli by CRISPR and CRISPRi systems toward the production of a chemical known as 1,4-butanediol, which is used in fiber manufacturing. In order to produce 1,4-butanediol, the authors alter the metabolic regulation of the Escherichia coli by CRISPR to induce point mutation in the gltA gene, knockout of the sad gene, and knock-in six genes (cat1, sucD, 4hbd, cat2, bld, and bdh). Whereas CRISPRi system used to knockdown the three competing genes (gabD, ybgC, and tesB) that affect the biosynthesis pathway of 1,4-butanediol. Consequently, the yield of 1,4-butanediol significantly increased from 0.9 to 1.8 g\/L.[93]\n Environmental biotechnology includes various disciplines that play an essential role in reducing environmental waste and providing environmentally safe processes, such as biofiltration and biodegradation.[94][95] The environment can be affected by biotechnologies, both positively and adversely. Vallero and others have argued that the difference between beneficial biotechnology (e.g., bioremediation is to clean up an oil spill or hazard chemical leak) versus the adverse effects stemming from biotechnological enterprises (e.g., flow of genetic material from transgenic organisms into wild strains) can be seen as applications and implications, respectively.[96] Cleaning up environmental wastes is an example of an application of environmental biotechnology; whereas loss of biodiversity or loss of containment of a harmful microbe are examples of environmental implications of biotechnology.\n Many cities have installed CityTrees, which use biotechnology to filter pollutants from urban atmospheres.[97]\n The regulation of genetic engineering concerns approaches taken by governments to assess and manage the risks associated with the use of genetic engineering technology, and the development and release of genetically modified organisms (GMO), including genetically modified crops and genetically modified fish. There are differences in the regulation of GMOs between countries, with some of the most marked differences occurring between the US and Europe.[98] Regulation varies in a given country depending on the intended use of the products of the genetic engineering. For example, a crop not intended for food use is generally not reviewed by authorities responsible for food safety.[99] The European Union differentiates between approval for cultivation within the EU and approval for import and processing. While only a few GMOs have been approved for cultivation in the EU a number of GMOs have been approved for import and processing.[100] The cultivation of GMOs has triggered a debate about the coexistence of GM and non-GM crops. Depending on the coexistence regulations, incentives for the cultivation of GM crops differ.[101]\n The EUginius (European GMO Initiative for a Unified Database System) database is intended to help companies, interested private users and competent authorities to find precise information on the presence, detection and identification of GMOs used in the European Union. The information is provided in English.\n In 1988, after prompting from the United States Congress, the National Institute of General Medical Sciences (National Institutes of Health) (NIGMS) instituted a funding mechanism for biotechnology training. Universities nationwide compete for these funds to establish Biotechnology Training Programs (BTPs). Each successful application is generally funded for five years then must be competitively renewed. Graduate students in turn compete for acceptance into a BTP; if accepted, then stipend, tuition and health insurance support are provided for two or three years during the course of their PhD thesis work. Nineteen institutions offer NIGMS supported BTPs.[102] Biotechnology training is also offered at the undergraduate level and in community colleges.\n But see also: Domingo, José L.; Bordonaba, Jordi Giné (2011). \"A literature review on the safety assessment of genetically modified plants\" (PDF). Environment International. 37 (4): 734–742. doi:10.1016\/j.envint.2011.01.003. PMID 21296423. Archived (PDF) from the original on October 9, 2022. In spite of this, the number of studies specifically focused on safety assessment of GM plants is still limited. However, it is important to remark that for the first time, a certain equilibrium in the number of research groups suggesting, on the basis of their studies, that a number of varieties of GM products (mainly maize and soybeans) are as safe and nutritious as the respective conventional non-GM plant, and those raising still serious concerns, was observed. Moreover, it is worth mentioning that most of the studies demonstrating that GM foods are as nutritional and safe as those obtained by conventional breeding, have been performed by biotechnology companies or associates, which are also responsible of commercializing these GM plants. Anyhow, this represents a notable advance in comparison with the lack of studies published in recent years in scientific journals by those companies. Krimsky, Sheldon (2015). \"An Illusory Consensus behind GMO Health Assessment\". Science, Technology, & Human Values. 40 (6): 883–914. doi:10.1177\/0162243915598381. S2CID 40855100. I began this article with the testimonials from respected scientists that there is literally no scientific controversy over the health effects of GMOs. My investigation into the scientific literature tells another story. And contrast: Panchin, Alexander Y.; Tuzhikov, Alexander I. (January 14, 2016). \"Published GMO studies find no evidence of harm when corrected for multiple comparisons\". Critical Reviews in Biotechnology. 37 (2): 213–217. doi:10.3109\/07388551.2015.1130684. ISSN 0738-8551. PMID 26767435. S2CID 11786594. Here, we show that a number of articles some of which have strongly and negatively influenced the public opinion on GM crops and even provoked political actions, such as GMO embargo, share common flaws in the statistical evaluation of the data. Having accounted for these flaws, we conclude that the data presented in these articles does not provide any substantial evidence of GMO harm.  The presented articles suggesting possible harm of GMOs received high public attention. However, despite their claims, they actually weaken the evidence for the harm and lack of substantial equivalency of studied GMOs. We emphasize that with over 1783 published articles on GMOs over the last 10 years it is expected that some of them should have reported undesired differences between GMOs and conventional crops even if no such differences exist in reality. and"}
{"key":"Biotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/Natural_science","headline":"Natural science - Wikipedia","content":"\nNatural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation.[1] Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.\n Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the \"laws of nature\".[2]\n Modern natural science succeeded more classical approaches to natural philosophy. Galileo, Kepler, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science.[3] Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on.[4] Today, \"natural history\" suggests observational descriptions aimed at popular audiences.[5]\n Philosophers of science have suggested several criteria, including Karl Popper's controversial falsifiability criterion, to help them differentiate scientific endeavors from non-scientific ones. Validity, accuracy, and quality control, such as peer review and repeatability of findings, are amongst the most respected criteria in today's global scientific community.\n In natural science, impossibility assertions come to be widely accepted as overwhelmingly probable rather than considered proved to the point of being unchallengeable. The basis for this strong acceptance is a combination of extensive evidence of something not occurring, combined with an underlying theory, very successful in making predictions, whose assumptions lead logically to the conclusion that something is impossible. While an impossibility assertion in natural science can never be absolutely proved, it could be refuted by the observation of a single counterexample. Such a counterexample would require that the assumptions underlying the theory that implied the impossibility be re-examined.\n This field encompasses a diverse set of disciplines that examines phenomena related to living organisms. The scale of study can range from sub-component biophysics up to complex ecologies. Biology is concerned with the characteristics, classification and behaviors of organisms, as well as how species were formed and their interactions with each other and the environment.\n The biological fields of botany, zoology, and medicine date back to early periods of civilization, while microbiology was introduced in the 17th century with the invention of the microscope. However, it was not until the 19th century that biology became a unified science. Once scientists discovered commonalities between all living things, it was decided they were best studied as a whole.\n Some key developments in biology were the discovery of genetics, evolution through natural selection, the germ theory of disease, and the application of the techniques of chemistry and physics at the level of the cell or organic molecule.\n Modern biology is divided into subdisciplines by the type of organism and by the scale being studied. Molecular biology is the study of the fundamental chemistry of life, while cellular biology is the examination of the cell; the basic building block of all life. At a higher level, anatomy and physiology look at the internal structures, and their functions, of an organism, while ecology looks at how various organisms interrelate.\n Earth science (also known as geoscience), is an all-embracing term for the sciences related to the planet Earth, including geology, geography, geophysics, geochemistry, climatology, glaciology, hydrology, meteorology, and oceanography.\n Although mining and precious stones have been human interests throughout the history of civilization, the development of the related sciences of economic geology and mineralogy did not occur until the 18th century. The study of the earth, particularly paleontology, blossomed in the 19th century. The growth of other disciplines, such as geophysics, in the 20th century, led to the development of the theory of plate tectonics in the 1960s, which has had a similar effect on the Earth sciences as the theory of evolution had on biology. Earth sciences today are closely linked to petroleum and mineral resources, climate research and to environmental assessment and remediation.\n Although sometimes considered in conjunction with the earth sciences, due to the independent development of its concepts, techniques, and practices and also the fact of it having a wide range of sub-disciplines under its wing, atmospheric science is also considered a separate branch of natural science. This field studies the characteristics of different layers of the atmosphere from ground level to the edge of the space. The timescale of the study also varies from day to century. Sometimes the field also includes the study of climatic patterns on planets other than earth.[6]\n The serious study of oceans began in the early- to the mid-20th century. As a field of natural science, it is relatively young but stand-alone programs offer specializations in the subject. Though some controversies remain as to the categorization of the field under earth sciences, interdisciplinary sciences, or as a separate field in its own right, most modern workers in the field agree that it has matured to a state that it has its own paradigms and practices.\n Planetary science or planetology, is the scientific study of planets, which include terrestrial planets like the Earth, and other types of planets, such as gas giants other celestial bodies, such as moons, asteroids, and comets. This largely includes the Solar System, but recently has started to expand to exoplanets, particularly terrestrial exoplanets. It explores various objects, spanning from micrometeoroids to gas giants, with the objective of establishing their composition, movements, genesis, interrelation, and past. Planetary science is an interdisciplinary domain, having originated from astronomy and Earth science, and currently encompassing a multitude of areas, such as planetary geology, cosmochemistry, atmospheric science, physics, oceanography, hydrology, theoretical planetology, glaciology, and exoplanetology. Related fields encompass space physics, which delves into the impact of the Sun on the bodies in the Solar System, and astrobiology.\n Planetary science comprises interconnected observational and theoretical branches. Observational research entails a combination of space exploration, primarily through robotic spacecraft missions utilizing remote sensing, and comparative experimental work conducted in Earth-based laboratories. The theoretical aspect involves extensive mathematical modelling and computer simulation.\n Typically, planetary scientists are situated within astronomy and physics or Earth sciences departments in universities or research centers. However, there are also dedicated planetary science institutes worldwide. Generally, individuals pursuing a career in planetary science undergo graduate-level studies in one of the Earth sciences, astronomy, astrophysics, geophysics, or physics. They then focus their research within the discipline of planetary science. Major conferences are held annually, and numerous peer reviewed journals cater to the diverse research interests in planetary science. Some planetary scientists are employed by private research centers and frequently engage in collaborative research initiatives.\n Constituting the scientific study of matter at the atomic and molecular scale, chemistry deals primarily with collections of atoms, such as gases, molecules, crystals, and metals. The composition, statistical properties, transformations, and reactions of these materials are studied. Chemistry also involves understanding the properties and interactions of individual atoms and molecules for use in larger-scale applications.\n Most chemical processes can be studied directly in a laboratory, using a series of (often well-tested) techniques for manipulating materials, as well as an understanding of the underlying processes. Chemistry is often called \"the central science\" because of its role in connecting the other natural sciences.\n Early experiments in chemistry had their roots in the system of alchemy, a set of beliefs combining mysticism with physical experiments. The science of chemistry began to develop with the work of Robert Boyle, the discoverer of gases, and Antoine Lavoisier, who developed the theory of the conservation of mass.\n The discovery of the chemical elements and atomic theory began to systematize this science, and researchers developed a fundamental understanding of states of matter, ions, chemical bonds and chemical reactions. The success of this science led to a complementary chemical industry that now plays a significant role in the world economy.\n Physics embodies the study of the fundamental constituents of the universe, the forces and interactions they exert on one another, and the results produced by these interactions. In general, physics is regarded as the fundamental science, because all other natural sciences use and obey the field's principles and laws. Physics relies heavily on mathematics as the logical framework for formulating and quantifying principles.\n The study of the principles of the universe has a long history and largely derives from direct observation and experimentation. The formulation of theories about the governing laws of the universe has been central to the study of physics from very early on, with philosophy gradually yielding to systematic, quantitative experimental testing and observation as the source of verification. Key historical developments in physics include Isaac Newton's theory of universal gravitation and classical mechanics, an understanding of electricity and its relation to magnetism, Einstein's theories of special and general relativity, the development of thermodynamics, and the quantum mechanical model of atomic and subatomic physics.\n The field of physics is extremely broad, and can include such diverse studies as quantum mechanics and theoretical physics, applied physics and optics. Modern physics is becoming increasingly specialized, where researchers tend to focus on a particular area rather than being \"universalists\" like Isaac Newton, Albert Einstein and Lev Landau, who worked in multiple areas.\n Astronomy is a natural science that studies celestial objects and phenomena. Objects of interest include planets, moons, stars, nebulae, galaxies, and comets. Astronomy is the study of everything in the universe beyond Earth's atmosphere. That includes objects we can see with our naked eyes. Astronomy is one of the oldest sciences.\n Astronomers of early civilizations performed methodical observations of the night sky, and astronomical artifacts have been found from much earlier periods. There are two types of astronomy: observational astronomy and theoretical astronomy. Observational astronomy is focused on acquiring and analyzing data, mainly using basic principles of physics while Theoretical astronomy is oriented towards the development of computer or analytical models to describe astronomical objects and phenomena.\n This discipline is the science of celestial objects and phenomena that originate outside the Earth's atmosphere. It is concerned with the evolution, physics, chemistry, meteorology, geology, and motion of celestial objects, as well as the formation and development of the universe.\n Astronomy includes the examination, study, and modeling of stars, planets, comets. Most of the information used by astronomers is gathered by remote observation, although some laboratory reproduction of celestial phenomena has been performed (such as the molecular chemistry of the interstellar medium). There is considerable overlap with physics and in some areas of earth science. There are also interdisciplinary fields such as astrophysics, planetary sciences, and cosmology, along with allied disciplines such as space physics and astrochemistry.\n While the origins of the study of celestial features and phenomena can be traced back to antiquity, the scientific methodology of this field began to develop in the middle of the 17th century. A key factor was Galileo's introduction of the telescope to examine the night sky in more detail.\n The mathematical treatment of astronomy began with Newton's development of celestial mechanics and the laws of gravitation, although it was triggered by earlier work of astronomers such as Kepler. By the 19th century, astronomy had developed into formal science, with the introduction of instruments such as the spectroscope and photography, along with much-improved telescopes and the creation of professional observatories.\n The distinctions between the natural science disciplines are not always sharp, and they share many cross-discipline fields. Physics plays a significant role in the other natural sciences, as represented by astrophysics, geophysics, chemical physics and biophysics. Likewise chemistry is represented by such fields as biochemistry, physical chemistry, geochemistry and astrochemistry.\n A particular example of a scientific discipline that draws upon multiple natural sciences is environmental science. This field studies the interactions of physical, chemical, geological, and biological components of the environment, with particular regard to the effect of human activities and the impact on biodiversity and sustainability. This science also draws upon expertise from other fields such as economics, law, and social sciences.\n A comparable discipline is oceanography, as it draws upon a similar breadth of scientific disciplines. Oceanography is sub-categorized into more specialized cross-disciplines, such as physical oceanography and marine biology. As the marine ecosystem is very large and diverse, marine biology is further divided into many subfields, including specializations in particular species.\n There is also a subset of cross-disciplinary fields that have strong currents that run counter to specialization by the nature of the problems that they address. Put another way: In some fields of integrative application, specialists in more than one field are a key part of the most dialog. Such integrative fields, for example, include nanoscience, astrobiology, and complex system informatics.\n Materials science is a relatively new, interdisciplinary field that deals with the study of matter and its properties; as well as the discovery and design of new materials. Originally developed through the field of metallurgy, the study of the properties of materials and solids has now expanded into all materials. The field covers the chemistry, physics, and engineering applications of materials including metals, ceramics, artificial polymers, and many others. The core of the field deals with relating the structure of materials with their properties.\n It is at the forefront of research in science and engineering. It is an important part of forensic engineering (the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property) and failure analysis, the latter being the key to understanding, for example, the cause of various aviation accidents. Many of the most pressing scientific problems that are faced today are due to the limitations of the materials that are available and, as a result, breakthroughs in this field are likely to have a significant impact on the future of technology.\n The basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a certain application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and how it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.\n Some scholars trace the origins of natural science as far back as pre-literate human societies, where understanding the natural world was necessary for survival.[7] People observed and built up knowledge about the behavior of animals and the usefulness of plants as food and medicine, which was passed down from generation to generation.[7] These primitive understandings gave way to more formalized inquiry around 3500 to 3000 BC in the Mesopotamian and Ancient Egyptian cultures, which produced the first known written evidence of natural philosophy, the precursor of natural science.[8] While the writings show an interest in astronomy, mathematics, and other aspects of the physical world, the ultimate aim of inquiry about nature's workings was in all cases religious or mythological, not scientific.[9]\n A tradition of scientific inquiry also emerged in Ancient China, where Taoist alchemists and philosophers experimented with elixirs to extend life and cure ailments.[10] They focused on the yin and yang, or contrasting elements in nature; the yin was associated with femininity and coldness, while yang was associated with masculinity and warmth.[11] The five phases – fire, earth, metal, wood, and water – described a cycle of transformations in nature. The water turned into wood, which turned into the fire when it burned. The ashes left by fire were earth.[12] Using these principles, Chinese philosophers and doctors explored human anatomy, characterizing organs as predominantly yin or yang, and understood the relationship between the pulse, the heart, and the flow of blood in the body centuries before it became accepted in the West.[13]\n Little evidence survives of how Ancient Indian cultures around the Indus River understood nature, but some of their perspectives may be reflected in the Vedas, a set of sacred Hindu texts.[13] They reveal a conception of the universe as ever-expanding and constantly being recycled and reformed.[13] Surgeons in the Ayurvedic tradition saw health and illness as a combination of three humors: wind, bile and phlegm.[13] A healthy life was the result of a balance among these humors.[13] In Ayurvedic thought, the body consisted of five elements: earth, water, fire, wind, and space.[13] Ayurvedic surgeons performed complex surgeries and developed a detailed understanding of human anatomy.[13]\n Pre-Socratic philosophers in Ancient Greek culture brought natural philosophy a step closer to direct inquiry about cause and effect in nature between 600 and 400 BC, although an element of magic and mythology remained.[14] Natural phenomena such as earthquakes and eclipses were explained increasingly in the context of nature itself instead of being attributed to angry gods.[14] Thales of Miletus, an early philosopher who lived from 625 to 546 BC, explained earthquakes by theorizing that the world floated on water and that water was the fundamental element in nature.[15] In the 5th century BC, Leucippus was an early exponent of atomism, the idea that the world is made up of fundamental indivisible particles.[16] Pythagoras applied Greek innovations in mathematics to astronomy, and suggested that the earth was spherical.[16]\n Later Socratic and Platonic thought focused on ethics, morals and art and did not attempt an investigation of the physical world; Plato criticized pre-Socratic thinkers as materialists and anti-religionists.[17] Aristotle, however, a student of Plato who lived from 384 to 322 BC, paid closer attention to the natural world in his philosophy.[18] In his History of Animals, he described the inner workings of 110 species, including the stingray, catfish and bee.[19] He investigated chick embryos by breaking open eggs and observing them at various stages of development.[20] Aristotle's works were influential through the 16th century, and he is considered to be the father of biology for his pioneering work in that science.[21] He also presented philosophies about physics, nature, and astronomy using inductive reasoning in his works Physics and Meteorology.[22]\n While Aristotle considered natural philosophy more seriously than his predecessors, he approached it as a theoretical branch of science.[23] Still, inspired by his work, Ancient Roman philosophers of the early 1st century AD, including Lucretius, Seneca and Pliny the Elder, wrote treatises that dealt with the rules of the natural world in varying degrees of depth.[24] Many Ancient Roman Neoplatonists of the 3rd to the 6th centuries also adapted Aristotle's teachings on the physical world to a philosophy that emphasized spiritualism.[25] Early medieval philosophers including Macrobius, Calcidius and Martianus Capella also examined the physical world, largely from a cosmological and cosmographical perspective, putting forth theories on the arrangement of celestial bodies and the heavens, which were posited as being composed of aether.[26]\n Aristotle's works on natural philosophy continued to be translated and studied amid the rise of the Byzantine Empire and Abbasid Caliphate.[27]\n In the Byzantine Empire, John Philoponus, an Alexandrian Aristotelian commentator and Christian theologian was the first who questioned Aristotle's teaching of physics. Unlike Aristotle who based his physics on verbal argument, Philoponus instead relied on observation and argued for observation rather than resorting to a verbal argument.[28] He introduced the theory of impetus. John Philoponus' criticism of Aristotelian principles of physics served as inspiration for Galileo Galilei during the Scientific Revolution.[29][30]\n A revival in mathematics and science took place during the time of the Abbasid Caliphate from the 9th century onward, when Muslim scholars expanded upon Greek and Indian natural philosophy.[31] The words alcohol, algebra and zenith all have Arabic roots.[32]\n Aristotle's works and other Greek natural philosophy did not reach the West until about the middle of the 12th century, when works were translated from Greek and Arabic into Latin.[33] The development of European civilization later in the Middle Ages brought with it further advances in natural philosophy.[34] European inventions such as the horseshoe, horse collar and crop rotation allowed for rapid population growth, eventually giving way to urbanization and the foundation of schools connected to monasteries and cathedrals in modern-day France and England.[35] Aided by the schools, an approach to Christian theology developed that sought to answer questions about nature and other subjects using logic.[36] This approach, however, was seen by some detractors as heresy.[36] By the 12th century, Western European scholars and philosophers came into contact with a body of knowledge of which they had previously been ignorant: a large corpus of works in Greek and Arabic that were preserved by Islamic scholars.[37] Through translation into Latin, Western Europe was introduced to Aristotle and his natural philosophy.[37] These works were taught at new universities in Paris and Oxford by the early 13th century, although the practice was frowned upon by the Catholic church.[38] A 1210 decree from the Synod of Paris ordered that \"no lectures are to be held in Paris either publicly or privately using Aristotle's books on natural philosophy or the commentaries, and we forbid all this under pain of ex-communication.\"[38]\n In the late Middle Ages, Spanish philosopher Dominicus Gundissalinus translated a treatise by the earlier Persian scholar Al-Farabi called On the Sciences into Latin, calling the study of the mechanics of nature Scientia naturalis, or natural science.[39] Gundissalinus also proposed his own classification of the natural sciences in his 1150 work On the Division of Philosophy.[39] This was the first detailed classification of the sciences based on Greek and Arab philosophy to reach Western Europe.[39] Gundissalinus defined natural science as \"the science considering only things unabstracted and with motion,\" as opposed to mathematics and sciences that rely on mathematics.[40] Following Al-Farabi, he then separated the sciences into eight parts, including physics, cosmology, meteorology, minerals science, and plant and animal science.[40]\n Later philosophers made their own classifications of the natural sciences. Robert Kilwardby wrote On the Order of the Sciences in the 13th century that classed medicine as a mechanical science, along with agriculture, hunting and theater while defining natural science as the science that deals with bodies in motion.[41] Roger Bacon, an English friar and philosopher, wrote that natural science dealt with \"a principle of motion and rest, as in the parts of the elements of fire, air, earth and water, and in all inanimate things made from them.\"[42] These sciences also covered plants, animals and celestial bodies.[42] Later in the 13th century, a Catholic priest and theologian Thomas Aquinas defined natural science as dealing with \"mobile beings\" and \"things which depend on a matter not only for their existence but also for their definition.\"[43] There was wide agreement among scholars in medieval times that natural science was about bodies in motion, although there was division about the inclusion of fields including medicine, music, and perspective.[44] Philosophers pondered questions including the existence of a vacuum, whether motion could produce heat, the colors of rainbows, the motion of the earth, whether elemental chemicals exist, and were in the atmosphere rain is formed.[45]\n In the centuries up through the end of the Middle Ages, natural science was often mingled with philosophies about magic and the occult.[46] Natural philosophy appeared in a wide range of forms, from treatises to encyclopedias to commentaries on Aristotle.[47] The interaction between natural philosophy and Christianity was complex during this period; some early theologians, including Tatian and Eusebius, considered natural philosophy an outcropping of pagan Greek science and were suspicious of it.[48] Although some later Christian philosophers, including Aquinas, came to see natural science as a means of interpreting scripture, this suspicion persisted until the 12th and 13th centuries.[49] The Condemnation of 1277, which forbade setting philosophy on a level equal with theology and the debate of religious constructs in a scientific context, showed the persistence with which Catholic leaders resisted the development of natural philosophy even from a theological perspective.[50] Aquinas and Albertus Magnus, another Catholic theologian of the era, sought to distance theology from science in their works.[51] \"I don't see what one's interpretation of Aristotle has to do with the teaching of the faith,\" he wrote in 1271.[52]\n By the 16th and 17th centuries, natural philosophy underwent an evolution beyond commentary on Aristotle as more early Greek philosophy was uncovered and translated.[53] The invention of the printing press in the 15th century, the invention of the microscope and telescope, and the Protestant Reformation fundamentally altered the social context in which scientific inquiry evolved in the West.[53] Christopher Columbus's discovery of a new world changed perceptions about the physical makeup of the world, while observations by Copernicus, Tyco Brahe and Galileo brought a more accurate picture of the solar system as heliocentric and proved many of Aristotle's theories about the heavenly bodies false.[54] Several 17th-century philosophers, including Thomas Hobbes, John Locke and Francis Bacon made a break from the past by rejecting Aristotle and his medieval followers outright, calling their approach to natural philosophy superficial.[55]\n The titles of Galileo's work Two New Sciences and Johannes Kepler's New Astronomy underscored the atmosphere of change that took hold in the 17th century as Aristotle was dismissed in favor of novel methods of inquiry into the natural world.[57] Bacon was instrumental in popularizing this change; he argued that people should use the arts and sciences to gain dominion over nature.[58] To achieve this, he wrote that \"human life [must] be endowed with discoveries and powers.\"[59] He defined natural philosophy as \"the knowledge of Causes and secret motions of things; and enlarging the bounds of Human Empire, to the effecting of all things possible.\"[57] Bacon proposed that scientific inquiry be supported by the state and fed by the collaborative research of scientists, a vision that was unprecedented in its scope, ambition, and forms at the time.[59] Natural philosophers came to view nature increasingly as a mechanism that could be taken apart and understood, much like a complex clock.[60] Natural philosophers including Isaac Newton, Evangelista Torricelli and Francesco Redi conducted experiments focusing on the flow of water, measuring atmospheric pressure using a barometer and disproving spontaneous generation.[61] Scientific societies and scientific journals emerged and were spread widely through the printing press, touching off the scientific revolution.[62] Newton in 1687 published his The Mathematical Principles of Natural Philosophy, or Principia Mathematica, which set the groundwork for physical laws that remained current until the 19th century.[63]\n Some modern scholars, including Andrew Cunningham, Perry Williams, and Floris Cohen, argue that natural philosophy is not properly called a science, and that genuine scientific inquiry began only with the scientific revolution.[64] According to Cohen, \"the emancipation of science from an overarching entity called 'natural philosophy is one defining characteristic of the Scientific Revolution.\"[64] Other historians of science, including Edward Grant, contend that the scientific revolution that blossomed in the 17th, 18th, and 19th centuries occurred when principles learned in the exact sciences of optics, mechanics, and astronomy began to be applied to questions raised by natural philosophy.[64] Grant argues that Newton attempted to expose the mathematical basis of nature – the immutable rules it obeyed – and in doing so joined natural philosophy and mathematics for the first time, producing an early work of modern physics.[65]\n The scientific revolution, which began to take hold in the 17th century, represented a sharp break from Aristotelian modes of inquiry.[66] One of its principal advances was the use of the scientific method to investigate nature. Data was collected and repeatable measurements were made in experiments.[67] Scientists then formed hypotheses to explain the results of these experiments.[68] The hypothesis was then tested using the principle of falsifiability to prove or disprove its accuracy.[68] The natural sciences continued to be called natural philosophy, but the adoption of the scientific method took science beyond the realm of philosophical conjecture and introduced a more structured way of examining nature.[66]\n Newton, an English mathematician, and physicist was the seminal figure in the scientific revolution.[69] Drawing on advances made in astronomy by Copernicus, Brahe, and Kepler, Newton derived the universal law of gravitation and laws of motion.[70] These laws applied both on earth and in outer space, uniting two spheres of the physical world previously thought to function independently of each other, according to separate physical rules.[71] Newton, for example, showed that the tides were caused by the gravitational pull of the moon.[72] Another of Newton's advances was to make mathematics a powerful explanatory tool for natural phenomena.[73] While natural philosophers had long used mathematics as a means of measurement and analysis, its principles were not used as a means of understanding cause and effect in nature until Newton.[73]\n In the 18th century and 19th century, scientists including Charles-Augustin de Coulomb, Alessandro Volta, and Michael Faraday built upon Newtonian mechanics by exploring electromagnetism, or the interplay of forces with positive and negative charges on electrically charged particles.[74] Faraday proposed that forces in nature operated in \"fields\" that filled space.[75] The idea of fields contrasted with the Newtonian construct of gravitation as simply \"action at a distance\", or the attraction of objects with nothing in the space between them to intervene.[75] James Clerk Maxwell in the 19th century unified these discoveries in a coherent theory of electrodynamics.[74] Using mathematical equations and experimentation, Maxwell discovered that space was filled with charged particles that could act upon themselves and each other and that they were a medium for the transmission of charged waves.[74]\n Significant advances in chemistry also took place during the scientific revolution. Antoine Lavoisier, a French chemist, refuted the phlogiston theory, which posited that things burned by releasing \"phlogiston\" into the air.[75] Joseph Priestley had discovered oxygen in the 18th century, but Lavoisier discovered that combustion was the result of oxidation.[75] He also constructed a table of 33 elements and invented modern chemical nomenclature.[75] Formal biological science remained in its infancy in the 18th century, when the focus lay upon the classification and categorization of natural life. This growth in natural history was led by Carl Linnaeus, whose 1735 taxonomy of the natural world is still in use. Linnaeus in the 1750s introduced scientific names for all his species.[76]\n By the 19th century, the study of science had come into the purview of professionals and institutions. In so doing, it gradually acquired the more modern name of natural science. The term scientist was coined by William Whewell in an 1834 review of Mary Somerville's On the Connexion of the Sciences.[77] But the word did not enter general use until nearly the end of the same century.[citation needed]\n According to a famous 1923 textbook, Thermodynamics and the Free Energy of Chemical Substances, by the American chemist Gilbert N. Lewis and the American physical chemist Merle Randall,[78] the natural sciences contain three great branches:\n Aside from the logical and mathematical sciences, there are three great branches of natural science which stand apart by reason of the variety of far reaching deductions drawn from a small number of primary postulates — they are mechanics, electrodynamics, and thermodynamics.[79] Today, natural sciences are more commonly divided into life sciences, such as botany and zoology; and physical sciences, which include physics, chemistry, astronomy, and Earth sciences.\n"}
{"key":"Biotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/Engineering_Science","headline":"Engineering physics - Wikipedia","content":"Engineering physics, or engineering science, refers to the study of the combined disciplines of physics, mathematics, chemistry, biology, and engineering, particularly computer, nuclear, electrical, electronic, aerospace, materials or mechanical engineering. By focusing on the scientific method as a rigorous basis, it seeks ways to apply, design, and develop new solutions in engineering.[1][2][3]\n Unlike traditional engineering disciplines, engineering science\/physics is not necessarily confined to a particular branch of science, engineering or physics. Instead, engineering science\/physics is meant to provide a more thorough grounding in applied physics for a selected specialty such as optics, quantum physics, materials science, applied mechanics, electronics, nanotechnology, microfabrication, microelectronics, computing, photonics, mechanical engineering, electrical engineering, nuclear engineering, biophysics, control theory, aerodynamics, energy, solid-state physics, etc. It is the discipline devoted to creating and optimizing engineering solutions through enhanced understanding and integrated application of mathematical, scientific, statistical, and engineering principles. The discipline is also meant for cross-functionality and bridges the gap between theoretical science and practical engineering with emphasis in research and development, design, and analysis.\n It is notable that in many languages the term for \"engineering physics\" would be directly translated into English as \"technical physics\". In some countries, both what would be translated as \"engineering physics\" and what would be translated as \"technical physics\" are disciplines leading to academic degrees, with the former specializing in nuclear power research, and the latter closer to engineering physics.[4] In some institutions, an engineering (or applied) physics major is a discipline or specialization within the scope of engineering science, or applied science.[5][6][7][8]\n In many universities, engineering science programs may be offered at the levels of B.Tech., B.Sc., M.Sc. and Ph.D. Usually, a core of basic and advanced courses in mathematics, physics, chemistry, and biology forms the foundation of the curriculum, while typical elective areas may include fluid dynamics, quantum physics, economics, plasma physics, relativity, solid mechanics, operations research, quantitative finance, information technology and engineering, dynamical systems, bioengineering, environmental engineering, computational engineering, engineering mathematics and statistics, solid-state devices, materials science, electromagnetism, nanoscience, nanotechnology, energy, and optics.\n Whereas typical engineering programs (undergraduate) generally focus on the application of established methods to the design and analysis of engineering solutions in defined fields (e.g. the traditional domains of civil or mechanical engineering), the engineering science programs (undergraduate) focus on the creation and use of more advanced experimental or computational techniques where standard approaches are inadequate (i.e., development of engineering solutions to contemporary problems in the physical and life sciences by applying fundamental principles).\n Qualified engineering physicists, with a degree in Engineering Physics, can work professionally as engineers and\/or physicists in the high technology industries and beyond, becoming domain experts in multiple engineering and scientific fields.[9][10][11]\n 38. https:\/\/ughb.stanford.edu\/degree-programs\/major-programs\/engineering-physics-program\n"}
{"key":"Biotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/K%C3%A1roly_Ereky","headline":"Károly Ereky - Wikipedia","content":"Károly Ereky (German: Karl Ereky; 20 October 1878 – 17 June 1952) was a Hungarian agricultural engineer. The term 'biotechnology' was coined by him in 1919.[1] He is regarded by some as the \"father\" of biotechnology.[2][3][4]\n Ereky was born on 18 October 1878 in Esztergom, Hungary as Károly Wittmann. His father was István Wittmann and his mother Mária Dukai Takách. (Among her relatives was Judit Dukai Takách (1795-1836) who was the first Hungarian female poet.) In 1893 he changed his name to Ereky. He had three brothers: Jenő, Ferenc and István. Ereky finished grammar school at Sümeg and Székesfehérvár. He attended the Technical University of Budapest and in 1900 received a degree in technical engineering.\n There may be a family connection between Ereky and compatriot Franz Wittmann, prominent electrical engineer and inventor of the Wittmann-oscilloscope.\n He then worked as machine designer for several paper and food industry companies in Vienna, Austria until 1905. He moved to Budapest and became an assistant professor in József\nTechnical University. In 1919 he became the Hungarian Minister of Food. He wrote over one hundred publications which were written in Hungarian and published in German. Ereky was also proficient in speaking both German and English.\n In 1922 he wrote a book on the mechanisms of chlorophyll and how it can be used for animal feeding.\nIn 1925 he wrote a book on leaf proteins as a possible food source which he also promoted as a commercial product.\n Ereky coined the word \"biotechnology\" in Hungary during 1919 in a book he published in Berlin called Biotechnologie der Fleisch-, Fett- und Milcherzeugung im landwirtschaftlichen Grossbetriebe (Biotechnology of Meat, Fat and Milk Production in an Agricultural Large-Scale Farm) where he described a technology based on converting raw materials into a more useful product.[5] He built a slaughterhouse for a thousand pigs and also a fattening farm with space for 50,000 pigs, raising over 100,000 pigs a year. The enterprise was enormous, becoming one of the largest and most profitable meat and fat operations in the world. Ereky further developed a theme that would be reiterated through the 20th century: biotechnology could provide solutions to societal crises, such as food and energy shortages. For Ereky, the term \"biotechnology\" indicated the process by which raw materials could be biologically upgraded into socially useful products.\n The book sold several thousand copies within few weeks in Germany. In 1921 the book was translated into Dutch.\n On 19 September 1946, Ereky was sent to the prison of Vác for 12 years by People's Tribunal for his counter-revolutionary role in Hungary. He died in prison on 17 June 1952 at the age of 74.\n"}
{"key":"Space Exploration","link":"https:\/\/en.wikipedia.org\/wiki\/Space Exploration","headline":"Space exploration - Wikipedia","content":"\n Space exploration is the use of astronomy and space technology to explore outer space.[1] While the exploration of space is currently carried out mainly by astronomers with telescopes, its physical exploration is conducted both by uncrewed robotic space probes and human spaceflight. Space exploration, like its classical form astronomy, is one of the main sources for space science.\n While the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical space exploration to become a reality. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries.[2]\n The early era of space exploration was driven by a \"Space Race\" between the Soviet Union and the United States. A driving force of the start of space exploration was during the Cold War. After the ability to create nuclear weapons, the narrative of defense\/offense left land and the power to control the air became the focus. Both the Soviet and the U.S. were fighting to prove their superiority in technology through exploring the unknown: space. In fact, the reason NASA was made was due to the response of Sputnik I.[3] The launch of the first human-made object to orbit Earth, the Soviet Union's Sputnik 1, on 4 October 1957, and the first Moon landing by the American Apollo 11 mission on 20 July 1969 are often taken as landmarks for this initial period. The Soviet space program achieved many of the first milestones, including the first living being in orbit in 1957, the first human spaceflight (Yuri Gagarin aboard Vostok 1) in 1961, the first spacewalk (by Alexei Leonov) on 18 March 1965, the first automatic landing on another celestial body in 1966, and the launch of the first space station (Salyut 1) in 1971. \nAfter the first 20 years of exploration, focus shifted from one-off flights to renewable hardware, such as the Space Shuttle program, and from competition to cooperation as with the International Space Station (ISS).\n With the substantial completion of the ISS[4] following STS-133 in March 2011, plans for space exploration by the U.S. remain in flux. Constellation, a Bush administration program for a return to the Moon by 2020[5] was judged inadequately funded and unrealistic by an expert review panel reporting in 2009.[6] \nThe Obama administration proposed a revision of Constellation in 2010 to focus on the development of the capability for crewed missions beyond low Earth orbit (LEO), envisioning extending the operation of the ISS beyond 2020, transferring the development of launch vehicles for human crews from NASA to the private sector, and developing technology to enable missions to beyond LEO, such as Earth–Moon L1, the Moon, Earth–Sun L2, near-Earth asteroids, and Phobos or Mars orbit.[7]\n In the 2000s, China initiated a successful crewed spaceflight program while India launched Chandraayan 1, while the European Union and Japan have also planned future crewed space missions. China, Russia, and Japan have advocated crewed missions to the Moon during the 21st century, while the European Union has advocated crewed missions to both the Moon and Mars during the 20th and 21st century.\n The first telescope is said to have been invented in 1608 in the Netherlands by an eyeglass maker named Hans Lippershey, but their first recorded use in astronomy was by Galileo Galilei in 1609.[8] In 1668 Isaac Newton built his own reflecting telescope, the first fully functional telescope of this kind, and a landmark for future developments due to its superior features over the previous Galilean telescope.[9]\n A string of discoveries in the Solar System (and beyond) followed, then and in the next centuries: the mountains of the Moon, the phases of Venus, the main satellites of Jupiter and Saturn, the rings of Saturn, many comets, the asteroids, the new planets Uranus and Neptune, and many more satellites.\n The Orbiting Astronomical Observatory 2 was the first space telescope launched 1968,[10] but the launching of Hubble Space Telescope in 1990[11] set a milestone. As of 1 December 2022, there were 5,284 confirmed exoplanets discovered. The Milky Way is estimated to contain 100–400 billion stars[12] and more than 100 billion planets.[13] There are at least 2 trillion galaxies in the observable universe.[14][15] HD1 is the most distant known object from Earth, reported as 33.4 billion light-years away.[16][17][18][19][20][21]\n MW 18014 was a German V-2 rocket test launch that took place on 20 June 1944, at the Peenemünde Army Research Center in Peenemünde. It was the first man-made object to reach outer space, attaining an apogee of 176 kilometers,[22] which is well above the Kármán line.[23] It was a vertical test launch. Although the rocket reached space, it did not reach orbital velocity, and therefore returned to Earth in an impact, becoming the first sub-orbital spaceflight.[24]\n The first successful orbital launch was of the Soviet uncrewed Sputnik 1 (\"Satellite 1\") mission on 4 October 1957. The satellite weighed about 83 kg (183 lb), and is believed to have orbited Earth at a height of about 250 km (160 mi). It had two radio transmitters (20 and 40 MHz), which emitted \"beeps\" that could be heard by radios around the globe. Analysis of the radio signals was used to gather information about the electron density of the ionosphere, while temperature and pressure data was encoded in the duration of radio beeps. The results indicated that the satellite was not punctured by a meteoroid. Sputnik 1 was launched by an R-7 rocket. It burned up upon re-entry on 3 January 1958.\n The first successful human spaceflight was Vostok 1 (\"East 1\"), carrying the 27-year-old Russian cosmonaut, Yuri Gagarin, on 12 April 1961. The spacecraft completed one orbit around the globe, lasting about 1 hour and 48 minutes. Gagarin's flight resonated around the world; it was a demonstration of the advanced Soviet space program and it opened an entirely new era in space exploration: human spaceflight.\n The first artificial object to reach another celestial body was Luna 2 reaching the Moon in 1959.[25] The first soft landing on another celestial body was performed by Luna 9 landing on the Moon on 3 February 1966.[26] Luna 10 became the first artificial satellite of the Moon, entering in a lunar orbit on 3 April 1966.[27]\n The first crewed landing on another celestial body was performed by Apollo 11 on 20 July 1969, landing on the Moon. There have been a total of six spacecraft with humans landing on the Moon starting from 1969 to the last human landing in 1972.\n The first interplanetary flyby was the 1961 Venera 1 flyby of Venus, though the 1962 Mariner 2 was the first flyby of Venus to return data (closest approach 34,773 kilometers). Pioneer 6 was the first satellite to orbit the Sun, launched on 16 December 1965. The other planets were first flown by in 1965 for Mars by Mariner 4, 1973 for Jupiter by Pioneer 10, 1974 for Mercury by Mariner 10, 1979 for Saturn by Pioneer 11, 1986 for Uranus by Voyager 2, 1989 for Neptune by Voyager 2. In 2015, the dwarf planets Ceres and Pluto were orbited by Dawn and passed by New Horizons, respectively. This accounts for flybys of each of the eight planets in the Solar System, the Sun, the Moon, and Ceres and Pluto (two of the five recognized dwarf planets).\n The first interplanetary surface mission to return at least limited surface data from another planet was the 1970 landing of Venera 7, which returned data to Earth for 23 minutes from Venus. In 1975 the Venera 9 was the first to return images from the surface of another planet, returning images from Venus. In 1971 the Mars 3 mission achieved the first soft landing on Mars returning data for almost 20 seconds. Later much longer duration surface missions were achieved, including over six years of Mars surface operation by Viking 1 from 1975 to 1982 and over two hours of transmission from the surface of Venus by Venera 13 in 1982, the longest ever Soviet planetary surface mission. Venus and Mars are the two planets outside of Earth on which humans have conducted surface missions with uncrewed robotic spacecraft.\n Salyut 1 was the first space station of any kind, launched into low Earth orbit by the Soviet Union on 19 April 1971. The International Space Station is currently the largest and oldest of the 2 current fully functional space stations, inhabited continuously since the year 2000. The other, Tiangong space station built by China, is now fully crewed and operational.\n Voyager 1 became the first human-made object to leave the Solar System into interstellar space on 25 August 2012. The probe passed the heliopause at 121 AU to enter interstellar space.[28]\n The Apollo 13 flight passed the far side of the Moon at an altitude of 254 kilometers (158 miles; 137 nautical miles) above the lunar surface, and 400,171 km (248,655 mi) from Earth, marking the record for the farthest humans have ever traveled from Earth in 1970.\n As of 26 November 2022[update] Voyager 1 was at a distance of 159 AU (23.8 billion km; 14.8 billion mi) from Earth.[29] It is the most distant human-made object from Earth.[30]\n Starting in the mid-20th century probes and then human mission were sent into Earth orbit, and then on to the Moon. Also, probes were sent throughout the known Solar System, and into Solar orbit. Uncrewed spacecraft have been sent into orbit around Saturn, Jupiter, Mars, Venus, and Mercury by the 21st century, and the most distance active spacecraft, Voyager 1 and 2 traveled beyond 100 times the Earth-Sun distance. The instruments were enough though that it is thought they have left the Sun's heliosphere, a sort of bubble of particles made in the Galaxy by the Sun's solar wind.\n The Sun is a major focus of space exploration. Being above the atmosphere in particular and Earth's magnetic field gives access to the solar wind and infrared and ultraviolet radiations that cannot reach Earth's surface. The Sun generates most space weather, which can affect power generation and transmission systems on Earth and interfere with, and even damage, satellites and space probes. Numerous spacecraft dedicated to observing the Sun, beginning with the Apollo Telescope Mount, have been launched and still others have had solar observation as a secondary objective. Parker Solar Probe, launched in 2018, will approach the Sun to within 1\/9th the orbit of Mercury.\n Mercury remains the least explored of the Terrestrial planets. As of May 2013, the Mariner 10 and MESSENGER missions have been the only missions that have made close observations of Mercury. MESSENGER entered orbit around Mercury in March 2011, to further investigate the observations made by Mariner 10 in 1975 (Munsell, 2006b).\nA third mission to Mercury, scheduled to arrive in 2025, BepiColombo is to include two probes. BepiColombo is a joint mission between Japan and the European Space Agency. MESSENGER and BepiColombo are intended to gather complementary data to help scientists understand many of the mysteries discovered by Mariner 10's flybys.\n Flights to other planets within the Solar System are accomplished at a cost in energy, which is described by the net change in velocity of the spacecraft, or delta-v. Due to the relatively high delta-v to reach Mercury and its proximity to the Sun, it is difficult to explore and orbits around it are rather unstable.\n Venus was the first target of interplanetary flyby and lander missions and, despite one of the most hostile surface environments in the Solar System, has had more landers sent to it (nearly all from the Soviet Union) than any other planet in the Solar System. The first flyby was the 1961 Venera 1, though the 1962 Mariner 2 was the first flyby to successfully return data. Mariner 2 has been followed by several other flybys by multiple space agencies often as part of missions using a Venus flyby to provide a gravitational assist en route to other celestial bodies. In 1967 Venera 4 became the first probe to enter and directly examine the atmosphere of Venus. In 1970, Venera 7 became the first successful lander to reach the surface of Venus and by 1985 it had been followed by eight additional successful Soviet Venus landers which provided images and other direct surface data. Starting in 1975 with the Soviet orbiter Venera 9 some ten successful orbiter missions have been sent to Venus, including later missions which were able to map the surface of Venus using radar to pierce the obscuring atmosphere.\n Space exploration has been used as a tool to understand Earth as a celestial object. Orbital missions can provide data for Earth that can be difficult or impossible to obtain from a purely ground-based point of reference.\n For example, the existence of the Van Allen radiation belts was unknown until their discovery by the United States' first artificial satellite, Explorer 1. These belts contain radiation trapped by Earth's magnetic fields, which currently renders construction of habitable space stations above 1000 km impractical.\nFollowing this early unexpected discovery, a large number of Earth observation satellites have been deployed specifically to explore Earth from a space-based perspective. These satellites have significantly contributed to the understanding of a variety of Earth-based phenomena. For instance, the hole in the ozone layer was found by an artificial satellite that was exploring Earth's atmosphere, and satellites have allowed for the discovery of archeological sites or geological formations that were difficult or impossible to otherwise identify.\n The Moon was the first celestial body to be the object of space exploration. It holds the distinctions of being the first remote celestial object to be flown by, orbited, and landed upon by spacecraft, and the only remote celestial object ever to be visited by humans.\n In 1959 the Soviets obtained the first images of the far side of the Moon, never previously visible to humans. The U.S. exploration of the Moon began with the Ranger 4 impactor in 1962. Starting in 1966 the Soviets successfully deployed a number of landers to the Moon which were able to obtain data directly from the Moon's surface; just four months later, Surveyor 1 marked the debut of a successful series of U.S. landers. The Soviet uncrewed missions culminated in the Lunokhod program in the early 1970s, which included the first uncrewed rovers and also successfully brought lunar soil samples to Earth for study. This marked the first (and to date the only) automated return of extraterrestrial soil samples to Earth. Uncrewed exploration of the Moon continues with various nations periodically deploying lunar orbiters, and in 2008 the Indian Moon Impact Probe and in 2023 the Chandrayaan-3 of India became the first spacecraft to land on the lunar south pole.\n Crewed exploration of the Moon began in 1968 with the Apollo 8 mission that successfully orbited the Moon, the first time any extraterrestrial object was orbited by humans. In 1969, the Apollo 11 mission marked the first time humans set foot upon another world. Crewed exploration of the Moon did not continue for long. The Apollo 17 mission in 1972 marked the sixth landing and the most recent human visit. Artemis 2 is scheduled to complete a crewed flyby of the Moon in 2025, and Artemis 3 will perform the first lunar landing since Apollo 17 with it scheduled for launch no earlier than 2026. Robotic missions are still pursued vigorously.\n The exploration of Mars has been an important part of the space exploration programs of the Soviet Union (later Russia), the United States, Europe, Japan and India. Dozens of robotic spacecraft, including orbiters, landers, and rovers, have been launched toward Mars since the 1960s. These missions were aimed at gathering data about current conditions and answering questions about the history of Mars. The questions raised by the scientific community are expected to not only give a better appreciation of the Red Planet but also yield further insight into the past, and possible future, of Earth.\n The exploration of Mars has come at a considerable financial cost with roughly two-thirds of all spacecraft destined for Mars failing before completing their missions, with some failing before they even began. Such a high failure rate can be attributed to the complexity and large number of variables involved in an interplanetary journey, and has led researchers to jokingly speak of The Great Galactic Ghoul[31] which subsists on a diet of Mars probes. This phenomenon is also informally known as the \"Mars Curse\".[32]  In contrast to overall high failure rates in the exploration of Mars, India has become the first country to achieve success of its maiden attempt.  India's Mars Orbiter Mission (MOM)[33][34][35] is one of the least expensive interplanetary missions ever undertaken with an approximate total cost of ₹ 450 Crore (US$73 million).[36][37] The first mission to Mars by any Arab country has been taken up by the United Arab Emirates. Called the Emirates Mars Mission, it was launched on 19 July 2020 and went into orbit around Mars on 9 February 2021. The uncrewed exploratory probe was named \"Hope Probe\" and was sent to Mars to study its atmosphere in detail.[38]\n The Russian space mission Fobos-Grunt, which launched on 9 November 2011 experienced a failure leaving it stranded in low Earth orbit.[39] It was to begin exploration of the Phobos and Martian circumterrestrial orbit, and study whether the moons of Mars, or at least Phobos, could be a \"trans-shipment point\" for spaceships traveling to Mars.[40]\n Until the advent of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes, their shapes and terrain remaining a mystery.\nSeveral asteroids have now been visited by probes, the first of which was Galileo, which flew past two: 951 Gaspra in 1991, followed by 243 Ida in 1993. Both of these lay near enough to Galileo's planned trajectory to Jupiter that they could be visited at acceptable cost. The first landing on an asteroid was performed by the NEAR Shoemaker probe in 2000, following an orbital survey of the object, 433 Eros. The dwarf planet Ceres and the asteroid 4 Vesta, two of the three largest asteroids, were visited by NASA's Dawn spacecraft, launched in 2007.\n Hayabusa was a robotic spacecraft developed by the Japan Aerospace Exploration Agency to return a sample of material from the small near-Earth asteroid 25143 Itokawa to Earth for further analysis. Hayabusa was launched on 9 May 2003 and rendezvoused with Itokawa in mid-September 2005. After arriving at Itokawa, Hayabusa studied the asteroid's shape, spin, topography, color, composition, density, and history. In November 2005, it landed on the asteroid twice to collect samples. The spacecraft returned to Earth on 13 June 2010.\n The exploration of Jupiter has consisted solely of a number of automated NASA spacecraft visiting the planet since 1973. A large majority of the missions have been \"flybys\", in which detailed observations are taken without the probe landing or entering orbit; such as in Pioneer and Voyager programs. The Galileo and Juno spacecraft are the only spacecraft to have entered the planet's orbit. As Jupiter is believed to have only a relatively small rocky core and no real solid surface, a landing mission is precluded.\n Reaching Jupiter from Earth requires a delta-v of 9.2 km\/s,[41] which is comparable to the 9.7 km\/s delta-v needed to reach low Earth orbit.[42] Fortunately, gravity assists through planetary flybys can be used to reduce the energy required at launch to reach Jupiter, albeit at the cost of a significantly longer flight duration.[41]\n Jupiter has 95 known moons, many of which have relatively little known information about them.\n Saturn has been explored only through uncrewed spacecraft launched by NASA, including one mission (Cassini–Huygens) planned and executed in cooperation with other space agencies. These missions consist of flybys in 1979 by Pioneer 11, in 1980 by Voyager 1, in 1982 by Voyager 2 and an orbital mission by the Cassini spacecraft, which lasted from 2004 until 2017.\n Saturn has at least 62 known moons, although the exact number is debatable since Saturn's rings are made up of vast numbers of independently orbiting objects of varying sizes. The largest of the moons is Titan, which holds the distinction of being the only moon in the Solar System with an atmosphere denser and thicker than that of Earth.  Titan holds the distinction of being the only object in the Outer Solar System that has been explored with a lander, the Huygens probe deployed by the Cassini spacecraft.\n The exploration of Uranus has been entirely through the Voyager 2 spacecraft, with no other visits currently planned. Given its axial tilt of 97.77°, with its polar regions exposed to sunlight or darkness for long periods, scientists were not sure what to expect at Uranus. The closest approach to Uranus occurred on 24 January 1986. Voyager 2 studied the planet's unique atmosphere and magnetosphere. Voyager 2 also examined its ring system and the moons of Uranus including all five of the previously known moons, while discovering an additional ten previously unknown moons.\n Images of Uranus proved to have a very uniform appearance, with no evidence of the dramatic storms or atmospheric banding evident on Jupiter and Saturn. Great effort was required to even identify a few clouds in the images of the planet. The magnetosphere of Uranus, however, proved to be unique, being profoundly affected by the planet's unusual axial tilt. In contrast to the bland appearance of Uranus itself, striking images were obtained of the Moons of Uranus, including evidence that Miranda had been unusually geologically active.\n The exploration of Neptune began with 25 August 1989 Voyager 2 flyby, the sole visit to the system as of 2024. The possibility of a Neptune Orbiter has been discussed, but no other missions have been given serious thought.\n Although the extremely uniform appearance of Uranus during Voyager 2's visit in 1986 had led to expectations that Neptune would also have few visible atmospheric phenomena, the spacecraft found that Neptune had obvious banding, visible clouds, auroras, and even a conspicuous anticyclone storm system rivaled in size only by Jupiter's Great Red Spot. Neptune also proved to have the fastest winds of any planet in the Solar System, measured as high as 2,100 km\/h.[43] Voyager 2 also examined Neptune's ring and moon system. It discovered 900 complete rings and additional partial ring \"arcs\" around Neptune. In addition to examining Neptune's three previously known moons, Voyager 2 also discovered five previously unknown moons, one of which, Proteus, proved to be the last largest moon in the system. Data from Voyager 2 supported the view that Neptune's largest moon, Triton, is a captured Kuiper belt object.[44]\n The dwarf planet Pluto presents significant challenges for spacecraft because of its great distance from Earth (requiring high velocity for reasonable trip times) and small mass (making capture into orbit very difficult at present). Voyager 1 could have visited Pluto, but controllers opted instead for a close flyby of Saturn's moon Titan, resulting in a trajectory incompatible with a Pluto flyby. Voyager 2 never had a plausible trajectory for reaching Pluto.[45]\n After an intense political battle, a mission to Pluto dubbed New Horizons was granted funding from the United States government in 2003.[46] New Horizons was launched successfully on 19 January 2006. In early 2007 the craft made use of a gravity assist from Jupiter. Its closest approach to Pluto was on 14 July 2015; scientific observations of Pluto began five months prior to closest approach and continued for 16 days after the encounter.\n The New Horizons mission also did a flyby of the small planetesimal Arrokoth, in the Kuiper belt, in 2019. This was its first extended mission.[47]\n Although many comets have been studied from Earth sometimes with centuries-worth of observations, only a few comets have been closely visited. In 1985, the International Cometary Explorer conducted the first comet fly-by (21P\/Giacobini-Zinner) before joining the Halley Armada studying the famous comet. The Deep Impact probe smashed into 9P\/Tempel to learn more about its structure and composition and the Stardust mission returned samples of another comet's tail. The Philae lander successfully landed on Comet Churyumov–Gerasimenko in 2014 as part of the broader Rosetta mission.\n Deep space exploration is the branch of astronomy, astronautics and space technology that is involved with the exploration of distant regions of outer space.[48] Physical exploration of space is conducted both by human spaceflights (deep-space astronautics) and by robotic spacecraft.\n Some of the best candidates for future deep space engine technologies include anti-matter, nuclear power and beamed propulsion.[49] The latter, beamed propulsion, appears to be the best candidate for deep space exploration presently available, since it uses known physics and known technology that is being developed for other purposes.[50]\n Breakthrough Starshot is a research and engineering project by the Breakthrough Initiatives to develop a proof-of-concept fleet of light sail spacecraft named StarChip,[51] to be capable of making the journey to the Alpha Centauri star system 4.37 light-years away. It was founded in 2016 by Yuri Milner, Stephen Hawking, and Mark Zuckerberg.[52][53]\n An article in science magazine Nature suggested the use of asteroids as a gateway for space exploration, with the ultimate destination being Mars. In order to make such an approach viable, three requirements need to be fulfilled: first, \"a thorough asteroid survey to find thousands of nearby bodies suitable for astronauts to visit\"; second, \"extending flight duration and distance capability to ever-increasing ranges out to Mars\"; and finally, \"developing better robotic vehicles and tools to enable astronauts to explore an asteroid regardless of its size, shape or spin\". Furthermore, using asteroids would provide astronauts with protection from galactic cosmic rays, with mission crews being able to land on them without great risk to radiation exposure.\n The James Webb Space Telescope (JWST or \"Webb\") is a space telescope that is the successor to the Hubble Space Telescope.[54][55] The JWST will provide greatly improved resolution and sensitivity over the Hubble, and will enable a broad range of investigations across the fields of astronomy and cosmology, including observing some of the most distant events and objects in the universe, such as the formation of the first galaxies. Other goals include understanding the formation of stars and planets, and direct imaging of exoplanets and novas.[56]\n The primary mirror of the JWST, the Optical Telescope Element, is composed of 18 hexagonal mirror segments made of gold-plated beryllium which combine to create a 6.5-meter (21 ft; 260 in) diameter mirror that is much larger than the Hubble's 2.4-meter (7.9 ft; 94 in) mirror.  Unlike the Hubble, which observes in the near ultraviolet, visible, and near infrared (0.1 to 1 μm) spectra, the JWST will observe in a lower frequency range, from long-wavelength visible light through mid-infrared (0.6 to 27 μm), which will allow it to observe high redshift objects that are too old and too distant for the Hubble to observe.[57] The telescope must be kept very cold in order to observe in the infrared without interference, so it will be deployed in space near the Earth–Sun L2 Lagrangian point, and a large sunshield made of silicon- and aluminum-coated Kapton will keep its mirror and instruments below 50 K (−220 °C; −370 °F).[58]\n The Artemis program is an ongoing crewed spaceflight program carried out by NASA, U.S. commercial spaceflight companies, and international partners such as ESA,[59] with the goal of landing \"the first woman and the next man\" on the Moon, specifically at the lunar south pole region by 2024. Artemis would be the next step towards the long-term goal of establishing a sustainable presence on the Moon, laying the foundation for private companies to build a lunar economy, and eventually sending humans to Mars.\n In 2017, the lunar campaign was authorized by Space Policy Directive 1, utilizing various ongoing spacecraft programs such as Orion, the Lunar Gateway, Commercial Lunar Payload Services, and adding an undeveloped crewed lander. The Space Launch System will serve as the primary launch vehicle for Orion, while commercial launch vehicles are planned for use to launch various other elements of the campaign.[60] NASA requested $1.6 billion in additional funding for Artemis for fiscal year 2020,[61] while the Senate Appropriations Committee requested from NASA a five-year budget profile[62] which is needed for evaluation and approval by Congress.[63][64]\n The research that is conducted by national space exploration agencies, such as NASA and Roscosmos, is one of the reasons supporters cite to justify government expenses. Economic analyses of the NASA programs often showed ongoing economic benefits (such as NASA spin-offs), generating many times the revenue of the cost of the program.[65] It is also argued that space exploration would lead to the extraction of resources on other planets and especially asteroids, which contain billions of dollars worth of minerals and metals. Such expeditions could generate a lot of revenue.[66] In addition, it has been argued that space exploration programs help inspire youth to study in science and engineering.[67] Space exploration also gives scientists the ability to perform experiments in other settings and expand humanity's knowledge.[68]\n Another claim is that space exploration is a necessity to mankind and that staying on Earth will lead to extinction. Some of the reasons are lack of natural resources, comets, nuclear war, and worldwide epidemic. Stephen Hawking, renowned British theoretical physicist, said that \"I don't think the human race will survive the next thousand years, unless we spread into space. There are too many accidents that can befall life on a single planet. But I'm an optimist. We will reach out to the stars.\"[69] Arthur C. Clarke (1950) presented a summary of motivations for the human exploration of space in his non-fiction semi-technical monograph Interplanetary Flight.[70] He argued that humanity's choice is essentially between expansion off Earth into space, versus cultural (and eventually biological) stagnation and death.\nThese motivations could be attributed to one of the first rocket scientists in NASA, Wernher von Braun, and his vision of humans moving beyond Earth. The basis of this plan was to:\n Develop multi-stage rockets capable of placing satellites, animals, and humans in space.\n Development of large, winged reusable spacecraft capable of carrying humans and equipment into Earth orbit in a way that made space access routine and cost-effective.\n Construction of a large, permanently occupied space station to be used as a platform both to observe Earth and from which to launch deep space expeditions.\n Launching the first human flights around the Moon, leading to the first landings of humans on the Moon, with the intent of exploring that body and establishing permanent lunar bases.\n \nAssembly and fueling of spaceships in Earth orbit for the purpose of sending humans to Mars with the intent of eventually colonizing that planet.[71] Known as the Von Braun Paradigm, the plan was formulated to lead humans in the exploration of space. Von Braun's vision of human space exploration served as the model for efforts in space exploration well into the twenty-first century, with NASA incorporating this approach into the majority of their projects.[71] The steps were followed out of order, as seen by the Apollo program reaching the moon before the space shuttle program was started, which in turn was used to complete the International Space Station. Von Braun's Paradigm formed NASA's drive for human exploration, in the hopes that humans discover the far reaches of the universe.\n NASA has produced a series of public service announcement videos supporting the concept of space exploration.[72]\n Overall, the public remains largely supportive of both crewed and uncrewed space exploration. According to an Associated Press Poll conducted in July 2003, 71% of U.S. citizens agreed with the statement that the space program is \"a good investment\", compared to 21% who did not.[73]\n Space advocacy and space policy[74] regularly invokes exploration as a human nature.[75]\n Spaceflight is the use of space technology to achieve the flight of spacecraft into and through outer space.\n Spaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other Earth observation satellites.\n A spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of Earth. Once in space, the motion of a spacecraft—both when unpropelled and when under propulsion—is covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.\n Satellites are used for a large number of purposes. Common types include military (spy) and civilian Earth observation satellites, communication satellites, navigation satellites, weather satellites, and research satellites. Space stations and human spacecraft in orbit are also satellites.\n The commercialization of space first started out with the launching of private satellites by NASA or other space agencies. Current examples of the commercial satellite use of space include satellite navigation systems, satellite television and satellite radio. The next step of commercialization of space was seen as human spaceflight. Flying humans safely to and from space had become routine to NASA.[76] Reusable spacecraft were an entirely new engineering challenge, something only seen in novels and films like Star Trek and War of the Worlds. Great names like Buzz Aldrin supported the use of making a reusable vehicle like the space shuttle. Aldrin held that reusable spacecraft were the key in making space travel affordable, stating that the use of \"passenger space travel is a huge potential market big enough to justify the creation of reusable launch vehicles\".[77] How can the public go against the words of one of America's best known heroes in space exploration? After all exploring space is the next great expedition, following the example of Lewis and Clark.Space tourism is the next step reusable vehicles in the commercialization of space. The purpose of this form of space travel is used by individuals for the purpose of personal pleasure.\n Private spaceflight companies such as SpaceX and Blue Origin, and commercial space stations such as the Axiom Space and the Bigelow Commercial Space Station have dramatically changed the landscape of space exploration, and will continue to do so in the near future.\n Astrobiology is the interdisciplinary study of life in the universe, combining aspects of astronomy, biology and geology.[78] It is focused primarily on the study of the origin, distribution and evolution of life. It is also known as exobiology (from Greek: έξω, exo, \"outside\").[79][80][81] The term \"Xenobiology\" has been used as well, but this is technically incorrect because its terminology means \"biology of the foreigners\".[82] Astrobiologists must also consider the possibility of life that is chemically entirely distinct from any life found on Earth.[83] In the Solar System some of the prime locations for current or past astrobiology are on Enceladus, Europa, Mars, and Titan.[84]\n To date, the longest human occupation of space is the International Space Station which has been in continuous use for 23 years, 140 days. Valeri Polyakov's record single spaceflight of almost 438 days aboard the Mir space station has not been surpassed. The health effects of space have been well documented through years of research conducted in the field of aerospace medicine. Analog environments similar to those one may experience in space travel (like deep sea submarines) have been used in this research to further explore the relationship between isolation and extreme environments.[85] It is imperative that the health of the crew be maintained as any deviation from baseline may compromise the integrity of the mission as well as the safety of the crew, hence the reason why astronauts must endure rigorous medical screenings and tests prior to embarking on any missions. However, it does not take long for the environmental dynamics of spaceflight to commence its toll on the human body; for example, space motion sickness (SMS) – a condition which affects the neurovestibular system and culminates in mild to severe signs and symptoms such as vertigo, dizziness, fatigue, nausea, and disorientation – plagues almost all space travelers within their first few days in orbit.[85] Space travel can also have a profound impact on the psyche of the crew members as delineated in anecdotal writings composed after their retirement. Space travel can adversely affect the body's natural biological clock (circadian rhythm); sleep patterns causing sleep deprivation and fatigue; and social interaction; consequently, residing in a Low Earth Orbit (LEO) environment for a prolonged amount of time can result in both mental and physical exhaustion.[85] Long-term stays in space reveal issues with bone and muscle loss in low gravity, immune system suppression, and radiation exposure. The lack of gravity causes fluid to rise upward which can cause pressure to build up in the eye, resulting in vision problems; the loss of bone minerals and densities; cardiovascular deconditioning; and decreased endurance and muscle mass.[86]\n Radiation is an insidious health hazard to space travelers as it is invisible and can cause cancer. When above the Earth's magnetic field spacecraft are no longer protected from the sun's radiation; the danger of radiation is even more potent in deep space. The hazards of radiation can be ameliorated through protective shielding on the spacecraft, alerts, and dosimetry.[87]\n Fortunately, with new and rapidly evolving technological advancements, those in Mission Control are able to monitor the health of their astronauts more closely utilizing telemedicine. One may not be able to completely evade the physiological effects of space flight, but they can be mitigated. For example, medical systems aboard space vessels such as the International Space Station (ISS) are well equipped and designed to counteract the effects of lack of gravity and weightlessness; on-board treadmills can help prevent muscle loss and reduce the risk of developing premature osteoporosis.[85][87] Additionally, a crew medical officer is appointed for each ISS mission and a flight surgeon is available 24\/7 via the ISS Mission Control Center located in Houston, Texas.[87] Although the interactions are intended to take place in real time, communications between the space and terrestrial crew may become delayed – sometimes by as much as 20 minutes[87] – as their distance from each other increases when the spacecraft moves further out of LEO; because of this the crew are trained and need to be prepared to respond to any medical emergencies that may arise on the vessel as the ground crew are hundreds of miles away. As one can see, travelling and possibly living in space poses many challenges. Many past and current concepts for the continued exploration and colonization of space focus on a return to the Moon as a \"stepping stone\" to the other planets, especially Mars. At the end of 2006 NASA announced they were planning to build a permanent Moon base with continual presence by 2024.[88]\n Beyond the technical factors that could make living in space more widespread, it has been suggested that the lack of private property, the inability or difficulty in establishing property rights in space, has been an impediment to the development of space for human habitation. Since the advent of space technology in the latter half of the twentieth century, the ownership of property in space has been murky, with strong arguments both for and against. In particular, the making of national territorial claims in outer space and on celestial bodies has been specifically proscribed by the Outer Space Treaty, which had been, as of 2012[update], ratified by all spacefaring nations.[89]\nSpace colonization, also called space settlement and space humanization, would be the permanent autonomous (self-sufficient) human habitation of locations outside Earth, especially of natural satellites or planets such as the Moon or Mars, using significant amounts of in-situ resource utilization.\n Participation and representation of humanity in space is an issue ever since the first phase of space exploration.[90] Some rights of non-spacefaring countries have been mostly secured through international space law, declaring space the \"province of all mankind\", understanding spaceflight as its resource, though sharing of space for all humanity is still criticized as imperialist and lacking.[90] Additionally to international inclusion, the inclusion of women and people of colour has also been lacking. To reach a more inclusive spaceflight some organizations like the Justspace Alliance[90] and IAU featured Inclusive Astronomy[91] have been formed in recent years.\n The first woman to go to space was Valentina Tereshkova. She flew in 1963 but it was not until the 1980s that another woman entered space again. All astronauts were required to be military test pilots at the time and women were not able to join this career, this is one reason for the delay in allowing women to join space crews.[citation needed] After the rule changed, Svetlana Savitskaya became the second woman to go to space, she was also from the Soviet Union. Sally Ride became the next woman in space and the first woman to fly to space through the United States program.\n Since then, eleven other countries have allowed women astronauts. The first all-female space walk occurred in 2018, including Christina Koch and Jessica Meir. They had both previously participated in space walks with NASA. The first woman to go to the Moon is planned for 2024.\n Despite these developments women are still underrepresented among astronauts and especially cosmonauts. Issues that block potential applicants from the programs, and limit the space missions they are able to go on, include:\n Artistry in and from space ranges from signals, capturing and arranging material like Yuri Gagarin's selfie in space or the image The Blue Marble, over drawings like the first one in space by cosmonaut and artist Alexei Leonov, music videos like Chris Hadfield's cover of Space Oddity on board the ISS, to permanent installations on celestial bodies like on the Moon.\n Solar System → Local Interstellar Cloud → Local Bubble → Gould Belt → Orion Arm → Milky Way → Milky Way subgroup → Local Group → Local Sheet → Virgo Supercluster → Laniakea Supercluster → Local Hole → Observable universe → UniverseEach arrow (→) may be read as \"within\" or \"part of\".\n"}
{"key":"Space Exploration","link":"https:\/\/en.wikipedia.org\/wiki\/Space_exploration","headline":"Space exploration - Wikipedia","content":"\n Space exploration is the use of astronomy and space technology to explore outer space.[1] While the exploration of space is currently carried out mainly by astronomers with telescopes, its physical exploration is conducted both by uncrewed robotic space probes and human spaceflight. Space exploration, like its classical form astronomy, is one of the main sources for space science.\n While the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical space exploration to become a reality. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries.[2]\n The early era of space exploration was driven by a \"Space Race\" between the Soviet Union and the United States. A driving force of the start of space exploration was during the Cold War. After the ability to create nuclear weapons, the narrative of defense\/offense left land and the power to control the air became the focus. Both the Soviet and the U.S. were fighting to prove their superiority in technology through exploring the unknown: space. In fact, the reason NASA was made was due to the response of Sputnik I.[3] The launch of the first human-made object to orbit Earth, the Soviet Union's Sputnik 1, on 4 October 1957, and the first Moon landing by the American Apollo 11 mission on 20 July 1969 are often taken as landmarks for this initial period. The Soviet space program achieved many of the first milestones, including the first living being in orbit in 1957, the first human spaceflight (Yuri Gagarin aboard Vostok 1) in 1961, the first spacewalk (by Alexei Leonov) on 18 March 1965, the first automatic landing on another celestial body in 1966, and the launch of the first space station (Salyut 1) in 1971. \nAfter the first 20 years of exploration, focus shifted from one-off flights to renewable hardware, such as the Space Shuttle program, and from competition to cooperation as with the International Space Station (ISS).\n With the substantial completion of the ISS[4] following STS-133 in March 2011, plans for space exploration by the U.S. remain in flux. Constellation, a Bush administration program for a return to the Moon by 2020[5] was judged inadequately funded and unrealistic by an expert review panel reporting in 2009.[6] \nThe Obama administration proposed a revision of Constellation in 2010 to focus on the development of the capability for crewed missions beyond low Earth orbit (LEO), envisioning extending the operation of the ISS beyond 2020, transferring the development of launch vehicles for human crews from NASA to the private sector, and developing technology to enable missions to beyond LEO, such as Earth–Moon L1, the Moon, Earth–Sun L2, near-Earth asteroids, and Phobos or Mars orbit.[7]\n In the 2000s, China initiated a successful crewed spaceflight program while India launched Chandraayan 1, while the European Union and Japan have also planned future crewed space missions. China, Russia, and Japan have advocated crewed missions to the Moon during the 21st century, while the European Union has advocated crewed missions to both the Moon and Mars during the 20th and 21st century.\n The first telescope is said to have been invented in 1608 in the Netherlands by an eyeglass maker named Hans Lippershey, but their first recorded use in astronomy was by Galileo Galilei in 1609.[8] In 1668 Isaac Newton built his own reflecting telescope, the first fully functional telescope of this kind, and a landmark for future developments due to its superior features over the previous Galilean telescope.[9]\n A string of discoveries in the Solar System (and beyond) followed, then and in the next centuries: the mountains of the Moon, the phases of Venus, the main satellites of Jupiter and Saturn, the rings of Saturn, many comets, the asteroids, the new planets Uranus and Neptune, and many more satellites.\n The Orbiting Astronomical Observatory 2 was the first space telescope launched 1968,[10] but the launching of Hubble Space Telescope in 1990[11] set a milestone. As of 1 December 2022, there were 5,284 confirmed exoplanets discovered. The Milky Way is estimated to contain 100–400 billion stars[12] and more than 100 billion planets.[13] There are at least 2 trillion galaxies in the observable universe.[14][15] HD1 is the most distant known object from Earth, reported as 33.4 billion light-years away.[16][17][18][19][20][21]\n MW 18014 was a German V-2 rocket test launch that took place on 20 June 1944, at the Peenemünde Army Research Center in Peenemünde. It was the first man-made object to reach outer space, attaining an apogee of 176 kilometers,[22] which is well above the Kármán line.[23] It was a vertical test launch. Although the rocket reached space, it did not reach orbital velocity, and therefore returned to Earth in an impact, becoming the first sub-orbital spaceflight.[24]\n The first successful orbital launch was of the Soviet uncrewed Sputnik 1 (\"Satellite 1\") mission on 4 October 1957. The satellite weighed about 83 kg (183 lb), and is believed to have orbited Earth at a height of about 250 km (160 mi). It had two radio transmitters (20 and 40 MHz), which emitted \"beeps\" that could be heard by radios around the globe. Analysis of the radio signals was used to gather information about the electron density of the ionosphere, while temperature and pressure data was encoded in the duration of radio beeps. The results indicated that the satellite was not punctured by a meteoroid. Sputnik 1 was launched by an R-7 rocket. It burned up upon re-entry on 3 January 1958.\n The first successful human spaceflight was Vostok 1 (\"East 1\"), carrying the 27-year-old Russian cosmonaut, Yuri Gagarin, on 12 April 1961. The spacecraft completed one orbit around the globe, lasting about 1 hour and 48 minutes. Gagarin's flight resonated around the world; it was a demonstration of the advanced Soviet space program and it opened an entirely new era in space exploration: human spaceflight.\n The first artificial object to reach another celestial body was Luna 2 reaching the Moon in 1959.[25] The first soft landing on another celestial body was performed by Luna 9 landing on the Moon on 3 February 1966.[26] Luna 10 became the first artificial satellite of the Moon, entering in a lunar orbit on 3 April 1966.[27]\n The first crewed landing on another celestial body was performed by Apollo 11 on 20 July 1969, landing on the Moon. There have been a total of six spacecraft with humans landing on the Moon starting from 1969 to the last human landing in 1972.\n The first interplanetary flyby was the 1961 Venera 1 flyby of Venus, though the 1962 Mariner 2 was the first flyby of Venus to return data (closest approach 34,773 kilometers). Pioneer 6 was the first satellite to orbit the Sun, launched on 16 December 1965. The other planets were first flown by in 1965 for Mars by Mariner 4, 1973 for Jupiter by Pioneer 10, 1974 for Mercury by Mariner 10, 1979 for Saturn by Pioneer 11, 1986 for Uranus by Voyager 2, 1989 for Neptune by Voyager 2. In 2015, the dwarf planets Ceres and Pluto were orbited by Dawn and passed by New Horizons, respectively. This accounts for flybys of each of the eight planets in the Solar System, the Sun, the Moon, and Ceres and Pluto (two of the five recognized dwarf planets).\n The first interplanetary surface mission to return at least limited surface data from another planet was the 1970 landing of Venera 7, which returned data to Earth for 23 minutes from Venus. In 1975 the Venera 9 was the first to return images from the surface of another planet, returning images from Venus. In 1971 the Mars 3 mission achieved the first soft landing on Mars returning data for almost 20 seconds. Later much longer duration surface missions were achieved, including over six years of Mars surface operation by Viking 1 from 1975 to 1982 and over two hours of transmission from the surface of Venus by Venera 13 in 1982, the longest ever Soviet planetary surface mission. Venus and Mars are the two planets outside of Earth on which humans have conducted surface missions with uncrewed robotic spacecraft.\n Salyut 1 was the first space station of any kind, launched into low Earth orbit by the Soviet Union on 19 April 1971. The International Space Station is currently the largest and oldest of the 2 current fully functional space stations, inhabited continuously since the year 2000. The other, Tiangong space station built by China, is now fully crewed and operational.\n Voyager 1 became the first human-made object to leave the Solar System into interstellar space on 25 August 2012. The probe passed the heliopause at 121 AU to enter interstellar space.[28]\n The Apollo 13 flight passed the far side of the Moon at an altitude of 254 kilometers (158 miles; 137 nautical miles) above the lunar surface, and 400,171 km (248,655 mi) from Earth, marking the record for the farthest humans have ever traveled from Earth in 1970.\n As of 26 November 2022[update] Voyager 1 was at a distance of 159 AU (23.8 billion km; 14.8 billion mi) from Earth.[29] It is the most distant human-made object from Earth.[30]\n Starting in the mid-20th century probes and then human mission were sent into Earth orbit, and then on to the Moon. Also, probes were sent throughout the known Solar System, and into Solar orbit. Uncrewed spacecraft have been sent into orbit around Saturn, Jupiter, Mars, Venus, and Mercury by the 21st century, and the most distance active spacecraft, Voyager 1 and 2 traveled beyond 100 times the Earth-Sun distance. The instruments were enough though that it is thought they have left the Sun's heliosphere, a sort of bubble of particles made in the Galaxy by the Sun's solar wind.\n The Sun is a major focus of space exploration. Being above the atmosphere in particular and Earth's magnetic field gives access to the solar wind and infrared and ultraviolet radiations that cannot reach Earth's surface. The Sun generates most space weather, which can affect power generation and transmission systems on Earth and interfere with, and even damage, satellites and space probes. Numerous spacecraft dedicated to observing the Sun, beginning with the Apollo Telescope Mount, have been launched and still others have had solar observation as a secondary objective. Parker Solar Probe, launched in 2018, will approach the Sun to within 1\/9th the orbit of Mercury.\n Mercury remains the least explored of the Terrestrial planets. As of May 2013, the Mariner 10 and MESSENGER missions have been the only missions that have made close observations of Mercury. MESSENGER entered orbit around Mercury in March 2011, to further investigate the observations made by Mariner 10 in 1975 (Munsell, 2006b).\nA third mission to Mercury, scheduled to arrive in 2025, BepiColombo is to include two probes. BepiColombo is a joint mission between Japan and the European Space Agency. MESSENGER and BepiColombo are intended to gather complementary data to help scientists understand many of the mysteries discovered by Mariner 10's flybys.\n Flights to other planets within the Solar System are accomplished at a cost in energy, which is described by the net change in velocity of the spacecraft, or delta-v. Due to the relatively high delta-v to reach Mercury and its proximity to the Sun, it is difficult to explore and orbits around it are rather unstable.\n Venus was the first target of interplanetary flyby and lander missions and, despite one of the most hostile surface environments in the Solar System, has had more landers sent to it (nearly all from the Soviet Union) than any other planet in the Solar System. The first flyby was the 1961 Venera 1, though the 1962 Mariner 2 was the first flyby to successfully return data. Mariner 2 has been followed by several other flybys by multiple space agencies often as part of missions using a Venus flyby to provide a gravitational assist en route to other celestial bodies. In 1967 Venera 4 became the first probe to enter and directly examine the atmosphere of Venus. In 1970, Venera 7 became the first successful lander to reach the surface of Venus and by 1985 it had been followed by eight additional successful Soviet Venus landers which provided images and other direct surface data. Starting in 1975 with the Soviet orbiter Venera 9 some ten successful orbiter missions have been sent to Venus, including later missions which were able to map the surface of Venus using radar to pierce the obscuring atmosphere.\n Space exploration has been used as a tool to understand Earth as a celestial object. Orbital missions can provide data for Earth that can be difficult or impossible to obtain from a purely ground-based point of reference.\n For example, the existence of the Van Allen radiation belts was unknown until their discovery by the United States' first artificial satellite, Explorer 1. These belts contain radiation trapped by Earth's magnetic fields, which currently renders construction of habitable space stations above 1000 km impractical.\nFollowing this early unexpected discovery, a large number of Earth observation satellites have been deployed specifically to explore Earth from a space-based perspective. These satellites have significantly contributed to the understanding of a variety of Earth-based phenomena. For instance, the hole in the ozone layer was found by an artificial satellite that was exploring Earth's atmosphere, and satellites have allowed for the discovery of archeological sites or geological formations that were difficult or impossible to otherwise identify.\n The Moon was the first celestial body to be the object of space exploration. It holds the distinctions of being the first remote celestial object to be flown by, orbited, and landed upon by spacecraft, and the only remote celestial object ever to be visited by humans.\n In 1959 the Soviets obtained the first images of the far side of the Moon, never previously visible to humans. The U.S. exploration of the Moon began with the Ranger 4 impactor in 1962. Starting in 1966 the Soviets successfully deployed a number of landers to the Moon which were able to obtain data directly from the Moon's surface; just four months later, Surveyor 1 marked the debut of a successful series of U.S. landers. The Soviet uncrewed missions culminated in the Lunokhod program in the early 1970s, which included the first uncrewed rovers and also successfully brought lunar soil samples to Earth for study. This marked the first (and to date the only) automated return of extraterrestrial soil samples to Earth. Uncrewed exploration of the Moon continues with various nations periodically deploying lunar orbiters, and in 2008 the Indian Moon Impact Probe and in 2023 the Chandrayaan-3 of India became the first spacecraft to land on the lunar south pole.\n Crewed exploration of the Moon began in 1968 with the Apollo 8 mission that successfully orbited the Moon, the first time any extraterrestrial object was orbited by humans. In 1969, the Apollo 11 mission marked the first time humans set foot upon another world. Crewed exploration of the Moon did not continue for long. The Apollo 17 mission in 1972 marked the sixth landing and the most recent human visit. Artemis 2 is scheduled to complete a crewed flyby of the Moon in 2025, and Artemis 3 will perform the first lunar landing since Apollo 17 with it scheduled for launch no earlier than 2026. Robotic missions are still pursued vigorously.\n The exploration of Mars has been an important part of the space exploration programs of the Soviet Union (later Russia), the United States, Europe, Japan and India. Dozens of robotic spacecraft, including orbiters, landers, and rovers, have been launched toward Mars since the 1960s. These missions were aimed at gathering data about current conditions and answering questions about the history of Mars. The questions raised by the scientific community are expected to not only give a better appreciation of the Red Planet but also yield further insight into the past, and possible future, of Earth.\n The exploration of Mars has come at a considerable financial cost with roughly two-thirds of all spacecraft destined for Mars failing before completing their missions, with some failing before they even began. Such a high failure rate can be attributed to the complexity and large number of variables involved in an interplanetary journey, and has led researchers to jokingly speak of The Great Galactic Ghoul[31] which subsists on a diet of Mars probes. This phenomenon is also informally known as the \"Mars Curse\".[32]  In contrast to overall high failure rates in the exploration of Mars, India has become the first country to achieve success of its maiden attempt.  India's Mars Orbiter Mission (MOM)[33][34][35] is one of the least expensive interplanetary missions ever undertaken with an approximate total cost of ₹ 450 Crore (US$73 million).[36][37] The first mission to Mars by any Arab country has been taken up by the United Arab Emirates. Called the Emirates Mars Mission, it was launched on 19 July 2020 and went into orbit around Mars on 9 February 2021. The uncrewed exploratory probe was named \"Hope Probe\" and was sent to Mars to study its atmosphere in detail.[38]\n The Russian space mission Fobos-Grunt, which launched on 9 November 2011 experienced a failure leaving it stranded in low Earth orbit.[39] It was to begin exploration of the Phobos and Martian circumterrestrial orbit, and study whether the moons of Mars, or at least Phobos, could be a \"trans-shipment point\" for spaceships traveling to Mars.[40]\n Until the advent of space travel, objects in the asteroid belt were merely pinpricks of light in even the largest telescopes, their shapes and terrain remaining a mystery.\nSeveral asteroids have now been visited by probes, the first of which was Galileo, which flew past two: 951 Gaspra in 1991, followed by 243 Ida in 1993. Both of these lay near enough to Galileo's planned trajectory to Jupiter that they could be visited at acceptable cost. The first landing on an asteroid was performed by the NEAR Shoemaker probe in 2000, following an orbital survey of the object, 433 Eros. The dwarf planet Ceres and the asteroid 4 Vesta, two of the three largest asteroids, were visited by NASA's Dawn spacecraft, launched in 2007.\n Hayabusa was a robotic spacecraft developed by the Japan Aerospace Exploration Agency to return a sample of material from the small near-Earth asteroid 25143 Itokawa to Earth for further analysis. Hayabusa was launched on 9 May 2003 and rendezvoused with Itokawa in mid-September 2005. After arriving at Itokawa, Hayabusa studied the asteroid's shape, spin, topography, color, composition, density, and history. In November 2005, it landed on the asteroid twice to collect samples. The spacecraft returned to Earth on 13 June 2010.\n The exploration of Jupiter has consisted solely of a number of automated NASA spacecraft visiting the planet since 1973. A large majority of the missions have been \"flybys\", in which detailed observations are taken without the probe landing or entering orbit; such as in Pioneer and Voyager programs. The Galileo and Juno spacecraft are the only spacecraft to have entered the planet's orbit. As Jupiter is believed to have only a relatively small rocky core and no real solid surface, a landing mission is precluded.\n Reaching Jupiter from Earth requires a delta-v of 9.2 km\/s,[41] which is comparable to the 9.7 km\/s delta-v needed to reach low Earth orbit.[42] Fortunately, gravity assists through planetary flybys can be used to reduce the energy required at launch to reach Jupiter, albeit at the cost of a significantly longer flight duration.[41]\n Jupiter has 95 known moons, many of which have relatively little known information about them.\n Saturn has been explored only through uncrewed spacecraft launched by NASA, including one mission (Cassini–Huygens) planned and executed in cooperation with other space agencies. These missions consist of flybys in 1979 by Pioneer 11, in 1980 by Voyager 1, in 1982 by Voyager 2 and an orbital mission by the Cassini spacecraft, which lasted from 2004 until 2017.\n Saturn has at least 62 known moons, although the exact number is debatable since Saturn's rings are made up of vast numbers of independently orbiting objects of varying sizes. The largest of the moons is Titan, which holds the distinction of being the only moon in the Solar System with an atmosphere denser and thicker than that of Earth.  Titan holds the distinction of being the only object in the Outer Solar System that has been explored with a lander, the Huygens probe deployed by the Cassini spacecraft.\n The exploration of Uranus has been entirely through the Voyager 2 spacecraft, with no other visits currently planned. Given its axial tilt of 97.77°, with its polar regions exposed to sunlight or darkness for long periods, scientists were not sure what to expect at Uranus. The closest approach to Uranus occurred on 24 January 1986. Voyager 2 studied the planet's unique atmosphere and magnetosphere. Voyager 2 also examined its ring system and the moons of Uranus including all five of the previously known moons, while discovering an additional ten previously unknown moons.\n Images of Uranus proved to have a very uniform appearance, with no evidence of the dramatic storms or atmospheric banding evident on Jupiter and Saturn. Great effort was required to even identify a few clouds in the images of the planet. The magnetosphere of Uranus, however, proved to be unique, being profoundly affected by the planet's unusual axial tilt. In contrast to the bland appearance of Uranus itself, striking images were obtained of the Moons of Uranus, including evidence that Miranda had been unusually geologically active.\n The exploration of Neptune began with 25 August 1989 Voyager 2 flyby, the sole visit to the system as of 2024. The possibility of a Neptune Orbiter has been discussed, but no other missions have been given serious thought.\n Although the extremely uniform appearance of Uranus during Voyager 2's visit in 1986 had led to expectations that Neptune would also have few visible atmospheric phenomena, the spacecraft found that Neptune had obvious banding, visible clouds, auroras, and even a conspicuous anticyclone storm system rivaled in size only by Jupiter's Great Red Spot. Neptune also proved to have the fastest winds of any planet in the Solar System, measured as high as 2,100 km\/h.[43] Voyager 2 also examined Neptune's ring and moon system. It discovered 900 complete rings and additional partial ring \"arcs\" around Neptune. In addition to examining Neptune's three previously known moons, Voyager 2 also discovered five previously unknown moons, one of which, Proteus, proved to be the last largest moon in the system. Data from Voyager 2 supported the view that Neptune's largest moon, Triton, is a captured Kuiper belt object.[44]\n The dwarf planet Pluto presents significant challenges for spacecraft because of its great distance from Earth (requiring high velocity for reasonable trip times) and small mass (making capture into orbit very difficult at present). Voyager 1 could have visited Pluto, but controllers opted instead for a close flyby of Saturn's moon Titan, resulting in a trajectory incompatible with a Pluto flyby. Voyager 2 never had a plausible trajectory for reaching Pluto.[45]\n After an intense political battle, a mission to Pluto dubbed New Horizons was granted funding from the United States government in 2003.[46] New Horizons was launched successfully on 19 January 2006. In early 2007 the craft made use of a gravity assist from Jupiter. Its closest approach to Pluto was on 14 July 2015; scientific observations of Pluto began five months prior to closest approach and continued for 16 days after the encounter.\n The New Horizons mission also did a flyby of the small planetesimal Arrokoth, in the Kuiper belt, in 2019. This was its first extended mission.[47]\n Although many comets have been studied from Earth sometimes with centuries-worth of observations, only a few comets have been closely visited. In 1985, the International Cometary Explorer conducted the first comet fly-by (21P\/Giacobini-Zinner) before joining the Halley Armada studying the famous comet. The Deep Impact probe smashed into 9P\/Tempel to learn more about its structure and composition and the Stardust mission returned samples of another comet's tail. The Philae lander successfully landed on Comet Churyumov–Gerasimenko in 2014 as part of the broader Rosetta mission.\n Deep space exploration is the branch of astronomy, astronautics and space technology that is involved with the exploration of distant regions of outer space.[48] Physical exploration of space is conducted both by human spaceflights (deep-space astronautics) and by robotic spacecraft.\n Some of the best candidates for future deep space engine technologies include anti-matter, nuclear power and beamed propulsion.[49] The latter, beamed propulsion, appears to be the best candidate for deep space exploration presently available, since it uses known physics and known technology that is being developed for other purposes.[50]\n Breakthrough Starshot is a research and engineering project by the Breakthrough Initiatives to develop a proof-of-concept fleet of light sail spacecraft named StarChip,[51] to be capable of making the journey to the Alpha Centauri star system 4.37 light-years away. It was founded in 2016 by Yuri Milner, Stephen Hawking, and Mark Zuckerberg.[52][53]\n An article in science magazine Nature suggested the use of asteroids as a gateway for space exploration, with the ultimate destination being Mars. In order to make such an approach viable, three requirements need to be fulfilled: first, \"a thorough asteroid survey to find thousands of nearby bodies suitable for astronauts to visit\"; second, \"extending flight duration and distance capability to ever-increasing ranges out to Mars\"; and finally, \"developing better robotic vehicles and tools to enable astronauts to explore an asteroid regardless of its size, shape or spin\". Furthermore, using asteroids would provide astronauts with protection from galactic cosmic rays, with mission crews being able to land on them without great risk to radiation exposure.\n The James Webb Space Telescope (JWST or \"Webb\") is a space telescope that is the successor to the Hubble Space Telescope.[54][55] The JWST will provide greatly improved resolution and sensitivity over the Hubble, and will enable a broad range of investigations across the fields of astronomy and cosmology, including observing some of the most distant events and objects in the universe, such as the formation of the first galaxies. Other goals include understanding the formation of stars and planets, and direct imaging of exoplanets and novas.[56]\n The primary mirror of the JWST, the Optical Telescope Element, is composed of 18 hexagonal mirror segments made of gold-plated beryllium which combine to create a 6.5-meter (21 ft; 260 in) diameter mirror that is much larger than the Hubble's 2.4-meter (7.9 ft; 94 in) mirror.  Unlike the Hubble, which observes in the near ultraviolet, visible, and near infrared (0.1 to 1 μm) spectra, the JWST will observe in a lower frequency range, from long-wavelength visible light through mid-infrared (0.6 to 27 μm), which will allow it to observe high redshift objects that are too old and too distant for the Hubble to observe.[57] The telescope must be kept very cold in order to observe in the infrared without interference, so it will be deployed in space near the Earth–Sun L2 Lagrangian point, and a large sunshield made of silicon- and aluminum-coated Kapton will keep its mirror and instruments below 50 K (−220 °C; −370 °F).[58]\n The Artemis program is an ongoing crewed spaceflight program carried out by NASA, U.S. commercial spaceflight companies, and international partners such as ESA,[59] with the goal of landing \"the first woman and the next man\" on the Moon, specifically at the lunar south pole region by 2024. Artemis would be the next step towards the long-term goal of establishing a sustainable presence on the Moon, laying the foundation for private companies to build a lunar economy, and eventually sending humans to Mars.\n In 2017, the lunar campaign was authorized by Space Policy Directive 1, utilizing various ongoing spacecraft programs such as Orion, the Lunar Gateway, Commercial Lunar Payload Services, and adding an undeveloped crewed lander. The Space Launch System will serve as the primary launch vehicle for Orion, while commercial launch vehicles are planned for use to launch various other elements of the campaign.[60] NASA requested $1.6 billion in additional funding for Artemis for fiscal year 2020,[61] while the Senate Appropriations Committee requested from NASA a five-year budget profile[62] which is needed for evaluation and approval by Congress.[63][64]\n The research that is conducted by national space exploration agencies, such as NASA and Roscosmos, is one of the reasons supporters cite to justify government expenses. Economic analyses of the NASA programs often showed ongoing economic benefits (such as NASA spin-offs), generating many times the revenue of the cost of the program.[65] It is also argued that space exploration would lead to the extraction of resources on other planets and especially asteroids, which contain billions of dollars worth of minerals and metals. Such expeditions could generate a lot of revenue.[66] In addition, it has been argued that space exploration programs help inspire youth to study in science and engineering.[67] Space exploration also gives scientists the ability to perform experiments in other settings and expand humanity's knowledge.[68]\n Another claim is that space exploration is a necessity to mankind and that staying on Earth will lead to extinction. Some of the reasons are lack of natural resources, comets, nuclear war, and worldwide epidemic. Stephen Hawking, renowned British theoretical physicist, said that \"I don't think the human race will survive the next thousand years, unless we spread into space. There are too many accidents that can befall life on a single planet. But I'm an optimist. We will reach out to the stars.\"[69] Arthur C. Clarke (1950) presented a summary of motivations for the human exploration of space in his non-fiction semi-technical monograph Interplanetary Flight.[70] He argued that humanity's choice is essentially between expansion off Earth into space, versus cultural (and eventually biological) stagnation and death.\nThese motivations could be attributed to one of the first rocket scientists in NASA, Wernher von Braun, and his vision of humans moving beyond Earth. The basis of this plan was to:\n Develop multi-stage rockets capable of placing satellites, animals, and humans in space.\n Development of large, winged reusable spacecraft capable of carrying humans and equipment into Earth orbit in a way that made space access routine and cost-effective.\n Construction of a large, permanently occupied space station to be used as a platform both to observe Earth and from which to launch deep space expeditions.\n Launching the first human flights around the Moon, leading to the first landings of humans on the Moon, with the intent of exploring that body and establishing permanent lunar bases.\n \nAssembly and fueling of spaceships in Earth orbit for the purpose of sending humans to Mars with the intent of eventually colonizing that planet.[71] Known as the Von Braun Paradigm, the plan was formulated to lead humans in the exploration of space. Von Braun's vision of human space exploration served as the model for efforts in space exploration well into the twenty-first century, with NASA incorporating this approach into the majority of their projects.[71] The steps were followed out of order, as seen by the Apollo program reaching the moon before the space shuttle program was started, which in turn was used to complete the International Space Station. Von Braun's Paradigm formed NASA's drive for human exploration, in the hopes that humans discover the far reaches of the universe.\n NASA has produced a series of public service announcement videos supporting the concept of space exploration.[72]\n Overall, the public remains largely supportive of both crewed and uncrewed space exploration. According to an Associated Press Poll conducted in July 2003, 71% of U.S. citizens agreed with the statement that the space program is \"a good investment\", compared to 21% who did not.[73]\n Space advocacy and space policy[74] regularly invokes exploration as a human nature.[75]\n Spaceflight is the use of space technology to achieve the flight of spacecraft into and through outer space.\n Spaceflight is used in space exploration, and also in commercial activities like space tourism and satellite telecommunications. Additional non-commercial uses of spaceflight include space observatories, reconnaissance satellites and other Earth observation satellites.\n A spaceflight typically begins with a rocket launch, which provides the initial thrust to overcome the force of gravity and propels the spacecraft from the surface of Earth. Once in space, the motion of a spacecraft—both when unpropelled and when under propulsion—is covered by the area of study called astrodynamics. Some spacecraft remain in space indefinitely, some disintegrate during atmospheric reentry, and others reach a planetary or lunar surface for landing or impact.\n Satellites are used for a large number of purposes. Common types include military (spy) and civilian Earth observation satellites, communication satellites, navigation satellites, weather satellites, and research satellites. Space stations and human spacecraft in orbit are also satellites.\n The commercialization of space first started out with the launching of private satellites by NASA or other space agencies. Current examples of the commercial satellite use of space include satellite navigation systems, satellite television and satellite radio. The next step of commercialization of space was seen as human spaceflight. Flying humans safely to and from space had become routine to NASA.[76] Reusable spacecraft were an entirely new engineering challenge, something only seen in novels and films like Star Trek and War of the Worlds. Great names like Buzz Aldrin supported the use of making a reusable vehicle like the space shuttle. Aldrin held that reusable spacecraft were the key in making space travel affordable, stating that the use of \"passenger space travel is a huge potential market big enough to justify the creation of reusable launch vehicles\".[77] How can the public go against the words of one of America's best known heroes in space exploration? After all exploring space is the next great expedition, following the example of Lewis and Clark.Space tourism is the next step reusable vehicles in the commercialization of space. The purpose of this form of space travel is used by individuals for the purpose of personal pleasure.\n Private spaceflight companies such as SpaceX and Blue Origin, and commercial space stations such as the Axiom Space and the Bigelow Commercial Space Station have dramatically changed the landscape of space exploration, and will continue to do so in the near future.\n Astrobiology is the interdisciplinary study of life in the universe, combining aspects of astronomy, biology and geology.[78] It is focused primarily on the study of the origin, distribution and evolution of life. It is also known as exobiology (from Greek: έξω, exo, \"outside\").[79][80][81] The term \"Xenobiology\" has been used as well, but this is technically incorrect because its terminology means \"biology of the foreigners\".[82] Astrobiologists must also consider the possibility of life that is chemically entirely distinct from any life found on Earth.[83] In the Solar System some of the prime locations for current or past astrobiology are on Enceladus, Europa, Mars, and Titan.[84]\n To date, the longest human occupation of space is the International Space Station which has been in continuous use for 23 years, 140 days. Valeri Polyakov's record single spaceflight of almost 438 days aboard the Mir space station has not been surpassed. The health effects of space have been well documented through years of research conducted in the field of aerospace medicine. Analog environments similar to those one may experience in space travel (like deep sea submarines) have been used in this research to further explore the relationship between isolation and extreme environments.[85] It is imperative that the health of the crew be maintained as any deviation from baseline may compromise the integrity of the mission as well as the safety of the crew, hence the reason why astronauts must endure rigorous medical screenings and tests prior to embarking on any missions. However, it does not take long for the environmental dynamics of spaceflight to commence its toll on the human body; for example, space motion sickness (SMS) – a condition which affects the neurovestibular system and culminates in mild to severe signs and symptoms such as vertigo, dizziness, fatigue, nausea, and disorientation – plagues almost all space travelers within their first few days in orbit.[85] Space travel can also have a profound impact on the psyche of the crew members as delineated in anecdotal writings composed after their retirement. Space travel can adversely affect the body's natural biological clock (circadian rhythm); sleep patterns causing sleep deprivation and fatigue; and social interaction; consequently, residing in a Low Earth Orbit (LEO) environment for a prolonged amount of time can result in both mental and physical exhaustion.[85] Long-term stays in space reveal issues with bone and muscle loss in low gravity, immune system suppression, and radiation exposure. The lack of gravity causes fluid to rise upward which can cause pressure to build up in the eye, resulting in vision problems; the loss of bone minerals and densities; cardiovascular deconditioning; and decreased endurance and muscle mass.[86]\n Radiation is an insidious health hazard to space travelers as it is invisible and can cause cancer. When above the Earth's magnetic field spacecraft are no longer protected from the sun's radiation; the danger of radiation is even more potent in deep space. The hazards of radiation can be ameliorated through protective shielding on the spacecraft, alerts, and dosimetry.[87]\n Fortunately, with new and rapidly evolving technological advancements, those in Mission Control are able to monitor the health of their astronauts more closely utilizing telemedicine. One may not be able to completely evade the physiological effects of space flight, but they can be mitigated. For example, medical systems aboard space vessels such as the International Space Station (ISS) are well equipped and designed to counteract the effects of lack of gravity and weightlessness; on-board treadmills can help prevent muscle loss and reduce the risk of developing premature osteoporosis.[85][87] Additionally, a crew medical officer is appointed for each ISS mission and a flight surgeon is available 24\/7 via the ISS Mission Control Center located in Houston, Texas.[87] Although the interactions are intended to take place in real time, communications between the space and terrestrial crew may become delayed – sometimes by as much as 20 minutes[87] – as their distance from each other increases when the spacecraft moves further out of LEO; because of this the crew are trained and need to be prepared to respond to any medical emergencies that may arise on the vessel as the ground crew are hundreds of miles away. As one can see, travelling and possibly living in space poses many challenges. Many past and current concepts for the continued exploration and colonization of space focus on a return to the Moon as a \"stepping stone\" to the other planets, especially Mars. At the end of 2006 NASA announced they were planning to build a permanent Moon base with continual presence by 2024.[88]\n Beyond the technical factors that could make living in space more widespread, it has been suggested that the lack of private property, the inability or difficulty in establishing property rights in space, has been an impediment to the development of space for human habitation. Since the advent of space technology in the latter half of the twentieth century, the ownership of property in space has been murky, with strong arguments both for and against. In particular, the making of national territorial claims in outer space and on celestial bodies has been specifically proscribed by the Outer Space Treaty, which had been, as of 2012[update], ratified by all spacefaring nations.[89]\nSpace colonization, also called space settlement and space humanization, would be the permanent autonomous (self-sufficient) human habitation of locations outside Earth, especially of natural satellites or planets such as the Moon or Mars, using significant amounts of in-situ resource utilization.\n Participation and representation of humanity in space is an issue ever since the first phase of space exploration.[90] Some rights of non-spacefaring countries have been mostly secured through international space law, declaring space the \"province of all mankind\", understanding spaceflight as its resource, though sharing of space for all humanity is still criticized as imperialist and lacking.[90] Additionally to international inclusion, the inclusion of women and people of colour has also been lacking. To reach a more inclusive spaceflight some organizations like the Justspace Alliance[90] and IAU featured Inclusive Astronomy[91] have been formed in recent years.\n The first woman to go to space was Valentina Tereshkova. She flew in 1963 but it was not until the 1980s that another woman entered space again. All astronauts were required to be military test pilots at the time and women were not able to join this career, this is one reason for the delay in allowing women to join space crews.[citation needed] After the rule changed, Svetlana Savitskaya became the second woman to go to space, she was also from the Soviet Union. Sally Ride became the next woman in space and the first woman to fly to space through the United States program.\n Since then, eleven other countries have allowed women astronauts. The first all-female space walk occurred in 2018, including Christina Koch and Jessica Meir. They had both previously participated in space walks with NASA. The first woman to go to the Moon is planned for 2024.\n Despite these developments women are still underrepresented among astronauts and especially cosmonauts. Issues that block potential applicants from the programs, and limit the space missions they are able to go on, include:\n Artistry in and from space ranges from signals, capturing and arranging material like Yuri Gagarin's selfie in space or the image The Blue Marble, over drawings like the first one in space by cosmonaut and artist Alexei Leonov, music videos like Chris Hadfield's cover of Space Oddity on board the ISS, to permanent installations on celestial bodies like on the Moon.\n Solar System → Local Interstellar Cloud → Local Bubble → Gould Belt → Orion Arm → Milky Way → Milky Way subgroup → Local Group → Local Sheet → Virgo Supercluster → Laniakea Supercluster → Local Hole → Observable universe → UniverseEach arrow (→) may be read as \"within\" or \"part of\".\n"}
{"key":"Space Exploration","link":"https:\/\/en.wikipedia.org\/wiki\/SpaceX","headline":"SpaceX - Wikipedia","content":"\n Space Exploration Technologies Corp. commonly referred to as SpaceX, is an American spacecraft manufacturer, launch service provider, defense contractor and satellite communications company headquartered in Hawthorne, California. The company was founded in 2002 by Elon Musk with the goal of reducing space transportation costs and ultimately developing a sustainable colony on Mars. The company currently operates the Falcon 9 and Falcon Heavy rockets along with the Dragon and Starship spacecraft.\n The company offers internet service via its Starlink satellites, which became the largest-ever satellite constellation in January 2020 and as of November 2023 comprised more than 5,000 small satellites in orbit.[7]\n Meanwhile, the company is developing Starship, a human-rated, fully-reusable, super heavy-lift launch system for interplanetary and orbital spaceflight. On its first flight in April 2023, it became the largest and most powerful rocket ever flown. The rocket reached space on its second flight that took place in November 2023.\n SpaceX is the first private company to develop a liquid-propellant rocket that has reached orbit; to launch, orbit, and recover a spacecraft; to send a spacecraft to the International Space Station; and to send astronauts to the International Space Station. It is also the first organization of any type to achieve a vertical propulsive landing of an orbital rocket booster and the first to reuse such a booster. The company's Falcon 9 rockets have landed and reflown more than 200 times.[8] As of December 2023, SpaceX has around $180 billion valuation.[9][10]\n In early 2001, Elon Musk met Robert Zubrin and donated US$100,000 to his Mars Society, joining its board of directors for a short time.[11]: 30–31  He gave a plenary talk at their fourth convention where he announced Mars Oasis, a project to land a greenhouse and grow plants on Mars.[12][13] Musk initially attempted to acquire a Dnepr ICBM for the project through Russian contacts from Jim Cantrell.[14]\n When Musk returned to Moscow, Russia, with Michael Griffin, they found the Russians increasingly unreceptive.[15][16] On the flight home Musk announced he could start a company to build the affordable rockets they needed instead.[16] By applying vertical integration,[15] using cheap commercial off-the-shelf components when possible,[16] and adopting the modular approach of modern software engineering, Musk believed SpaceX could significantly cut launch price.[16] Griffin would later be appointed NASA administrator,[17] and play a part in the formation of the COTS program.[18][19]\n In early 2002, Elon Musk started to look for staff for his company, soon to be named SpaceX. Musk approached five people for the initial positions at the fledgling company, including Michael Griffin who was offered the position of Chief Engineer, Jim Cantrel and John Garvey (Cantrel and Garvey would later found the company Vector Launch), rocket engineer Tom Mueller, and Chris Thompson.[20][21] SpaceX was first headquartered in a warehouse in El Segundo, California. Early SpaceX employees, such as Tom Mueller (CTO), Gwynne Shotwell (COO), and Chris Thompson (VP of Operations), came from neighboring TRW and Boeing corporations. By November 2005, the company had 160 employees.[22] Musk personally interviewed and approved all of SpaceX's early employees.[23]\nMusk has stated that one of his goals with SpaceX is to decrease the cost and improve the reliability of access to space, ultimately by a factor of ten.[24]\n SpaceX developed its first orbital launch vehicle, the Falcon 1, with internal funding.[25][26]\nThe Falcon 1 was an expendable two-stage-to-orbit small-lift launch vehicle.\nThe total development cost of Falcon 1 was approximately $90 million[27] to $100 million.[28] The Falcon rocket series was named after Star Wars's Millennium Falcon fictional spacecraft.[29]\n In 2004, SpaceX protested against NASA to the Government Accountability Office (GAO) because of a sole-source contract awarded to Kistler Aerospace. Before the GAO could respond, NASA withdrew the contract, and formed the COTS program.[30][31] In 2005, SpaceX announced plans to pursue a human-rated commercial space program through the end of the decade, a program that would later become the Dragon spacecraft.[32]\nIn 2006, the company was selected by NASA and awarded $396 million to provide crew and cargo resupply demonstration contracts to the ISS under the COTS program.[33]\n The first two Falcon 1 launches were purchased by the United States Department of Defense under the DARPA Falcon Project which evaluated new US launch vehicles suitable for use in hypersonic missile delivery for Prompt Global Strike.[26][34][35] The first three launches of the rocket, between 2006 and 2008, all resulted in failures, which almost ended the company. Financing for Tesla Motors had failed, as well,[36] and consequently Tesla, SolarCity, and Musk personally were all nearly bankrupt at the same time.[37] Musk was reportedly \"waking from nightmares, screaming and in physical pain\" because of the stress.[38]\n The financial situation started to turn around with the first successful launch achieved on the fourth attempt on 28 September 2008. Musk split his remaining $30 million between SpaceX and Tesla, and NASA awarded the first Commercial Resupply Services contract awarding $1.6 billion to SpaceX in December, thus financially saving the company.[39] Based on these factors and the further business operations they enabled, the Falcon 1 was soon retired following its second successful, and fifth total, launch in July 2009; this allowed SpaceX to focus company resources on the development of a larger orbital rocket, the Falcon 9.[40] Gwynne Shotwell was also promoted to company president at this time, for her role in successfully negotiating the CRS contract.[41]\n SpaceX originally intended to follow its light Falcon 1 launch vehicle with an intermediate capacity vehicle, the Falcon 5.[42] The company instead decided in 2005 to proceed with the development of the Falcon 9, a reusable heavier lift vehicle. Development of the Falcon 9 was accelerated by NASA, which committed to purchasing several commercial flights if specific capabilities were demonstrated. This started with seed money from the Commercial Orbital Transportation Services (COTS) program in 2006.[43]\nThe overall contract award was $278 million to provide development funding for the Dragon spacecraft, Falcon 9, and demonstration launches of Falcon 9 with Dragon.[43]\nAs part of this contract, the Falcon 9 launched for the first time in June 2010 with the Dragon Spacecraft Qualification Unit, using a mockup of the Dragon spacecraft.\n The first operational Dragon spacecraft was launched in December 2010 aboard COTS Demo Flight 1, the Falcon 9's second flight, and safely returned to Earth after two orbits, completing all its mission objectives.[44] By December 2010, the SpaceX production line was manufacturing one Falcon 9 and Dragon every three months.[45]\n In April 2011, as part of its second-round Commercial Crew Development (CCDev) program, NASA issued a $75 million contract for SpaceX to develop an integrated launch escape system for Dragon in preparation for human-rating it as a crew transport vehicle to the ISS.[46] NASA awarded SpaceX a fixed-price Space Act Agreement (SAA) to produce a detailed design of the crew transportation system in August 2012.[47]\n In early 2012, approximately two-thirds of SpaceX stock was owned by Musk[48] and his seventy million shares were then estimated to be worth $875 million on private markets,[49] valuing SpaceX at $1.3 billion.[50] In May 2012, with the Dragon C2+ launch Dragon became the first commercial spacecraft to deliver cargo to the International Space Station.[51]\nAfter the flight, the company private equity valuation nearly doubled to $2.4 billion or $20\/share.[52][53] By that time, SpaceX had operated on total funding of approximately $1 billion over its first decade of operation. Of this, private equity provided approximately $200 million, with Musk investing approximately $100 million and other investors having put in about $100 million.[54]\n SpaceX's active reusability test program began in late 2012 with testing low-altitude, low-speed aspects of the landing technology.[55] The Falcon 9 prototypes performed vertical takeoffs and landings (VTOL). High-velocity, high-altitude tests of the booster atmospheric return technology began in late 2013.[55]\n SpaceX launched the first commercial mission for a private customer in 2013. In 2014, SpaceX won nine contracts out of the 20 that were openly competed worldwide.[56]\nThat year Arianespace requested that European governments provide additional subsidies to face the competition from SpaceX.[57][58]\nBeginning in 2014, SpaceX capabilities and pricing also began to affect the market for launch of U.S. military payloads, which for nearly a decade had been dominated by the large U.S. launch provider United Launch Alliance (ULA).[59]\nThe monopoly had allowed launch costs by the U.S. provider to rise to over $400 million over the years.[60]\nIn September 2014, NASA's Director of Commercial Spaceflight, Kevin Crigler, awarded SpaceX the Commercial Crew Transportation Capability (CCtCap) contract to finalize the development of the Crew Transportation System. The contract included several technical and certification milestones, an uncrewed flight test, a crewed flight test, and six operational missions after certification.[47]\n In January 2015, SpaceX raised $1 billion in funding from Google and Fidelity, in exchange for 8.33% of the company, establishing the company valuation at approximately $12 billion.[61]\nThe same month SpaceX announced the development of a new satellite constellation, called Starlink, to provide global broadband internet service with 4,000 satellites.[62]\n The Falcon 9 had its first major failure in late June 2015, when the seventh ISS resupply mission, CRS-7 exploded two minutes into the flight. The problem was traced to a failed 2-foot-long steel strut that held a helium pressure vessel, which broke free due to the force of acceleration. This caused a breach and allowed high-pressure helium to escape into the low-pressure propellant tank, causing the failure.[63]\n SpaceX first achieved a successful landing and recovery of a first stage in December 2015 with Falcon 9 Flight 20.[64]\nIn April 2016, the company achieved the first successful landing on the autonomous spaceport drone ship (ASDS) Of Course I Still Love You in the Atlantic Ocean.[65]\nBy October 2016, following the successful landings, SpaceX indicated they were offering their customers a 10% price discount if they choose to fly their payload on a reused Falcon 9 first stage.[66]\n A second major rocket failure happened in early September 2016, when a Falcon 9 exploded during a propellant fill operation for a standard pre-launch static fire test. The payload, the AMOS-6 communications satellite valued at $200 million, was destroyed.[67] The explosion was caused by the liquid oxygen that is used as propellant turning so cold that it solidified and ignited with carbon composite helium vessels.[68] Though not considered an unsuccessful flight, the rocket explosion sent the company into a four-month launch hiatus while it worked out what went wrong. SpaceX returned to flight in January 2017.[69]\n Later that year, in March 2017, SpaceX launched a returned Falcon 9 for the SES-10 satellite. This was the first time a re-launch of a payload-carrying orbital rocket went back to space.[70] The first stage was recovered again, also making it the first landing of a reused orbital class rocket.[71]\n In July 2017, the company raised $350 million, which raised its valuation to $21 billion.[72]\nIn 2017, SpaceX achieved a 45% global market share for awarded commercial launch contracts.[73]\nBy March 2018, SpaceX had more than 100 launches on its manifest representing about $12 billion in contract revenue.[74] The contracts included both commercial and government (NASA\/DOD) customers.[75] This made SpaceX the leading global commercial launch provider measured by manifested launches.[76]\n In 2017, SpaceX formed a subsidiary, The Boring Company,[77] and began work to construct a short test tunnel on and adjacent to the SpaceX headquarters and manufacturing facility, using a small number of SpaceX employees,[78] which was completed in May 2018,[79] and opened to the public in December 2018.[80] During 2018, The Boring Company was spun out into a separate corporate entity with 6% of the equity going to SpaceX, less than 10% to early employees, and the remainder of the equity to Elon Musk.[80]\n In 2019 Spacex raised $1.33 billion of capital across three funding rounds.[81]\nBy May 2019, the valuation of SpaceX had risen to $33.3 billion[82] and reached $36 billion by March 2020.[83]\n On 19 August 2020, after a $1.9 billion funding round, one of the largest single fundraising pushes by any privately held company, SpaceX's valuation increased to $46 billion.[84][85][86]\n In February 2021, SpaceX raised an additional $1.61 billion in an equity round from 99 investors[87] at a per share value of approximately $420,[86] raising the company valuation to approximately $74 billion. By 2021, SpaceX had raised more than $6 billion in equity financing. Most of the capital raised since 2019 has been used to support the operational fielding of the Starlink satellite constellation and the development and manufacture of the Starship launch vehicle.[87] By October 2021, the valuation of SpaceX had risen to $100.3 billion.[88] On 16 April 2021, Starship HLS won the contract, and will play a critical role in the Artemis program.[89] By 2021, SpaceX had entered into agreements with Google Cloud Platform and Microsoft Azure to provide on-ground computer and networking services for Starlink.[90] A new round of financing in 2022 values SpaceX at $127 billion.[91]\n In July 2021, SpaceX unveiled another drone ship named A Shortfall of Gravitas, landing a booster from CRS-23 on it for the first time on 29 August 2021.[92] Within the first 130 days of 2022, SpaceX had 18 rocket launches and two astronaut splashdowns. On 13 December 2021, company CEO Elon Musk announced that the company was starting a carbon dioxide removal program that would convert captured carbon into rocket fuel,[93][94] after he announced a $100 million donation to the X Prize Foundation the previous February to provide the monetary rewards to winners in a contest to develop the best carbon capture technology.[95][96]\n In August 2022, Reuters reported that the European Space Agency (ESA) began initial discussions with SpaceX that could lead to the company's launchers being used temporarily, given that Russia blocked access to Soyuz rockets amid the Russian invasion of Ukraine.[97] Since that same invasion and in the greater war between Russia and Ukraine, Starlink was extensively used.[98]\n In 2022, SpaceX's Falcon 9 also became the world record holder for the most launches of a single vehicle type in a single year.[99][100][non-primary source needed] SpaceX launched a rocket approximately every six days in 2022, with 61 launches in total. All but one (a Falcon Heavy in November) was on a Falcon 9 rocket.[99]\n In November 2023, SpaceX announced it would acquire its parachute supplier Pioneer Aerospace out of bankruptcy for $2.2 million.[101][102]\n In January 2019, SpaceX announced it would lay off 10% of its workforce to help finance the Starship and Starlink projects.[103]\nThe purpose of the Starship vehicle is to enable large-scale transit of humans and cargo to the Moon, Mars, and beyond.[104] Spacex's Starship is the largest and most powerful rocket ever flown, with a payload capacity of 100+ tons.[105][106] Construction of initial prototypes and tests for Starship started in early 2019 in Florida and Texas. All Starship construction and testing moved to the new SpaceX South Texas launch site later that year.\n On 20 April 2023, Starship's first orbital flight test ended in a mid-air explosion over the Gulf of Mexico before booster separation. After launch, multiple engines in the booster progressively failed, causing the vehicle to reach max q later than planned. \"Max q\" is the theoretical point of maximal mechanical stress which occurs during the launch sequence of a space vehicle. In the case of a rocket that must be self-destructed during its ascent, max q occurs at the point of self-destruction. Eventually, the vehicle lost control and spun erratically until the automated flight termination system was activated, which intentionally destroyed the rocket. Elon Musk, SpaceX, and other individuals familiar with the space industry have referred to the test flight as a success.[107][108]\n Musk said at the time that it would take between \"six to eight weeks\" to get the infrastructure prepared for another launch. In October 2023, a senior SpaceX executive stated the company had been ready to launch the next test flight since September. He accused government regulators of disrupting the project's progress, adding the delay could lead to China beating US astronauts back to the Moon.[109][110]\n On 18 November 2023, SpaceX launched its second integrated Starship test, with both vehicles flying for a few minutes before separately exploding.[111][112][113][114]\n In early March 2024 SpaceX announced that it was targeting 14 March as the tentative launch date for its next unmanned Starship launch configuration flight test, pending the issuance of a \"launch license\" by the FAA. This license was granted on 13 March 2024.[115] On 14 March 2023 at 13:25 UTC Starship launched for the 3rd time and for the first time Starship reached its planned suborbital trajectory. The flight ended with the booster experiencing a malfunction shortly before landing and the ship being lost during reentry over the Indian Ocean. [105][106]\n A significant milestone was achieved in May 2020, when SpaceX successfully launched two NASA astronauts (Doug Hurley and Bob Behnken) into orbit on a Crew Dragon spacecraft during Crew Dragon Demo-2, making SpaceX the first private company to send astronauts to the International Space Station and marking the first crewed orbital launch from American soil in 9 years.[116][117]\nThe mission launched from Kennedy Space Center Launch Complex 39A (LC-39A) of the Kennedy Space Center in Florida.[118]\n In May 2019, SpaceX launched the first large batch of 60 Starlink satellites, beginning to deploy what would become the world's largest commercial satellite constellation the following year.[119] In 2022 most Spacex launches focused on Starlink, a consumer internet business that sends batches of internet-beaming satellites and now has over 2,200 satellites in orbit.[120]\n On 16 July 2021 SpaceX entered an agreement to acquire Swarm Technologies, a private company building a low Earth orbit satellite constellation for communications with Internet of things (IoT) devices, for $524 million.[121][4]\n In December 2022, the U.S. Federal Communications Commission (FCC) approved to launch up to 7,500 of SpaceX's next-generation satellites in its Starlink internet network.[122]\n SpaceX has developed three launch vehicles. The small-lift Falcon 1 was the first launch vehicle developed and was retired in 2009. The medium-lift Falcon 9 and the heavy-lift Falcon Heavy are both operational.\n Falcon 1 was a small rocket capable of placing several hundred kilograms into low Earth orbit. It launched five times between 2006 and 2009, of which 2 were successful.[141] The Falcon 1 was the first privately funded, liquid-fueled rocket to reach orbit.[123]\n Falcon 9 is a medium-lift launch vehicle capable of delivering up to 22,800 kilograms (50,265 lb) to orbit, competing with the Delta IV and the Atlas V rockets, as well as other launch providers around the world. It has nine Merlin engines in its first stage. The Falcon 9 v1.0 rocket successfully reached orbit on its first attempt on 4 June 2010. Its third flight, COTS Demo Flight 2, launched on 22 May 2012 and launched the first commercial spacecraft to reach and dock with the International Space Station (ISS).[51] The vehicle was upgraded to Falcon 9 v1.1 in 2013, Falcon 9 Full Thrust in 2015, and finally to Falcon 9 Block 5 in 2018. The first stage of Falcon 9 is designed to retro propulsively land, be recovered, and reflown.[142]\n Falcon Heavy is a heavy-lift launch vehicle capable of delivering up to 63,800 kg (140,700 lb) to Low Earth orbit (LEO) or 26,700 kg (58,900 lb) to geosynchronous transfer orbit (GTO). It uses three slightly modified Falcon 9 first-stage cores with a total of 27 Merlin 1D engines.[143][144] The Falcon Heavy successfully flew its inaugural mission on 6 February 2018, launching Musk's personal Tesla Roadster into heliocentric orbit[145]\n Both the Falcon 9 and Falcon Heavy are certified to conduct launches for the National Security Space Launch (NSSL).[146][147] As of 19 March 2024, the Falcon 9 and Falcon Heavy have been launched 320 times, resulting in 318 full mission successes, one partial success, and one in-flight failure. In addition, a Falcon 9 experienced a pre-flight failure before a static fire test in 2016.[148][149]\n SpaceX is developing a fully reusable super-heavy lift launch system known as Starship. It comprises a reusable first stage, called Super Heavy, and the reusable Starship second stage space vehicle. The system is intended to supersede the company's existing launch vehicle hardware by the early 2020s.[150][151]\n Since the founding of SpaceX in 2002, the company has developed several rocket engines – Merlin, Kestrel, and Raptor – for use in launch vehicles,[152][153] Draco for the reaction control system of the Dragon series of spacecraft,[154] and SuperDraco for abort capability in Crew Dragon.[155]\n Merlin is a family of rocket engines that uses liquid oxygen (LOX) and RP-1 propellants. Merlin was first used to power the Falcon 1's first stage and is now used on both stages of the Falcon 9 and Falcon Heavy vehicles.[156]\nKestrel uses the same propellants and was used as the Falcon 1 rocket's second-stage main engine.[153][157]\n Draco and SuperDraco are hypergolic liquid-propellant rocket engines. Draco engines are used on the reaction control system of the Dragon and Dragon 2 spacecraft.[154] The SuperDraco engine is more powerful, and eight SuperDraco engines provide launch escape capability for crewed Dragon 2 spacecraft during an abort scenario.[158]\n Raptor is a new family of liquid oxygen and liquid methane-fueled full-flow staged combustion cycle engines to power the first and second stages of the in-development Starship launch system.[152] Development versions were test-fired in late 2016,[159] and the engine flew for the first time in 2019, powering the Starhopper vehicle to an altitude of 20 m (66 ft).[160]\n SpaceX has developed the Dragon spacecraft to transport cargo and crew to the International Space Station. The first version of Dragon, used only for cargo, was first launched in 2010.[44] The currently operational second generation Dragon spacecraft, known as Dragon 2, conducted its first flight, without crew, to the ISS in early 2019, followed by a crewed flight of Dragon 2 in 2020.[116]\nThe cargo variant of Dragon 2 flew for the first time in December 2020, for a resupply to the Space Station as part of the CRS contract with NASA.[161]\n In March 2020 SpaceX revealed the Dragon XL, designed as a resupply spacecraft for NASA's planned Lunar Gateway space station under a Gateway Logistics Services (GLS) contract.[162] Dragon XL is planned to launch on the Falcon Heavy, and is able to transport over 5,000 kg (11,000 lb) to the Gateway. Dragon XL will be docked at the Gateway for six to twelve months at a time.[163]\n SpaceX routinely returns the first stage of Falcon 9 and Falcon Heavy rockets after orbital launches. The rocket flights and lands at a predetermined landing site using only its propulsion systems.[164]\nWhen propellant margins do not permit a return to launch site (RTLS), rockets return to a floating landing platform in the ocean, called autonomous spaceport drone ships (ASDS).[165]\n SpaceX also plans to introduce floating launch platforms. These are modified oil rigs to use in the 2020s to provide a sea launch option for their second-generation launch vehicle: the heavy-lift Starship system, consisting of the Super Heavy booster and Starship second stage.\n Starlink is an internet satellite constellation under development by SpaceX that consists of thousands of cross-linked communications satellites in ~550 km orbits. Owned and operated by SpaceX, its goal is to address the significant unmet demand worldwide for low-cost broadband capabilities.[166]\n Development began in 2015, and initial prototype test-flight satellites were launched on the SpaceX Paz satellite mission in 2017. In May 2019, SpaceX launched the first batch of 60 satellites aboard a Falcon 9.[167] Initial test operation of the constellation began in late 2020[168] and first orders were taken in early 2021.[169] Customers were told to expect internet service speeds of 50 Mbit\/s to 150 Mbit\/s and latency from 20 ms to 40 ms.[170] In December 2022, Starlink reached over 1 million subscribers worldwide.[171]\n The planned large number of Starlink satellites has been criticized by astronomers due to concerns over light pollution,[172][173][174] with the brightness of Starlink satellites in both optical and radio wavelengths interfering with scientific observations.[175] In response, SpaceX has implemented several upgrades to Starlink satellites aimed at reducing their brightness.[176] The large number of satellites employed by Starlink also creates long-term dangers of space debris collisions.[177][178] However, the satellites are equipped with krypton-fueled Hall thrusters which allow them to de-orbit at the end of their life. They are also designed to avoid collisions based on uplinked tracking data autonomously.[179]\n In December 2022, SpaceX announced Starshield, a program to incorporate military or government entity payloads on board a Starlink-derived satellite bus. The Space Development Agency is a key customer procuring satellites for a space-based missile defense system.[180][181]\n In June 2015, SpaceX announced that they would sponsor a Hyperloop competition, and would build a 1.6 km (0.99 mi) long subscale test track near SpaceX's headquarters for the competitive events.[182][183] The company has held the annual competition since 2017.[184]\n In collaboration with doctors and academic researchers, SpaceX invited all employees to participate in the creation of a COVID-19 antibody-testing program in 2020. As such, 4300 employees volunteered to provide blood samples resulting in a peer-reviewed scientific paper crediting eight SpaceX employees as coauthors and suggesting that a certain level of COVID-19 antibodies may provide lasting protection against the virus.[185][186]\n In July 2018, Musk arranged for his employees to build a mini-submarine to assist the rescue of children stuck in a flooded cavern in Thailand.[187] Richard Stanton, leader of the international rescue diving team, urged Musk to facilitate the construction of the vehicle as a back-up, in case flooding worsened.[188][189] Engineers at SpaceX and The Boring Company built the mini-submarine from a Falcon 9 liquid oxygen transfer tube in eight hours and personally delivered it to Thailand.[190][191] By this time, however, eight of the 12 children had already been rescued using full face masks and oxygen under anesthesia; consequently Thai authorities declined to use the submarine.[187]\n NASA's PACE (plankton, Aerosol, Cloud, ocean Ecosystem) satellite launched on a SpaceX Falcon 9 rocket from the Cape Canaveral Space Force Stattion's Space Launch Complex 40 at 12.03 PM IST on Thursday, 8 February 2024. The spacecraft was separated from the rocket's second stage a few minutes after that and entered a sun-synchronous orbit. The Falcon 9 rocket also stuck its landing completing the 4th completed flight for this particular Falcon 9 rocket.[192]\n SpaceX is headquartered in Hawthorne, California, which also serves as its primary manufacturing plant.[193] The company operates a research and major operation in Redmond, Washington, owns a test site in Texas[194] and operates three launch sites, with another under development. SpaceX also operates regional offices in Texas, Virginia, and Washington, D.C.[75] SpaceX was incorporated in the state of Delaware.[195]\n SpaceX Headquarters is located in the Los Angeles suburb of Hawthorne, California. The large three-story facility, originally built by Northrop Corporation to build Boeing 747 fuselages,[193] houses SpaceX's office space, mission control, and Falcon 9 manufacturing facilities.[196]\n The area has one of the largest concentrations of space sector headquarters, facilities, and\/or subsidiaries in the U.S., including Boeing\/McDonnell Douglas main satellite building campuses, The Aerospace Corporation, Raytheon, NASA's Jet Propulsion Laboratory, United States Space Force's Space Systems Command at Los Angeles Air Force Base, Lockheed Martin, BAE Systems, Northrop Grumman, and AECOM, etc., with a large pool of aerospace engineers and recent college engineering graduates.[193]\n SpaceX uses a high degree of vertical integration in the production of its rockets and rocket engines.[15] SpaceX builds its rocket engines, rocket stages, spacecraft, principal avionics and all software in-house in their Hawthorne facility, which is unusual for the space industry.[15]\n In January 2015, SpaceX announced it would be entering the satellite production business and global satellite internet business. The first satellite facility is a 30,000 sq ft (2,800 m2) office building located in Redmond, Washington. As of January 2017, a second facility in Redmond was acquired with 40,625 sq ft (3,774.2 m2) and has become a research and development laboratory for the satellites.[197] In July 2016, SpaceX acquired an additional 8,000 sq ft (740 m2) office space in Irvine, California to focus on satellite communications.[198]\n SpaceX operates its Rocket Development and Test Facility in McGregor, Texas. All SpaceX rocket engines are tested on rocket test stands,[194] and low-altitude VTVL flight testing of the Falcon 9 Grasshopper in 2012–2013 were carried out at McGregor.[199] Testing of the much larger Starship prototypes is conducted at the SpaceX Starbase near Brownsville, Texas.[196]\n The company purchased the McGregor facilities from Beal Aerospace, where it refitted the largest test stand for Falcon 9 engine testing. SpaceX has made many improvements to the facility since its purchase and has also extended the acreage by purchasing several pieces of adjacent farmland. As of October 2012[update], the McGregor facility had seven test stands that are operated \"18 hours a day, six days a week\"[200] and is building more test stands because production is ramping up and the company has a large manifest in the next several years.[201] In addition to routine testing, Dragon capsules (following recovery after an orbital mission), are shipped to McGregor for de-fueling, cleanup, and refurbishment for reuse in future missions.[202]\n SpaceX currently operates four orbital launch sites, at Cape Canaveral Space Force Station and Kennedy Space Center in Florida and Vandenberg Space Force Base in California for Falcon rockets, and Starbase near Brownsville, Texas for Starship. SpaceX has indicated that they see a niche for each of the four orbital facilities and that they have sufficient launch business to fill each pad.[203] The Vandenberg launch site enables highly inclined orbits (66–145°), while Cape Canaveral and Kennedy enable orbits of medium inclination (28.5–55°).[204] Larger inclinations, including SSO, are possible from Florida by overflying Cuba.[205]\n Before it was retired, all Falcon 1 launches took place at the Ronald Reagan Ballistic Missile Defense Test Site on Omelek Island of the Marshall Islands.[206]\n In April 2007, the Pentagon approved the use of Cape Canaveral Space Launch Complex 40 (SLC-40) by SpaceX.[207] The site has been used since 2010 for Falcon 9 launches, mainly to low Earth and geostationary orbits. The former Launch Complex 13 at Cape Canaveral, now renamed Landing Zones 1 and 2, has since 2015 been used for Falcon 9 first-stage booster landings.[208]\n Vandenberg Space Launch Complex 4 (SLC-4E) was leased from the military in 2011 and is used for payloads to polar orbits. The Vandenberg site can launch both Falcon 9 and Falcon Heavy vehicles,[209] but cannot launch to low inclination orbits. The neighboring SLC-4W was converted to Landing Zone 4 in 2015 for booster landings.[210]\n On 14 April 2014, SpaceX signed a 20-year lease for Kennedy Space Center Launch Complex 39A.[211] The pad was subsequently modified to support Falcon 9 and Falcon Heavy launches. As of 2024[update] it is the only pad that supports Falcon Heavy launches. SpaceX launched its first crewed mission to the ISS from Launch Pad 39A on 30 May 2020.[212] Pad 39A has been prepared since 2019 to eventually accommodate Starship launches. With delays in launch FAA permits for Boca Chica, the 39A Starship preparation was accelerated in 2022.[213]\n SpaceX manufactures and flies Starship test vehicles from the SpaceX Starbase in Boca Chica near Brownsville, Texas, having announced first plans for the launch facility in August 2014.[214][215] The Federal Aviation Administration (FAA) issued the permit in July 2014.[216] SpaceX broke ground on the new launch facility in 2014 with construction ramping up in the latter half of 2015,[217] with the first suborbital launches from the facility in 2019[196] and orbital launches starting in 2023. Some residents of Boca Chica Village, Brownsville, and environmental activists criticized the site along with Starship development program in various aspects.[218][219]\n SpaceX won demonstration and actual supply contracts from NASA for the International Space Station (ISS) with technology the company developed. SpaceX is also certified for U.S. military launches of Evolved Expendable Launch Vehicle-class (EELV) payloads. With approximately thirty missions on the manifest for 2018 alone, SpaceX represents over $12 billion under contract.[75]\n In 2006, SpaceX won a NASA Commercial Orbital Transportation Services (COTS) Phase 1 contract to demonstrate cargo delivery to the International Space Station (ISS), with a possible contract option for crew transport.[220] Through this contract, designed by NASA to provide \"seed money\" through Space Act Agreements for developing new capabilities, NASA paid SpaceX $396 million to develop the cargo configuration of the Dragon spacecraft, while SpaceX developed the Falcon 9 launch vehicle with their resources.[221] These Space Act Agreements have been shown to have saved NASA millions of dollars in development costs, making rocket development 4–10 times cheaper than if produced by NASA alone.[222]\n In December 2010 the launch of the SpaceX COTS Demo Flight 1 mission, SpaceX became the first private company to successfully launch, orbit, and recover a spacecraft.[223] Dragon successfully berthed with the ISS during SpaceX COTS Demo Flight 2 in May 2012, a first for a private spacecraft.[224]\n Commercial Resupply Services (CRS) is a series of contracts awarded by NASA from 2008 to 2016 for the delivery of cargo and supplies to the International Space Station on commercially operated spacecraft. The first CRS contracts were signed in 2008 and awarded $1.6 billion to SpaceX for 12 cargo transport missions, covering deliveries to 2016.[225] SpaceX CRS-1, the first of the 12 planned resupply missions, launched in October 2012, achieved orbit, berthed, and remained on station for 20 days, before re-entering the atmosphere and splashing down in the Pacific Ocean.[226]\n CRS missions have flown approximately twice a year to the ISS since then. In 2015, NASA extended the Phase 1 contracts by ordering an additional three resupply flights from SpaceX, and then extended the contract further for a total of twenty cargo missions to the ISS.[227][225][228] The final Dragon 1 mission, SpaceX CRS-20, departed the ISS in April 2020, and Dragon was subsequently retired from service. A second phase of contracts was awarded in January 2016 with SpaceX as one of the awardees. SpaceX will fly up to nine additional CRS flights with the upgraded Dragon 2 spacecraft.[229][230] In March 2020, NASA contracted SpaceX to develop the Dragon XL spacecraft to send supplies to the Lunar Gateway space station. Dragon XL will be launched on a Falcon Heavy.[231]\n SpaceX is responsible for the transportation of NASA astronauts to and from the ISS. The NASA contracts started as part of the Commercial Crew Development (CCDev) program, aimed at developing commercially operated spacecraft capable of delivering astronauts to the ISS. The first contract was awarded to SpaceX in 2011,[232][233] followed by another in 2012 to continue development and testing of its Dragon 2 spacecraft.[234]\n In September 2014, NASA chose SpaceX and Boeing as the two companies that would be funded to develop systems to transport U.S. crews to and from the ISS.[235] SpaceX won $2.6 billion to complete and certify Dragon 2 by 2017. The contracts called for at least one crewed flight test with at least one NASA astronaut aboard. Once Crew Dragon received NASA human-spaceflight certification, the contract required SpaceX to conduct at least two, and as many as six, crewed missions to the space station.[235]\n SpaceX completed the first key flight test of its Crew Dragon spacecraft, a Pad Abort Test, in May 2015,[236] and successfully conducted a full uncrewed test flight in early 2019. The capsule docked to the ISS and then splashed down in the Atlantic Ocean.[237] In January 2020, SpaceX conducted an in-flight abort test, the last test flight before flying crew, in which the Dragon spacecraft fired its launch escape engines in a simulated abort scenario.[238]\n On 30 May 2020, the Crew Dragon Demo-2 mission was launched to the International Space Station with NASA astronauts Bob Behnken and Doug Hurley, the first time a crewed vehicle had launched from the U.S. since 2011, and the first commercial crewed launch to the ISS.[239]\nThe Crew-1 mission was successfully launched to the International Space Station on 16 November 2020, with NASA astronauts Michael Hopkins, Victor Glover and Shannon Walker along with JAXA astronaut Soichi Noguchi,[240] all members of the Expedition 64 crew.[241] On 23 April 2021, Crew-2 was launched to the International Space Station with NASA astronauts Shane Kimbrough and K. Megan McArthur, JAXA astronaut Akihiko Hoshide, and ESA astronaut Thomas Pesquet.[242] The Crew-2 mission successfully docked on 24 April 2021.[243]\n SpaceX also offers paid crewed spaceflights for private individuals. The first of these missions, Inspiration4, launched in 2021 on behalf of Shift4 Payments CEO Jared Isaacman. The mission launched the Crew Dragon Resilience from the Florida Kennedy Space Center's Launch Complex 39A atop a Falcon 9 launch vehicle, placed the Dragon capsule into low Earth orbit, and ended successfully about three days later when the Resilience splashed down in the Atlantic Ocean. All four crew members received commercial astronaut training from SpaceX. The training included lessons in orbital mechanics, operating in a microgravity environment, stress testing, emergency-preparedness training, and mission simulations.[244]\n In 2005, SpaceX announced that it had been awarded an Indefinite Delivery\/Indefinite Quantity (IDIQ) contract, allowing the United States Air Force to purchase up to $100 million worth of launches from the company.[245] Three years later, NASA announced that it had awarded an IDIQ Launch Services contract to SpaceX for up to $1 billion, depending on the number of missions awarded.[246] In December 2012, SpaceX announced its first two launch contracts with the United States Department of Defense (DoD). The United States Air Force Space and Missile Systems Center awarded SpaceX two EELV-class missions: Deep Space Climate Observatory (DSCOVR) and Space Test Program 2 (STP-2). DSCOVR was launched on a Falcon 9 launch vehicle in 2015, while STP-2 was launched on a Falcon Heavy on 25 June 2019.[247]\n The Falcon 9 v1.1 was certified for National Security Space Launch (NSSL) in 2015, allowing SpaceX to contract launch services to the Air Force for any payloads classified under national security.[146]\nThis broke the monopoly held since 2006 by United Launch Alliance (ULA) over U.S. Air Force launches of classified payloads.[248]\nIn April 2016, the U.S. Air Force awarded the first such national security launch to SpaceX to launch the second GPS III satellite for $82.7 million.[249] This was approximately 40% less than the estimated cost for similar previous missions.[250] SpaceX also launched the third GPS III launch on 20 June 2020.[251] In March 2018, SpaceX secured an additional $290 million contract from the U.S. Air Force to launch another three GPS III satellites.[252]\n The U.S. National Reconnaissance Office (NRO) also purchased launches from SpaceX, with the first taking place on 1 May 2017.[253] In February 2019, SpaceX secured a $297 million contract from the U.S. Air Force to launch another three national security missions, all slated to launch no earlier than FY 2021.[254] In August 2020, the U.S. Space Force awarded its National Security Space Launch (NSSL) contracts for the following 5–7 years. SpaceX won a contract for $316 million for one launch. In addition, SpaceX will handle 40% of the U.S. military's satellite launch requirements over the period.[255]\n SpaceX also designs and launches custom military satellites for the Space Development Agency as part of a new missile defense system in low Earth orbit.[256] The constellation would give the United States capabilities to sense, target and potentially intercept nuclear missiles and hypersonic weapons launched from anywhere on Earth.[257] Both China and Russia brought concerns to the United Nations about the program,[258] and various organizations warn it could be destabilizing and trigger an arms race in space.[259][260]\n SpaceX's low launch prices, especially for communications satellites flying to geostationary transfer orbit (GTO), have resulted in market pressure on its competitors to lower their own prices.[15] Prior to 2013, the openly competed comsat launch market had been dominated by Arianespace (flying the Ariane 5) and International Launch Services (flying the Proton).[261] With a published price of $56.5 million per launch to low Earth orbit, Falcon 9 rockets were the cheapest in the industry.[262] European satellite operators are pushing the ESA to reduce launch prices of the Ariane 5 and the future Ariane 6 rockets as a result of competition from SpaceX.[263]\n SpaceX ended the United Launch Alliance (ULA) monopoly of U.S. military payloads when it began to compete for national security launches. In 2015, anticipating a slump in domestic, military, and spy launches, ULA stated that it would go out of business unless it won commercial satellite launch orders.[264] To that end, ULA announced a major restructuring of processes and workforce to decrease launch costs by half.[265][266]\n Congressional testimony by SpaceX in 2017 suggested that the NASA Space Act Agreement process of \"setting only a high-level requirement for cargo transport to the space station [while] leaving the details to industry\" had allowed SpaceX to design and develop the Falcon 9 rocket on its own at a substantially lower cost. According to NASA's own independently verified numbers, SpaceX's total development cost for the Falcon 9 rocket, including the Falcon 1 rocket, was estimated at $390 million. In 2011, NASA estimated that it would have cost the agency about $4 billion to develop a rocket like the Falcon 9 booster based upon NASA's traditional contracting processes, about ten times more.[222] In May 2020, NASA administrator Jim Bridenstine remarked that thanks to NASA's investments into SpaceX, the United States has 70% of the commercial launch market, a major improvement since 2012 when there were no commercial launches from the country.[267]\n In November 2022, the company announced COO Gwynne Shotwell and vice president Mark Juncosa would oversee Starbase, its Texas launch facility, along with Omead Afshar, who at the time oversaw operations for Tesla in Texas. Shyamal Patel, who was senior director of operations at the site, would shift to its Cape Canaveral site. CNBC reported that these executive moves demonstrated \"the sense of urgency within the company to get Starship flying.\"[280][281][282]\n \n According to former NASA deputy administrator Lori Garver, the company overall has a male-dominated employee culture,  similar to that of the spaceflight industry in general.[283] In December 2021, claims of workplace sexual harassment from five former SpaceX employees, ranging from interns to full engineers, were published.[284] The former employees claimed to have experienced unwanted advances and uncomfortable interactions.[285] Additionally, the accounts included claims of a culture of sexual harassment existing at the company and one where complaints made to executives, managers, and human resources officers went largely unaddressed.[286]\n In May 2022, a Business Insider article alleged that Musk engaged in sexual misconduct with a SpaceX flight attendant in a private jet in 2016 citing an anonymous friend of the flight attendant.[287] In response, some employees collaborated on an open letter condemning \"Elon's harmful Twitter behavior\".[288] It also asks the company to clearly define SpaceX's \"no-asshole\" and \"zero tolerance\" policies, which it says is unequally enforced from one employee to the next. The next day, Gwynne Shotwell announced that those employees who were involved with the letter had been terminated and claimed that unsponsored, unsolicited surveys were sent to employees during the work day and that some felt pressured to sign the letter.[289]\n The company has also been described as having a work culture that pushes employees to work excessively and is described as fostering a burnout culture.[290] According to a memo by Blue Origin, a rival aerospace company,[291][292][293] SpaceX expected very long work hours, work on weekends, and limited use of holidays.[290]\n Bundled references\n"}
{"key":"Space Exploration","link":"https:\/\/en.wikipedia.org\/wiki\/Exploration","headline":"Exploration - Wikipedia","content":"\n Exploration is the process of exploring, an activity which has some expectation of discovery. Organised exploration is largely a human activity, but exploratory activity is common to most organisms capable of directed locomotion and the ability to learn, and has been described in, amongst others, social insects foraging behaviour, where feedback from returning individuals affects the activity of other members of the group.[1]\n Exploration has been defined as:\n In all these definitions there is an implication of novelty, or unfamiliarity or the expectation of discovery in the exploration, whereas a survey implies directed examination, but not necessarily discovery of any previously unknown or unexpected information. The activities are not mutually exclusive, and often occur simultaneously to a variable extent. The same field of investigation or region may be explored at different times by different explorers with different motivations, who may make similar or different discoveries.\n Intrinsic exploration involves activity that is not directed towards a specific goal other than the activity itself.[3]\n Extrinsic exploration has the same meaning as appetitive behavior.[4][clarification needed] It is directed towards a specific goal.\n Curiosity is a quality related to inquisitive thinking and activities such as exploration, investigation, and learning, evident by observation in humans and other animals.[5][6] Curiosity has been found to be a strong motivation for exploration. When the potential external rewards are uncertain or unclear, an intrinsic drive is more likely to motivate exploration sufficiently to achieve results. Factors suggested to underlie curiosity include gain of information, utility of the information, perceived progress, and novelty.[7] Exploratory behavior is the movements of people and other animals while becoming familiar with new environments, even when there is no obvious biological advantage to it. A lack of exploratory behaviour may be considered an indication of fearfulness or emotionality.[8]\n Travelling with the expectation of discovery is often motivated by a combinion of aspects of inspective and diversive exploration. There are three major subdivisions of this class of exploration, based on the technology involved.\n Geographical exploration, sometimes considered the default meaning for the more general term exploration, is the practice of discovering lands and regions of the planet Earth remote or relatively inaccessible from the origin of the explorer.[12] The surface of the Earth not covered by water has been relatively comprehensively explored, as access is generally relatively straightforward, but underwater and subterranean areas are far less known, and even at the surface, much is still to be discovered in detail in the more remote and inaccessible wilderness areas.\n Two major eras of geographical exploration occurred in human history:  The first, covering most of Human history, saw people moving out of Africa, settling in new lands, and developing distinct cultures in relative isolation.[13] Early explorers settled in Europe and Asia; about 14,000 years ago, some crossed the Ice Age land bridge from Siberia to Alaska, and moved southwards to settle in the Americas.[12] For the most part, these cultures were ignorant of each other's existence.[13] The second period of exploration, occurring over the last 10,000 years, saw increased cross-cultural exchange through trade and exploration, and marked a new era of cultural intermingling, and more recently, convergence.[13]\n Early writings about exploration date back to the 4th millennium B.C. in ancient Egypt. One of the earliest and most impactful thinkers on exploration was Ptolemy in the 2nd century AD. Between the 5th century and 15th century AD, most exploration was done by Chinese and Arab explorers. This was followed by the Age of Discovery after European scholars rediscovered the works of early Latin and Greek geographers. While the Age of Discovery was partly driven by land routes outside of Europe becoming unsafe,[14] and a desire for conquest, the 17th century also saw exploration driven by nobler motives, including scientific discovery and the expansion of knowledge about the world.[12] This broader knowledge of the world's geography meant that people were able to make world maps, depicting all land known. The first modern atlas was the Theatrum Orbis Terrarum, published by Abraham Ortelius, which included a world map that depicted all of Earth's continents.[15]: 32 \n Underwater exploration is the exploration of any underwater environment, either by direct observation by the explorer, or by remote observation and measurement under the direction of the investigators. Systematic, targeted exploration, with simultaneous survey, and recording of data, followed by data processing, interpretation and publication, is the most effective method to increase understanding of the ocean and other underwater regions, so they can be effectively managed, conserved, regulated, and their resources discovered, accessed, and used. Less than 10% of the ocean has been mapped in any detail, even less has been visually observed, and the total diversity of life and distribution of populations is similarly incompletely known.[16]\n Space exploration is the use of astronomy and space technology to explore outer space.[17] While the exploration of space is currently carried out mainly by astronomers with telescopes, its physical exploration is conducted both by uncrewed robotic space probes and human spaceflight. Space exploration, like its classical form astronomy, is one of the main sources for space science.\n While the observation of objects in space, known as astronomy, predates reliable recorded history, it was the development of large and relatively efficient rockets during the mid-twentieth century that allowed physical extraterrestrial exploration to become a reality. Common rationales for exploring space include advancing scientific research, national prestige, uniting different nations, ensuring the future survival of humanity, and developing military and strategic advantages against other countries.[18]\n Urban exploration is the exploration of manmade structures, usually abandoned ruins or hidden components of the manmade environment. Photography and historical interest\/documentation are heavily featured in the hobby, sometimes involving trespassing onto private property.[19]\n The activity presents various risks, including physical danger and, if done illegally and\/or without permission, the possibility of arrest and punishment. Some activities associated with urban exploration violate local or regional laws and certain broadly interpreted anti-terrorism laws, or can be considered trespassing or invasion of privacy.[20]\n Systematic investigation is done in an orderly and organised manner, generally following a plan, though it should be a flexible plan, which is amenable to rational adaptation to suit circumstances, as the concept of exploration accepts the possibility of the unexpected being encountered, and the plan must survive such encounters to remain useful.[citation needed]\n Science is a rigorous, systematic endeavor that builds and organizes knowledge in the form of testable explanations and predictions about the world.[21][22] Modern science is typically divided into three major branches:[23] natural sciences (e.g., physics, chemistry, and biology), which study the physical world; the social sciences (e.g., economics, psychology, and sociology), which study individuals and societies;[24][25] and the formal sciences (e.g., logic, mathematics, and theoretical computer science), which study formal systems, governed by axioms and rules.[26][27] There is disagreement whether the formal sciences are science disciplines,[28][29][30] because they do not rely on empirical evidence.[31][29] Applied sciences are disciplines that use scientific knowledge for practical purposes, such as in engineering and medicine.[32][33][34]\n Prospecting for minerals is an example of systematic investigation and of inspective exploration. Traditionally prospecting relied on direct observation of mineralisation in rock outcrops or in sediments, but more recently also includes the use of geologic, geophysical, and geochemical tools to search for anomalies which can narrow the search area. The area to be prospected should be covered sufficiently to minimise the risk of missing something important, but can take into account previous experience that certain geological evidence correlates with a very low probability of finding the desired minerals, and other evidence indicates a high probability, making it efficient to concentrate on the areas of high probability when they are found, and to skip areas of very low probability. Once an anomaly has been identified and interpreted to be a prospect, more detailed exploration of the potential reserve can be done by soil sampling, drilling, seismic surveys, and similar methods to assess the most appropriate method and type of mining and the economic potential.[35]\n Diagnosis is the identification of the nature and cause of a given phenomenon. Diagnosis is used in many different disciplines, such as medicine, forensic science and engineering failure analysis, with variations in the use of logic, analytics, and experience, to determine causality.[36] A diagnostic examination explores the available evidence to try to identify likely causes for observed effects, and may also investigate further with the intention to discover additional relevant evidence. This is an instance of inspective and extrinsic exploration.\n Exploration as the pursuit of first hand experience and knowledge is often an example of diversive and intrinsic exploration when done for personal satisfaction and entertainment, though it may also be for purposes of learning or verifying the information provided by others, which is an extrinsic motivation, and which is likely to be characterised by a relatively systematic approach. As the personal aspect of the experience is central to this type of exploration, the same region or range of experiences may be explored repeatedly by different people, for each can have a reasonable expectation of personal discovery.\n Wandering about in the hope or expectation of serendipitous discovery may also be considered a form of diversive exploration. This form of exploration may be done in a physical or information environment, such as exploring the internet, also known as web surfing.[37]\n Exploratory behavior has been defined as behavior directed toward getting information about the environment,[38]  or to locate things such as food or individuals. Exploration usually follows a sequence, in which four stages can be identified.[39] The first phase is search, in which the subject moves around to contact relevant stimuli, to which the subject pays attention, and may approach and investigate. The sequence may be interrupted by flight if danger is recognised, or a return to search if the stimulus is not interesting  or useful.[40][41][42][43]\n A tendency to explore a new environment has been recognised in a wide range of free-moving animals from invertebrates to primates. Various forms of exploratory behaviour in animas have been analysed and categorised since 1960.[3]\n Explorers:\n"}
{"key":"Space Exploration","link":"https:\/\/en.wikipedia.org\/wiki\/Buzz_Aldrin","headline":"Buzz Aldrin - Wikipedia","content":"\n Buzz Aldrin (\/ˈɔːldrɪn\/; born Edwin Eugene Aldrin Jr.; January 20, 1930) is an American former astronaut, engineer and fighter pilot. He made three spacewalks as pilot of the 1966 Gemini 12 mission, and was the Lunar Module Eagle pilot on the 1969 Apollo 11 mission. He was the second person to walk on the Moon after mission commander Neil Armstrong.\n Born in Glen Ridge, New Jersey, Aldrin graduated third in the class of 1951 from the United States Military Academy at West Point with a degree in mechanical engineering. He was commissioned into the United States Air Force and served as a jet fighter pilot during the Korean War. He flew 66 combat missions and shot down two MiG-15 aircraft.\n After earning a Doctor of Science degree in astronautics from the Massachusetts Institute of Technology (MIT), Aldrin was selected as a member of NASA's Astronaut Group 3, making him the first astronaut with a doctoral degree. His doctoral thesis, Line-of-Sight Guidance Techniques for Manned Orbital Rendezvous, earned him the nickname \"Dr. Rendezvous\" from fellow astronauts. His first space flight was in 1966 on Gemini 12, during which he spent over five hours on extravehicular activity. Three years later, Aldrin set foot on the Moon at 03:15:16 on July 21, 1969 (UTC), nineteen minutes after Armstrong first touched the surface, while command module pilot Michael Collins remained in lunar orbit. A Presbyterian elder, Aldrin became the first person to hold a religious ceremony on the Moon when he privately took communion.\n After leaving NASA in 1971, Aldrin became Commandant of the U.S. Air Force Test Pilot School. He retired from the Air Force in 1972 after 21 years of service. His autobiographies Return to Earth (1973) and Magnificent Desolation (2009) recount his struggles with clinical depression and alcoholism in the years after leaving NASA. Aldrin continues to advocate for space exploration, particularly a human mission to Mars. He developed the Aldrin cycler, a special spacecraft trajectory that makes travel to Mars more efficient in terms of time and propellant. He has been accorded numerous honors, including the Presidential Medal of Freedom in 1969.\n Aldrin was born Edwin Eugene Aldrin Jr. on January 20, 1930, at Mountainside Hospital in Glen Ridge, New Jersey.[1] His parents, Edwin Eugene Aldrin Sr. and Marion Aldrin (née Moon), lived in neighboring Montclair.[2] His father was an Army aviator during World War I and the assistant commandant of the Army's test pilot school at McCook Field, Ohio, from 1919 to 1922, but left the Army in 1928 and became an executive at Standard Oil.[3] Aldrin had two sisters: Madeleine, who was four years older, and Fay Ann, who was a year and a half older.[4] His nickname, which became his legal first name in 1988,[5][6] arose as a result of Fay's mispronouncing \"brother\" as \"buzzer\", which was then shortened to \"Buzz\".[4][7] He was a Boy Scout, achieving the rank of Tenderfoot Scout.[8]\n Aldrin did well in school, maintaining an A average.[9] He played football and was the starting center for Montclair High School's undefeated 1946 state champion team.[10][11] His father wanted him to go to the United States Naval Academy in Annapolis, Maryland, and enrolled him at nearby Severn School, a preparatory school for Annapolis, and even secured him a Naval Academy appointment from Albert W. Hawkes, one of the United States senators from New Jersey.[12] Aldrin attended Severn School in 1946,[13] but had other ideas about his future career. He suffered from seasickness and considered ships a distraction from flying airplanes. He faced down his father and told him to ask Hawkes to change the nomination to the United States Military Academy at West Point, New York.[12]\n Aldrin entered West Point in 1947.[5] He did well academically, finishing first in his class his plebe (first) year.[9] Aldrin was also an excellent athlete, competing in pole vault for the academy track and field team.[14][15] In 1950, he traveled with a group of West Point cadets to Japan and the Philippines to study the military government policies of Douglas MacArthur.[16] During the trip, the Korean War broke out.[17] On June 5, 1951, Aldrin graduated third in the class of 1951 with a Bachelor of Science degree in mechanical engineering.[18]\n Among the top of his class, Aldrin had his choice of assignments. He chose the United States Air Force, which had become a separate service in 1947 while Aldrin was still at West Point and did not yet have its own academy.[19][a] He was commissioned as a second lieutenant and underwent basic flight training in T-6 Texans at Bartow Air Base in Florida. His classmates included Sam Johnson, who later became a prisoner of war in Vietnam; the two became friends. At one point, Aldrin attempted a double Immelmann turn in a T-28 Trojan and suffered a grayout. He recovered in time to pull out at about 2,000 feet (610 m), averting what would have been a fatal crash.[21]\n When Aldrin was deciding what sort of aircraft he should fly, his father advised him to choose bombers, because command of a bomber crew gave an opportunity to learn and hone leadership skills, which could open up better prospects for career advancement. Aldrin chose instead to fly fighters. He moved to Nellis Air Force Base in Las Vegas, where he learned to fly the F-80 Shooting Star and the F-86 Sabre. Like most jet fighter pilots of the era, he preferred the latter.[21]\n In December 1952, Aldrin was assigned to the 16th Fighter-Interceptor Squadron, which was part of the 51st Fighter-Interceptor Wing. At the time it was based at Suwon Air Base, about 20 miles (32 km) south of Seoul, and was engaged in combat operations as part of the Korean War.[18][22] During an acclimatization flight, his main fuel system froze at 100 percent power, which would have soon used up all his fuel. He was able to override the setting manually, but this required holding a button down, which in turn made it impossible to also use his radio. He barely managed to make it back under enforced radio silence. He flew 66 combat missions in F-86 Sabres in Korea and shot down two MiG-15 aircraft.[22][23]\n The first MiG-15 he shot down was on May 14, 1953. Aldrin was flying about 5 miles (8.0 km) south of the Yalu River, when he saw two MiG-15 fighters below him. Aldrin opened fire on one of the MiGs, whose pilot may never have seen him coming.[22][24] The June 8, 1953, issue of Life magazine featured gun camera footage taken by Aldrin of the pilot ejecting from his damaged aircraft.[25]\n Aldrin's second aerial victory came on June 4, 1953, when he accompanied aircraft from the 39th Fighter-Interceptor Squadron in an attack on an airbase in North Korea. Their newer aircraft were faster than his and he had trouble keeping up. He then spotted a MiG approaching from above. This time, Aldrin and his opponent spotted each other at about the same time. They went through a series of scissor maneuvers, attempting to get behind the other. Aldrin was first to do so, but his gun sight jammed. He then manually sighted his gun and fired. He then had to pull out, as the two aircraft had gotten too low for the dogfight to continue. Aldrin saw the MiG's canopy open and the pilot eject, although Aldrin was uncertain whether there was sufficient time for a parachute to open.[24][26] For his service in Korea, he was awarded two Distinguished Flying Crosses and three Air Medals.[27]\n Aldrin's year-long tour ended in December 1953, by which time the fighting in Korea had ended. Aldrin was assigned as an aerial gunnery instructor at Nellis.[18] In December 1954 he became an aide-de-camp to Brigadier General Don Z. Zimmerman, the Dean of Faculty at the nascent United States Air Force Academy, which opened in 1955.[28][29] That same year, he graduated from the Squadron Officer School at Maxwell Air Force Base in Alabama.[30] From 1956 to 1959 he flew F-100 Super Sabres equipped with nuclear weapons as a flight commander in the 22nd Fighter Squadron, 36th Fighter Wing, stationed at Bitburg Air Base in West Germany.[18][24][28] Among his squadron colleagues was Ed White, who had been a year behind him at West Point. After White left West Germany to study for a master's degree at the University of Michigan in aeronautical engineering, he wrote to Aldrin encouraging him to do the same.[15]\n Through the Air Force Institute of Technology, Aldrin enrolled as a graduate student at the Massachusetts Institute of Technology in 1959 intending to earn a master's degree.[31] Richard Battin was the professor for his astrodynamics class. Two other USAF officers who later became astronauts, David Scott and Edgar Mitchell, took the course around this time. Another USAF officer, Charles Duke, also took the course and wrote his 1964 master's degree at MIT under the supervision of Laurence R. Young.[32]\n Aldrin enjoyed the classwork and soon decided to pursue a doctorate instead.[31] In January 1963, he earned a Sc.D. degree in astronautics.[28][33] His doctoral thesis was Line-of-Sight Guidance Techniques for Manned Orbital Rendezvous, the dedication of which read: \"In the hopes that this work may in some way contribute to their exploration of space, this is dedicated to the crew members of this country's present and future manned space programs. If only I could join them in their exciting endeavors!\"[33] Aldrin chose his doctoral thesis in the hope that it would help him be selected as an astronaut, although it meant foregoing test pilot training, which was a prerequisite at the time.[31]\n After completing his doctorate Aldrin was assigned to the Gemini Target Office of the Air Force Space Systems Division in Los Angeles,[15] working with the Lockheed Aircraft Corporation on enhancing the maneuver capabilities of the Agena target vehicle which was to be used by NASA's Project Gemini. He was then posted to the Space Systems Division's field office at NASA's Manned Spacecraft Center in Houston, where he was involved in integrating Department of Defense experiments into Project Gemini flights.[34]\n Aldrin initially applied to join the astronaut corps when NASA's Astronaut Group 2 was selected in 1962. His application was rejected on the grounds that he was not a test pilot. Aldrin was aware of the requirement and asked for a waiver but the request was turned down.[35] On May 15, 1963, NASA announced another round of selections, this time with the requirement that applicants had either test pilot experience or 1,000 hours of flying time in jet aircraft.[36] Aldrin had over 2,500 hours of flying time, of which 2,200 was in jets.[34] His selection as one of fourteen members of NASA's Astronaut Group 3 was announced on October 18, 1963.[37] This made him the first astronaut with a doctoral degree which, combined with his expertise in orbital mechanics, earned him the nickname \"Dr. Rendezvous\" from his fellow astronauts.[38][39][40] Although Aldrin was both the most educated and the rendezvous expert in the astronaut corps,[14] he was aware that the nickname was not always intended as a compliment.[15] Upon completion of initial training, each new astronaut was assigned a field of expertise; in Aldrin's case, it was mission planning, trajectory analysis, and flight plans.[41][42]\n Jim Lovell and Aldrin were selected as the backup crew of Gemini 10, commander and pilot respectively. Backup crews usually became the prime crew of the third following mission, but the last scheduled mission in the program was Gemini 12.[43] The February 28, 1966, deaths of the Gemini 9 prime crew, Elliot See and Charles Bassett, in an air crash, led to Lovell and Aldrin being moved up one mission to backup for Gemini 9, which put them in position as prime crew for Gemini 12.[44][45] They were designated its prime crew on June 17, 1966, with Gordon Cooper and Gene Cernan as their backups.[46]\n Initially, Gemini 12's mission objectives were uncertain. As the last scheduled mission, it was primarily intended to complete tasks that had not been successfully or fully carried out on earlier missions.[47] While NASA had successfully performed rendezvous during Project Gemini, the gravity-gradient stabilization test on Gemini 11 was unsuccessful. NASA also had concerns about extravehicular activity (EVA). Cernan on Gemini 9 and Richard Gordon on Gemini 11 had suffered from fatigue carrying out tasks during EVA, but Michael Collins had a successful EVA on Gemini 10, which suggested that the order in which he had performed his tasks was an important factor.[48][49]\n It therefore fell to Aldrin to complete Gemini's EVA goals. NASA formed a committee to give him a better chance of success. It dropped the test of the Air Force's astronaut maneuvering unit (AMU) that had given Gordon trouble on Gemini 11 so Aldrin could focus on EVA. NASA revamped the training program, opting for underwater training over parabolic flight. Aircraft flying a parabolic trajectory had given astronauts an experience of weightlessness in training, but there was a delay between each parabola which gave astronauts several minutes of rest. It also encouraged performing tasks quickly, whereas in space they had to be done slowly and deliberately. Training in a viscous, buoyant fluid gave a better simulation. NASA also placed additional handholds on the capsule, which were increased from nine on Gemini 9 to 44 on Gemini 12, and created workstations where he could anchor his feet.[48][49]\n Gemini 12's main objectives were to rendezvous with a target vehicle, and fly the spacecraft and target vehicle together using gravity-gradient stabilization, perform docked maneuvers using the Agena propulsion system to change orbit, conduct a tethered stationkeeping exercise and three EVAs, and demonstrate an automatic reentry. Gemini 12 also carried 14 scientific, medical, and technological experiments.[50] It was not a trailblazing mission; rendezvous from above had already been successfully performed by Gemini 9, and the tethered vehicle exercise by Gemini 11. Even gravity-gradient stabilization had been attempted by Gemini 11, albeit unsuccessfully.[49]\n Gemini 12 was launched from Launch Complex 19 at Cape Canaveral on 20:46 UTC on November 11, 1966. The Gemini Agena Target Vehicle had been launched about an hour and a half before.[50] The mission's first major objective was to rendezvous with this target vehicle. As the target and Gemini 12 capsule drew closer together, radar contact between the two deteriorated until it became unusable, forcing the crew to rendezvous manually. Aldrin used a sextant and rendezvous charts he helped create to give Lovell the right information to put the spacecraft in position to dock with the target vehicle.[51] Gemini 12 achieved the fourth docking with an Agena target vehicle.[52]\n The next task was to practice undocking and docking again. On undocking, one of the three latches caught, and Lovell had to use the Gemini's thrusters to free the spacecraft. Aldrin then docked again successfully a few minutes later. The flight plan then called for the Agena main engine to be fired to take the docked spacecraft into a higher orbit, but eight minutes after the Agena had been launched, it had suffered a loss of chamber pressure. The Mission and Flight Directors therefore decided not to risk the main engine. This would be the only mission objective that was not achieved.[52] Instead, the Agena's secondary propulsion system was used to allow the spacecraft to view the solar eclipse of November 12, 1966, over South America, which Lovell and Aldrin photographed through the spacecraft windows.[50]\n Aldrin performed three EVAs. The first was a standup EVA on November 12, in which the spacecraft door was opened and he stood up, but did not leave the spacecraft. The standup EVA mimicked some of the actions he would do during his free-flight EVA, so he could compare the effort expended between the two. It set an EVA record of two hours and twenty minutes. The next day Aldrin performed his free-flight EVA. He climbed across the newly installed hand-holds to the Agena and installed the cable needed for the gravity-gradient stabilization experiment. Aldrin performed numerous tasks, including installing electrical connectors and testing tools that would be needed for Project Apollo. A dozen two-minute rest periods prevented him from becoming fatigued. His second EVA concluded after two hours and six minutes. A third, 55-minute standup EVA was conducted on November 14, during which Aldrin took photographs, conducted experiments, and discarded some unneeded items.[50][53]\n On November 15, the crew initiated the automatic reentry system and splashed down in the Atlantic Ocean, where they were picked up by a helicopter, which took them to the awaiting aircraft carrier USS Wasp.[50][54] After the mission, his wife realized he had fallen into a depression, something she had not seen before.[51]\n Lovell and Aldrin were assigned to an Apollo crew with Neil Armstrong as commander, Lovell as command module pilot (CMP), and Aldrin as lunar module pilot (LMP). Their assignment as the backup crew of Apollo 9 was announced on November 20, 1967.[55] Due to design and manufacturing delays in the lunar module (LM), Apollo 8 and Apollo 9 swapped prime and backup crews, and Armstrong's crew became the backup for Apollo 8. Under the normal crew rotation scheme, Armstrong was expected to command Apollo 11.[56]\n Michael Collins, the CMP on the Apollo 8 prime crew, required surgery to remove a bone spur on his spine.[57] Lovell took his place on the Apollo 8 crew. When Collins recovered he joined Armstrong's crew as CMP. In the meantime, Fred Haise filled in as backup LMP, and Aldrin as backup CMP for Apollo 8.[58] While the CMP usually occupied the center couch on launch, Aldrin occupied it rather than Collins, as he had already been trained to operate its console on liftoff before Collins arrived.[59]\n Apollo 11 was the second American space mission made up entirely of astronauts who had already flown in space,[60] the first being Apollo 10.[61] The next would not be flown until STS-26 in 1988.[60] Deke Slayton, who was responsible for astronaut flight assignments, gave Armstrong the option to replace Aldrin with Lovell, since some thought Aldrin was difficult to work with. Armstrong thought it over for a day before declining. He had no issues working with Aldrin, and thought Lovell deserved his own command.[62]\n Early versions of the EVA checklist had the lunar module pilot as the first to step onto the lunar surface. However, when Aldrin learned that this might be amended, he lobbied within NASA for the original procedure to be followed. Multiple factors contributed to the final decision, including the physical positioning of the astronauts within the compact lunar lander, which made it easier for Armstrong to be the first to exit the spacecraft. Furthermore, there was little support for Aldrin's views among senior astronauts who would command later Apollo missions.[63] Collins has commented that he thought Aldrin \"resents not being first on the Moon more than he appreciates being second\".[64]\nAldrin and Armstrong did not have time to perform much geological training. The first lunar landing focused more on landing on the Moon and making it safely back to Earth than the scientific aspects of the mission. The duo was briefed by NASA and USGS geologists. They made one geological field trip to West Texas. The press followed them, and a helicopter made it hard for Aldrin and Armstrong to hear their instructor.[65]\n On the morning of July 16, 1969, an estimated one million spectators watched the launch of Apollo 11 from the highways and beaches in the vicinity of Cape Canaveral, Florida. The launch was televised live in 33 countries, with an estimated 25 million viewers in the United States alone. Millions more listened to radio broadcasts.[66][67] Propelled by a Saturn V rocket, Apollo 11 lifted off from Launch Complex 39 at the Kennedy Space Center on July 16, 1969, at 13:32:00 UTC (9:32:00 EDT),[68] and entered Earth orbit twelve minutes later. After one and a half orbits, the S-IVB third-stage engine pushed the spacecraft onto its trajectory toward the Moon. About thirty minutes later, the transposition, docking, and extraction maneuver was performed: this involved separating the command module Columbia from the spent S-IVB stage; turning around; and docking with, and extracting, the lunar module Eagle. The combined spacecraft then headed for the Moon, while the S-IVB stage continued on a trajectory past the Moon.[69]\n On July 19 at 17:21:50 UTC, Apollo 11 passed behind the Moon and fired its service propulsion engine to enter lunar orbit.[69] In the thirty orbits that followed,[70] the crew saw passing views of their landing site in the southern Sea of Tranquillity about 12 miles (19 km) southwest of the crater Sabine D.[71] At 12:52:00 UTC on July 20, Aldrin and Armstrong entered Eagle, and began the final preparations for lunar descent. At 17:44:00 Eagle separated from the Columbia.[69] Collins, alone aboard Columbia, inspected Eagle as it pirouetted before him to ensure the craft was not damaged and that the landing gear had correctly deployed.[72][73]\n Throughout the descent, Aldrin called out navigation data to Armstrong, who was busy piloting the Eagle.[74] Five minutes into the descent burn, and 6,000 feet (1,800 m) above the surface of the Moon, the LM guidance computer (LGC) distracted the crew with the first of several unexpected alarms that indicated that it could not complete all its tasks in real time and had to postpone some of them.[75] Due to the 1202\/1201 program alarms caused by spurious rendezvous radar inputs to the LGC,[76] Armstrong manually landed the Eagle instead of using the computer's autopilot. The Eagle landed at 20:17:40 UTC on Sunday July 20 with about 25 seconds of fuel left.[77]\n As a Presbyterian elder, Aldrin was the first and only person to hold a religious ceremony on the Moon. He radioed Earth: \"I'd like to take this opportunity to ask every person listening in, whoever and wherever they may be, to pause for a moment and contemplate the events of the past few hours, and to give thanks in his or her own way.\"[78] Using a kit given to him by his pastor,[79] he took communion and read Jesus's words from the New Testament's John 15:5, as Aldrin records it: \"I am the vine. You are the branches. Whoever remains in me, and I in him, will bear much fruit; for you can do nothing without me.\"[80] But he kept this ceremony secret because of a lawsuit over the reading of Genesis on Apollo 8.[81] In 1970 he commented: \"It was interesting to think that the very first liquid ever poured on the Moon, and the first food eaten there, were communion elements.\"[82]\n On reflection in his 2009 book, Aldrin said, \"Perhaps, if I had it to do over again, I would not choose to celebrate communion. Although it was a deeply meaningful experience for me, it was a Christian sacrament, and we had come to the moon in the name of all mankind – be they Christians, Jews, Muslims, animists, agnostics, or atheists. But at the time I could think of no better way to acknowledge the enormity of the Apollo 11 experience than by giving thanks to God.\"[83] Aldrin shortly hit upon a more universally human reference on the voyage back to Earth by publicly broadcasting his reading of the Old Testament's Psalm 8:3–4, as Aldrin records: \"When I considered the heavens, the work of Thy fingers, the moon and the stars which Thou hast ordained, what is man that Thou art mindful of him.\"[84] Photos of these liturgical documents reveal the conflict's development as Aldrin expresses faith.[85]\n Preparations for the EVA began at 23:43.[69] Once Armstrong and Aldrin were ready to go outside, Eagle was depressurized, and the hatch was opened at 02:39:33 on July 21.[69][86] Aldrin set foot on the Moon at 03:15:16 on July 21, 1969 (UTC), nineteen minutes after Armstrong first touched the surface.[69] Armstrong and Aldrin became the first and second people, respectively, to walk on the Moon. Aldrin's first words after he set foot on the Moon were \"Beautiful view\", to which Armstrong asked \"Isn't that something? Magnificent sight out here.\" Aldrin answered, \"Magnificent desolation.\"[87] Aldrin and Armstrong had trouble erecting the Lunar Flag Assembly, but with some effort secured it into the surface. Aldrin saluted the flag while Armstrong photographed the scene. Aldrin positioned himself in front of the video camera and began experimenting with different locomotion methods to move about the lunar surface to aid future moonwalkers.[88] During these experiments, President Nixon called the duo to congratulate them on the successful landing. Nixon closed with, \"Thank you very much, and all of us look forward to seeing you on the Hornet on Thursday.\"[89] Aldrin replied, \"I look forward to that very much, sir.\"[89][90]\n After the call, Aldrin began photographing and inspecting the spacecraft to document and verify its condition before their flight. Aldrin and Armstrong then set up a seismometer, to detect moonquakes, and a laser beam reflector. While Armstrong inspected a crater, Aldrin began the difficult task of hammering a metal tube into the surface to obtain a core sample.[91] Most of the iconic photographs of an astronaut on the Moon taken by the Apollo 11 astronauts are of Aldrin; Armstrong appears in just two color photographs. \"As the sequence of lunar operations evolved,\" Aldrin explained, \"Neil had the camera most of the time, and the majority of the pictures taken on the Moon that include an astronaut are of me. It wasn't until we were back on Earth and in the Lunar Receiving Laboratory looking over the pictures that we realized there were few pictures of Neil. My fault perhaps, but we had never simulated this during our training.\"[92]\n Aldrin reentered Eagle first but, as he tells it, before ascending the module's   ladder he became the first person to urinate on the Moon.[93] With some difficulty they lifted film and two sample boxes containing 21.55 kilograms (47.5 lb) of lunar surface material to the hatch using a flat cable pulley device.[94] Armstrong reminded Aldrin of a bag of memorial items in his sleeve pocket, and Aldrin tossed the bag down. It contained a mission patch for the Apollo 1 flight that Ed White never flew due to his death in a cabin fire during the launch rehearsal; medallions commemorating Yuri Gagarin, the first man in space (who had died the previous year in a test flight accident), and Vladimir Komarov, the first man to die in a space flight, and a silicon disk etched with goodwill messages from 73 nations.[95] After transferring to LM life support, the explorers lightened the ascent stage for the return to lunar orbit by tossing out their backpacks, lunar overshoes, an empty Hasselblad camera, and other equipment. The hatch was closed again at 05:01, and they repressurized the lunar module and settled down to sleep.[96]\n At 17:54 UTC, they lifted off in Eagle's ascent stage to rejoin Collins aboard Columbia in lunar orbit.[69] After rendezvous with Columbia, the ascent stage was jettisoned into lunar orbit, and Columbia made its way back to Earth.[97] It splashed down in the Pacific 2,660 km (1,440 nmi) east of Wake Island at 16:50 UTC (05:50 local time) on July 24.[69][98] The total mission duration was 195 hours, 18 minutes, 35 seconds.[99]\n Bringing back pathogens from the lunar surface was considered a possibility, albeit remote, so divers passed biological isolation garments (BIGs) to the astronauts, and assisted them into the life raft. The astronauts were winched on board the recovery helicopter, and flown to the aircraft carrier USS Hornet,[100] where they spent the first part of the Earth-based portion of 21 days of quarantine.[101] On August 13, the three astronauts rode in ticker-tape parades in their honor in New York and Chicago, attended by an estimated six million people.[102] An official state dinner that evening in Los Angeles celebrated the flight. President Richard Nixon honored each of them with the highest American civilian award, the Presidential Medal of Freedom (with distinction).[103][104]\n On September 16, 1969, the astronauts addressed a joint session of Congress where they thanked the representatives for their past support and implored them to continue funding the space effort.[105][106] The astronauts embarked on a 38-day world tour on September 29 that brought the astronauts to 22 foreign countries and included visits with leaders of multiple countries.[107] The last leg of the tour included Australia, South Korea, and Japan; the crew returned to the US on November 5, 1969.[108][109]\n After Apollo 11, Aldrin was kept busy giving speeches and making public appearances. In October 1970, he joined Soviet cosmonauts Andriyan Nikolayev and Vitaly Sevastyanov on their tour of the NASA space centers. He was also involved in the design of the Space Shuttle. With the Apollo program coming to an end, Aldrin, now a colonel, saw few prospects at NASA, and decided to return to the Air Force on July 1, 1971.[110] During his NASA career, he had spent 289 hours and 53 minutes in space, of which 7 hours and 52 minutes was in EVA.[28]\n Aldrin hoped to become Commandant of Cadets at the United States Air Force Academy, but the job went to his West Point classmate Hoyt S. Vandenberg Jr. Aldrin was made Commandant of the USAF Aerospace Research Pilot School at Edwards Air Force Base, California. Aldrin had neither managerial nor test pilot experience, but a third of the training curriculum was devoted to astronaut training and students flew a modified F-104 Starfighter to the edge of space.[111] Fellow Group 3 astronaut and moonwalker Alan Bean considered him well qualified for the job.[112]\n Aldrin did not get along well with his superior, Brigadier General Robert M. White, who had earned his USAF astronaut wings flying the X-15. Aldrin's celebrity status led people to defer to him more than the higher-ranking general.[113] There were two crashes at Edwards, of an A-7 Corsair II and a T-33. No people died, but the aircraft were destroyed and the accidents were attributed to insufficient supervision, which placed the blame on Aldrin. What he had hoped would be an enjoyable job became a highly stressful one.[114]\n Aldrin went to see the base surgeon. In addition to signs of depression, he experienced neck and shoulder pains, and hoped that the latter might explain the former.[115] He was hospitalized for depression at Wilford Hall Medical Center for four weeks.[116] His mother had committed suicide in May 1968, and he was plagued with guilt that his fame after Gemini 12 had contributed. His mother's father had also committed suicide, and he believed he inherited depression from them.[117] At the time there was great stigma related to mental illness and he was aware that it could not only be career-ending, but could result in his being ostracized socially.[115]\n In February 1972, General George S. Brown paid a visit to Edwards and informed Aldrin that the USAF Aerospace Research Pilot School was being renamed the USAF Test Pilot School and the astronaut training was being dropped. With the Apollo program winding down, and Air Force budgets being cut, the Air Force's interest in space diminished.[114] Aldrin elected to retire as a colonel on March 1, 1972, after 21 years of service. His father and General Jimmy Doolittle, a close friend of his father, attended the formal retirement ceremony.[114]\n Aldrin's father died on December 28, 1974, from complications following a heart attack.[118] Aldrin's autobiographies, Return to Earth (1973) and Magnificent Desolation (2009), recounted his struggles with clinical depression and alcoholism in the years after leaving NASA.[119][120][121] Encouraged by a therapist to take a regular job, Aldrin worked selling used cars, at which he had no talent.[122] Periods of hospitalization and sobriety alternated with bouts of heavy drinking. Eventually he was arrested for disorderly conduct. Finally, in October 1978, he quit drinking for good. Aldrin attempted to help others with drinking problems, including actor William Holden. Holden's girlfriend Stefanie Powers had portrayed Marianne, a woman with whom Aldrin had an affair, in the 1976 TV movie version of Return to Earth. Aldrin was saddened by Holden's alcohol-related death in 1981.[123]\n On September 9, 2002, Aldrin was lured to a Beverly Hills hotel on the pretext of being interviewed for a Japanese children's television show on the subject of space.[124] When he arrived, Moon landing conspiracy theorist Bart Sibrel accosted him with a film crew and demanded he swear on a Bible that the Moon landings were not faked. After a brief confrontation, during which Sibrel followed Aldrin despite being told to leave him alone, and called him \"a coward, a liar, and a thief\" the 72-year-old Aldrin punched Sibrel in the jaw, which was caught on camera by Sibrel's film crew. Aldrin said he had acted to defend himself and his stepdaughter. Witnesses said Sibrel had aggressively poked Aldrin with a Bible. Additional mitigating factors were that Sibrel sustained no visible injury and did not seek medical attention, and that Aldrin had no criminal record. The police declined to press charges against Aldrin.[125][126]\n In 2005, while being interviewed for a Science Channel documentary titled First on the Moon: The Untold Story, Aldrin told an interviewer they had seen an unidentified flying object (UFO). The documentary makers omitted the crew's conclusion that they probably saw one of the four detached spacecraft adapter panels from the upper stage of the Saturn V rocket. The panels had been jettisoned before the separation maneuver so they closely followed the spacecraft until the first mid-course correction. When Aldrin appeared on The Howard Stern Show on August 15, 2007, Stern asked him about the supposed UFO sighting. Aldrin confirmed that there was no such sighting of anything deemed extraterrestrial and said they were, and are, \"99.9 percent\" sure the object was the detached panel.[128][129] According to Aldrin his words had been taken out of context. He made a request to the Science Channel to make a correction, but was refused.[130]\n In December 2016, Aldrin was part of a tourist group visiting the Amundsen–Scott South Pole Station in Antarctica when he fell ill and was evacuated, first to McMurdo Station and from there to Christchurch, New Zealand.[131] At 86 years of age, Aldrin's visit made him the oldest person to reach the South Pole. He had traveled to the North Pole in 1998.[132][133]\n After leaving NASA, Aldrin continued to advocate for space exploration. In 1985 he joined the University of North Dakota (UND)'s College of Aerospace Sciences at the invitation of John D. Odegard, the dean of the college. Aldrin helped to develop UND's Space Studies program and brought David Webb from NASA to serve as the department's first chair.[134] To further promote space exploration, and to commemorate the 40th anniversary of the first lunar landing, Aldrin teamed up with Snoop Dogg, Quincy Jones, Talib Kweli, and Soulja Boy to create the rap single and video \"Rocket Experience\", proceeds from which were donated to Aldrin's non-profit foundation, ShareSpace.[135] He is also a member of the Mars Society's Steering committee.[136]\n In 1985, Aldrin proposed a special spacecraft trajectory now known as the Aldrin cycler.[137][138] Cycler trajectories offer reduced cost of repeated travel to Mars by using less propellant. The Aldrin cycler provided a five and a half month journey from the Earth to Mars, with a return trip to Earth of the same duration on a twin cycler orbit. Aldrin continues to research this concept with engineers from Purdue University.[139] In 1996 Aldrin founded Starcraft Boosters, Inc. (SBI) to design reusable rocket launchers.[140]\n In December 2003, Aldrin published an opinion piece in The New York Times criticizing NASA's objectives. In it, he voiced concern about NASA's development of a spacecraft \"limited to transporting four astronauts at a time with little or no cargo carrying capability\" and declared the goal of sending astronauts back to the Moon was \"more like reaching for past glory than striving for new triumphs\".[141]\n In a June 2013 opinion piece in The New York Times, Aldrin supported a human mission to Mars and which viewed the Moon \"not as a destination but more a point of departure, one that places humankind on a trajectory to homestead Mars and become a two-planet species.\"[142] In August 2015, Aldrin, in association with the Florida Institute of Technology, presented a master plan to NASA for consideration where astronauts, with a tour of duty of ten years, establish a colony on Mars before the year 2040.[143]\n Aldrin was awarded the Air Force Distinguished Service Medal (DSM) in 1969 for his role as lunar module pilot on Apollo 11.[144] He was awarded an oak leaf cluster in 1972 in lieu of a second DSM for his role in both the Korean War and in the space program,[144] and the Legion of Merit for his role in the Gemini and Apollo programs.[144] During a 1966 ceremony marking the end of the Gemini program, Aldrin was awarded the NASA Exceptional Service Medal by President Johnson at LBJ Ranch.[145] He was awarded the NASA Distinguished Service Medal in 1970 for the Apollo 11 mission.[146][147] Aldrin was one of ten Gemini astronauts inducted into the International Space Hall of Fame in 1982.[148][149] He was also inducted into the U.S. Astronaut Hall of Fame in 1993,[150][151] the National Aviation Hall of Fame in 2000,[152] and the New Jersey Hall of Fame in 2008.[153]\nThe Toy Story character Buzz Lightyear was named in honor of Buzz Aldrin.[154]\n In 1999, while celebrating the 30th anniversary of the lunar landing, Vice President Al Gore, who was also the vice-chancellor of the Smithsonian Institution's Board of Regents, presented the Apollo 11 crew with the Smithsonian Institution's Langley Gold Medal for aviation. After the ceremony, the crew went to the White House and presented President Bill Clinton with an encased Moon rock.[155][156] The Apollo 11 crew was awarded the New Frontier Congressional Gold Medal in the Capitol Rotunda in 2011. During the ceremony, NASA administrator Charles Bolden said, \"Those of us who have had the privilege to fly in space followed the trail they forged.\"[157][158]\n The Apollo 11 crew were awarded the Collier Trophy in 1969. The National Aeronautic Association president awarded a duplicate trophy to Collins and Aldrin at a ceremony.[159] The crew was awarded the 1969 General Thomas D. White USAF Space Trophy.[160] The National Space Club named the crew the winners of the 1970 Dr. Robert H. Goddard Memorial Trophy, awarded annually for the greatest achievement in spaceflight.[161] They received the international Harmon Trophy for aviators in 1970,[162][163] conferred to them by Vice President Spiro Agnew in 1971.[164] Agnew also presented them the Hubbard Medal of the National Geographic Society in 1970. He told them, \"You've won a place alongside Christopher Columbus in American history\".[165] In 1970, the Apollo 11 team were co-winners of the Iven C. Kincheloe award from the Society of Experimental Test Pilots along with Darryl Greenamyer who broke the world speed record for piston engine airplanes.[166] For contributions to the television industry, they were honored with round plaques on the Hollywood Walk of Fame.[167]\n In 2001, President George W. Bush appointed Aldrin to the Commission on the Future of the United States Aerospace Industry.[168] Aldrin received the 2003 Humanitarian Award from Variety, the Children's Charity, which, according to the organization, \"is given to an individual who has shown unusual understanding, empathy, and devotion to mankind.\"[169] In 2006, the Space Foundation awarded him its highest honor, the General James E. Hill Lifetime Space Achievement Award.[170]\n Aldrin received honorary degrees from six colleges and universities,[28] and was named as the Chancellor of the International Space University in 2015.[171] He was a member of the National Space Society's Board of Governors,[172] and has served as the organization's chairman. In 2016, his hometown middle school in Montclair, New Jersey, was renamed Buzz Aldrin Middle School.[173] The Aldrin crater on the Moon near the Apollo 11 landing site and Asteroid 6470 Aldrin are named in his honor.[148]\n In 2019, Aldrin was awarded the Starmus Festival's Stephen Hawking Medal for Science Communication for Lifetime Achievement.[174] On his 93rd birthday he was honored by Living Legends of Aviation.[175] On May 5, 2023, he received an honorary promotion to the rank of brigadier general in the United States Air Force, as well as being made an honorary Space Force guardian.[176][177][178] On May 24, 2023, he received the 40 over 40 award by Monaco Voice, calling him \"Guardian of the Galaxy\".[179]\n Aldrin has been married four times. His first marriage was on December 29, 1954, to Joan Archer, a Rutgers University and Columbia University alumna with a master's degree. They had three children, James, Janice and Andrew. They filed for divorce in 1974.[180][181] His second wife was Beverly Van Zile, whom he married on December 31, 1975,[182] and divorced in 1978. His third wife was Lois Driggs Cannon, whom he married on February 14, 1988.[183] Their divorce was finalized in December 2012. The settlement included 50 percent of their $475,000 bank account and $9,500 a month plus 30 percent of his annual income, estimated at more than $600,000.[184][185] As of 2017,[update] he had one grandson, Jeffrey Schuss, born to his daughter Janice, and three great-grandsons and one great-granddaughter.[186]\n In 2018, Aldrin was involved in a legal dispute with his children Andrew and Janice and former business manager Christina Korp over their claims that he was mentally impaired through dementia and Alzheimer's disease. His children alleged that he made new friends who were alienating him from the family and encouraging him to spend his savings at a high rate. They sought to be named legal guardians so they could control his finances.[187] In June, Aldrin filed a lawsuit against Andrew, Janice, Korp, and businesses and foundations run by the family.[188] Aldrin alleged that Janice was not acting in his financial interest and that Korp was exploiting the elderly. He sought to remove Andrew's control of Aldrin's social media accounts, finances, and businesses. The situation ended when his children withdrew their petition and he dropped the lawsuit in March 2019, several months before the 50th anniversary of the Apollo 11 mission.[189]\n On January 20, 2023, his 93rd birthday, Aldrin announced on Twitter that he had married for the fourth time, to his 63-year-old companion, Anca Faur.[190][175]\n Aldrin is an active supporter of the Republican Party, headlining fundraisers for its members of Congress[191] and endorsing its candidates. He appeared at a rally for George W. Bush in 2004 and campaigned for Paul Rancatore in Florida in 2008, Mead Treadwell in Alaska in 2014[192] and Dan Crenshaw in Texas in 2018.[193] He appeared at the 2019 State of the Union Address as a guest of President Donald Trump.[194]\n Buzz Aldrin is the first Freemason to set foot on the Moon.[195] Aldrin was initiated into Freemasonry at Oak Park Lodge No. 864 in Alabama and raised at Lawrence N. Greenleaf Lodge, No. 169 in Colorado.[196]\n By the time Aldrin stepped onto the lunar surface, he was a member of two Masonic lodges: Montclair Lodge No. 144 in New Jersey and Clear Lake Lodge No. 1417 in Seabrook, Texas, where he was invited to serve on the High Council and was ordained in the 33rd degree of the Ancient and Accepted Scottish Rite.[197]\n Aldrin is also a member of York Rite and Arabia Shrine Temple of Houston.[198]\n In 2007, Aldrin confirmed to Time magazine that he had recently had a face-lift, joking that the g-forces he was exposed to in space \"caused a sagging jowl that needed some attention\".[199]\n Following the 2012 death of his Apollo 11 colleague Neil Armstrong, Aldrin said he was\n ... deeply saddened by the passing ... I know I am joined by many millions of others from around the world in mourning the passing of a true American hero and the best pilot I ever knew ... I had truly hoped that on July 20, 2019, Neil, Mike and I would be standing together to commemorate the 50th Anniversary of our moon landing.[200] Aldrin has primarily resided in the Los Angeles area, including Beverly Hills and Laguna Beach since 1985.[201][202] In 2014, he sold his Westwood condominium;[203] this was after his third divorce in 2012. He also lives in Satellite Beach, Florida.[204][205][when?]\n Aldrin has been a teetotaler since 1978.[206]\n Aldrin has been portrayed by:\n"}
{"key":"Nanotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/Nanotechnology","headline":"Nanotechnology - Wikipedia","content":"\n Nanotechnology was defined by the National Nanotechnology Initiative as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers (nm). At this scale, commonly known as the nanoscale, surface area and quantum mechanical effects become important in describing properties of matter. The definition of nanotechnology is inclusive of all types of research and technologies that deal with these special properties. It is therefore common to see the plural form \"nanotechnologies\" as well as \"nanoscale technologies\" to refer to the broad range of research and applications whose common trait is size.[1] An earlier description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology.[2]\n Nanotechnology as defined by size is naturally broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage,[3][4] engineering,[5] microfabrication,[6] and molecular engineering.[7] The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly,[8] from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.\n Scientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications, such as in nanomedicine, nanoelectronics, biomaterials energy production, and consumer products. On the other hand, nanotechnology raises many of the same issues as any new technology, including concerns about the toxicity and environmental impact of nanomaterials,[9] and their potential effects on global economics, as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.\n The concepts that seeded nanotechnology were first discussed in 1959 by physicist Richard Feynman in his talk There's Plenty of Room at the Bottom, in which he described the possibility of synthesis via direct manipulation of atoms.\n The term \"nano-technology\" was first used by Norio Taniguchi in 1974, though it was not widely known. Inspired by Feynman's concepts, K. Eric Drexler used the term \"nanotechnology\" in his 1986 book Engines of Creation: The Coming Era of Nanotechnology, which proposed the idea of a nanoscale \"assembler\" which would be able to build a copy of itself and of other items of arbitrary complexity with atomic control. Also in 1986, Drexler co-founded The Foresight Institute (with which he is no longer affiliated) to help increase public awareness and understanding of nanotechnology concepts and implications.\n The emergence of nanotechnology as a field in the 1980s occurred through convergence of Drexler's theoretical and public work, which developed and popularized a conceptual framework for nanotechnology, and high-visibility experimental advances that drew additional wide-scale attention to the prospects of atomic control of matter. In the 1980s, two major breakthroughs sparked the growth of nanotechnology in the modern era. First, the invention of the scanning tunneling microscope in 1981 which enabled visualization of individual atoms and bonds, and was successfully used to manipulate individual atoms in 1989. The microscope's developers Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory received a Nobel Prize in Physics in 1986.[10][11] Binnig, Quate and Gerber also invented the analogous atomic force microscope that year.\n Second, fullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry.[12][13] C60 was not initially described as nanotechnology; the term was used regarding subsequent work with related carbon nanotubes (sometimes called graphene tubes or Bucky tubes) which suggested potential applications for nanoscale electronics and devices. The discovery of carbon nanotubes is largely attributed to Sumio Iijima of NEC in 1991,[14] for which Iijima won the inaugural 2008 Kavli Prize in Nanoscience.\n In the early 2000s, the field garnered increased scientific, political, and commercial attention that led to both controversy and progress. Controversies emerged regarding the definitions and potential implications of nanotechnologies, exemplified by the Royal Society's report on nanotechnology.[15] Challenges were raised regarding the feasibility of applications envisioned by advocates of molecular nanotechnology, which culminated in a public debate between Drexler and Smalley in 2001 and 2003.[16]\n Meanwhile, commercialization of products based on advancements in nanoscale technologies began emerging. These products are limited to bulk applications of nanomaterials and do not involve atomic control of matter. Some examples include the Silver Nano platform for using silver nanoparticles as an antibacterial agent, nanoparticle-based transparent sunscreens, carbon fiber strengthening using silica nanoparticles, and carbon nanotubes for stain-resistant textiles.[17][18]\n Governments moved to promote and fund research into nanotechnology, such as in the U.S. with the National Nanotechnology Initiative, which formalized a size-based definition of nanotechnology and established funding for research on the nanoscale, and in Europe via the European Framework Programmes for Research and Technological Development.\n By the mid-2000s new and serious scientific attention began to flourish.  Projects emerged to produce nanotechnology roadmaps[19][20]  which center on atomically precise manipulation of matter and discuss existing and projected capabilities, goals, and applications.\n Nanotechnology is the science and engineering of functional systems at the molecular scale. This covers both current work and concepts that are more advanced. In its original sense, nanotechnology refers to the projected ability to construct items from the bottom up, using techniques and tools being developed today to make complete, high-performance products.\n One nanometer (nm) is one billionth, or 10−9, of a meter. By comparison, typical carbon–carbon bond lengths, or the spacing between these atoms in a molecule, are in the range 0.12–0.15 nm, and a DNA double-helix has a diameter around 2 nm. On the other hand, the smallest cellular life-forms, the bacteria of the genus Mycoplasma, are around 200 nm in length. By convention, nanotechnology is taken as the scale range 1 to 100 nm following the definition used by the National Nanotechnology Initiative in the US. The lower limit is set by the size of atoms (hydrogen has the smallest atoms, which are approximately a quarter of a nm kinetic diameter) since nanotechnology must build its devices from atoms and molecules. The upper limit is more or less arbitrary but is around the size below which the phenomena not observed in larger structures start to become apparent and can be made use of in the nano device.[21] These new phenomena make nanotechnology distinct from devices that are merely miniaturized versions of an equivalent macroscopic device; such devices are on a larger scale and come under the description of microtechnology.[22]\n To put that scale in another context, the comparative size of a nanometer to a meter is the same as that of a marble to the size of the earth.[23] Or another way of putting it: a nanometer is the amount an average man's beard grows in the time it takes him to raise the razor to his face.[23]\n Two main approaches are used in nanotechnology. In the \"bottom-up\" approach, materials and devices are built from molecular components which assemble themselves chemically by principles of molecular recognition.[24] In the \"top-down\" approach, nano-objects are constructed from larger entities without atomic-level control.[25]\n Areas of physics such as nanoelectronics, nanomechanics, nanophotonics and nanoionics have evolved during the last few decades to provide a basic scientific foundation of nanotechnology.\n Several phenomena become pronounced as the size of the system decreases. These include statistical mechanical effects, as well as quantum mechanical effects, for example the \"quantum size effect\" where the electronic properties of solids are altered with great reductions in particle size. This effect does not come into play by going from macro to micro dimensions. However, quantum effects can become significant when the nanometer size range is reached, typically at distances of 100 nanometers or less, the so-called quantum realm. Additionally, a number of physical (mechanical, electrical, optical, etc.) properties change when compared to macroscopic systems. One example is the increase in surface area to volume ratio altering mechanical, thermal and catalytic properties of materials. Diffusion and reactions at nanoscale, nanostructures materials and nanodevices with fast ion transport are generally referred to nanoionics. Mechanical properties of nanosystems are of interest in the nanomechanics research. The catalytic activity of nanomaterials also opens potential risks in their interaction with biomaterials.\n Materials reduced to the nanoscale can show different properties compared to what they exhibit on a macroscale, enabling unique applications. For instance, opaque substances can become transparent (copper); stable materials can turn combustible (aluminium); insoluble materials may become soluble (gold). A material such as gold, which is chemically inert at normal scales, can serve as a potent chemical catalyst at nanoscales. Much of the fascination with nanotechnology stems from these quantum and surface phenomena that matter exhibits at the nanoscale.[26]\n Modern synthetic chemistry has reached the point where it is possible to prepare small molecules to almost any structure. These methods are used today to manufacture a wide variety of useful chemicals such as pharmaceuticals or commercial polymers. This ability raises the question of extending this kind of control to the next-larger level, seeking methods to assemble these single molecules into supramolecular assemblies consisting of many molecules arranged in a well defined manner.\n These approaches utilize the concepts of molecular self-assembly and\/or supramolecular chemistry to automatically arrange themselves into some useful conformation through a bottom-up approach. The concept of molecular recognition is especially important: molecules can be designed so that a specific configuration or arrangement is favored due to non-covalent intermolecular forces. The Watson–Crick basepairing rules are a direct result of this, as is the specificity of an enzyme being targeted to a single substrate, or the specific folding of the protein itself. Thus, two or more components can be designed to be complementary and mutually attractive so that they make a more complex and useful whole.\n Such bottom-up approaches should be capable of producing devices in parallel and be much cheaper than top-down methods, but could potentially be overwhelmed as the size and complexity of the desired assembly increases. Most useful structures require complex and thermodynamically unlikely arrangements of atoms. Nevertheless, there are many examples of self-assembly based on molecular recognition in biology, most notably Watson–Crick basepairing and enzyme-substrate interactions. The challenge for nanotechnology is whether these principles can be used to engineer new constructs in addition to natural ones.\n Molecular nanotechnology, sometimes called molecular manufacturing, describes engineered nanosystems (nanoscale machines) operating on the molecular scale. Molecular nanotechnology is especially associated with the molecular assembler, a machine that can produce a desired structure or device atom-by-atom using the principles of mechanosynthesis. Manufacturing in the context of productive nanosystems is not related to, and should be clearly distinguished from, the conventional technologies used to manufacture nanomaterials such as carbon nanotubes and nanoparticles.\n When the term \"nanotechnology\" was independently coined and popularized by Eric Drexler (who at the time was unaware of an earlier usage by Norio Taniguchi) it referred to a future manufacturing technology based on molecular machine systems. The premise was that molecular-scale biological analogies of traditional machine components demonstrated molecular machines were possible: by the countless examples found in biology, it is known that sophisticated, stochastically optimized biological machines can be produced.\n It is hoped that developments in nanotechnology will make possible their construction by some other means, perhaps using biomimetic principles. However, Drexler and other researchers[27] have proposed that advanced nanotechnology, although perhaps initially implemented by biomimetic means, ultimately could be based on mechanical engineering principles, namely, a manufacturing technology based on the mechanical functionality of these components (such as gears, bearings, motors, and structural members) that would enable programmable, positional assembly to atomic specification.[28] The physics and engineering performance of exemplar designs were analyzed in Drexler's book Nanosystems: Molecular Machinery, Manufacturing, and Computation.[2]\n In general it is very difficult to assemble devices on the atomic scale, as one has to position atoms on other atoms of comparable size and stickiness. Another view, put forth by Carlo Montemagno,[29] is that future nanosystems will be hybrids of silicon technology and biological molecular machines. Richard Smalley argued that mechanosynthesis are impossible due to the difficulties in mechanically manipulating individual molecules.\n This led to an exchange of letters in the ACS publication Chemical & Engineering News in 2003.[30] Though biology clearly demonstrates that molecular machine systems are possible, non-biological molecular machines are today only in their infancy. Leaders in research on non-biological molecular machines are Alex Zettl and his colleagues at Lawrence Berkeley Laboratories and UC Berkeley.[31] They have constructed at least three distinct molecular devices whose motion is controlled from the desktop with changing voltage: a nanotube nanomotor, a molecular actuator,[32] and a nanoelectromechanical relaxation oscillator.[33] See nanotube nanomotor for more examples.\n An experiment indicating that positional molecular assembly is possible was performed by Ho and Lee at Cornell University in 1999. They used a scanning tunneling microscope to move an individual carbon monoxide molecule (CO) to an individual iron atom (Fe) sitting on a flat silver crystal, and chemically bound the CO to the Fe by applying a voltage.\n The nanomaterials field includes subfields which develop or study materials having unique properties arising from their nanoscale dimensions.[36]\n These seek to arrange smaller components into more complex assemblies.\n These seek to create smaller devices by using larger ones to direct their assembly.\n These seek to develop components of a desired functionality without regard to how they might be assembled.\n These subfields seek to anticipate what inventions nanotechnology might yield, or attempt to propose an agenda along which inquiry might progress. These often take a big-picture view of nanotechnology, with more emphasis on its societal implications than the details of how such inventions could actually be created.\n Nanomaterials can be classified in 0D, 1D, 2D and 3D nanomaterials. The dimensionality play a major role in determining the characteristic of nanomaterials including physical, chemical and biological characteristics. With the decrease in dimensionality, an increase in surface-to-volume ratio is observed. This indicate that smaller dimensional nanomaterials have higher surface area compared to 3D nanomaterials. Recently, two dimensional (2D) nanomaterials are extensively investigated for electronic, biomedical, drug delivery and biosensor applications.\n There are several important modern developments. The atomic force microscope (AFM) and the Scanning Tunneling Microscope (STM) are two early versions of scanning probes that launched nanotechnology. There are other types of scanning probe microscopy. Although conceptually similar to the scanning confocal microscope developed by Marvin Minsky in 1961 and the scanning acoustic microscope (SAM) developed by Calvin Quate and coworkers in the 1970s, newer scanning probe microscopes have much higher resolution, since they are not limited by the wavelength of sound or light.\n The tip of a scanning probe can also be used to manipulate nanostructures (a process called positional assembly). Feature-oriented scanning methodology may be a promising way to implement these nanomanipulations in automatic mode.[54][55] However, this is still a slow process because of low scanning velocity of the microscope.\n Various techniques of nanolithography such as optical lithography, X-ray lithography, dip pen nanolithography, electron beam lithography or nanoimprint lithography were also developed. Lithography is a top-down fabrication technique where a bulk material is reduced in size to nanoscale pattern.\n Another group of nanotechnological techniques include those used for fabrication of nanotubes and nanowires, those used in semiconductor fabrication such as deep ultraviolet lithography, electron beam lithography, focused ion beam machining, nanoimprint lithography, atomic layer deposition, and molecular vapor deposition, and further including molecular self-assembly techniques such as those employing di-block copolymers. The precursors of these techniques preceded the nanotech era, and are extensions in the development of scientific advancements rather than techniques which were devised with the sole purpose of creating nanotechnology and which were results of nanotechnology research.[56]\n The top-down approach anticipates nanodevices that must be built piece by piece in stages, much as manufactured items are made. Scanning probe microscopy is an important technique both for characterization and synthesis of nanomaterials. Atomic force microscopes and scanning tunneling microscopes can be used to look at surfaces and to move atoms around. By designing different tips for these microscopes, they can be used for carving out structures on surfaces and to help guide self-assembling structures. By using, for example, feature-oriented scanning approach, atoms or molecules can be moved around on a surface with scanning probe microscopy techniques.[54][55] At present, it is expensive and time-consuming for mass production but very suitable for laboratory experimentation.\n In contrast, bottom-up techniques build or grow larger structures atom by atom or molecule by molecule. These techniques include chemical synthesis, self-assembly and positional assembly. Dual polarisation interferometry is one tool suitable for characterisation of self assembled thin films. Another variation of the bottom-up approach is molecular beam epitaxy or MBE. Researchers at Bell Telephone Laboratories like John R. Arthur. Alfred Y. Cho, and Art C. Gossard developed and implemented MBE as a research tool in the late 1960s and 1970s. Samples made by MBE were key to the discovery of the fractional quantum Hall effect for which the 1998 Nobel Prize in Physics was awarded. MBE allows scientists to lay down atomically precise layers of atoms and, in the process, build up complex structures. Important for research on semiconductors, MBE is also widely used to make samples and devices for the newly emerging field of spintronics.\n However, new therapeutic products, based on responsive nanomaterials, such as the ultradeformable, stress-sensitive Transfersome vesicles, are under development and already approved for human use in some countries.[57]\n As of August 21, 2008, the Project on Emerging Nanotechnologies estimates that over 800 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week.[18] The project lists all of the products in a publicly accessible online database. Most applications are limited to the use of \"firstgeneration\" passive nanomaterials which includes titanium dioxide in sunscreen, cosmetics, surface coatings,[58] and some food products; Carbon allotropes used to produce gecko tape; silver in food packaging, clothing, disinfectants and household appliances; zinc oxide in sunscreens and cosmetics, surface coatings, paints and outdoor furniture varnishes; and cerium oxide as a fuel catalyst.[17]\n Further applications allow tennis balls to last longer, golf balls to fly straighter, and even bowling balls to become more durable and have a harder surface. Trousers and socks have been infused with nanotechnology so that they will last longer and keep people cool in the summer. Bandages are being infused with silver nanoparticles to heal cuts faster.[59] Video game consoles and personal computers may become cheaper, faster, and contain more memory thanks to nanotechnology.[60] Also, to build structures for on chip computing with light, for example on chip optical quantum information processing, and picosecond transmission of information.[61]\n Nanotechnology may have the ability to make existing medical applications cheaper and easier to use in places like the general practitioners' offices and at homes.[62] Cars are being manufactured using nanomaterials in such ways that car parts require fewer metals during manufacturing and less fuel to operate in the future.[63]\n Nanoencapsulation is a technology involving the enclosure of active substances within carriers or particles of nanometer size. Typically, these carriers are nanoparticles that offer various advantages, such as enhanced bioavailability, controlled release, targeted delivery, and protection of the encapsulated substances. In the medical field, nanoencapsulation plays a significant role in drug delivery and therapeutic strategies. It facilitates more efficient drug administration, minimizing side effects, and increasing treatment effectiveness by encapsulating drugs within nanoparticles. Nanoencapsulation is particularly useful for improving the bioavailability of poorly water-soluble drugs, enabling controlled and sustained drug release, and supporting the development of targeted therapies. These features collectively contribute to advancements in medical treatments and patient care.[64][65]\n Scientists are now turning to nanotechnology in an attempt to develop diesel engines with cleaner exhaust fumes. Platinum is currently used as the diesel engine catalyst in these engines. The catalyst is what cleans the exhaust fume particles. First, a reduction catalyst is employed to take nitrogen atoms from NOx molecules in order to free oxygen. Next the oxidation catalyst oxidizes the hydrocarbons and carbon monoxide to form carbon dioxide and water.[citation needed] Platinum is used in both the reduction and the oxidation catalysts.[66] Using platinum though, is inefficient in that it is expensive and unsustainable. Danish company InnovationsFonden invested DKK 15 million in a search for new catalyst substitutes using nanotechnology. The goal of the project, launched in the autumn of 2014, is to maximize surface area and minimize the amount of material required. Objects tend to minimize their surface energy; two drops of water, for example, will join to form one drop and decrease surface area. If the catalyst's surface area that is exposed to the exhaust fumes is maximized, efficiency of the catalyst is maximized. The team working on this project aims to create nanoparticles that will not merge. Every time the surface is optimized, material is saved. Thus, creating these nanoparticles will increase the effectiveness of the resulting diesel engine catalyst—in turn leading to cleaner exhaust fumes—and will decrease cost. If successful, the team hopes to reduce platinum use by 25%.[67]\n Nanotechnology also has a prominent role in the fast developing field of Tissue Engineering. When designing scaffolds, researchers attempt to mimic the nanoscale features of a cell's microenvironment to direct its differentiation down a suitable lineage.[68] For example, when creating scaffolds to support the growth of bone, researchers may mimic osteoclast resorption pits.[69]\n Researchers have successfully used DNA origami-based nanobots capable of carrying out logic functions to achieve targeted drug delivery in cockroaches. It is said that the computational power of these nanobots can be scaled up to that of a Commodore 64.[70]\n An area of concern is the effect that industrial-scale manufacturing and use of nanomaterials would have on human health and the environment, as suggested by nanotoxicology research. For these reasons, some groups advocate that nanotechnology be regulated by governments. Others counter that overregulation would stifle scientific research and the development of beneficial innovations. Public health research agencies, such as the National Institute for Occupational Safety and Health are actively conducting research on potential health effects stemming from exposures to nanoparticles.[71][72]\n Some nanoparticle products may have unintended consequences. Researchers have discovered that bacteriostatic silver nanoparticles used in socks to reduce foot odor are being released in the wash.[73] These particles are then flushed into the waste water stream and may destroy bacteria which are critical components of natural ecosystems, farms, and waste treatment processes.[74]\n Public deliberations on risk perception in the US and UK carried out by the Center for Nanotechnology in Society found that participants were more positive about nanotechnologies for energy applications than for health applications, with health applications raising moral and ethical dilemmas such as cost and availability.[75]\n Experts, including director of the Woodrow Wilson Center's Project on Emerging Nanotechnologies David Rejeski, have testified[76] that successful commercialization depends on adequate oversight, risk research strategy, and public engagement. Berkeley, California is currently the only city in the United States to regulate nanotechnology;[77] In 2008, Cambridge, Massachusetts considered enacting a similar law,[78] but ultimately rejected it.[79]\n Nanofibers are used in several areas and in different products, in everything from aircraft wings to tennis rackets. Inhaling airborne nanoparticles and nanofibers may lead to a number of pulmonary diseases, e.g. fibrosis.[80] Researchers have found that when rats breathed in nanoparticles, the particles settled in the brain and lungs, which led to significant increases in biomarkers for inflammation and stress response[81] and that nanoparticles induce skin aging through oxidative stress in hairless mice.[82][83]\n A two-year study at UCLA's School of Public Health found lab mice consuming nano-titanium dioxide showed DNA and chromosome damage to a degree \"linked to all the big killers of man, namely cancer, heart disease, neurological disease and aging\".[84]\n A Nature Nanotechnology study suggests some forms of carbon nanotubes – a poster child for the \"nanotechnology revolution\" – could be as harmful as asbestos if inhaled in sufficient quantities. Anthony Seaton of the Institute of Occupational Medicine in Edinburgh, Scotland, who contributed to the article on carbon nanotubes said \"We know that some of them probably have the potential to cause mesothelioma. So those sorts of materials need to be handled very carefully.\"[85] In the absence of specific regulation forthcoming from governments, Paull and Lyons (2008) have called for an exclusion of engineered nanoparticles in food.[86] A newspaper article reports that workers in a paint factory developed serious lung disease and nanoparticles were found in their lungs.[87][88][89][90]\n Calls for tighter regulation of nanotechnology have occurred alongside a growing debate related to the human health and safety risks of nanotechnology.[91] There is significant debate about who is responsible for the regulation of nanotechnology. Some regulatory agencies currently cover some nanotechnology products and processes (to varying degrees) – by \"bolting on\" nanotechnology to existing regulations – there are clear gaps in these regimes.[92] Davies (2008) has proposed a regulatory road map describing steps to deal with these shortcomings.[93]\n Stakeholders concerned by the lack of a regulatory framework to assess and control risks associated with the release of nanoparticles and nanotubes have drawn parallels with bovine spongiform encephalopathy (\"mad cow\" disease), thalidomide, genetically modified food,[94] nuclear energy, reproductive technologies, biotechnology, and asbestosis. Andrew Maynard, chief science advisor to the Woodrow Wilson Center's Project on Emerging Nanotechnologies, concludes that there is insufficient funding for human health and safety research, and as a result there is currently limited understanding of the human health and safety risks associated with nanotechnology.[95] As a result, some academics have called for stricter application of the precautionary principle, with delayed marketing approval, enhanced labelling and additional safety data development requirements in relation to certain forms of nanotechnology.[96]\n The Royal Society report[15] identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that \"manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure\" (p. xiii).\n The Center for Nanotechnology in Society has found that people respond to nanotechnologies differently, depending on application – with participants in public deliberations more positive about nanotechnologies for energy than health applications – suggesting that any public calls for nano regulations may differ by technology sector.[75]\n"}
{"key":"Nanotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/Nanotechnology","headline":"Nanotechnology - Wikipedia","content":"\n Nanotechnology was defined by the National Nanotechnology Initiative as the manipulation of matter with at least one dimension sized from 1 to 100 nanometers (nm). At this scale, commonly known as the nanoscale, surface area and quantum mechanical effects become important in describing properties of matter. The definition of nanotechnology is inclusive of all types of research and technologies that deal with these special properties. It is therefore common to see the plural form \"nanotechnologies\" as well as \"nanoscale technologies\" to refer to the broad range of research and applications whose common trait is size.[1] An earlier description of nanotechnology referred to the particular technological goal of precisely manipulating atoms and molecules for fabrication of macroscale products, also now referred to as molecular nanotechnology.[2]\n Nanotechnology as defined by size is naturally broad, including fields of science as diverse as surface science, organic chemistry, molecular biology, semiconductor physics, energy storage,[3][4] engineering,[5] microfabrication,[6] and molecular engineering.[7] The associated research and applications are equally diverse, ranging from extensions of conventional device physics to completely new approaches based upon molecular self-assembly,[8] from developing new materials with dimensions on the nanoscale to direct control of matter on the atomic scale.\n Scientists currently debate the future implications of nanotechnology. Nanotechnology may be able to create many new materials and devices with a vast range of applications, such as in nanomedicine, nanoelectronics, biomaterials energy production, and consumer products. On the other hand, nanotechnology raises many of the same issues as any new technology, including concerns about the toxicity and environmental impact of nanomaterials,[9] and their potential effects on global economics, as well as speculation about various doomsday scenarios. These concerns have led to a debate among advocacy groups and governments on whether special regulation of nanotechnology is warranted.\n The concepts that seeded nanotechnology were first discussed in 1959 by physicist Richard Feynman in his talk There's Plenty of Room at the Bottom, in which he described the possibility of synthesis via direct manipulation of atoms.\n The term \"nano-technology\" was first used by Norio Taniguchi in 1974, though it was not widely known. Inspired by Feynman's concepts, K. Eric Drexler used the term \"nanotechnology\" in his 1986 book Engines of Creation: The Coming Era of Nanotechnology, which proposed the idea of a nanoscale \"assembler\" which would be able to build a copy of itself and of other items of arbitrary complexity with atomic control. Also in 1986, Drexler co-founded The Foresight Institute (with which he is no longer affiliated) to help increase public awareness and understanding of nanotechnology concepts and implications.\n The emergence of nanotechnology as a field in the 1980s occurred through convergence of Drexler's theoretical and public work, which developed and popularized a conceptual framework for nanotechnology, and high-visibility experimental advances that drew additional wide-scale attention to the prospects of atomic control of matter. In the 1980s, two major breakthroughs sparked the growth of nanotechnology in the modern era. First, the invention of the scanning tunneling microscope in 1981 which enabled visualization of individual atoms and bonds, and was successfully used to manipulate individual atoms in 1989. The microscope's developers Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory received a Nobel Prize in Physics in 1986.[10][11] Binnig, Quate and Gerber also invented the analogous atomic force microscope that year.\n Second, fullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry.[12][13] C60 was not initially described as nanotechnology; the term was used regarding subsequent work with related carbon nanotubes (sometimes called graphene tubes or Bucky tubes) which suggested potential applications for nanoscale electronics and devices. The discovery of carbon nanotubes is largely attributed to Sumio Iijima of NEC in 1991,[14] for which Iijima won the inaugural 2008 Kavli Prize in Nanoscience.\n In the early 2000s, the field garnered increased scientific, political, and commercial attention that led to both controversy and progress. Controversies emerged regarding the definitions and potential implications of nanotechnologies, exemplified by the Royal Society's report on nanotechnology.[15] Challenges were raised regarding the feasibility of applications envisioned by advocates of molecular nanotechnology, which culminated in a public debate between Drexler and Smalley in 2001 and 2003.[16]\n Meanwhile, commercialization of products based on advancements in nanoscale technologies began emerging. These products are limited to bulk applications of nanomaterials and do not involve atomic control of matter. Some examples include the Silver Nano platform for using silver nanoparticles as an antibacterial agent, nanoparticle-based transparent sunscreens, carbon fiber strengthening using silica nanoparticles, and carbon nanotubes for stain-resistant textiles.[17][18]\n Governments moved to promote and fund research into nanotechnology, such as in the U.S. with the National Nanotechnology Initiative, which formalized a size-based definition of nanotechnology and established funding for research on the nanoscale, and in Europe via the European Framework Programmes for Research and Technological Development.\n By the mid-2000s new and serious scientific attention began to flourish.  Projects emerged to produce nanotechnology roadmaps[19][20]  which center on atomically precise manipulation of matter and discuss existing and projected capabilities, goals, and applications.\n Nanotechnology is the science and engineering of functional systems at the molecular scale. This covers both current work and concepts that are more advanced. In its original sense, nanotechnology refers to the projected ability to construct items from the bottom up, using techniques and tools being developed today to make complete, high-performance products.\n One nanometer (nm) is one billionth, or 10−9, of a meter. By comparison, typical carbon–carbon bond lengths, or the spacing between these atoms in a molecule, are in the range 0.12–0.15 nm, and a DNA double-helix has a diameter around 2 nm. On the other hand, the smallest cellular life-forms, the bacteria of the genus Mycoplasma, are around 200 nm in length. By convention, nanotechnology is taken as the scale range 1 to 100 nm following the definition used by the National Nanotechnology Initiative in the US. The lower limit is set by the size of atoms (hydrogen has the smallest atoms, which are approximately a quarter of a nm kinetic diameter) since nanotechnology must build its devices from atoms and molecules. The upper limit is more or less arbitrary but is around the size below which the phenomena not observed in larger structures start to become apparent and can be made use of in the nano device.[21] These new phenomena make nanotechnology distinct from devices that are merely miniaturized versions of an equivalent macroscopic device; such devices are on a larger scale and come under the description of microtechnology.[22]\n To put that scale in another context, the comparative size of a nanometer to a meter is the same as that of a marble to the size of the earth.[23] Or another way of putting it: a nanometer is the amount an average man's beard grows in the time it takes him to raise the razor to his face.[23]\n Two main approaches are used in nanotechnology. In the \"bottom-up\" approach, materials and devices are built from molecular components which assemble themselves chemically by principles of molecular recognition.[24] In the \"top-down\" approach, nano-objects are constructed from larger entities without atomic-level control.[25]\n Areas of physics such as nanoelectronics, nanomechanics, nanophotonics and nanoionics have evolved during the last few decades to provide a basic scientific foundation of nanotechnology.\n Several phenomena become pronounced as the size of the system decreases. These include statistical mechanical effects, as well as quantum mechanical effects, for example the \"quantum size effect\" where the electronic properties of solids are altered with great reductions in particle size. This effect does not come into play by going from macro to micro dimensions. However, quantum effects can become significant when the nanometer size range is reached, typically at distances of 100 nanometers or less, the so-called quantum realm. Additionally, a number of physical (mechanical, electrical, optical, etc.) properties change when compared to macroscopic systems. One example is the increase in surface area to volume ratio altering mechanical, thermal and catalytic properties of materials. Diffusion and reactions at nanoscale, nanostructures materials and nanodevices with fast ion transport are generally referred to nanoionics. Mechanical properties of nanosystems are of interest in the nanomechanics research. The catalytic activity of nanomaterials also opens potential risks in their interaction with biomaterials.\n Materials reduced to the nanoscale can show different properties compared to what they exhibit on a macroscale, enabling unique applications. For instance, opaque substances can become transparent (copper); stable materials can turn combustible (aluminium); insoluble materials may become soluble (gold). A material such as gold, which is chemically inert at normal scales, can serve as a potent chemical catalyst at nanoscales. Much of the fascination with nanotechnology stems from these quantum and surface phenomena that matter exhibits at the nanoscale.[26]\n Modern synthetic chemistry has reached the point where it is possible to prepare small molecules to almost any structure. These methods are used today to manufacture a wide variety of useful chemicals such as pharmaceuticals or commercial polymers. This ability raises the question of extending this kind of control to the next-larger level, seeking methods to assemble these single molecules into supramolecular assemblies consisting of many molecules arranged in a well defined manner.\n These approaches utilize the concepts of molecular self-assembly and\/or supramolecular chemistry to automatically arrange themselves into some useful conformation through a bottom-up approach. The concept of molecular recognition is especially important: molecules can be designed so that a specific configuration or arrangement is favored due to non-covalent intermolecular forces. The Watson–Crick basepairing rules are a direct result of this, as is the specificity of an enzyme being targeted to a single substrate, or the specific folding of the protein itself. Thus, two or more components can be designed to be complementary and mutually attractive so that they make a more complex and useful whole.\n Such bottom-up approaches should be capable of producing devices in parallel and be much cheaper than top-down methods, but could potentially be overwhelmed as the size and complexity of the desired assembly increases. Most useful structures require complex and thermodynamically unlikely arrangements of atoms. Nevertheless, there are many examples of self-assembly based on molecular recognition in biology, most notably Watson–Crick basepairing and enzyme-substrate interactions. The challenge for nanotechnology is whether these principles can be used to engineer new constructs in addition to natural ones.\n Molecular nanotechnology, sometimes called molecular manufacturing, describes engineered nanosystems (nanoscale machines) operating on the molecular scale. Molecular nanotechnology is especially associated with the molecular assembler, a machine that can produce a desired structure or device atom-by-atom using the principles of mechanosynthesis. Manufacturing in the context of productive nanosystems is not related to, and should be clearly distinguished from, the conventional technologies used to manufacture nanomaterials such as carbon nanotubes and nanoparticles.\n When the term \"nanotechnology\" was independently coined and popularized by Eric Drexler (who at the time was unaware of an earlier usage by Norio Taniguchi) it referred to a future manufacturing technology based on molecular machine systems. The premise was that molecular-scale biological analogies of traditional machine components demonstrated molecular machines were possible: by the countless examples found in biology, it is known that sophisticated, stochastically optimized biological machines can be produced.\n It is hoped that developments in nanotechnology will make possible their construction by some other means, perhaps using biomimetic principles. However, Drexler and other researchers[27] have proposed that advanced nanotechnology, although perhaps initially implemented by biomimetic means, ultimately could be based on mechanical engineering principles, namely, a manufacturing technology based on the mechanical functionality of these components (such as gears, bearings, motors, and structural members) that would enable programmable, positional assembly to atomic specification.[28] The physics and engineering performance of exemplar designs were analyzed in Drexler's book Nanosystems: Molecular Machinery, Manufacturing, and Computation.[2]\n In general it is very difficult to assemble devices on the atomic scale, as one has to position atoms on other atoms of comparable size and stickiness. Another view, put forth by Carlo Montemagno,[29] is that future nanosystems will be hybrids of silicon technology and biological molecular machines. Richard Smalley argued that mechanosynthesis are impossible due to the difficulties in mechanically manipulating individual molecules.\n This led to an exchange of letters in the ACS publication Chemical & Engineering News in 2003.[30] Though biology clearly demonstrates that molecular machine systems are possible, non-biological molecular machines are today only in their infancy. Leaders in research on non-biological molecular machines are Alex Zettl and his colleagues at Lawrence Berkeley Laboratories and UC Berkeley.[31] They have constructed at least three distinct molecular devices whose motion is controlled from the desktop with changing voltage: a nanotube nanomotor, a molecular actuator,[32] and a nanoelectromechanical relaxation oscillator.[33] See nanotube nanomotor for more examples.\n An experiment indicating that positional molecular assembly is possible was performed by Ho and Lee at Cornell University in 1999. They used a scanning tunneling microscope to move an individual carbon monoxide molecule (CO) to an individual iron atom (Fe) sitting on a flat silver crystal, and chemically bound the CO to the Fe by applying a voltage.\n The nanomaterials field includes subfields which develop or study materials having unique properties arising from their nanoscale dimensions.[36]\n These seek to arrange smaller components into more complex assemblies.\n These seek to create smaller devices by using larger ones to direct their assembly.\n These seek to develop components of a desired functionality without regard to how they might be assembled.\n These subfields seek to anticipate what inventions nanotechnology might yield, or attempt to propose an agenda along which inquiry might progress. These often take a big-picture view of nanotechnology, with more emphasis on its societal implications than the details of how such inventions could actually be created.\n Nanomaterials can be classified in 0D, 1D, 2D and 3D nanomaterials. The dimensionality play a major role in determining the characteristic of nanomaterials including physical, chemical and biological characteristics. With the decrease in dimensionality, an increase in surface-to-volume ratio is observed. This indicate that smaller dimensional nanomaterials have higher surface area compared to 3D nanomaterials. Recently, two dimensional (2D) nanomaterials are extensively investigated for electronic, biomedical, drug delivery and biosensor applications.\n There are several important modern developments. The atomic force microscope (AFM) and the Scanning Tunneling Microscope (STM) are two early versions of scanning probes that launched nanotechnology. There are other types of scanning probe microscopy. Although conceptually similar to the scanning confocal microscope developed by Marvin Minsky in 1961 and the scanning acoustic microscope (SAM) developed by Calvin Quate and coworkers in the 1970s, newer scanning probe microscopes have much higher resolution, since they are not limited by the wavelength of sound or light.\n The tip of a scanning probe can also be used to manipulate nanostructures (a process called positional assembly). Feature-oriented scanning methodology may be a promising way to implement these nanomanipulations in automatic mode.[54][55] However, this is still a slow process because of low scanning velocity of the microscope.\n Various techniques of nanolithography such as optical lithography, X-ray lithography, dip pen nanolithography, electron beam lithography or nanoimprint lithography were also developed. Lithography is a top-down fabrication technique where a bulk material is reduced in size to nanoscale pattern.\n Another group of nanotechnological techniques include those used for fabrication of nanotubes and nanowires, those used in semiconductor fabrication such as deep ultraviolet lithography, electron beam lithography, focused ion beam machining, nanoimprint lithography, atomic layer deposition, and molecular vapor deposition, and further including molecular self-assembly techniques such as those employing di-block copolymers. The precursors of these techniques preceded the nanotech era, and are extensions in the development of scientific advancements rather than techniques which were devised with the sole purpose of creating nanotechnology and which were results of nanotechnology research.[56]\n The top-down approach anticipates nanodevices that must be built piece by piece in stages, much as manufactured items are made. Scanning probe microscopy is an important technique both for characterization and synthesis of nanomaterials. Atomic force microscopes and scanning tunneling microscopes can be used to look at surfaces and to move atoms around. By designing different tips for these microscopes, they can be used for carving out structures on surfaces and to help guide self-assembling structures. By using, for example, feature-oriented scanning approach, atoms or molecules can be moved around on a surface with scanning probe microscopy techniques.[54][55] At present, it is expensive and time-consuming for mass production but very suitable for laboratory experimentation.\n In contrast, bottom-up techniques build or grow larger structures atom by atom or molecule by molecule. These techniques include chemical synthesis, self-assembly and positional assembly. Dual polarisation interferometry is one tool suitable for characterisation of self assembled thin films. Another variation of the bottom-up approach is molecular beam epitaxy or MBE. Researchers at Bell Telephone Laboratories like John R. Arthur. Alfred Y. Cho, and Art C. Gossard developed and implemented MBE as a research tool in the late 1960s and 1970s. Samples made by MBE were key to the discovery of the fractional quantum Hall effect for which the 1998 Nobel Prize in Physics was awarded. MBE allows scientists to lay down atomically precise layers of atoms and, in the process, build up complex structures. Important for research on semiconductors, MBE is also widely used to make samples and devices for the newly emerging field of spintronics.\n However, new therapeutic products, based on responsive nanomaterials, such as the ultradeformable, stress-sensitive Transfersome vesicles, are under development and already approved for human use in some countries.[57]\n As of August 21, 2008, the Project on Emerging Nanotechnologies estimates that over 800 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week.[18] The project lists all of the products in a publicly accessible online database. Most applications are limited to the use of \"firstgeneration\" passive nanomaterials which includes titanium dioxide in sunscreen, cosmetics, surface coatings,[58] and some food products; Carbon allotropes used to produce gecko tape; silver in food packaging, clothing, disinfectants and household appliances; zinc oxide in sunscreens and cosmetics, surface coatings, paints and outdoor furniture varnishes; and cerium oxide as a fuel catalyst.[17]\n Further applications allow tennis balls to last longer, golf balls to fly straighter, and even bowling balls to become more durable and have a harder surface. Trousers and socks have been infused with nanotechnology so that they will last longer and keep people cool in the summer. Bandages are being infused with silver nanoparticles to heal cuts faster.[59] Video game consoles and personal computers may become cheaper, faster, and contain more memory thanks to nanotechnology.[60] Also, to build structures for on chip computing with light, for example on chip optical quantum information processing, and picosecond transmission of information.[61]\n Nanotechnology may have the ability to make existing medical applications cheaper and easier to use in places like the general practitioners' offices and at homes.[62] Cars are being manufactured using nanomaterials in such ways that car parts require fewer metals during manufacturing and less fuel to operate in the future.[63]\n Nanoencapsulation is a technology involving the enclosure of active substances within carriers or particles of nanometer size. Typically, these carriers are nanoparticles that offer various advantages, such as enhanced bioavailability, controlled release, targeted delivery, and protection of the encapsulated substances. In the medical field, nanoencapsulation plays a significant role in drug delivery and therapeutic strategies. It facilitates more efficient drug administration, minimizing side effects, and increasing treatment effectiveness by encapsulating drugs within nanoparticles. Nanoencapsulation is particularly useful for improving the bioavailability of poorly water-soluble drugs, enabling controlled and sustained drug release, and supporting the development of targeted therapies. These features collectively contribute to advancements in medical treatments and patient care.[64][65]\n Scientists are now turning to nanotechnology in an attempt to develop diesel engines with cleaner exhaust fumes. Platinum is currently used as the diesel engine catalyst in these engines. The catalyst is what cleans the exhaust fume particles. First, a reduction catalyst is employed to take nitrogen atoms from NOx molecules in order to free oxygen. Next the oxidation catalyst oxidizes the hydrocarbons and carbon monoxide to form carbon dioxide and water.[citation needed] Platinum is used in both the reduction and the oxidation catalysts.[66] Using platinum though, is inefficient in that it is expensive and unsustainable. Danish company InnovationsFonden invested DKK 15 million in a search for new catalyst substitutes using nanotechnology. The goal of the project, launched in the autumn of 2014, is to maximize surface area and minimize the amount of material required. Objects tend to minimize their surface energy; two drops of water, for example, will join to form one drop and decrease surface area. If the catalyst's surface area that is exposed to the exhaust fumes is maximized, efficiency of the catalyst is maximized. The team working on this project aims to create nanoparticles that will not merge. Every time the surface is optimized, material is saved. Thus, creating these nanoparticles will increase the effectiveness of the resulting diesel engine catalyst—in turn leading to cleaner exhaust fumes—and will decrease cost. If successful, the team hopes to reduce platinum use by 25%.[67]\n Nanotechnology also has a prominent role in the fast developing field of Tissue Engineering. When designing scaffolds, researchers attempt to mimic the nanoscale features of a cell's microenvironment to direct its differentiation down a suitable lineage.[68] For example, when creating scaffolds to support the growth of bone, researchers may mimic osteoclast resorption pits.[69]\n Researchers have successfully used DNA origami-based nanobots capable of carrying out logic functions to achieve targeted drug delivery in cockroaches. It is said that the computational power of these nanobots can be scaled up to that of a Commodore 64.[70]\n An area of concern is the effect that industrial-scale manufacturing and use of nanomaterials would have on human health and the environment, as suggested by nanotoxicology research. For these reasons, some groups advocate that nanotechnology be regulated by governments. Others counter that overregulation would stifle scientific research and the development of beneficial innovations. Public health research agencies, such as the National Institute for Occupational Safety and Health are actively conducting research on potential health effects stemming from exposures to nanoparticles.[71][72]\n Some nanoparticle products may have unintended consequences. Researchers have discovered that bacteriostatic silver nanoparticles used in socks to reduce foot odor are being released in the wash.[73] These particles are then flushed into the waste water stream and may destroy bacteria which are critical components of natural ecosystems, farms, and waste treatment processes.[74]\n Public deliberations on risk perception in the US and UK carried out by the Center for Nanotechnology in Society found that participants were more positive about nanotechnologies for energy applications than for health applications, with health applications raising moral and ethical dilemmas such as cost and availability.[75]\n Experts, including director of the Woodrow Wilson Center's Project on Emerging Nanotechnologies David Rejeski, have testified[76] that successful commercialization depends on adequate oversight, risk research strategy, and public engagement. Berkeley, California is currently the only city in the United States to regulate nanotechnology;[77] In 2008, Cambridge, Massachusetts considered enacting a similar law,[78] but ultimately rejected it.[79]\n Nanofibers are used in several areas and in different products, in everything from aircraft wings to tennis rackets. Inhaling airborne nanoparticles and nanofibers may lead to a number of pulmonary diseases, e.g. fibrosis.[80] Researchers have found that when rats breathed in nanoparticles, the particles settled in the brain and lungs, which led to significant increases in biomarkers for inflammation and stress response[81] and that nanoparticles induce skin aging through oxidative stress in hairless mice.[82][83]\n A two-year study at UCLA's School of Public Health found lab mice consuming nano-titanium dioxide showed DNA and chromosome damage to a degree \"linked to all the big killers of man, namely cancer, heart disease, neurological disease and aging\".[84]\n A Nature Nanotechnology study suggests some forms of carbon nanotubes – a poster child for the \"nanotechnology revolution\" – could be as harmful as asbestos if inhaled in sufficient quantities. Anthony Seaton of the Institute of Occupational Medicine in Edinburgh, Scotland, who contributed to the article on carbon nanotubes said \"We know that some of them probably have the potential to cause mesothelioma. So those sorts of materials need to be handled very carefully.\"[85] In the absence of specific regulation forthcoming from governments, Paull and Lyons (2008) have called for an exclusion of engineered nanoparticles in food.[86] A newspaper article reports that workers in a paint factory developed serious lung disease and nanoparticles were found in their lungs.[87][88][89][90]\n Calls for tighter regulation of nanotechnology have occurred alongside a growing debate related to the human health and safety risks of nanotechnology.[91] There is significant debate about who is responsible for the regulation of nanotechnology. Some regulatory agencies currently cover some nanotechnology products and processes (to varying degrees) – by \"bolting on\" nanotechnology to existing regulations – there are clear gaps in these regimes.[92] Davies (2008) has proposed a regulatory road map describing steps to deal with these shortcomings.[93]\n Stakeholders concerned by the lack of a regulatory framework to assess and control risks associated with the release of nanoparticles and nanotubes have drawn parallels with bovine spongiform encephalopathy (\"mad cow\" disease), thalidomide, genetically modified food,[94] nuclear energy, reproductive technologies, biotechnology, and asbestosis. Andrew Maynard, chief science advisor to the Woodrow Wilson Center's Project on Emerging Nanotechnologies, concludes that there is insufficient funding for human health and safety research, and as a result there is currently limited understanding of the human health and safety risks associated with nanotechnology.[95] As a result, some academics have called for stricter application of the precautionary principle, with delayed marketing approval, enhanced labelling and additional safety data development requirements in relation to certain forms of nanotechnology.[96]\n The Royal Society report[15] identified a risk of nanoparticles or nanotubes being released during disposal, destruction and recycling, and recommended that \"manufacturers of products that fall under extended producer responsibility regimes such as end-of-life regulations publish procedures outlining how these materials will be managed to minimize possible human and environmental exposure\" (p. xiii).\n The Center for Nanotechnology in Society has found that people respond to nanotechnologies differently, depending on application – with participants in public deliberations more positive about nanotechnologies for energy than health applications – suggesting that any public calls for nano regulations may differ by technology sector.[75]\n"}
{"key":"Nanotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/History_of_nanotechnology","headline":"History of nanotechnology - Wikipedia","content":"The history of nanotechnology traces the development of the concepts and experimental work falling under the broad category of nanotechnology.  Although nanotechnology is a relatively recent development in scientific research, the development of its central  concepts happened over a longer period of time. The emergence of nanotechnology in the 1980s was caused by the convergence of experimental advances such as the invention of the scanning tunneling microscope in 1981 and the discovery of fullerenes in 1985, with the elucidation and popularization of a conceptual framework for the goals of nanotechnology beginning with the 1986 publication of the book Engines of Creation.  The field was subject to growing public awareness and controversy in the early 2000s, with prominent debates about both its potential implications as well as the feasibility of the applications envisioned by advocates of molecular nanotechnology, and with governments moving to promote and fund research into nanotechnology.  The early 2000s also saw the beginnings of commercial applications of nanotechnology, although these were limited to bulk applications of nanomaterials rather than the transformative applications envisioned by the field.\n Carbon nanotubes have been found in pottery from Keeladi, India, dating to c. 600–300 BC, though it is not known how they formed or whether the substance containing them was employed deliberately.[1] Cementite nanowires have been observed in Damascus steel, a material dating back to c. 900 AD, their origin and means of manufacture also unknown.[2]\n Although nanoparticles are associated with modern science, they were used by artisans as far back as the ninth century in Mesopotamia for creating a glittering effect on the surface of pots.[3][4]\n In modern times, pottery from the Middle Ages and Renaissance often retains a distinct gold- or copper-colored metallic glitter. This luster is caused by a metallic film that was applied to the transparent surface of a glazing, which contains silver and copper nanoparticles dispersed homogeneously in the glassy matrix of the ceramic glaze. These nanoparticles are created by the artisans by adding copper and silver salts and oxides together with vinegar, ochre, and clay on the surface of previously glazed pottery. The technique originated in the Muslim world. As Muslims were not allowed to use gold in artistic representations, they sought a way to create a similar effect without using real gold. The solution they found was using luster.[4][5]\n The American physicist Richard Feynman lectured, \"There's Plenty of Room at the Bottom,\" at an American Physical Society meeting at Caltech on December 29, 1959, which is often held to have provided inspiration for the field of nanotechnology. Feynman had described a process by which the ability to manipulate individual atoms and molecules might be developed, using one set of precise tools to build and operate another proportionally smaller set, so on down to the needed scale. In the course of this, he noted, scaling issues would arise from the changing magnitude of various physical phenomena: gravity would become less important, surface tension and Van der Waals attraction would become more important.[6]\n After Feynman's death, a scholar studying the historical development of nanotechnology has concluded that his actual role in catalyzing nanotechnology research was limited, based on recollections from many of the people active in the nascent field in the 1980s and 1990s. Chris Toumey, a cultural anthropologist at the University of South Carolina, found that the published versions of Feynman's talk had a negligible influence in the twenty years after it was first published, as measured by citations in the scientific literature, and not much more influence in the decade after the Scanning Tunneling Microscope was invented in 1981.  Subsequently, interest in “Plenty of Room” in the scientific literature greatly increased in the early 1990s.  This is probably because the term “nanotechnology” gained serious attention just before that time, following its use by K. Eric Drexler in his 1986 book, Engines of Creation: The Coming Era of Nanotechnology, which took the Feynman concept of a billion tiny factories and added the idea that they could make more copies of themselves via computer control instead of control by a human operator; and in a cover article headlined \"Nanotechnology\",[7][8] published later that year in a mass-circulation science-oriented magazine, Omni.  Toumey's analysis also includes comments from distinguished scientists in nanotechnology who say that “Plenty of Room” did not influence their early work, and in fact most of them had not read it until a later date.[9][10]\n These and other developments hint that the retroactive rediscovery of Feynman's “Plenty of Room” gave nanotechnology a packaged history that provided an early date of December 1959, plus a connection to the charisma and genius of Richard Feynman. Feynman's stature as a Nobel laureate and as an iconic figure in 20th century science surely helped advocates of nanotechnology and provided a valuable intellectual link to the past.[11]\n Japanese scientist Norio Taniguchi of Tokyo University of Science was the first to use the term \"nano-technology\" in a 1974 conference,[12] to describe semiconductor processes such as thin film deposition and ion beam milling exhibiting characteristic control on the order of a nanometer.  His definition was, \"'Nano-technology' mainly consists of the processing of, separation, consolidation, and deformation of materials by one atom or one molecule.\"  However, the term was not used again until 1981 when Eric Drexler, who was unaware of Taniguchi's prior use of the term, published his first paper on nanotechnology in 1981.[13][14][15]\n In the 1980s the idea of nanotechnology as a deterministic, rather than stochastic, handling of individual atoms and molecules was conceptually explored in depth by K. Eric Drexler, who promoted the technological significance of nano-scale phenomena and devices through speeches and two influential books.\n In 1980, Drexler encountered Feynman's provocative 1959 talk \"There's Plenty of Room at the Bottom\" while preparing his initial scientific paper on the subject, “Molecular Engineering: An approach to the development of general capabilities for molecular manipulation,” published in the Proceedings of the National Academy of Sciences in 1981.[16]  The term \"nanotechnology\" (which paralleled Taniguchi's \"nano-technology\") was independently applied by Drexler in his 1986 book Engines of Creation: The Coming Era of Nanotechnology, which proposed the idea of a nanoscale \"assembler\" which would be able to build a copy of itself and of other items of arbitrary complexity. He also first published the term \"grey goo\" to describe what might happen if a hypothetical self-replicating machine, capable of independent operation, were constructed and released.  Drexler's vision of nanotechnology is often called \"Molecular Nanotechnology\" (MNT) or \"molecular manufacturing.\"\n His 1991 Ph.D. work at the MIT Media Lab was the first doctoral degree on the topic of molecular nanotechnology and (after some editing) his thesis, \"Molecular Machinery and Manufacturing with Applications to Computation,\"[17] was published as Nanosystems: Molecular Machinery, Manufacturing, and Computation,[18] which received the Association of American Publishers award for Best Computer Science Book of 1992.  Drexler founded the Foresight Institute in 1986 with the mission of \"Preparing for nanotechnology.”  Drexler is no longer a member of the Foresight Institute.[citation needed]\n In nanoelectronics, nanoscale thickness was demonstrated in the gate oxide and thin films used in transistors as early as the 1960s, but it was not until the late 1990s that MOSFETs (metal–oxide–semiconductor field-effect transistors) with nanoscale gate length were demonstrated. Nanotechnology and nanoscience got a boost in the early 1980s with two major developments: the birth of cluster science and the invention of the scanning tunneling microscope (STM). These developments led to the discovery of fullerenes in 1985 and the structural assignment of carbon nanotubes in 1991. The development of FinFET in the 1990s aldo laid the foundations for modern nanoelectronic semiconductor device fabrication.\n The scanning tunneling microscope, an instrument for imaging surfaces at the atomic level, was developed in 1981 by Gerd Binnig and Heinrich Rohrer at IBM Zurich Research Laboratory, for which they were awarded the Nobel Prize in Physics in 1986.[19][20]  Binnig, Calvin Quate and Christoph Gerber invented the first atomic force microscope in 1986. The first commercially available atomic force microscope was introduced in 1989.\n IBM researcher Don Eigler was the first to manipulate atoms using a scanning tunneling microscope in 1989. He used 35 Xenon atoms to spell out the IBM logo.[21]  He shared the 2010 Kavli Prize in Nanoscience for this work.[22]\n Interface and colloid science had existed for nearly a century before they became associated with nanotechnology.[23][24]  The first observations and size measurements of nanoparticles had been made during the first decade of the 20th century by Richard Adolf Zsigmondy, winner of the 1925 Nobel Prize in Chemistry, who made a detailed study of gold sols and other nanomaterials with sizes down to 10 nm using an ultramicroscope which was capable of visualizing particles much smaller than the light wavelength.[25]  Zsigmondy was also the first to use the term \"nanometer\" explicitly for characterizing particle size.  In the 1920s, Irving Langmuir, winner of the 1932 Nobel Prize in Chemistry, and Katharine B. Blodgett introduced the concept of a monolayer, a layer of material one molecule thick.  In the early 1950s, Derjaguin and Abrikosova conducted the first measurement of surface forces.[26]\n In 1974 the process of atomic layer deposition for depositing uniform thin films one atomic layer at a time was developed and patented by Tuomo Suntola and co-workers in Finland.[27]\n In another development, the synthesis and properties of semiconductor nanocrystals were studied. This led to a fast increasing number of semiconductor nanoparticles of quantum dots.\n Fullerenes were discovered in 1985 by Harry Kroto, Richard Smalley, and Robert Curl, who together won the 1996 Nobel Prize in Chemistry.  Smalley's research in physical chemistry investigated formation of inorganic and semiconductor clusters using pulsed molecular beams and time of flight mass spectrometry.  As a consequence of this expertise, Curl introduced him to Kroto in order to investigate a question about the constituents of astronomical dust.  These are carbon rich grains expelled by old stars such as R Corona Borealis. The result of this collaboration was the discovery of C60 and the fullerenes as the third allotropic form of carbon.  Subsequent discoveries included the endohedral fullerenes, and the larger family of fullerenes the following year.[28][29]\n The discovery of carbon nanotubes is largely attributed to Sumio Iijima of NEC in 1991, although carbon nanotubes have been produced and observed under a variety of conditions prior to 1991.[30]  Iijima's discovery of multi-walled carbon nanotubes in the insoluble material of arc-burned graphite rods in 1991[31] and Mintmire, Dunlap, and White's independent prediction that if single-walled carbon nanotubes could be made, then they would exhibit remarkable conducting properties[32] helped create the initial buzz that is now associated with carbon nanotubes. Nanotube research accelerated greatly following the independent discoveries[33][34] by Bethune at IBM[35] and Iijima at NEC of single-walled carbon nanotubes and methods to specifically produce them by adding transition-metal catalysts to the carbon in an arc discharge.\n In the early 1990s Huffman and Kraetschmer, of the University of Arizona, discovered how to synthesize and purify large quantities of fullerenes.  This opened the door to their characterization and functionalization by hundreds of investigators in government and industrial laboratories.  Shortly after, rubidium doped C60 was found to be a mid temperature (Tc = 32 K) superconductor.  At a meeting of the Materials Research Society in 1992, Dr. Thomas Ebbesen (NEC) described to a spellbound audience his discovery and characterization of carbon nanotubes.[citation needed]  This event sent those in attendance and others downwind of his presentation into their laboratories to reproduce and push those discoveries forward.  Using the same or similar tools as those used by Huffman and Kratschmer, hundreds of researchers further developed the field of nanotube-based nanotechnology.\n The National Nanotechnology Initiative is a United States federal nanotechnology research and development program. “The NNI serves as the central point of communication, cooperation, and collaboration for all Federal agencies engaged in nanotechnology research, bringing together the expertise needed to advance this broad and complex field.\"[36] Its goals are to advance a world-class nanotechnology research and development (R&D) program, foster the transfer of new technologies into products for commercial and public benefit, develop and sustain educational resources, a skilled workforce, and the supporting infrastructure and tools to advance nanotechnology, and support responsible development of nanotechnology. The initiative was spearheaded by Mihail Roco, who formally proposed the National Nanotechnology Initiative to the Office of Science and Technology Policy during the Clinton administration in 1999, and was a key architect in its development.  He is currently the Senior Advisor for Nanotechnology at the National Science Foundation, as well as the founding chair of the National Science and Technology Council subcommittee on Nanoscale Science, Engineering and Technology.[37]\n President Bill Clinton advocated nanotechnology development. In a 21 January 2000 speech[38] at the California Institute of Technology, Clinton said, \"Some of our research goals may take twenty or more years to achieve, but that is precisely why there is an important role for the federal government.\"  Feynman's stature and concept of atomically precise fabrication played a role in securing funding for nanotechnology research, as mentioned in President Clinton's speech:\n My budget supports a major new National Nanotechnology Initiative, worth $500 million. Caltech is no stranger to the idea of nanotechnology the ability to manipulate matter at the atomic and molecular level. Over 40 years ago, Caltech's own Richard Feynman asked, \"What would happen if we could arrange the atoms one by one the way we want them?\"[39] President George W. Bush further increased funding for nanotechnology.  On December 3, 2003, Bush signed into law the 21st Century Nanotechnology Research and Development Act,[40] which authorizes expenditures for five of the participating agencies totaling US$3.63 billion over four years.[41]  The NNI budget supplement for Fiscal Year 2009 provides $1.5 billion to the NNI, reflecting steady growth in the nanotechnology investment.[42]\n \"Why the future doesn't need us\" is an article written by Bill Joy, then Chief Scientist at Sun Microsystems, in the April 2000 issue of Wired magazine. In the article, he argues that \"Our most powerful 21st-century technologies — robotics, genetic engineering, and nanotech — are threatening to make humans an endangered species.\" Joy argues that developing technologies provide a much greater danger to humanity than any technology before it has ever presented. In particular, he focuses on genetics, nanotechnology and robotics. He argues that 20th-century technologies of destruction, such as the nuclear bomb, were limited to large governments, due to the complexity and cost of such devices, as well as the difficulty in acquiring the required materials. He also voices concern about increasing computer power. His worry is that computers will eventually become more intelligent than we are, leading to such dystopian scenarios as robot rebellion. He notably quotes the Unabomber on this topic.  After the publication of the article, Bill Joy suggested assessing technologies to gauge their implicit dangers, as well as having scientists refuse to work on technologies that have the potential to cause harm.\n In the AAAS Science and Technology Policy Yearbook 2001 article titled A Response to Bill Joy and the Doom-and-Gloom Technofuturists, Bill Joy was criticized for having technological tunnel vision on his prediction, by failing to consider social factors.[43]  In Ray Kurzweil's The Singularity Is Near, he questioned the regulation of potentially dangerous technology, asking \"Should we tell the millions of people afflicted with cancer and other devastating conditions that we are canceling the development of all bioengineered treatments because there is a risk that these same technologies may someday be used for malevolent purposes?\".\n Prey is a 2002 novel by Michael Crichton which features an artificial swarm of nanorobots which develop intelligence and threaten their human inventors.  The novel generated concern within the nanotechnology community that the novel could negatively affect public perception of nanotechnology by creating fear of a similar scenario in real life.[44]\n Richard Smalley, best known for co-discovering the soccer ball-shaped “buckyball” molecule and a leading advocate of nanotechnology and its many applications, was an outspoken critic of the idea of molecular assemblers, as advocated by Eric Drexler.  In 2001 he introduced scientific objections to them[45] attacking the notion of universal assemblers in a 2001 Scientific American article, leading to a rebuttal later that year from Drexler and colleagues,[46] and eventually to an exchange of open letters in 2003.[47]\n Smalley criticized Drexler's work on nanotechnology as naive, arguing that chemistry is extremely complicated, reactions are hard to control, and that a universal assembler is science fiction.  Smalley believed that such assemblers were not physically possible and introduced scientific objections to them. His two principal technical objections, which he had termed the “fat fingers problem\" and the \"sticky fingers problem”, argued against the feasibility of molecular assemblers being able to precisely select and place individual atoms. He also believed that Drexler's speculations about apocalyptic dangers of molecular assemblers threaten the public support for development of nanotechnology.\n Smalley first argued that \"fat fingers\" made MNT impossible. He later argued that nanomachines would have to resemble chemical enzymes more than Drexler's assemblers and could only work in water. He believed these would exclude the possibility of \"molecular assemblers\" that worked by precision picking and placing of individual atoms.  Also, Smalley argued that nearly all of modern chemistry involves reactions that take place in a solvent (usually water), because the small molecules of a solvent contribute many things, such as lowering binding energies for transition states.  Since nearly all known chemistry requires a solvent, Smalley felt that Drexler's proposal to use a high vacuum environment was not feasible.\n Smalley also believed that Drexler's speculations about apocalyptic dangers of self-replicating machines that have been equated with \"molecular assemblers\" would threaten the public support for development of nanotechnology. To address the debate between Drexler and Smalley regarding molecular assemblers, Chemical & Engineering News published a point-counterpoint consisting of an exchange of letters that addressed the issues.[47]\n Drexler and coworkers responded to these two issues[46] in a 2001 publication.  Drexler and colleagues noted that Drexler never proposed universal assemblers able to make absolutely anything, but instead proposed more limited assemblers able to make a very wide variety of things. They challenged the relevance of Smalley's arguments to the more specific proposals advanced in Nanosystems.  Drexler maintained that both were straw man arguments, and in the case of enzymes, Prof. Klibanov wrote in 1994, \"...using an enzyme in organic solvents eliminates several obstacles...\"[48] Drexler also addresses this in Nanosystems by showing mathematically that well designed catalysts can provide the effects of a solvent and can fundamentally be made even more efficient than a solvent\/enzyme reaction could ever be.  Drexler had difficulty in getting Smalley to respond, but in December 2003, Chemical & Engineering News carried a 4-part debate.[47]\n Ray Kurzweil spends four pages in his book 'The Singularity Is Near' to showing that Richard Smalley's arguments are not valid, and disputing them point by point.  Kurzweil ends by stating that Drexler's visions are very practicable and even happening already.[49]\n The Royal Society and Royal Academy of Engineering's 2004 report on the implications of nanoscience and nanotechnologies[50] was inspired by Prince Charles' concerns about nanotechnology, including molecular manufacturing. However, the report spent almost no time on molecular manufacturing.[51]  In fact, the word \"Drexler\" appears only once in the body of the report (in passing), and \"molecular manufacturing\" or \"molecular nanotechnology\" not at all.  The report covers various risks of nanoscale technologies, such as nanoparticle toxicology.  It also provides a useful overview of several nanoscale fields. The report contains an annex (appendix) on grey goo, which cites a weaker variation of Richard Smalley's contested argument against molecular manufacturing. It concludes that there is no evidence that autonomous, self replicating nanomachines will be developed in the foreseeable future, and suggests that regulators should be more concerned with issues of nanoparticle toxicology.\n The early 2000s saw the beginnings of the use of nanotechnology in commercial products, although most applications are limited to the bulk use of passive nanomaterials.  Examples include titanium dioxide and zinc oxide nanoparticles in sunscreen, cosmetics and some food products; silver nanoparticles in food packaging, clothing, disinfectants and household appliances such as Silver Nano; carbon nanotubes for stain-resistant textiles; and cerium oxide as a fuel catalyst.[52]  As of March 10, 2011, the Project on Emerging Nanotechnologies estimated that over 1300 manufacturer-identified nanotech products are publicly available, with new ones hitting the market at a pace of 3–4 per week.[53]\n The National Science Foundation funded researcher David Berube to study the field of nanotechnology[when?]. His findings are published in the monograph Nano-Hype: The Truth Behind the Nanotechnology Buzz.  This study concludes that much of what is sold as “nanotechnology” is in fact a recasting of straightforward materials science, which is leading to a “nanotech industry built solely on selling nanotubes, nanowires, and the like” which will “end up with a few suppliers selling low margin products in huge volumes.\" Further applications which require actual manipulation or arrangement of nanoscale components await further research. Though technologies branded with the term 'nano' are sometimes little related to and fall far short of the most ambitious and transformative technological goals of the sort in molecular manufacturing proposals, the term still connotes such ideas. According to Berube, there may be a danger that a \"nano bubble\" will form, or is forming already, from the use of the term by scientists and entrepreneurs to garner funding, regardless of interest in the transformative possibilities of more ambitious and far-sighted work.[54]\n Invention of ionizable cationic lipids at the turn of the 21st century allowed subsequent development of solid lipid nanoparticles, which in the 2020s became the most successful and well-known non-viral nanoparticle drug delivery system due to their use in several mRNA vaccines during the COVID-19 pandemic.\n"}
{"key":"Nanotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/List_of_nanotechnology_organizations","headline":"List of nanotechnology organizations - Wikipedia","content":"\n This is a list of organizations involved in nanotechnology.\n Centre for nanoscience and technology, pondicherry university\n Centre for Research of Nanotechnology, University of Kashmir\n nanoCLO (SMC-Pvt) Ltd. Pakistan. Manufacturers of Nanofibers Membranes.\n"}
{"key":"Nanotechnology","link":"https:\/\/en.wikipedia.org\/wiki\/Nanotechnology_in_fiction","headline":"Nanotechnology in fiction - Wikipedia","content":"The use of nanotechnology in fiction has attracted scholarly attention.[1][2][3][4] The first use of the distinguishing concepts of nanotechnology was \"There's Plenty of Room at the Bottom\", a talk given by physicist Richard Feynman in 1959. K. Eric Drexler's 1986 book Engines of Creation introduced the general public to the concept of nanotechnology. Since then, nanotechnology has been used frequently in a diverse range of fiction, often as a justification for unusual or far-fetched occurrences featured in speculative fiction.[5]\n In 1931, Boris Zhitkov wrote a short story called Microhands (Микроруки), where the narrator builds for himself a pair of microscopic remote manipulators, and uses them for fine tasks like eye surgery. When he attempts to build even smaller manipulators to be manipulated by the first pair, the story goes into detail about the problem of regular materials behaving differently on a microscopic scale.\n In his 1956 short story The Next Tenants, Arthur C. Clarke describes tiny machines that operate at the micrometre scale – although not strictly nanoscale (billionth of a meter), they are the first fictional example of the concepts now associated with nanotechnology.\n A concept similar to nanotechnology, called \"micromechanical devices\", was described in Lem's 1959 novel Eden These devices were used by the aliens as \"seeds\" to grow a wall around the human spaceship.  [6]\nStanislaw Lem's 1964 novel The Invincible involves the discovery of an artificial ecosystem of minuscule robots, although like in Clarke's story they are larger than what is strictly meant by the term 'nanotechnology'.\n Robert Silverberg's 1969 short story How It Was when the Past Went Away describes nanotechnology[clarification needed] being used in the construction of stereo loudspeakers, with a thousand speakers per inch.[5]\n The 1984 novel Peace on Earth by Stanislaw Lem tells about small bacteria-sized nanorobots looking like normal dust (developed by artificial intelligence placed by humans on the Moon in the era of cold warfare) that has later came to Earth and are replicating, destroying all weapons, modern technology and software, leaving living organisms (as there were no living organisms on the Moon) intact.\n The 1985 novel Blood Music by Greg Bear (originally a 1983 short story) features genetically engineered white blood cells that eventually learn to manipulate matter on an atomic scale.\n The 1991 novelization of Terminator 2: Judgment Day, authored by Randall Frakes, expands the origin story of the T-1000 Terminator through the inclusion of a prologue set in the future. It is explained that the T-1000 is a 'Nanomorph', that was created by Skynet, through the use of programmable Nanotechnology. This was only implied in the film itself.\n The 1992 novel Assemblers of Infinity is a science-fiction novel authored by Kevin J. Anderson and Doug Beason. The plot line makes specific mention of nano-assembly and nano-disassembly robots, along with admonitions regarding the dangers that these bacteria-sized machines might pose.\n In Kim Stanley Robinson's Red Mars (1992), the extraordinary tensile strength of carbon nanotubes is used to create a tether for a space elevator, which connects Mars to an asteroid that has been led into orbit around the planet. The space elevator speeds travel of people and materials between Earth and Mars, but creates tension between factions — and is later destroyed.\n Neal Stephenson's 1995 novel The Diamond Age is set in a world where nanotechnology is commonplace. Nanoscale warfare, fabrication at the molecular scale, and self-assembling islands all exist.\n The morphing technology in Animorphs is described as a form of nanotechnology that allows its users to transform into other animal and alien species, as well as members of their own species.\n The Trinity Blood series features an alien nanomachine found on Mars which is present in the body of the protagonist, Abel Nighroad. These nanomachines are known as Krusnik nanomachines, and feed on the cells of vampires.\n Nanobots (called Nanoes) are central to Stel Pavlou's novel Decipher (2001).\n Michael Crichton's novel Prey (2002) is a cautionary tale about the possible risks of developing nanotechnology.[7] In Prey, a swarm of molecule-sized nanorobots develop intelligence and become a large scale threat.\n Robert Ludlum's 2005 novel The Lazarus Vendetta also focuses around nanotechnology, focusing mainly on its ability to cure cancer.\n The 2006 children's novel The Doomsday Dust (book 4 in the Spy Gear Adventures series by Rick Barba) features a nanite swarm as the villain.\n A nanomorph, term first coined by science fiction writer David Pulver in 1986's GURPS Robots, is a fictional robot entirely made of nanomachines. Its brain is distributed throughout its whole body, which also acts as an all-around sensor, hence making it impossible to surprise as long as the target is on line of sight. A nanomorph is arguably the robotic ultimate in versatility, maybe even in power.[citation needed] Further uses of the concept could include using parts of its body as a tracking device, splitting the body for doing several tasks, or merging two nanomorphs in a greater one, or else gliding\/flying in an ornithopter-like way (by molding itself like a giant, articulated kite). A common but facultative (without this feature, it would still qualify as a nanomorph) improvement is the ability to cover itself with specific colors and textures in a realistic looking manner (the ultimate being to look like a human, à la doppelgänger).\n In the Expanse series, the protomolecule was created by a race of ancient aliens and sent to star systems across the galaxy to terraform planets into habitable worlds and develop a gateway network to facilitate interstellar travel. The protomolecule had the ability to consume biomass and technology and utilize them to serve various different functions. Numerous violent conflicts were fought between factions of humanity, notably the United Nations, Martian Congressional Republic, Outer Planets Alliance, and Laconian Empire, for control of this technology.  \n Rudy Rucker's 2007 novel Postsingular depicts the protagonists' experiences with three generations of swarm nanorobots: \"nants\", which are initially tasked with consuming the Earth and Moon (including organic life) to create a virtual Earth before being reversed; \"orphids\", tasked with a more ubiquitous and assistive role in augmenting reality, interfacing with human thought and even enabling interdimensional travel; and \"silps\", which accomplish the Singularity by replacing the previous generations of nanorobots and enable organic beings and inorganic material on Earth to express sentient, intelligible thought to each other. Rucker's 2009 sequel Hylozoic continues the protagonists' experiences with silps after the Singularity as they enable interstellar and interdimensional travel and communication with Earth by extraterrestrial beings who have already experienced similar Singularities themselves.   \n One of the first mentions on a television show was an announcement to students over the school loudspeakers in the 1987 Max Headroom episode, \"Academy\" that, \"Nanotechnology pod test results are posted in the Submicron Lab for your viewing.\"\n The anime series Ghost in the Shell: Stand Alone Complex employs a plotline heavily involved in the use of \"micromachines\" as a form of treatment against complex diseases after a subject undergoing cyberisation.\n In the Star Trek universe, from Star Trek: The Next Generation onward, the Borg use nanomachines, referred to as nanoprobes, to assimilate individuals into their collective. In another episode, an experiment by Wesley Crusher gone awry led to nanites developing a collective intelligence and interfering with ship systems, eventually being deposited on a planet to establish their own civilization.\n On the television show Red Dwarf, nanobots played a notable role in series VII to IX. Nanobots are nanotechnology created to be a self-repair system for androids like Kryten as they can also change anything into anything else. Kryten's nanobots grow bored of their duties and take over the ship Red Dwarf, leaving the crew to try and recapture it aboard the smaller Starbug. In the end the ship they are chasing is actually a smaller Red Dwarf built by the nanobots (which evaded their scanners in the end by coming aboard Starbug), with the rest being changed into a planet. Once the crew discover this and find the nanobots, they force them to rebuild Red Dwarf (as well as Dave Lister's then-missing arm). In the end the nanobots build an enhanced Red Dwarf based on the original design plans. They also resurrect the original full crew killed in the first episode.\n The episode The New Breed of the show Outer Limits featured nanobots.\n Nanobots were also featured during the Sci-Fi Channel era of Mystery Science Theater 3000, where they were known as \"nanites\". They were depicted on the show as microscopic, bug-like, freestanding robots with distinct personalities.\n Nanotechnology appeared several times in the TV series Stargate SG-1 and Stargate Atlantis, in the form of the replicators and the Asurans, respectively. A \"nanovirus\" is also seen in Stargate Atlantis.\n In Cowboy Bebop: The Movie (2001), a criminal blows up a tanker trunk containing a nanobot virus that instantly kills thousands.\n In the 2003 film Agent Cody Banks, a scientist creates nanobots programmed to clean up oil spills.\n In the 2004 film I, Robot, nanites are used to wipe out artificial intelligence in the event of a malfunction and are depicted as a liquid containing tiny silver objects.\n In the 2005 Doctor Who television episode \"The Empty Child\/The Doctor Dances\" a metal cylinder falls from space and lands in World War II-era London, releasing nanobots which transform every human they come into contact with into gas mask-wearing zombies, like the first human they encountered, a gas mask-wearing child.\n In the 2008 film The Day the Earth Stood Still, the alien robot \"GORT\" disintegrates into a swarm of self-replicating nanobots shaped like bugs that cover Earth and destroy all humans and artificial structures by seemingly devouring them within seconds.\n The revamped Knight Rider television series and TV movie incorporate nanotechnology into the Knight Industries Three Thousand (KITT), allowing it to change color and shape, as well as providing abilities such as self-regeneration.\n In the 2009 film G.I. Joe: The Rise of Cobra, the main plot is to save the world from a warhead containing deadly nanobots called the \"Nanomites\", which if detonated over a city could destroy it in hours.\n The popular NBC science fiction show, Revolution, is based on a worldwide blackout due to the manipulation of nanotechnology.\n In 2010 Generator Rex was shown on Cartoon Network. It was based on a laboratory experiment going wrong and infecting the world with bad \"Nanites\" which turned people into monsters known as E.V.Os.\n In the Ben 10 series, there is a nanotechnology-based alien species called Nanochips, who first appeared in the live-action movie Ben 10: Alien Swarm.\n Nanotechnology is featured heavily within the Terminator film series. The 1991 film Terminator 2: Judgment Day and 2015 film Terminator: Genisys feature the T-1000 terminator. The T-1000 is composed of Mimetic Polyalloy, a liquid metal that utilizes nanites for shapeshifting abilities; Giving the T-1000 the ability to mimic anyone it samples through physical contact.  It can also form its arms into blades and stabbing weapons and instantly recover from any damage. In the 2003 film Terminator 3: Rise of the Machines a new terminator, the T-X, also utilities Mimetic Polyalloy for shapeshifting abilities; like the T-1000 it can mimic anyone it touches. The T-X is also equipped with nanotechnological transjectors, and can infect and control other machines using nanites.\n In Terminator Genisys, human resistance leader John Connor is infected with \"machine phase matter\" by a T-5000 terminator, transforming John into a \"T-3000\". The T-3000, like the T-1000 and T-X units, has shapeshifting and replication abilities. This unit's deadly structure gives the T-3000 the unique ability to instantly scatter into particles and then quickly reform to avoid harmful impact as well as instantly recovering from damage.\n In the 2014 film Transcendence, the uploaded consciousness of Will Caster (Johnny Depp) uses nanotechnology to turn himself, and the local townsfolk, into a self-healing defense force with superhuman strength.\n In Venture Brothers Season 6 Episode 3 \"Faking Miracles\" a laboratory accident leads nanobots to enter Dean Venture's body. Billy Quizboy and Peter White take remote control of the nanobots, inadvertently torturing Dean to showcase the power of the nanobots to Dr. Venture. Eventually they are used, unbeknownst to Dean, to improve his intelligence so that he can pass an entrance examination for college. In the post-credit scene Dean painfully urinates them out like a set of kidney stones.\n Nanotechnology is featured in the Marvel Cinematic Universe (MCU):\n In PlanetSide and PlanetSide 2, nanites are used to fabricate weapons, vehicles, structures, equipment, and even resurrect human bodies. The development of rebirthing technology has allowed soldiers achieve immortality by downloading their consciousness into a new body composed entirely of nanites.\n In Rise of the Robots and Rise 2: Resurrection, A nanomorth features a gynoid known as the Supervisor which composed of Chromium element, a liquid metal that utilizes nanites for shapeshifting abilities and a hive mind constructed from trillions of nanobots in a sealed central chamber within Metropolis 4 . Due to the corruption of the EGO virus which infect the Supervisor, she now controls the Electrocorp and all other machines in Metropolis 4.\n In Total Annihilation nanobots are used to build structures.\n In some games of the Mortal Kombat series, the character Smoke is a cloud of nanobots.\n In System Shock 2 (1999), \"nanites\" are used as currency as well as a type of weapon ammo.\n In Deus Ex (2000), nanotechnology is an important part of both the plot and game mechanics. A very dangerous technology in the wrong hands, it provides a number of superhuman abilities to the protagonist along with novel approaches to weaponry such as the coveted Dragon's Tooth Sword.\n The MMORPG Anarchy Online (launched 2001) is set on a planet with well-developed nanotechnology, which generally is used as magic in fantasy-themed games.\n In Metal Gear Solid (1998) the protagonist got nanomachines to supply and administer adrenalin, nutrients, sugar, nootropics, and benzedrine and to recharge a Codec's battery. The protagonist of Metal Gear Solid 2 (2001) had artificial blood infused with nanomachine that served functions such as healing. Metal Gear Solid 4 (2008) featured a great deal of nanotechnology, such as the Sons of the Patriots, an artificial intelligence\/nanomachine network that regulated and enhanced the actions of every lawful combatant in the world. In Metal Gear Rising Revengeance (2013) the main antagonist Senator Armstrong also augments himself with nanotechnology.\n In Red Faction (2001), nanotechnology is used on Mars to control miners, and Red Faction Guerilla (2009) features nanotechnology, in particular a device called the Nano Forge, as a major plot point.\n The computer game Hostile Waters features a narrative involving nanotech assemblers.\n In the Ratchet & Clank series, the health system involves nanotechnology. The nanotech can be upgraded by purchase in the first game, or by defeating enemies in other games of the series.\n Nanotechnology is also found in Crysis (2007), Crysis 2 (2011), and Crysis 3 (2013).  The protagonists of these games are equipped with a \"Nano Suit\", which enables them to become stronger, invisible, heavily armored, etc.\n In Marvel: Ultimate Alliance 2 (2009), Reed Richards creates nanites that are meant to control the minds of supervillains. However, the nanites evolve into a group mind called the Fold which serves as the primary antagonist for the game.\n In SpaceChem the player has to build molecular assembler\/disassemblers using nanomachines called \"Waldos\" controlled by a visual programming language.\n The Distant Stars expansion for Stellaris heavily features nanotechnology in many aspects.\n In the manga series Battle Angel Alita: Last Order, nanotechnology is referenced numerously and its use is heavily restricted, owing to the loss of Mercury as a potential planetary colony due to a grey goo catastrophe. Its danger and control has become one of the main driving narratives in the story.\n In Dx13: Nano A Mano[8] - a manga series by Kirupagaren Kanni - the protagonist uses nanobots to create a giant mecha, which is remotely controlled by custom-built equipment such as electronic glove, microphones, cameras, etc.\n Nanomites appear in the G.I. Joe Reinstated series published by Devil's Due.\n In the anime and manga series Black Cat, Eve has the ability to manipulate nanomachines. Nanobots are later used for a variety of purposes, from turning victims into berserk warriors to granting Creed Diskenth immortality.\n In the anime and manga series To Love-Ru, the Transformation Weapons Golden Darkness and Mea Kurosaki have nanomachines within them, in the same manner as Eve from Black Cat.\n In the anime and manga series Project ARMS, the ARMS are weapons made from many nanomachines imbued into compatible biological beings, granting them a great variety of combative abilities and regeneration. The four protagonists each have an ARMS that have artificial intelligence, but the Keith series and the modulated ARMS do not.\n In the LEGO franchise BIONICLE, it is eventually revealed that all characters from the 2001–2008 storyline are biomechanical nanobots (though roughly human-sized, given the size of the gigantic robot they inhabit (12,192 km tall)).\n One of the earliest appearances of nanotech in comics was the Technovore from Iron Man 294 (July 1993).\n In several X-Men storylines, nano-sentinels appear, either used to modify human beings into Prime Sentinels (including the character Fantomex), or to infect mutants and attack their cells.\n"}
{"key":"Renewable Energy","link":"https:\/\/en.wikipedia.org\/wiki\/Renewable Energy","headline":"Renewable energy - Wikipedia","content":"\n Renewable energy, green energy, or low-carbon energy is energy from renewable resources that are naturally replenished on a human timescale. Renewable resources include sunlight, wind, the movement of water, and geothermal heat.[1][2][3][4] Although most renewable energy sources are sustainable, some are not. For example, some biomass sources are considered unsustainable at current rates of exploitation.[5][6] Renewable energy is often used for electricity generation, heating and cooling. Renewable energy projects are typically large-scale, but they are also suited to rural and remote areas and developing countries, where energy is often crucial in human development.[7][8]\n Renewable energy is often deployed together with further electrification, which has several benefits: electricity can move heat or objects efficiently, and is clean at the point of consumption.[9][10] From 2011 to 2021, renewable energy grew from 20% to 28% of global electricity supply. Use of fossil energy shrank from 68% to 62%, and nuclear from 12% to 10%. The share of hydropower decreased from 16% to 15% while power from sun and wind increased from 2% to 10%. Biomass and geothermal energy grew from 2% to 3%. There are 3,146 gigawatts installed in 135 countries, while 156 countries have laws regulating the renewable energy sector.[11][12] In 2021, China accounted for almost half of the global increase in renewable electricity.[13]\n Globally there are over 10 million jobs associated with the renewable energy industries, with solar photovoltaics being the largest renewable employer.[14] Renewable energy systems are rapidly becoming more efficient and cheaper and their share of total energy consumption is increasing,[15] with a large majority of worldwide newly installed electricity capacity being renewable.[16] In most countries, photovoltaic solar or onshore wind are the cheapest new-build electricity.[17]\n Many nations around the world already have renewable energy contributing more than 20% of their total energy supply, with some generating over half their electricity from renewables.[18] A few countries generate all their electricity using renewable energy.[19] National renewable energy markets are projected to continue to grow strongly in the 2020s and beyond.[20] According to the IEA, to achieve net zero emissions by 2050, 90% of global electricity generation will need to be produced from renewable sources.[21] Some studies say that a global transition to 100% renewable energy across all sectors – power, heat, transport and industry – is feasible and economically viable.[22][23][24]\n Renewable energy resources exist over wide geographical areas, in contrast to fossil fuels, which are concentrated in a limited number of countries. Deployment of renewable energy and energy efficiency technologies is resulting in significant energy security, climate change mitigation, and economic benefits.[25] However renewables are being hindered by hundreds of billions of dollars of fossil fuel subsidies.[26] In international public opinion surveys there is strong support for renewables such as solar power and wind power.[27][28] In 2022 the International Energy Agency asked countries to solve policy, regulatory, permitting and financing obstacles to adding more renewables, to have a better chance of reaching net zero carbon emissions by 2050.[29]\n Renewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:[32]\n Renewable energy is derived from natural processes that are replenished constantly. In its various forms, it derives directly from the sun, or from heat generated deep within the earth. Included in the definition is electricity and heat generated from solar, wind, ocean, hydropower, biomass, geothermal resources, and biofuels and hydrogen derived from renewable resources. Renewable energy stands in contrast to fossil fuels, which are being used far more quickly than they are being replenished. According to several studies, renewable energy is more evenly distributed than fossil fuels globally and a wider deployment of renewables should lead to more decentralized global energy system with fewer geopolitical tensions among states.[34][35] Renewable energy resources and significant opportunities for energy efficiency exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency, and technological diversification of energy sources, would result in significant energy security and economic benefits.[25] Solar and wind power have become much cheaper.[36] In some cases it will be cheaper to transition to these sources as opposed to continuing to use the current, inefficient, fossil fuels. It would also reduce environmental pollution such as air pollution caused by the burning of fossil fuels, and improve public health, reduce premature mortalities due to pollution and save associated health costs that could amount to trillions of dollars annually.[37][38] Multiple analyses of decarbonization strategies have found that quantified health benefits can significantly offset the costs of implementing these strategies.[39][40]\n Climate change concerns, coupled with the continuing fall in the costs of some renewable energy equipment, such as wind turbines and solar panels, are driving increased use of renewables.[27] New government spending, regulation and policies helped the industry weather the global financial crisis better than many other sectors.[41] As of 2019[update], however, according to the International Renewable Energy Agency, renewables overall share in the energy mix (including power, heat and transport) needs to grow six times faster, in order to keep the rise in average global temperatures \"well below\" 2.0 °C (3.6 °F) during the present century, compared to pre-industrial levels.[42]\n A household's solar panels, and batteries if they have them, can often either be used for just that household or if connected to an electrical grid can be aggregated with millions of others.[43]  According to the research, a nation must reach a certain point in its growth before it can take use of more renewable energy. In our words, its addition changed how crucial input factors (labor and capital) connect to one another, lowering their overall elasticity and increasing the apparent economies of scale.[44][clarification needed] The United Nations' eighth Secretary-General Ban Ki-moon said that renewable energy has the ability to lift the poorest nations to new levels of prosperity.[45] Renewables supply more than 20% of energy in at least 30 nations.[46] Although many countries have various policy targets for longer-term shares of renewable energy these tend to be only for the power sector,[47] including a 40% target of all electricity generated for the European Union by 2030.[48]\n Renewable energy often displaces conventional fuels in four areas: electricity generation, hot water\/space heating, transportation, and rural (off-grid) energy services.[49]\n More than a quarter of electricity is generated from renewables as of 2021.[50] One of the efforts to decarbonize transportation is the increased use of electric vehicles (EVs).[51] Despite that and the use of biofuels, such as biojet, less than 4% of transport energy is from renewables.[52] Occasionally hydrogen fuel cells are used for heavy transport.[53] Meanwhile, in the future electrofuels may also play a greater role in decarbonizing hard-to-abate sectors like aviation and maritime shipping.[54]\n Solar water heating makes an important contribution to renewable heat in many countries, most notably in China, which now has 70% of the global total (180 GWth). Most of these systems are installed on multi-family apartment buildings[55] and meet a portion of the hot water needs of an estimated 50–60 million households in China. Worldwide, total installed solar water heating systems meet a portion of the water heating needs of over 70 million households.\n Heat pumps provide both heating and cooling, and also flatten the electric demand curve and are thus an increasing priority.[56] Renewable thermal energy is also growing rapidly.[57] About 10% of heating and cooling energy is from renewables.[50]\n Solar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, concentrated solar power (CSP), concentrator photovoltaics, solar architecture and artificial photosynthesis.[63][64][obsolete source]Most new renewable energy is solar.[65] Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert, and distribute solar energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air. Active solar technologies encompass solar thermal energy, using solar collectors for heating, and solar power, converting sunlight into electricity either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).[66]\n A photovoltaic system converts light into electrical direct current (DC) by taking advantage of the photoelectric effect.[67] Solar PV has turned into a multi-billion, fast-growing industry, continues to improve its cost-effectiveness, and has the most potential of any renewable technologies together with CSP.[68][69] Concentrated solar power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Commercial concentrated solar power plants were first developed in the 1980s. CSP-Stirling has by far the highest efficiency among all solar energy technologies.\n In 2011, the International Energy Agency declared that \"the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared\".[63] Solar power accounts for 505 GW annually, which is about 2% of the world's electricity. Solar energy can be harnessed anywhere that receives sunlight; however, the amount of solar energy that can be harnessed for electricity generation is influenced by weather conditions, geographic location and time of day.[70]\n According to chapter 6 of the IPCC 2022 climate mitigation report, the global potential of direct solar energy far exceeds that of any other renewable energy resource. It is well beyond the total amount of energy needed in order to support mitigation over the current century.[51] Australia has the largest proportion of solar electricity in the world, supplying 9.9% of the country's electrical demand in 2020.[71] More than 30 per cent of Australian households now have rooftop solar PV, with a combined capacity exceeding 11 GW.[72]\n There are, however, environmental implications of scaling up solar energy. In particular, the demand for raw materials such as aluminum poses concerns over the carbon footprint that will result from harvesting raw materials needed to implement solar energy.[73]\n Photovoltaics (PV) is rapidly-growing with global capacity increasing from 230 GW at the end of 2015 to 890 GW in 2021.[74] PV uses solar cells assembled into solar panels to convert sunlight into electricity. PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station. The predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time, and reached grid parity.[77]\n Building-integrated photovoltaics or \"onsite\" PV systems use existing land and structures and generate power close to where it is consumed.[78]\n Photovoltaics grew fastest in China between 2016 and 2021 adding 560 GW, more than all advanced economies combined. Solar PV's installed power capacity is poised to surpass that of coal by 2027, becoming the largest in the world.[79] This requires an increase of installed PV capacity to 4,600 GW, of which more than half is expected to be deployed in China and India.[80][81]\n Commercial concentrated solar power plants were first developed in the 1980s. As the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and gigawatt-scale solar power stations are being built. Many solar photovoltaic power stations have been built, mainly in Europe, China and the United States.[82] The 1.5 GW Tengger Desert Solar Park, in China is the world's largest PV power station. Many of these plants are integrated with agriculture and some use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems.\n Solar thermal energy (STE) is a form of energy and a technology for harnessing solar energy to generate thermal energy for use in industry, and in the residential and commercial sectors.\n Solar thermal collectors are classified by the United States Energy Information Administration as low-, medium-, or high-temperature collectors. Low-temperature collectors are generally unglazed and used to heat swimming pools or to heat ventilation air. Medium-temperature collectors are also usually flat plates but are used for heating water or air for residential and commercial use.\n High-temperature collectors concentrate sunlight using mirrors or lenses and are generally used for fulfilling heat requirements up to 300 deg C \/ 20 bar pressure in industries, and for electric power production. Two categories include Concentrated Solar Thermal (CST) for fulfilling heat requirements in industries, and Concentrated Solar Power (CSP) when the heat collected is used for electric power generation. CST and CSP are not replaceable in terms of application.\n Air flow can be used to run wind turbines. Modern utility-scale wind turbines range from around 600 kW to 9 MW of rated power. The power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine.[87] Areas where winds are stronger and more constant, such as offshore and high-altitude sites, are preferred locations for wind farms.\n Wind-generated electricity met nearly 4% of global electricity demand in 2015, with nearly 63 GW of new wind power capacity installed. Wind energy was the leading source of new capacity in Europe, the US and Canada, and the second largest in China. In Denmark, wind energy met more than 40% of its electricity demand while Ireland, Portugal and Spain each met nearly 20%.[88]\n Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore, and likely also industrial use of new types of VAWT turbines in addition to the horizontal axis units currently in use. As offshore wind speeds average ~90% greater than that of land, offshore resources can contribute substantially more energy than land-stationed turbines.[89]\n Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. Water can generate electricity with a conversion efficiency of about 90%, which is the highest rate in renewable energy.[93] There are many forms of water energy:\n Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010.[needs update] Of the top 50 countries by percentage of electricity generated from renewables, 46 are primarily hydroelectric.[97] There are now seven hydroelectricity stations larger than 10 GW (10,000 MW) worldwide, see table below.\n Much hydropower is flexible, thus complementing wind and solar.[98] Wave power, which captures the energy of ocean surface waves, and tidal power, converting the energy of tides, are two forms of hydropower with future potential; however, they are not yet widely employed commercially.[99] A demonstration project operated by the Ocean Renewable Power Company on the coast of Maine, and connected to the grid, harnesses tidal power from the Bay of Fundy, location of the world's highest tidal flow. Ocean thermal energy conversion, which uses the temperature difference between cooler deep and warmer surface waters, currently has no economic feasibility.[100][101]\n In 2021, the world renewable hydropower capacity was 1,360 GW.[79] Only a third of the world's estimated hydroelectric potential of 14,000 TWh\/year has been developed.[102][103] New hydropower projects face opposition from local communities due to their large impact, including relocation of communities and flooding of wildlife habitats and farming land.[104] High cost and lead times from permission process, including environmental and risk assessments, with lack of environmental and social acceptance are therefore the primary challenges for new developments.[105] It is popular to repower old dams thereby increasing their efficiency and capacity as well as quicker responsiveness on the grid.[106] Where circumstances permit existing dams such as the Russell Dam built in 1985 may be updated with \"pump back\" facilities for pumped-storage which is useful for peak loads or to support intermittent wind and solar power. Because dispatchable power is more valuable than VRE[107][108] countries with large hydroelectric developments such as Canada and Norway are spending billions to expand their grids to trade with neighboring countries having limited hydro.[109]\n Biomass is biological material derived from living, or recently living organisms. It commonly refers to plants or plant-derived materials. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel in solid, liquid or gaseous form. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: thermal, chemical, and biochemical methods. Wood was the largest biomass energy source as of 2012;[113] examples include forest residues – such as dead trees, branches and tree stumps, yard clippings, wood chips and even municipal solid waste. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo,[114] and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).\n Plant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy.[115] The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first-generation biofuel.\n Biomass can be converted to other usable forms of energy such as methane gas[116] or transportation fuels such as ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gas – also called landfill gas or biogas. Crops, such as corn and sugarcane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products such as vegetable oils and animal fats.[117] There is a great deal of research involving algal fuel or algae-derived biomass due to the fact that it is a non-food resource, grows around 20 times faster than other types of food crops, such as corn and soy, and can be grown almost anywhere.[118][119] Once harvested, it can be fermented to produce biofuels such as ethanol, butanol, and methane, as well as biodiesel and hydrogen. The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks).\n Biomass, biogas and biofuels are burned to produce heat\/power and in doing so can harm the environment. Pollutants such as sulphurous oxides (SOx), nitrous oxides (NOx), and particulate matter (PM) are produced from the combustion of biomass. With regards to traditional use of biomass for heating and cooking, the World Health Organization estimates that 3.7 million prematurely died from outdoor air pollution in 2012 while indoor pollution from biomass burning effects over 3 billion people worldwide.[120][121]\n Bioenergy global capacity in 2021 was 158 GW. Biofuels avoided 4.4% of global transport fuel demand in 2021.[79]\n Biofuels include a wide range of fuels which are derived from biomass. The term covers solid, liquid, and gaseous fuels.[122] Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas, landfill gas and synthetic gas. Bioethanol is an alcohol made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugarcane and, more recently, sweet sorghum. The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa.[123]\n With advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil. The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency, biofuels do not address global warming concerns.[124] Biodiesel is made from vegetable oils, animal fats or recycled greases. It can be used as a fuel for vehicles in its pure form, or more commonly as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. Biofuels provided 2.7% of the world's transport fuel in 2010.[125][needs update]\n Policies in more than 80 countries support biofuels demand.[79]\n Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter.[126] Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste (bagasse) is used to produce heat and power.[127] There are no longer light vehicles in Brazil running on pure gasoline.[128]\n Biojet is expected to be important for short-term reduction of carbon dioxide emissions from long-haul flights.[129]\n High temperature geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet and from radioactive decay of minerals (in currently uncertain[134] but possibly roughly equal[135] proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective geothermal originates from the Greek roots geo, meaning earth, and thermos, meaning heat.\n The heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – 6,400 kilometres (4,000 mi) down. At the core, temperatures may reach over 5,000 °C (9,030 °F). Heat conducts from the core to the surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to 371 °C (700 °F).[136]\n Low temperature geothermal[56] refers to the use of the outer crust of the Earth as a thermal battery to facilitate renewable thermal energy for heating and cooling buildings, and other refrigeration and industrial uses. In this form of geothermal, a geothermal heat pump and ground-coupled heat exchanger are used together to move heat energy into the Earth (for cooling) and out of the Earth (for heating) on a varying seasonal basis. Low-temperature geothermal (generally referred to as \"GHP\"[clarification needed]) is an increasingly important renewable technology because it both reduces total annual energy loads associated with heating and cooling, and it also flattens the electric demand curve eliminating the extreme summer and winter peak electric supply requirements. Thus low temperature geothermal\/GHP is becoming an increasing national[clarification needed] priority with multiple tax credit support[137] and focus as part of the ongoing movement toward net zero energy.[138]\n Geothermal power is cost effective, reliable, sustainable, and environmentally friendly,[139] but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are usually much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.\n In 2017, the United States led the world in geothermal electricity production with 12.9 GW of installed capacity.[74] The largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California.[140] The Philippines follows the US as the second highest producer of geothermal power in the world, with 1.9 GW of capacity online.[74]\n Global geothermal capacity in 2021 was 15 GW.[79]\n There are also other renewable energy technologies that are still under development, including cellulosic ethanol, hot-dry-rock geothermal power, and marine energy.[141] These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.[141]\n There are numerous organizations within the academic, federal,[clarification needed] and commercial sectors conducting large-scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.[142]\nMultiple government supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners.[143]\n Enhanced geothermal systems (EGS) are a new type of geothermal power technology that does not require natural convective hydrothermal resources. The vast majority of geothermal energy within drilling reach is in dry and non-porous rock.[144] EGS technologies \"enhance\" and\/or create geothermal resources in this \"hot dry rock (HDR)\" through hydraulic fracturing. EGS and HDR technologies, such as hydrothermal geothermal, are expected to be baseload resources that produce power 24 hours a day like a fossil plant. Distinct from hydrothermal, HDR and EGS may be feasible anywhere in the world, depending on the economic limits of drill depth. Good locations are over deep granite covered by a thick (3–5 km or 1.9–3.1 mi) layer of insulating sediments which slow heat loss.[145] There are HDR and EGS systems currently being developed and tested in France, Australia, Japan, Germany, the U.S., and Switzerland. The largest EGS project in the world is a 25 megawatt demonstration plant currently being developed in the Cooper Basin, Australia. The Cooper Basin has the potential to generate 5,000–10,000 MW.\n Marine energy (also sometimes referred to as ocean energy) is the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world's oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries. The term marine energy encompasses wave power – power from surface waves, marine current power - power from marine hydrokinetic streams (e.g., the Gulf Stream), and tidal power – obtained from the kinetic energy of large bodies of moving water. Reverse electrodialysis (RED) is a technology for generating electricity by mixing fresh river water and salty sea water in large power cells designed for this purpose; as of 2016, it is being tested at a small scale (50 kW). Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water. The oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.[146][147][page needed]\n Passive daytime radiative cooling (PDRC) uses the coldness of outer space as a renewable energy source to achieve daytime cooling that can be used in many applications,[151][152][153] such as indoor space cooling,[154][155] outdoor urban heat island mitigation,[156][157] and solar cell efficiency.[158][159] PDRC surfaces are designed to be high in solar reflectance to minimize heat gain and strong in longwave infrared (LWIR) thermal radiation heat transfer.[160] On a planetary scale, it has been proposed as a way to slow and reverse global warming.[150][161] PDRC applications are deployed as sky-facing surfaces, similar to other renewable energy sources such as photovoltaic systems and solar thermal collectors.[159] PDRC became possible with the ability to suppress solar heating using photonic metamaterials, first published in a study by Raman et al. to the scientific community in 2014.[158][162] PDRC applications for indoor space cooling is growing with an estimated \"market size of ~$27 billion in 2025.\"[163]\n Earth emits roughly 1017 W of infrared thermal radiation that flows toward the cold outer space. Solar energy hits the surface and atmosphere of the earth and produces heat. Using various theorized devices like emissive energy harvester (EEH) or thermoradiative diode, this energy flow can be converted into electricity. In theory, this technology can be used during nighttime.[164][165]\n Producing liquid fuels from oil-rich (fat-rich) varieties of algae is an ongoing research topic. Various microalgae grown in open or closed systems are being tried including some systems that can be set up in brownfield and desert lands.[166]\n Collection of static electricity charges from water droplets on metal surfaces is an experimental technology that would be especially useful in low-income countries with relative air humidity over 60%.[167]\n Breeder reactors could, in principle, extract almost all of the energy contained in uranium or thorium, decreasing fuel requirements by a factor of 100 compared to widely used once-through light water reactors, which extract less than 1% of the energy in the actinide metal (uranium or thorium) mined from the earth.[168] The high fuel-efficiency of breeder reactors could greatly reduce concerns about fuel supply, energy used in mining, and storage of radioactive waste. With seawater uranium extraction (currently too expensive to be economical), there is enough fuel for breeder reactors to satisfy the world's energy needs for 5 billion years at 1983's total energy consumption rate, thus making nuclear energy effectively a renewable energy.[169][170] In addition to seawater the average crustal granite rocks contain significant quantities of uranium and thorium that with breeder reactors can supply abundant energy for the remaining lifespan of the sun on the main sequence of stellar evolution.[171]\n Artificial photosynthesis uses techniques including nanotechnology to store solar electromagnetic energy in chemical bonds by splitting water to produce hydrogen and then using carbon dioxide to make methanol.[172] Researchers in this field strived to design molecular mimics of photosynthesis that use a wider region of the solar spectrum, employ catalytic systems made from abundant, inexpensive materials that are robust, readily repaired, non-toxic, stable in a variety of environmental conditions and perform more efficiently allowing a greater proportion of photon energy to end up in the storage compounds, i.e., carbohydrates (rather than building and sustaining living cells).[173] However, prominent research faces hurdles, Sun Catalytix a MIT spin-off stopped scaling up their prototype fuel-cell in 2012 because it offers few savings over other ways to make hydrogen from sunlight.[174]\n Renewable energy production from some sources such as wind and solar is more variable and more geographically spread than technology based on fossil fuels and nuclear. While integrating it into the wider energy system is feasible, it does lead to some additional challenges such as increased production volatility and decreased system inertia.[175] Implementation of energy storage, using a wide variety of renewable energy technologies, and implementing a smart grid in which energy is automatically used at the moment it is produced can reduce risks and costs of renewable energy implementation.[175][176]: 15–16 \n Sector coupling of the power generation sector with other sectors may increase flexibility: for example the transport sector can be coupled by charging electric vehicles and sending electricity from vehicle to grid.[177] Similarly the industry sector can be coupled by hydrogen produced by electrolysis,[178] and the buildings sector by thermal energy storage for space heating and cooling.[179]\n Electrical energy storage is a collection of methods used to store electrical energy. Electrical energy is stored during times when production (especially from intermittent sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption. Pumped-storage hydroelectricity accounts for more than 85% of all grid power storage.[180] Batteries are increasingly being deployed for storage[181] and grid ancillary services[182] and for domestic storage.[183] Green hydrogen is a more economical means of long-term renewable energy storage, in terms of capital expenditures compared to pumped hydroelectric or batteries.[184][185]\n Most new renewables are solar, followed by wind then hydro then bioenergy.[186] Investment in renewables, especially solar, tends to be more effective in creating jobs than coal, gas or oil.[187][188] Worldwide, renewables employ about 12 million people as of 2020, with solar PV being the technology employing the most at almost 4 million.[189] However, as of February 2024, the world's supply of workforce for solar energy is lagging greatly behind demand as universities worldwide still produce more workforce for fossil fuels than for renewable energy industries.[190]\n The International Renewable Energy Agency (IRENA) stated that ~86% (187 GW) of renewable capacity added in 2022 had lower costs than electricity generated from fossil fuels.[191] IRENA also stated that capacity added since 2000 reduced electricity bills in 2022 by at least $520 billion, and that in non-OECD countries, the lifetime savings of 2022 capacity additions will reduce costs by up to $580 billion.[191]\n * = 2018. All other values for 2019.\n The results of a recent review of the literature concluded that as greenhouse gas (GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies.[206]\n In the decade of 2010–2019, worldwide investment in renewable energy capacity excluding large hydropower amounted to US$2.7 trillion, of which the top countries China contributed US$818 billion, the United States contributed US$392.3 billion, Japan contributed US$210.9 billion, Germany contributed US$183.4 billion, and the United Kingdom contributed US$126.5 billion.[207] This was an increase of over three and possibly four times the equivalent amount invested in the decade of 2000–2009 (no data is available for 2000–2003).[207]\n As of 2022, an estimated 28% of the world's electricity was generated by renewables. This is up from 19% in 1990.[208]\n A December 2022 report by the IEA forecasts that over 2022-2027, renewables are seen growing by almost 2 400 GW in its main forecast, equal to the entire installed power capacity of China in 2021. This is an 85% acceleration from the previous five years, and almost 30% higher than what the IEA forecast in its 2021 report, making its largest ever upward revision. Renewables are set to account for over 90% of global electricity capacity expansion over the forecast period.[79] To achieve net zero emissions by 2050, IEA believes that 90% of global electricity generation will need to be produced from renewable sources.[21]\n In June 2022 IEA Executive Director Fatih Birol said that countries should invest more in renewables to \"ease the pressure on consumers from high fossil fuel prices, make our energy systems more secure, and get the world on track to reach our climate goals.”[210]\n China's five year plan to 2025 includes increasing direct heating by renewables such as geothermal and solar thermal.[211]\n REPowerEU, the EU plan to escape dependence on fossil Russian gas, is expected to call for much more green hydrogen.[212]\n After a transitional period,[213] renewable energy production is expected to make up most of the world's energy production. In 2018, the risk management firm, DNV GL, forecasts that the world's primary energy mix will be split equally between fossil and non-fossil sources by 2050.[214]\n In July 2014, WWF and the World Resources Institute convened a discussion among a number of major US companies who had declared their intention to increase their use of renewable energy. These discussions identified a number of \"principles\" which companies seeking greater access to renewable energy considered important market deliverables. These principles included choice (between suppliers and between products), cost competitiveness, longer term fixed price supplies, access to third-party financing vehicles, and collaboration.[215]\n UK statistics released in September 2020 noted that \"the proportion of demand met from renewables varies from a low of 3.4 per cent (for transport, mainly from biofuels) to highs of over 20 per cent for 'other final users', which is largely the service and commercial sectors that consume relatively large quantities of electricity, and industry\".[216]\n In some locations, individual households can opt to purchase renewable energy through a consumer green energy program.\n Renewable energy in developing countries is an increasingly used alternative to fossil fuel energy, as these countries scale up their energy supplies and address energy poverty. Renewable energy technology was once seen as unaffordable for developing countries.[217] However, since 2015, investment in non-hydro renewable energy has been higher in developing countries than in developed countries, and comprised 54% of global renewable energy investment in 2019.[218] The International Energy Agency forecasts that renewable energy will provide the majority of energy supply growth through 2030 in Africa and Central and South America, and 42% of supply growth in China.[219]\n In Kenya, the Olkaria V Geothermal Power Station is one of the largest in the world.[221] The Grand Ethiopia Renaissance Dam project incorporates wind turbines.[222] Once completed, Morocco's Ouarzazate Solar Power Station is projected to provide power to over a million people.[223]\n Policies to support renewable energy have been vital in their expansion. Where Europe dominated in establishing energy policy in the early 2000s, most countries around the world now have some form of energy policy.[225]\n The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed in 2009, with 75 countries signing the charter of IRENA.[226] As of April 2019, IRENA has 160 member states.[227] The then United Nations Secretary-General Ban Ki-moon has said that renewable energy can lift the poorest nations to new levels of prosperity,[45] and in September 2011 he launched the UN Sustainable Energy for All initiative to improve energy access, efficiency and the deployment of renewable energy.[228]\n The 2015 Paris Agreement on climate change motivated many countries to develop or improve renewable energy policies.[20] In 2017, a total of 121 countries adopted some form of renewable energy policy.[225] National targets that year existed in 176 countries.[20] In addition, there is also a wide range of policies at the state\/provincial, and local levels.[125] Some public utilities help plan or install residential energy upgrades.\n Many national, state and local governments have created green banks. A green bank is a quasi-public financial institution that uses public capital to leverage private investment in clean energy technologies.[229] Green banks use a variety of financial tools to bridge market gaps that hinder the deployment of clean energy.\n Climate neutrality by the year 2050 is the main goal of the European Green Deal.[230] For the European Union to reach their target of climate neutrality, one goal is to decarbonise its energy system by aiming to achieve \"net-zero greenhouse gas emissions by 2050.\"[231]\n 100% renewable energy is the goal of the use renewable resources for all energy. 100% renewable energy for electricity, heating, cooling and transport is motivated by climate change, pollution and other environmental issues, as well as economic and energy security concerns. Shifting the total global primary energy supply to renewable sources requires a transition of the energy system, since most of today's energy is derived from non-renewable fossil fuels.\n Research into this topic is fairly new, with very few studies published before 2009, but has gained increasing attention in recent years. The majority of studies show that a global transition to 100% renewable energy across all sectors – power, heat, transport and industry – is feasible and economically viable.[232][233][234][235][need quotation to verify] A cross-sectoral, holistic approach is seen as an important feature of 100% renewable energy systems and is based on the assumption \"that the best solutions can be found only if one focuses on the synergies between the sectors\" of the energy system such as electricity, heat, transport or industry.[236]\n The International Renewable Energy Agency's (IRENA) 2023 report on renewable energy finance highlights steady investment growth since 2018: USD 348 billion in 2020 (a 5.6% increase from 2019), USD 430 billion in 2021 (24% up from 2020), and USD 499 billion in 2022 (16% higher). This trend is driven by increasing recognition of renewable energy's role in mitigating climate change and enhancing energy security, along with investor interest in alternatives to fossil fuels. Policies such as feed-in tariffs in China and Vietnam have significantly increased renewable adoption. Furthermore, from 2013 to 2022, installation costs for solar photovoltaic (PV), onshore wind, and offshore wind fell by 69%, 33%, and 45%, respectively, making renewables more cost-effective.[239][240]\n From 2020 to 2022, solar technology investments almost doubled from USD 162 billion to USD 308 billion, driven by the sector's increasing maturity and cost reductions, particularly in solar photovoltaic (PV), which accounted for 90% of total investments. China and the United States were the main recipients, collectively making up about half of all solar investments since 2013. Despite reductions in Japan and India due to policy changes and COVID-19, growth in China, the United States, and a significant increase from Vietnam's feed-in tariff program offset these declines. Globally, the solar sector added 714 gigawatts (GW) of solar PV and concentrated solar power (CSP) capacity between 2013 and 2021, with a notable rise in large-scale solar heating installations in 2021, especially in China, Europe, Turkey, and Mexico.[240]\n Investments in wind technologies reached USD 161 billion in 2020, with onshore wind dominating at 80% of total investments from 2013 to 2022. Offshore wind investments nearly doubled to USD 41 billion between 2019 and 2020, primarily due to policy incentives in China and expansion in Europe. Global wind capacity increased by 557 GW between 2013 and 2021, with capacity additions increasing by an average of 19% each year.[240]\n Between 2013 and 2022, the renewable energy sector underwent a significant realignment of investment priorities. Investment in solar and wind energy technologies markedly increased. In contrast, other renewable technologies such as hydropower (including pumped storage hydropower), biomass, biofuels, geothermal, and marine energy experienced a substantial decrease in financial investment. Notably, from 2017 to 2022, investment in these alternative renewable technologies declined by 45%, falling from USD 35 billion to USD 17 billion.[240]\n Renewable electricity generation by wind and solar is variable. This results in reduced capacity factor and may require keeping some gas-fired power plants or other dispatchable generation on standby[243][244][245] until there is enough energy storage, demand response, grid improvement, and\/or base load power from non-intermittent sources like hydropower, nuclear power or bioenergy.\n The market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs, coupled with high oil prices, peak oil, oil wars, oil spills, promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization.[27][better source needed]\n The International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.[246]\n Solar power plants may compete with arable land,[248][249] while on-shore wind farms face opposition due to aesthetic concerns and noise, which is impacting both humans and wildlife.[250][251][252][need quotation to verify]In the United States, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.[253] These concerns, when directed against renewable energy, are sometimes described as \"not in my back yard\" attitude (NIMBY).\n A 2011 UK Government document states that \"projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake\".[254] In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.[255][256]\n Whether nuclear power should be considered a form of renewable energy is an ongoing subject of debate. Statutory definitions of renewable energy usually exclude many present nuclear energy technologies, with the notable exception of the state of Utah.[257] Dictionary-sourced definitions of renewable energy technologies often omit or explicitly exclude mention of nuclear energy sources, with an exception made for the natural nuclear decay heat generated within the Earth.[258][259]\n The most common fuel used in conventional nuclear fission power stations, uranium-235 is \"non-renewable\" according to the Energy Information Administration, the organization however is silent on the recycled MOX fuel.[259] The National Renewable Energy Laboratory does not mention nuclear power in its \"energy basics\" definition.[260]\n From around 2010 onwards, the geopolitical impact of the growing use of renewable energy has been discussed.[263] Some argue that former fossil fuels exporters will experience a weakening of their position in international affairs, while countries with abundant renewable energy resources will be strengthened.[264] Also some countries rich in critical materials for renewable energy technologies are expected to rise in importance in international affairs.[265][266]\n The GeGaLo index of geopolitical gains and losses assesses how the geopolitical position of 156 countries may change if the world fully transitions to renewable energy resources. Former fossil fuels exporters are expected to lose power, while the positions of former fossil fuel importers and countries rich in renewable energy resources is expected to strengthen.[267] Sourcing of required materials, ownership of key infrastructure assets and the design of grids all require geopolitics consideration.[268][269][270]\n Transitions to renewable energy have many geopolitical implications such as the potential of revenue losses leading to political instability in insufficiently prepared fossil-fuel-exporting economies, albeit it is unclear whether the transition will increase or reduce conflict overall. In particular, a study hypothesizes that a \"configuration emerges in which fossil fuel importers are better off decarbonizing, competitive fossil fuel exporters are better off flooding markets and uncompetitive fossil fuel producers—rather than benefitting from 'free-riding'—suffer from their exposure to stranded assets and lack of investment in decarbonization technologies\".[271][272]\n A study found that transition from fossil fuels to renewable energy systems reduces risks from mining, trade and political dependence because renewable energy systems don't need fuel – they depend on trade only for the acquisition of materials and components during construction.[273]\n Nations rich in solar and wind energy could become major energy exporters.[274]\n Trade in hydrogen could fundamentally redraw the geography of the global energy trade, and international governance and investments that seek to scale up the hydrogen economy could reduce \"the risk of market fragmentation, carbon lock-in, and intensified geo-economic rivalry\".[275][274][276] Electricity will overtake other energy carriers by 2050, accounting for almost 50% of total energy consumption (up from 22% in 2015). Given the limitations of using solely electricity, clean hydrogen has significant potential in a number of industries.[277][278] Hydrogen has the potential to be long-term stored in the electricity and heating industries.[279]\n In 2019, oil and gas companies were listed by Forbes with sales of US$4.8 trillion, about 5% of the global GDP.[280] Net importers such as China and the EU would gain advantages from a transition to low-carbon technologies driven by technological development, energy efficiency or climate change policy, while Russia, the USA or Canada could see their fossil fuel industries nearly shut down.[281] On the other hand, countries with large areas such as Australia, Russia, China, the US, Canada and Brazil and also Africa and the Middle East have a potential for huge installations of renewable energy. The production of renewable energy technologies requires rare-earth elements with new supply chains.[282]\n In October 2021, European Commissioner for Climate Action Frans Timmermans suggested \"the best answer\" to the 2021 global energy crisis is \"to reduce our reliance on fossil fuels.\"[283] He said those blaming the European Green Deal were doing so \"for perhaps ideological reasons or sometimes economic reasons in protecting their vested interests.\"[283] Some critics blamed the European Union Emissions Trading System (EU ETS) and closure of nuclear plants for contributing to the energy crisis.[284][285][286] European Commission President Ursula von der Leyen said that Europe is \"too reliant\" on natural gas and too dependent on natural gas imports. According to Von der Leyen, \"The answer has to do with diversifying our suppliers ... and, crucially, with speeding up the transition to clean energy.\"[287]\n The renewable energy transition requires increased extraction of certain metals and minerals.[288] This impacts the environment and can lead to environmental conflict.[289]\n The International Energy Agency does not recognise shortages of resources but states that supply could struggle to keep pace with the world's climate ambitions. Electric vehicles (EV) and battery storage are expected to cause the most demand. Wind farms and solar PV are less consuming. The extension of electrical grids requires large amounts of copper and aluminium. The IEA recommends to scale up recycling. By 2040, quantities of copper, lithium, cobalt, and nickel from spent batteries could reduce combined primary supply requirements for these minerals by around 10%.[288]\n The demand for lithium by 2040 is expected to grow by the factor of 42. Graphite and nickel exploration is predicted to grow about 20-fold. For each of the most relevant minerals and metals, a significant share of resources are concentrated in only one country: copper in Chile, nickel in Indonesia, rare earths in China, cobalt in the Democratic Republic of the Congo (DRC), and lithium in Australia. China dominates processing of them all.[288]\n A controversial approach is deep sea mining. Minerals can be collected from new sources like polymetallic nodules lying on the seabed,[290] but this could damage biodiversity.[291]\n Moving to modern renewable energy has very large health benefits due to reducing air pollution from fossil fuels.[292][293][294][295]\n Renewable sources other than biomass such as wind power, photovoltaics, and hydroelectricity have the advantage of being able to conserve water, lower pollution[296] and reduce CO2 emissions.\n Solar panels change the albedo of the surface, so if used on a very large scale (such as covering 20% of the Sahara Desert), could change global weather patterns.[297]\n Installations used to produce wind, solar and hydropower are an increasing threat to key conservation areas, with facilities built in areas set aside for nature conservation and other environmentally sensitive areas. They are often much larger than fossil fuel power plants, needing areas of land up to 10 times greater than coal or gas to produce equivalent energy amounts.[298] More than 2000 renewable energy facilities are built, and more are under construction, in areas of environmental importance and threaten the habitats of plant and animal species across the globe. The authors' team emphasized that their work should not be interpreted as anti-renewables because renewable energy is crucial for reducing carbon emissions. The key is ensuring that renewable energy facilities are built in places where they do not damage biodiversity.[299]\n The transition to renewable energy depends on non-renewable resources, such as mined metals.[248] Manufacturing of photovoltaic panels, wind turbines and batteries requires significant amounts of rare-earth elements[300] which has significant social and environmental impact if mined in forests and protected areas.[301] Due to co-occurrence of rare-earth and radioactive elements (thorium, uranium and radium), rare-earth mining results in production of low-level radioactive waste.[302]\n In 2020 scientists published a world map of areas that contain renewable energy materials as well as estimations of their overlaps with \"Key Biodiversity Areas\", \"Remaining Wilderness\" and \"Protected Areas\". The authors assessed that careful strategic planning is needed.[303][304][305] Solar panels are recycled to reduce electronic waste and create a source for materials that would otherwise need to be mined,[306] but such business is still small and work is ongoing to improve and scale-up the process.[307][308][309]\n Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. The oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from more than a million years ago. The use of biomass for fire did not become commonplace until many hundreds of thousands of years later.[310] Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships in the Persian Gulf and on the Nile.[311] From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times.[312] Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass.\n In 1885, Werner Siemens, commenting on the discovery of the photovoltaic effect in the solid state, wrote:\n In conclusion, I would say that however great the scientific importance of this discovery may be, its practical value will be no less obvious when we reflect that the supply of solar energy is both without limit and without cost, and that it will continue to pour down upon us for countless ages after all the coal deposits of the earth have been exhausted and forgotten.[313] Max Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus (The Protestant Ethic and the Spirit of Capitalism), published in 1905.[314] Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 Scientific American article: \"in the far distant future, natural fuels having been exhausted [solar power] will remain as the only means of existence of the human race\".[315]\n The theory of peak oil was published in 1956.[316] In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity-generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.[317]\n New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.[41]\n"}
{"key":"Renewable Energy","link":"https:\/\/en.wikipedia.org\/wiki\/Renewable_energy","headline":"Renewable energy - Wikipedia","content":"\n Renewable energy, green energy, or low-carbon energy is energy from renewable resources that are naturally replenished on a human timescale. Renewable resources include sunlight, wind, the movement of water, and geothermal heat.[1][2][3][4] Although most renewable energy sources are sustainable, some are not. For example, some biomass sources are considered unsustainable at current rates of exploitation.[5][6] Renewable energy is often used for electricity generation, heating and cooling. Renewable energy projects are typically large-scale, but they are also suited to rural and remote areas and developing countries, where energy is often crucial in human development.[7][8]\n Renewable energy is often deployed together with further electrification, which has several benefits: electricity can move heat or objects efficiently, and is clean at the point of consumption.[9][10] From 2011 to 2021, renewable energy grew from 20% to 28% of global electricity supply. Use of fossil energy shrank from 68% to 62%, and nuclear from 12% to 10%. The share of hydropower decreased from 16% to 15% while power from sun and wind increased from 2% to 10%. Biomass and geothermal energy grew from 2% to 3%. There are 3,146 gigawatts installed in 135 countries, while 156 countries have laws regulating the renewable energy sector.[11][12] In 2021, China accounted for almost half of the global increase in renewable electricity.[13]\n Globally there are over 10 million jobs associated with the renewable energy industries, with solar photovoltaics being the largest renewable employer.[14] Renewable energy systems are rapidly becoming more efficient and cheaper and their share of total energy consumption is increasing,[15] with a large majority of worldwide newly installed electricity capacity being renewable.[16] In most countries, photovoltaic solar or onshore wind are the cheapest new-build electricity.[17]\n Many nations around the world already have renewable energy contributing more than 20% of their total energy supply, with some generating over half their electricity from renewables.[18] A few countries generate all their electricity using renewable energy.[19] National renewable energy markets are projected to continue to grow strongly in the 2020s and beyond.[20] According to the IEA, to achieve net zero emissions by 2050, 90% of global electricity generation will need to be produced from renewable sources.[21] Some studies say that a global transition to 100% renewable energy across all sectors – power, heat, transport and industry – is feasible and economically viable.[22][23][24]\n Renewable energy resources exist over wide geographical areas, in contrast to fossil fuels, which are concentrated in a limited number of countries. Deployment of renewable energy and energy efficiency technologies is resulting in significant energy security, climate change mitigation, and economic benefits.[25] However renewables are being hindered by hundreds of billions of dollars of fossil fuel subsidies.[26] In international public opinion surveys there is strong support for renewables such as solar power and wind power.[27][28] In 2022 the International Energy Agency asked countries to solve policy, regulatory, permitting and financing obstacles to adding more renewables, to have a better chance of reaching net zero carbon emissions by 2050.[29]\n Renewable energy flows involve natural phenomena such as sunlight, wind, tides, plant growth, and geothermal heat, as the International Energy Agency explains:[32]\n Renewable energy is derived from natural processes that are replenished constantly. In its various forms, it derives directly from the sun, or from heat generated deep within the earth. Included in the definition is electricity and heat generated from solar, wind, ocean, hydropower, biomass, geothermal resources, and biofuels and hydrogen derived from renewable resources. Renewable energy stands in contrast to fossil fuels, which are being used far more quickly than they are being replenished. According to several studies, renewable energy is more evenly distributed than fossil fuels globally and a wider deployment of renewables should lead to more decentralized global energy system with fewer geopolitical tensions among states.[34][35] Renewable energy resources and significant opportunities for energy efficiency exist over wide geographical areas, in contrast to other energy sources, which are concentrated in a limited number of countries. Rapid deployment of renewable energy and energy efficiency, and technological diversification of energy sources, would result in significant energy security and economic benefits.[25] Solar and wind power have become much cheaper.[36] In some cases it will be cheaper to transition to these sources as opposed to continuing to use the current, inefficient, fossil fuels. It would also reduce environmental pollution such as air pollution caused by the burning of fossil fuels, and improve public health, reduce premature mortalities due to pollution and save associated health costs that could amount to trillions of dollars annually.[37][38] Multiple analyses of decarbonization strategies have found that quantified health benefits can significantly offset the costs of implementing these strategies.[39][40]\n Climate change concerns, coupled with the continuing fall in the costs of some renewable energy equipment, such as wind turbines and solar panels, are driving increased use of renewables.[27] New government spending, regulation and policies helped the industry weather the global financial crisis better than many other sectors.[41] As of 2019[update], however, according to the International Renewable Energy Agency, renewables overall share in the energy mix (including power, heat and transport) needs to grow six times faster, in order to keep the rise in average global temperatures \"well below\" 2.0 °C (3.6 °F) during the present century, compared to pre-industrial levels.[42]\n A household's solar panels, and batteries if they have them, can often either be used for just that household or if connected to an electrical grid can be aggregated with millions of others.[43]  According to the research, a nation must reach a certain point in its growth before it can take use of more renewable energy. In our words, its addition changed how crucial input factors (labor and capital) connect to one another, lowering their overall elasticity and increasing the apparent economies of scale.[44][clarification needed] The United Nations' eighth Secretary-General Ban Ki-moon said that renewable energy has the ability to lift the poorest nations to new levels of prosperity.[45] Renewables supply more than 20% of energy in at least 30 nations.[46] Although many countries have various policy targets for longer-term shares of renewable energy these tend to be only for the power sector,[47] including a 40% target of all electricity generated for the European Union by 2030.[48]\n Renewable energy often displaces conventional fuels in four areas: electricity generation, hot water\/space heating, transportation, and rural (off-grid) energy services.[49]\n More than a quarter of electricity is generated from renewables as of 2021.[50] One of the efforts to decarbonize transportation is the increased use of electric vehicles (EVs).[51] Despite that and the use of biofuels, such as biojet, less than 4% of transport energy is from renewables.[52] Occasionally hydrogen fuel cells are used for heavy transport.[53] Meanwhile, in the future electrofuels may also play a greater role in decarbonizing hard-to-abate sectors like aviation and maritime shipping.[54]\n Solar water heating makes an important contribution to renewable heat in many countries, most notably in China, which now has 70% of the global total (180 GWth). Most of these systems are installed on multi-family apartment buildings[55] and meet a portion of the hot water needs of an estimated 50–60 million households in China. Worldwide, total installed solar water heating systems meet a portion of the water heating needs of over 70 million households.\n Heat pumps provide both heating and cooling, and also flatten the electric demand curve and are thus an increasing priority.[56] Renewable thermal energy is also growing rapidly.[57] About 10% of heating and cooling energy is from renewables.[50]\n Solar energy, radiant light and heat from the sun, is harnessed using a range of ever-evolving technologies such as solar heating, photovoltaics, concentrated solar power (CSP), concentrator photovoltaics, solar architecture and artificial photosynthesis.[63][64][obsolete source]Most new renewable energy is solar.[65] Solar technologies are broadly characterized as either passive solar or active solar depending on the way they capture, convert, and distribute solar energy. Passive solar techniques include orienting a building to the Sun, selecting materials with favorable thermal mass or light dispersing properties, and designing spaces that naturally circulate air. Active solar technologies encompass solar thermal energy, using solar collectors for heating, and solar power, converting sunlight into electricity either directly using photovoltaics (PV), or indirectly using concentrated solar power (CSP).[66]\n A photovoltaic system converts light into electrical direct current (DC) by taking advantage of the photoelectric effect.[67] Solar PV has turned into a multi-billion, fast-growing industry, continues to improve its cost-effectiveness, and has the most potential of any renewable technologies together with CSP.[68][69] Concentrated solar power (CSP) systems use lenses or mirrors and tracking systems to focus a large area of sunlight into a small beam. Commercial concentrated solar power plants were first developed in the 1980s. CSP-Stirling has by far the highest efficiency among all solar energy technologies.\n In 2011, the International Energy Agency declared that \"the development of affordable, inexhaustible and clean solar energy technologies will have huge longer-term benefits. It will increase countries' energy security through reliance on an indigenous, inexhaustible and mostly import-independent resource, enhance sustainability, reduce pollution, lower the costs of mitigating climate change, and keep fossil fuel prices lower than otherwise. These advantages are global. Hence the additional costs of the incentives for early deployment should be considered learning investments; they must be wisely spent and need to be widely shared\".[63] Solar power accounts for 505 GW annually, which is about 2% of the world's electricity. Solar energy can be harnessed anywhere that receives sunlight; however, the amount of solar energy that can be harnessed for electricity generation is influenced by weather conditions, geographic location and time of day.[70]\n According to chapter 6 of the IPCC 2022 climate mitigation report, the global potential of direct solar energy far exceeds that of any other renewable energy resource. It is well beyond the total amount of energy needed in order to support mitigation over the current century.[51] Australia has the largest proportion of solar electricity in the world, supplying 9.9% of the country's electrical demand in 2020.[71] More than 30 per cent of Australian households now have rooftop solar PV, with a combined capacity exceeding 11 GW.[72]\n There are, however, environmental implications of scaling up solar energy. In particular, the demand for raw materials such as aluminum poses concerns over the carbon footprint that will result from harvesting raw materials needed to implement solar energy.[73]\n Photovoltaics (PV) is rapidly-growing with global capacity increasing from 230 GW at the end of 2015 to 890 GW in 2021.[74] PV uses solar cells assembled into solar panels to convert sunlight into electricity. PV systems range from small, residential and commercial rooftop or building integrated installations, to large utility-scale photovoltaic power station. The predominant PV technology is crystalline silicon, while thin-film solar cell technology accounts for about 10 percent of global photovoltaic deployment. In recent years, PV technology has improved its electricity generating efficiency, reduced the installation cost per watt as well as its energy payback time, and reached grid parity.[77]\n Building-integrated photovoltaics or \"onsite\" PV systems use existing land and structures and generate power close to where it is consumed.[78]\n Photovoltaics grew fastest in China between 2016 and 2021 adding 560 GW, more than all advanced economies combined. Solar PV's installed power capacity is poised to surpass that of coal by 2027, becoming the largest in the world.[79] This requires an increase of installed PV capacity to 4,600 GW, of which more than half is expected to be deployed in China and India.[80][81]\n Commercial concentrated solar power plants were first developed in the 1980s. As the cost of solar electricity has fallen, the number of grid-connected solar PV systems has grown into the millions and gigawatt-scale solar power stations are being built. Many solar photovoltaic power stations have been built, mainly in Europe, China and the United States.[82] The 1.5 GW Tengger Desert Solar Park, in China is the world's largest PV power station. Many of these plants are integrated with agriculture and some use tracking systems that follow the sun's daily path across the sky to generate more electricity than fixed-mounted systems.\n Solar thermal energy (STE) is a form of energy and a technology for harnessing solar energy to generate thermal energy for use in industry, and in the residential and commercial sectors.\n Solar thermal collectors are classified by the United States Energy Information Administration as low-, medium-, or high-temperature collectors. Low-temperature collectors are generally unglazed and used to heat swimming pools or to heat ventilation air. Medium-temperature collectors are also usually flat plates but are used for heating water or air for residential and commercial use.\n High-temperature collectors concentrate sunlight using mirrors or lenses and are generally used for fulfilling heat requirements up to 300 deg C \/ 20 bar pressure in industries, and for electric power production. Two categories include Concentrated Solar Thermal (CST) for fulfilling heat requirements in industries, and Concentrated Solar Power (CSP) when the heat collected is used for electric power generation. CST and CSP are not replaceable in terms of application.\n Air flow can be used to run wind turbines. Modern utility-scale wind turbines range from around 600 kW to 9 MW of rated power. The power available from the wind is a function of the cube of the wind speed, so as wind speed increases, power output increases up to the maximum output for the particular turbine.[87] Areas where winds are stronger and more constant, such as offshore and high-altitude sites, are preferred locations for wind farms.\n Wind-generated electricity met nearly 4% of global electricity demand in 2015, with nearly 63 GW of new wind power capacity installed. Wind energy was the leading source of new capacity in Europe, the US and Canada, and the second largest in China. In Denmark, wind energy met more than 40% of its electricity demand while Ireland, Portugal and Spain each met nearly 20%.[88]\n Globally, the long-term technical potential of wind energy is believed to be five times total current global energy production, or 40 times current electricity demand, assuming all practical barriers needed were overcome. This would require wind turbines to be installed over large areas, particularly in areas of higher wind resources, such as offshore, and likely also industrial use of new types of VAWT turbines in addition to the horizontal axis units currently in use. As offshore wind speeds average ~90% greater than that of land, offshore resources can contribute substantially more energy than land-stationed turbines.[89]\n Since water is about 800 times denser than air, even a slow flowing stream of water, or moderate sea swell, can yield considerable amounts of energy. Water can generate electricity with a conversion efficiency of about 90%, which is the highest rate in renewable energy.[93] There are many forms of water energy:\n Hydropower is produced in 150 countries, with the Asia-Pacific region generating 32 percent of global hydropower in 2010.[needs update] Of the top 50 countries by percentage of electricity generated from renewables, 46 are primarily hydroelectric.[97] There are now seven hydroelectricity stations larger than 10 GW (10,000 MW) worldwide, see table below.\n Much hydropower is flexible, thus complementing wind and solar.[98] Wave power, which captures the energy of ocean surface waves, and tidal power, converting the energy of tides, are two forms of hydropower with future potential; however, they are not yet widely employed commercially.[99] A demonstration project operated by the Ocean Renewable Power Company on the coast of Maine, and connected to the grid, harnesses tidal power from the Bay of Fundy, location of the world's highest tidal flow. Ocean thermal energy conversion, which uses the temperature difference between cooler deep and warmer surface waters, currently has no economic feasibility.[100][101]\n In 2021, the world renewable hydropower capacity was 1,360 GW.[79] Only a third of the world's estimated hydroelectric potential of 14,000 TWh\/year has been developed.[102][103] New hydropower projects face opposition from local communities due to their large impact, including relocation of communities and flooding of wildlife habitats and farming land.[104] High cost and lead times from permission process, including environmental and risk assessments, with lack of environmental and social acceptance are therefore the primary challenges for new developments.[105] It is popular to repower old dams thereby increasing their efficiency and capacity as well as quicker responsiveness on the grid.[106] Where circumstances permit existing dams such as the Russell Dam built in 1985 may be updated with \"pump back\" facilities for pumped-storage which is useful for peak loads or to support intermittent wind and solar power. Because dispatchable power is more valuable than VRE[107][108] countries with large hydroelectric developments such as Canada and Norway are spending billions to expand their grids to trade with neighboring countries having limited hydro.[109]\n Biomass is biological material derived from living, or recently living organisms. It commonly refers to plants or plant-derived materials. As an energy source, biomass can either be used directly via combustion to produce heat, or indirectly after converting it to various forms of biofuel in solid, liquid or gaseous form. Conversion of biomass to biofuel can be achieved by different methods which are broadly classified into: thermal, chemical, and biochemical methods. Wood was the largest biomass energy source as of 2012;[113] examples include forest residues – such as dead trees, branches and tree stumps, yard clippings, wood chips and even municipal solid waste. Industrial biomass can be grown from numerous types of plants, including miscanthus, switchgrass, hemp, corn, poplar, willow, sorghum, sugarcane, bamboo,[114] and a variety of tree species, ranging from eucalyptus to oil palm (palm oil).\n Plant energy is produced by crops specifically grown for use as fuel that offer high biomass output per hectare with low input energy.[115] The grain can be used for liquid transportation fuels while the straw can be burned to produce heat or electricity. Plant biomass can also be degraded from cellulose to glucose through a series of chemical treatments, and the resulting sugar can then be used as a first-generation biofuel.\n Biomass can be converted to other usable forms of energy such as methane gas[116] or transportation fuels such as ethanol and biodiesel. Rotting garbage, and agricultural and human waste, all release methane gas – also called landfill gas or biogas. Crops, such as corn and sugarcane, can be fermented to produce the transportation fuel, ethanol. Biodiesel, another transportation fuel, can be produced from left-over food products such as vegetable oils and animal fats.[117] There is a great deal of research involving algal fuel or algae-derived biomass due to the fact that it is a non-food resource, grows around 20 times faster than other types of food crops, such as corn and soy, and can be grown almost anywhere.[118][119] Once harvested, it can be fermented to produce biofuels such as ethanol, butanol, and methane, as well as biodiesel and hydrogen. The biomass used for electricity generation varies by region. Forest by-products, such as wood residues, are common in the United States. Agricultural waste is common in Mauritius (sugar cane residue) and Southeast Asia (rice husks).\n Biomass, biogas and biofuels are burned to produce heat\/power and in doing so can harm the environment. Pollutants such as sulphurous oxides (SOx), nitrous oxides (NOx), and particulate matter (PM) are produced from the combustion of biomass. With regards to traditional use of biomass for heating and cooking, the World Health Organization estimates that 3.7 million prematurely died from outdoor air pollution in 2012 while indoor pollution from biomass burning effects over 3 billion people worldwide.[120][121]\n Bioenergy global capacity in 2021 was 158 GW. Biofuels avoided 4.4% of global transport fuel demand in 2021.[79]\n Biofuels include a wide range of fuels which are derived from biomass. The term covers solid, liquid, and gaseous fuels.[122] Liquid biofuels include bioalcohols, such as bioethanol, and oils, such as biodiesel. Gaseous biofuels include biogas, landfill gas and synthetic gas. Bioethanol is an alcohol made by fermenting the sugar components of plant materials and it is made mostly from sugar and starch crops. These include maize, sugarcane and, more recently, sweet sorghum. The latter crop is particularly suitable for growing in dryland conditions, and is being investigated by International Crops Research Institute for the Semi-Arid Tropics for its potential to provide fuel, along with food and animal feed, in arid parts of Asia and Africa.[123]\n With advanced technology being developed, cellulosic biomass, such as trees and grasses, are also used as feedstocks for ethanol production. Ethanol can be used as a fuel for vehicles in its pure form, but it is usually used as a gasoline additive to increase octane and improve vehicle emissions. Bioethanol is widely used in the United States and in Brazil. The energy costs for producing bio-ethanol are almost equal to, the energy yields from bio-ethanol. However, according to the European Environment Agency, biofuels do not address global warming concerns.[124] Biodiesel is made from vegetable oils, animal fats or recycled greases. It can be used as a fuel for vehicles in its pure form, or more commonly as a diesel additive to reduce levels of particulates, carbon monoxide, and hydrocarbons from diesel-powered vehicles. Biodiesel is produced from oils or fats using transesterification and is the most common biofuel in Europe. Biofuels provided 2.7% of the world's transport fuel in 2010.[125][needs update]\n Policies in more than 80 countries support biofuels demand.[79]\n Since the 1970s, Brazil has had an ethanol fuel program which has allowed the country to become the world's second largest producer of ethanol (after the United States) and the world's largest exporter.[126] Brazil's ethanol fuel program uses modern equipment and cheap sugarcane as feedstock, and the residual cane-waste (bagasse) is used to produce heat and power.[127] There are no longer light vehicles in Brazil running on pure gasoline.[128]\n Biojet is expected to be important for short-term reduction of carbon dioxide emissions from long-haul flights.[129]\n High temperature geothermal energy is from thermal energy generated and stored in the Earth. Thermal energy is the energy that determines the temperature of matter. Earth's geothermal energy originates from the original formation of the planet and from radioactive decay of minerals (in currently uncertain[134] but possibly roughly equal[135] proportions). The geothermal gradient, which is the difference in temperature between the core of the planet and its surface, drives a continuous conduction of thermal energy in the form of heat from the core to the surface. The adjective geothermal originates from the Greek roots geo, meaning earth, and thermos, meaning heat.\n The heat that is used for geothermal energy can be from deep within the Earth, all the way down to Earth's core – 6,400 kilometres (4,000 mi) down. At the core, temperatures may reach over 5,000 °C (9,030 °F). Heat conducts from the core to the surrounding rock. Extremely high temperature and pressure cause some rock to melt, which is commonly known as magma. Magma convects upward since it is lighter than the solid rock. This magma then heats rock and water in the crust, sometimes up to 371 °C (700 °F).[136]\n Low temperature geothermal[56] refers to the use of the outer crust of the Earth as a thermal battery to facilitate renewable thermal energy for heating and cooling buildings, and other refrigeration and industrial uses. In this form of geothermal, a geothermal heat pump and ground-coupled heat exchanger are used together to move heat energy into the Earth (for cooling) and out of the Earth (for heating) on a varying seasonal basis. Low-temperature geothermal (generally referred to as \"GHP\"[clarification needed]) is an increasingly important renewable technology because it both reduces total annual energy loads associated with heating and cooling, and it also flattens the electric demand curve eliminating the extreme summer and winter peak electric supply requirements. Thus low temperature geothermal\/GHP is becoming an increasing national[clarification needed] priority with multiple tax credit support[137] and focus as part of the ongoing movement toward net zero energy.[138]\n Geothermal power is cost effective, reliable, sustainable, and environmentally friendly,[139] but has historically been limited to areas near tectonic plate boundaries. Recent technological advances have expanded the range and size of viable resources, especially for applications such as home heating, opening a potential for widespread exploitation. Geothermal wells release greenhouse gases trapped deep within the earth, but these emissions are usually much lower per energy unit than those of fossil fuels. As a result, geothermal power has the potential to help mitigate global warming if widely deployed in place of fossil fuels.\n In 2017, the United States led the world in geothermal electricity production with 12.9 GW of installed capacity.[74] The largest group of geothermal power plants in the world is located at The Geysers, a geothermal field in California.[140] The Philippines follows the US as the second highest producer of geothermal power in the world, with 1.9 GW of capacity online.[74]\n Global geothermal capacity in 2021 was 15 GW.[79]\n There are also other renewable energy technologies that are still under development, including cellulosic ethanol, hot-dry-rock geothermal power, and marine energy.[141] These technologies are not yet widely demonstrated or have limited commercialization. Many are on the horizon and may have potential comparable to other renewable energy technologies, but still depend on attracting sufficient attention and research, development and demonstration (RD&D) funding.[141]\n There are numerous organizations within the academic, federal,[clarification needed] and commercial sectors conducting large-scale advanced research in the field of renewable energy. This research spans several areas of focus across the renewable energy spectrum. Most of the research is targeted at improving efficiency and increasing overall energy yields.[142]\nMultiple government supported research organizations have focused on renewable energy in recent years. Two of the most prominent of these labs are Sandia National Laboratories and the National Renewable Energy Laboratory (NREL), both of which are funded by the United States Department of Energy and supported by various corporate partners.[143]\n Enhanced geothermal systems (EGS) are a new type of geothermal power technology that does not require natural convective hydrothermal resources. The vast majority of geothermal energy within drilling reach is in dry and non-porous rock.[144] EGS technologies \"enhance\" and\/or create geothermal resources in this \"hot dry rock (HDR)\" through hydraulic fracturing. EGS and HDR technologies, such as hydrothermal geothermal, are expected to be baseload resources that produce power 24 hours a day like a fossil plant. Distinct from hydrothermal, HDR and EGS may be feasible anywhere in the world, depending on the economic limits of drill depth. Good locations are over deep granite covered by a thick (3–5 km or 1.9–3.1 mi) layer of insulating sediments which slow heat loss.[145] There are HDR and EGS systems currently being developed and tested in France, Australia, Japan, Germany, the U.S., and Switzerland. The largest EGS project in the world is a 25 megawatt demonstration plant currently being developed in the Cooper Basin, Australia. The Cooper Basin has the potential to generate 5,000–10,000 MW.\n Marine energy (also sometimes referred to as ocean energy) is the energy carried by ocean waves, tides, salinity, and ocean temperature differences. The movement of water in the world's oceans creates a vast store of kinetic energy, or energy in motion. This energy can be harnessed to generate electricity to power homes, transport and industries. The term marine energy encompasses wave power – power from surface waves, marine current power - power from marine hydrokinetic streams (e.g., the Gulf Stream), and tidal power – obtained from the kinetic energy of large bodies of moving water. Reverse electrodialysis (RED) is a technology for generating electricity by mixing fresh river water and salty sea water in large power cells designed for this purpose; as of 2016, it is being tested at a small scale (50 kW). Offshore wind power is not a form of marine energy, as wind power is derived from the wind, even if the wind turbines are placed over water. The oceans have a tremendous amount of energy and are close to many if not most concentrated populations. Ocean energy has the potential of providing a substantial amount of new renewable energy around the world.[146][147][page needed]\n Passive daytime radiative cooling (PDRC) uses the coldness of outer space as a renewable energy source to achieve daytime cooling that can be used in many applications,[151][152][153] such as indoor space cooling,[154][155] outdoor urban heat island mitigation,[156][157] and solar cell efficiency.[158][159] PDRC surfaces are designed to be high in solar reflectance to minimize heat gain and strong in longwave infrared (LWIR) thermal radiation heat transfer.[160] On a planetary scale, it has been proposed as a way to slow and reverse global warming.[150][161] PDRC applications are deployed as sky-facing surfaces, similar to other renewable energy sources such as photovoltaic systems and solar thermal collectors.[159] PDRC became possible with the ability to suppress solar heating using photonic metamaterials, first published in a study by Raman et al. to the scientific community in 2014.[158][162] PDRC applications for indoor space cooling is growing with an estimated \"market size of ~$27 billion in 2025.\"[163]\n Earth emits roughly 1017 W of infrared thermal radiation that flows toward the cold outer space. Solar energy hits the surface and atmosphere of the earth and produces heat. Using various theorized devices like emissive energy harvester (EEH) or thermoradiative diode, this energy flow can be converted into electricity. In theory, this technology can be used during nighttime.[164][165]\n Producing liquid fuels from oil-rich (fat-rich) varieties of algae is an ongoing research topic. Various microalgae grown in open or closed systems are being tried including some systems that can be set up in brownfield and desert lands.[166]\n Collection of static electricity charges from water droplets on metal surfaces is an experimental technology that would be especially useful in low-income countries with relative air humidity over 60%.[167]\n Breeder reactors could, in principle, extract almost all of the energy contained in uranium or thorium, decreasing fuel requirements by a factor of 100 compared to widely used once-through light water reactors, which extract less than 1% of the energy in the actinide metal (uranium or thorium) mined from the earth.[168] The high fuel-efficiency of breeder reactors could greatly reduce concerns about fuel supply, energy used in mining, and storage of radioactive waste. With seawater uranium extraction (currently too expensive to be economical), there is enough fuel for breeder reactors to satisfy the world's energy needs for 5 billion years at 1983's total energy consumption rate, thus making nuclear energy effectively a renewable energy.[169][170] In addition to seawater the average crustal granite rocks contain significant quantities of uranium and thorium that with breeder reactors can supply abundant energy for the remaining lifespan of the sun on the main sequence of stellar evolution.[171]\n Artificial photosynthesis uses techniques including nanotechnology to store solar electromagnetic energy in chemical bonds by splitting water to produce hydrogen and then using carbon dioxide to make methanol.[172] Researchers in this field strived to design molecular mimics of photosynthesis that use a wider region of the solar spectrum, employ catalytic systems made from abundant, inexpensive materials that are robust, readily repaired, non-toxic, stable in a variety of environmental conditions and perform more efficiently allowing a greater proportion of photon energy to end up in the storage compounds, i.e., carbohydrates (rather than building and sustaining living cells).[173] However, prominent research faces hurdles, Sun Catalytix a MIT spin-off stopped scaling up their prototype fuel-cell in 2012 because it offers few savings over other ways to make hydrogen from sunlight.[174]\n Renewable energy production from some sources such as wind and solar is more variable and more geographically spread than technology based on fossil fuels and nuclear. While integrating it into the wider energy system is feasible, it does lead to some additional challenges such as increased production volatility and decreased system inertia.[175] Implementation of energy storage, using a wide variety of renewable energy technologies, and implementing a smart grid in which energy is automatically used at the moment it is produced can reduce risks and costs of renewable energy implementation.[175][176]: 15–16 \n Sector coupling of the power generation sector with other sectors may increase flexibility: for example the transport sector can be coupled by charging electric vehicles and sending electricity from vehicle to grid.[177] Similarly the industry sector can be coupled by hydrogen produced by electrolysis,[178] and the buildings sector by thermal energy storage for space heating and cooling.[179]\n Electrical energy storage is a collection of methods used to store electrical energy. Electrical energy is stored during times when production (especially from intermittent sources such as wind power, tidal power, solar power) exceeds consumption, and returned to the grid when production falls below consumption. Pumped-storage hydroelectricity accounts for more than 85% of all grid power storage.[180] Batteries are increasingly being deployed for storage[181] and grid ancillary services[182] and for domestic storage.[183] Green hydrogen is a more economical means of long-term renewable energy storage, in terms of capital expenditures compared to pumped hydroelectric or batteries.[184][185]\n Most new renewables are solar, followed by wind then hydro then bioenergy.[186] Investment in renewables, especially solar, tends to be more effective in creating jobs than coal, gas or oil.[187][188] Worldwide, renewables employ about 12 million people as of 2020, with solar PV being the technology employing the most at almost 4 million.[189] However, as of February 2024, the world's supply of workforce for solar energy is lagging greatly behind demand as universities worldwide still produce more workforce for fossil fuels than for renewable energy industries.[190]\n The International Renewable Energy Agency (IRENA) stated that ~86% (187 GW) of renewable capacity added in 2022 had lower costs than electricity generated from fossil fuels.[191] IRENA also stated that capacity added since 2000 reduced electricity bills in 2022 by at least $520 billion, and that in non-OECD countries, the lifetime savings of 2022 capacity additions will reduce costs by up to $580 billion.[191]\n * = 2018. All other values for 2019.\n The results of a recent review of the literature concluded that as greenhouse gas (GHG) emitters begin to be held liable for damages resulting from GHG emissions resulting in climate change, a high value for liability mitigation would provide powerful incentives for deployment of renewable energy technologies.[206]\n In the decade of 2010–2019, worldwide investment in renewable energy capacity excluding large hydropower amounted to US$2.7 trillion, of which the top countries China contributed US$818 billion, the United States contributed US$392.3 billion, Japan contributed US$210.9 billion, Germany contributed US$183.4 billion, and the United Kingdom contributed US$126.5 billion.[207] This was an increase of over three and possibly four times the equivalent amount invested in the decade of 2000–2009 (no data is available for 2000–2003).[207]\n As of 2022, an estimated 28% of the world's electricity was generated by renewables. This is up from 19% in 1990.[208]\n A December 2022 report by the IEA forecasts that over 2022-2027, renewables are seen growing by almost 2 400 GW in its main forecast, equal to the entire installed power capacity of China in 2021. This is an 85% acceleration from the previous five years, and almost 30% higher than what the IEA forecast in its 2021 report, making its largest ever upward revision. Renewables are set to account for over 90% of global electricity capacity expansion over the forecast period.[79] To achieve net zero emissions by 2050, IEA believes that 90% of global electricity generation will need to be produced from renewable sources.[21]\n In June 2022 IEA Executive Director Fatih Birol said that countries should invest more in renewables to \"ease the pressure on consumers from high fossil fuel prices, make our energy systems more secure, and get the world on track to reach our climate goals.”[210]\n China's five year plan to 2025 includes increasing direct heating by renewables such as geothermal and solar thermal.[211]\n REPowerEU, the EU plan to escape dependence on fossil Russian gas, is expected to call for much more green hydrogen.[212]\n After a transitional period,[213] renewable energy production is expected to make up most of the world's energy production. In 2018, the risk management firm, DNV GL, forecasts that the world's primary energy mix will be split equally between fossil and non-fossil sources by 2050.[214]\n In July 2014, WWF and the World Resources Institute convened a discussion among a number of major US companies who had declared their intention to increase their use of renewable energy. These discussions identified a number of \"principles\" which companies seeking greater access to renewable energy considered important market deliverables. These principles included choice (between suppliers and between products), cost competitiveness, longer term fixed price supplies, access to third-party financing vehicles, and collaboration.[215]\n UK statistics released in September 2020 noted that \"the proportion of demand met from renewables varies from a low of 3.4 per cent (for transport, mainly from biofuels) to highs of over 20 per cent for 'other final users', which is largely the service and commercial sectors that consume relatively large quantities of electricity, and industry\".[216]\n In some locations, individual households can opt to purchase renewable energy through a consumer green energy program.\n Renewable energy in developing countries is an increasingly used alternative to fossil fuel energy, as these countries scale up their energy supplies and address energy poverty. Renewable energy technology was once seen as unaffordable for developing countries.[217] However, since 2015, investment in non-hydro renewable energy has been higher in developing countries than in developed countries, and comprised 54% of global renewable energy investment in 2019.[218] The International Energy Agency forecasts that renewable energy will provide the majority of energy supply growth through 2030 in Africa and Central and South America, and 42% of supply growth in China.[219]\n In Kenya, the Olkaria V Geothermal Power Station is one of the largest in the world.[221] The Grand Ethiopia Renaissance Dam project incorporates wind turbines.[222] Once completed, Morocco's Ouarzazate Solar Power Station is projected to provide power to over a million people.[223]\n Policies to support renewable energy have been vital in their expansion. Where Europe dominated in establishing energy policy in the early 2000s, most countries around the world now have some form of energy policy.[225]\n The International Renewable Energy Agency (IRENA) is an intergovernmental organization for promoting the adoption of renewable energy worldwide. It aims to provide concrete policy advice and facilitate capacity building and technology transfer. IRENA was formed in 2009, with 75 countries signing the charter of IRENA.[226] As of April 2019, IRENA has 160 member states.[227] The then United Nations Secretary-General Ban Ki-moon has said that renewable energy can lift the poorest nations to new levels of prosperity,[45] and in September 2011 he launched the UN Sustainable Energy for All initiative to improve energy access, efficiency and the deployment of renewable energy.[228]\n The 2015 Paris Agreement on climate change motivated many countries to develop or improve renewable energy policies.[20] In 2017, a total of 121 countries adopted some form of renewable energy policy.[225] National targets that year existed in 176 countries.[20] In addition, there is also a wide range of policies at the state\/provincial, and local levels.[125] Some public utilities help plan or install residential energy upgrades.\n Many national, state and local governments have created green banks. A green bank is a quasi-public financial institution that uses public capital to leverage private investment in clean energy technologies.[229] Green banks use a variety of financial tools to bridge market gaps that hinder the deployment of clean energy.\n Climate neutrality by the year 2050 is the main goal of the European Green Deal.[230] For the European Union to reach their target of climate neutrality, one goal is to decarbonise its energy system by aiming to achieve \"net-zero greenhouse gas emissions by 2050.\"[231]\n 100% renewable energy is the goal of the use renewable resources for all energy. 100% renewable energy for electricity, heating, cooling and transport is motivated by climate change, pollution and other environmental issues, as well as economic and energy security concerns. Shifting the total global primary energy supply to renewable sources requires a transition of the energy system, since most of today's energy is derived from non-renewable fossil fuels.\n Research into this topic is fairly new, with very few studies published before 2009, but has gained increasing attention in recent years. The majority of studies show that a global transition to 100% renewable energy across all sectors – power, heat, transport and industry – is feasible and economically viable.[232][233][234][235][need quotation to verify] A cross-sectoral, holistic approach is seen as an important feature of 100% renewable energy systems and is based on the assumption \"that the best solutions can be found only if one focuses on the synergies between the sectors\" of the energy system such as electricity, heat, transport or industry.[236]\n The International Renewable Energy Agency's (IRENA) 2023 report on renewable energy finance highlights steady investment growth since 2018: USD 348 billion in 2020 (a 5.6% increase from 2019), USD 430 billion in 2021 (24% up from 2020), and USD 499 billion in 2022 (16% higher). This trend is driven by increasing recognition of renewable energy's role in mitigating climate change and enhancing energy security, along with investor interest in alternatives to fossil fuels. Policies such as feed-in tariffs in China and Vietnam have significantly increased renewable adoption. Furthermore, from 2013 to 2022, installation costs for solar photovoltaic (PV), onshore wind, and offshore wind fell by 69%, 33%, and 45%, respectively, making renewables more cost-effective.[239][240]\n From 2020 to 2022, solar technology investments almost doubled from USD 162 billion to USD 308 billion, driven by the sector's increasing maturity and cost reductions, particularly in solar photovoltaic (PV), which accounted for 90% of total investments. China and the United States were the main recipients, collectively making up about half of all solar investments since 2013. Despite reductions in Japan and India due to policy changes and COVID-19, growth in China, the United States, and a significant increase from Vietnam's feed-in tariff program offset these declines. Globally, the solar sector added 714 gigawatts (GW) of solar PV and concentrated solar power (CSP) capacity between 2013 and 2021, with a notable rise in large-scale solar heating installations in 2021, especially in China, Europe, Turkey, and Mexico.[240]\n Investments in wind technologies reached USD 161 billion in 2020, with onshore wind dominating at 80% of total investments from 2013 to 2022. Offshore wind investments nearly doubled to USD 41 billion between 2019 and 2020, primarily due to policy incentives in China and expansion in Europe. Global wind capacity increased by 557 GW between 2013 and 2021, with capacity additions increasing by an average of 19% each year.[240]\n Between 2013 and 2022, the renewable energy sector underwent a significant realignment of investment priorities. Investment in solar and wind energy technologies markedly increased. In contrast, other renewable technologies such as hydropower (including pumped storage hydropower), biomass, biofuels, geothermal, and marine energy experienced a substantial decrease in financial investment. Notably, from 2017 to 2022, investment in these alternative renewable technologies declined by 45%, falling from USD 35 billion to USD 17 billion.[240]\n Renewable electricity generation by wind and solar is variable. This results in reduced capacity factor and may require keeping some gas-fired power plants or other dispatchable generation on standby[243][244][245] until there is enough energy storage, demand response, grid improvement, and\/or base load power from non-intermittent sources like hydropower, nuclear power or bioenergy.\n The market for renewable energy technologies has continued to grow. Climate change concerns and increasing in green jobs, coupled with high oil prices, peak oil, oil wars, oil spills, promotion of electric vehicles and renewable electricity, nuclear disasters and increasing government support, are driving increasing renewable energy legislation, incentives and commercialization.[27][better source needed]\n The International Energy Agency has stated that deployment of renewable technologies usually increases the diversity of electricity sources and, through local generation, contributes to the flexibility of the system and its resistance to central shocks.[246]\n Solar power plants may compete with arable land,[248][249] while on-shore wind farms face opposition due to aesthetic concerns and noise, which is impacting both humans and wildlife.[250][251][252][need quotation to verify]In the United States, the Massachusetts Cape Wind project was delayed for years partly because of aesthetic concerns. However, residents in other areas have been more positive. According to a town councilor, the overwhelming majority of locals believe that the Ardrossan Wind Farm in Scotland has enhanced the area.[253] These concerns, when directed against renewable energy, are sometimes described as \"not in my back yard\" attitude (NIMBY).\n A 2011 UK Government document states that \"projects are generally more likely to succeed if they have broad public support and the consent of local communities. This means giving communities both a say and a stake\".[254] In countries such as Germany and Denmark many renewable projects are owned by communities, particularly through cooperative structures, and contribute significantly to overall levels of renewable energy deployment.[255][256]\n Whether nuclear power should be considered a form of renewable energy is an ongoing subject of debate. Statutory definitions of renewable energy usually exclude many present nuclear energy technologies, with the notable exception of the state of Utah.[257] Dictionary-sourced definitions of renewable energy technologies often omit or explicitly exclude mention of nuclear energy sources, with an exception made for the natural nuclear decay heat generated within the Earth.[258][259]\n The most common fuel used in conventional nuclear fission power stations, uranium-235 is \"non-renewable\" according to the Energy Information Administration, the organization however is silent on the recycled MOX fuel.[259] The National Renewable Energy Laboratory does not mention nuclear power in its \"energy basics\" definition.[260]\n From around 2010 onwards, the geopolitical impact of the growing use of renewable energy has been discussed.[263] Some argue that former fossil fuels exporters will experience a weakening of their position in international affairs, while countries with abundant renewable energy resources will be strengthened.[264] Also some countries rich in critical materials for renewable energy technologies are expected to rise in importance in international affairs.[265][266]\n The GeGaLo index of geopolitical gains and losses assesses how the geopolitical position of 156 countries may change if the world fully transitions to renewable energy resources. Former fossil fuels exporters are expected to lose power, while the positions of former fossil fuel importers and countries rich in renewable energy resources is expected to strengthen.[267] Sourcing of required materials, ownership of key infrastructure assets and the design of grids all require geopolitics consideration.[268][269][270]\n Transitions to renewable energy have many geopolitical implications such as the potential of revenue losses leading to political instability in insufficiently prepared fossil-fuel-exporting economies, albeit it is unclear whether the transition will increase or reduce conflict overall. In particular, a study hypothesizes that a \"configuration emerges in which fossil fuel importers are better off decarbonizing, competitive fossil fuel exporters are better off flooding markets and uncompetitive fossil fuel producers—rather than benefitting from 'free-riding'—suffer from their exposure to stranded assets and lack of investment in decarbonization technologies\".[271][272]\n A study found that transition from fossil fuels to renewable energy systems reduces risks from mining, trade and political dependence because renewable energy systems don't need fuel – they depend on trade only for the acquisition of materials and components during construction.[273]\n Nations rich in solar and wind energy could become major energy exporters.[274]\n Trade in hydrogen could fundamentally redraw the geography of the global energy trade, and international governance and investments that seek to scale up the hydrogen economy could reduce \"the risk of market fragmentation, carbon lock-in, and intensified geo-economic rivalry\".[275][274][276] Electricity will overtake other energy carriers by 2050, accounting for almost 50% of total energy consumption (up from 22% in 2015). Given the limitations of using solely electricity, clean hydrogen has significant potential in a number of industries.[277][278] Hydrogen has the potential to be long-term stored in the electricity and heating industries.[279]\n In 2019, oil and gas companies were listed by Forbes with sales of US$4.8 trillion, about 5% of the global GDP.[280] Net importers such as China and the EU would gain advantages from a transition to low-carbon technologies driven by technological development, energy efficiency or climate change policy, while Russia, the USA or Canada could see their fossil fuel industries nearly shut down.[281] On the other hand, countries with large areas such as Australia, Russia, China, the US, Canada and Brazil and also Africa and the Middle East have a potential for huge installations of renewable energy. The production of renewable energy technologies requires rare-earth elements with new supply chains.[282]\n In October 2021, European Commissioner for Climate Action Frans Timmermans suggested \"the best answer\" to the 2021 global energy crisis is \"to reduce our reliance on fossil fuels.\"[283] He said those blaming the European Green Deal were doing so \"for perhaps ideological reasons or sometimes economic reasons in protecting their vested interests.\"[283] Some critics blamed the European Union Emissions Trading System (EU ETS) and closure of nuclear plants for contributing to the energy crisis.[284][285][286] European Commission President Ursula von der Leyen said that Europe is \"too reliant\" on natural gas and too dependent on natural gas imports. According to Von der Leyen, \"The answer has to do with diversifying our suppliers ... and, crucially, with speeding up the transition to clean energy.\"[287]\n The renewable energy transition requires increased extraction of certain metals and minerals.[288] This impacts the environment and can lead to environmental conflict.[289]\n The International Energy Agency does not recognise shortages of resources but states that supply could struggle to keep pace with the world's climate ambitions. Electric vehicles (EV) and battery storage are expected to cause the most demand. Wind farms and solar PV are less consuming. The extension of electrical grids requires large amounts of copper and aluminium. The IEA recommends to scale up recycling. By 2040, quantities of copper, lithium, cobalt, and nickel from spent batteries could reduce combined primary supply requirements for these minerals by around 10%.[288]\n The demand for lithium by 2040 is expected to grow by the factor of 42. Graphite and nickel exploration is predicted to grow about 20-fold. For each of the most relevant minerals and metals, a significant share of resources are concentrated in only one country: copper in Chile, nickel in Indonesia, rare earths in China, cobalt in the Democratic Republic of the Congo (DRC), and lithium in Australia. China dominates processing of them all.[288]\n A controversial approach is deep sea mining. Minerals can be collected from new sources like polymetallic nodules lying on the seabed,[290] but this could damage biodiversity.[291]\n Moving to modern renewable energy has very large health benefits due to reducing air pollution from fossil fuels.[292][293][294][295]\n Renewable sources other than biomass such as wind power, photovoltaics, and hydroelectricity have the advantage of being able to conserve water, lower pollution[296] and reduce CO2 emissions.\n Solar panels change the albedo of the surface, so if used on a very large scale (such as covering 20% of the Sahara Desert), could change global weather patterns.[297]\n Installations used to produce wind, solar and hydropower are an increasing threat to key conservation areas, with facilities built in areas set aside for nature conservation and other environmentally sensitive areas. They are often much larger than fossil fuel power plants, needing areas of land up to 10 times greater than coal or gas to produce equivalent energy amounts.[298] More than 2000 renewable energy facilities are built, and more are under construction, in areas of environmental importance and threaten the habitats of plant and animal species across the globe. The authors' team emphasized that their work should not be interpreted as anti-renewables because renewable energy is crucial for reducing carbon emissions. The key is ensuring that renewable energy facilities are built in places where they do not damage biodiversity.[299]\n The transition to renewable energy depends on non-renewable resources, such as mined metals.[248] Manufacturing of photovoltaic panels, wind turbines and batteries requires significant amounts of rare-earth elements[300] which has significant social and environmental impact if mined in forests and protected areas.[301] Due to co-occurrence of rare-earth and radioactive elements (thorium, uranium and radium), rare-earth mining results in production of low-level radioactive waste.[302]\n In 2020 scientists published a world map of areas that contain renewable energy materials as well as estimations of their overlaps with \"Key Biodiversity Areas\", \"Remaining Wilderness\" and \"Protected Areas\". The authors assessed that careful strategic planning is needed.[303][304][305] Solar panels are recycled to reduce electronic waste and create a source for materials that would otherwise need to be mined,[306] but such business is still small and work is ongoing to improve and scale-up the process.[307][308][309]\n Prior to the development of coal in the mid 19th century, nearly all energy used was renewable. The oldest known use of renewable energy, in the form of traditional biomass to fuel fires, dates from more than a million years ago. The use of biomass for fire did not become commonplace until many hundreds of thousands of years later.[310] Probably the second oldest usage of renewable energy is harnessing the wind in order to drive ships over water. This practice can be traced back some 7000 years, to ships in the Persian Gulf and on the Nile.[311] From hot springs, geothermal energy has been used for bathing since Paleolithic times and for space heating since ancient Roman times.[312] Moving into the time of recorded history, the primary sources of traditional renewable energy were human labor, animal power, water power, wind, in grain crushing windmills, and firewood, a traditional biomass.\n In 1885, Werner Siemens, commenting on the discovery of the photovoltaic effect in the solid state, wrote:\n In conclusion, I would say that however great the scientific importance of this discovery may be, its practical value will be no less obvious when we reflect that the supply of solar energy is both without limit and without cost, and that it will continue to pour down upon us for countless ages after all the coal deposits of the earth have been exhausted and forgotten.[313] Max Weber mentioned the end of fossil fuel in the concluding paragraphs of his Die protestantische Ethik und der Geist des Kapitalismus (The Protestant Ethic and the Spirit of Capitalism), published in 1905.[314] Development of solar engines continued until the outbreak of World War I. The importance of solar energy was recognized in a 1911 Scientific American article: \"in the far distant future, natural fuels having been exhausted [solar power] will remain as the only means of existence of the human race\".[315]\n The theory of peak oil was published in 1956.[316] In the 1970s environmentalists promoted the development of renewable energy both as a replacement for the eventual depletion of oil, as well as for an escape from dependence on oil, and the first electricity-generating wind turbines appeared. Solar had long been used for heating and cooling, but solar panels were too costly to build solar farms until 1980.[317]\n New government spending, regulation and policies helped the industry weather the 2009 economic crisis better than many other sectors.[41]\n"}
{"key":"Renewable Energy","link":"https:\/\/en.wikipedia.org\/wiki\/Concentrated_solar_power","headline":"Concentrated solar power - Wikipedia","content":"\n Concentrated solar power (CSP, also known as concentrating solar power, concentrated solar thermal) systems generate solar power by using mirrors or lenses to concentrate a large area of sunlight into a receiver.[1] Electricity is generated when the concentrated light is converted to heat (solar thermal energy), which drives a heat engine (usually a steam turbine) connected to an electrical power generator[2][3][4] or powers a thermochemical reaction.[5][6][7]\n As of 2021, global installed capacity of concentrated solar power stood at 6.8 GW.[8] As of 2023, with the inclusion of three new CSP projects in construction in China[9] and in Dubai in the UAE,[9] the total is now 7.5 GW. The US National Renewable Energy Laboratory (NREL) maintains a full database of the current state of all CSP plants globally, whether under construction, shut down, or operating. The data includes comprehensive details such as capacity, type of power block components, number of thermal energy storage hours, and turbine sizes.[10]\n As a thermal energy generating power station, CSP has more in common with thermal power stations such as coal, gas, or geothermal. A CSP plant can incorporate thermal energy storage, which stores energy either in the form of sensible heat or as latent heat (for example, using molten salt), which enables these plants to continue supplying electricity whenever it is needed, day or night.[11] This makes CSP a dispatchable form of solar. Dispatchable renewable energy is particularly valuable in places where there is already a high penetration of photovoltaics (PV), such as California,[12] because demand for electric power peaks near sunset just as PV capacity ramps down (a phenomenon referred to as duck curve).[13]\n CSP is often compared to photovoltaic solar (PV) since they both use solar energy. While solar PV experienced huge growth during the 2010s, due to falling prices,[14][15] solar CSP growth has been slow due to technical difficulties and high prices. In 2017, CSP represented less than 2% of worldwide installed capacity of solar electricity plants.[16] \nHowever, CSP can more easily store energy during the night, making it more competitive with dispatchable generators and baseload plants.[17][18][19][20]\n The DEWA project in Dubai, under construction in 2019, held the world record for lowest CSP price in 2017 at US$73 per MWh[21] for its 700 MW combined trough and tower project: 600 MW of trough, 100 MW of tower with 15 hours of thermal energy storage daily.\nBase-load CSP tariff in the extremely dry Atacama region of Chile reached below $50\/MWh in 2017 auctions.[22][23]\n A legend has it that Archimedes used a \"burning glass\" to concentrate sunlight on the invading Roman fleet and repel them from Syracuse. In 1973 a Greek scientist, Dr. Ioannis Sakkas, curious about whether Archimedes could really have destroyed the Roman fleet in 212 BC, lined up nearly 60 Greek sailors, each holding an oblong mirror tipped to catch the sun's rays and direct them at a tar-covered plywood silhouette 49 m (160 ft) away. The ship caught fire after a few minutes; however, historians continue to doubt the Archimedes story.[24]\n In 1866, Auguste Mouchout used a parabolic trough to produce steam for the first solar steam engine. The first patent for a solar collector was obtained by the Italian Alessandro Battaglia in Genoa, Italy, in 1886. Over the following years, invеntors such as John Ericsson and Frank Shuman developed concentrating solar-powered dеvices for irrigation, refrigеration, and locomоtion. In 1913 Shuman finished a 55 horsepower (41 kW) parabolic solar thermal energy station in Maadi, Egypt for irrigation.[25][26][27][28] The first solar-power system using a mirror dish was built by Dr. R.H. Goddard, who was already well known for his research on liquid-fueled rockets and wrote an article in 1929 in which he asserted that all the previous obstacles had been addressed.[29]\n Professor Giovanni Francia (1911–1980) designed and built the first concentrated-solar plant, which entered into operation in Sant'Ilario, near Genoa, Italy in 1968. This plant had the architecture of today's power tower plants with a solar receiver in the center of a field of solar collectors. The plant was able to produce 1 MW with superheated steam at 100 bar and 500 °C.[30] The 10 MW Solar One power tower was developed in Southern California in 1981. Solar One was converted into Solar Two in 1995, implementing a new design with a molten salt mixture (60% sodium nitrate, 40% potassium nitrate) as the receiver working fluid and as a storage medium. The molten salt approach proved effective, and Solar Two operated successfully until it was decommissioned in 1999.[31] The parabolic-trough technology of the nearby Solar Energy Generating Systems (SEGS), begun in 1984, was more workable. The 354 MW SEGS was the largest solar power plant in the world, until 2014.\n No commercial concentrated solar was constructed from 1990 when SEGS was completed until 2006 when the Compact linear Fresnel reflector system at Liddell Power Station in Australia was built. Few other plants were built with this design although the 5 MW Kimberlina Solar Thermal Energy Plant opened in 2009.\n In 2007, 75 MW Nevada Solar One was built, a trough design and the first large plant since SEGS. Between 2010\n Due to the success of Solar Two, a commercial power plant, called Solar Tres Power Tower, was built in Spain in 2011, later renamed Gemasolar Thermosolar Plant. Gemasolar's results paved the way for further plants of its type. Ivanpah Solar Power Facility was constructed at the same time but without thermal storage, using natural gas to preheat water each morning.\n Most concentrated solar power plants use the parabolic trough design, instead of the power tower or Fresnel systems. There have also been variations of parabolic trough systems like the integrated solar combined cycle (ISCC) which combines troughs and conventional fossil fuel heat systems.\n CSP was originally treated as a competitor to photovoltaics, and Ivanpah was built without energy storage, although Solar Two had included several hours of thermal storage. By 2015, prices for photovoltaic plants had fallen and PV commercial power was selling for 1⁄3 of contemporary CSP contracts.[32][33] However, increasingly, CSP was being bid with 3 to 12 hours of thermal energy storage, making CSP a dispatchable form of solar energy.[34] As such, it is increasingly seen as competing with natural gas and PV with batteries for flexible, dispatchable power.\n CSP is used to produce electricity (sometimes called solar thermoelectricity, usually generated through steam). Concentrated-solar technology systems use mirrors or lenses with tracking systems to focus a large area of sunlight onto a small area. The concentrated light is then used as heat or as a heat source for a conventional power plant (solar thermoelectricity). The solar concentrators used in CSP systems can often also be used to provide industrial process heating or cooling, such as in solar air conditioning.\n Concentrating technologies exist in four optical types, namely parabolic trough, dish, concentrating linear Fresnel reflector, and solar power tower.[35] Parabolic trough and concentrating linear Fresnel reflectors are classified as linear focus collector types, while dish and solar tower are point focus types. Linear focus collectors achieve medium concentration factors (50 suns and over), and point focus collectors achieve high concentration factors (over 500 suns). Although simple, these solar concentrators are quite far from the theoretical maximum concentration.[36][37] For example, the parabolic-trough concentration gives about 1⁄3 of the theoretical maximum for the design acceptance angle, that is, for the same overall tolerances for the system. Approaching the theoretical maximum may be achieved by using more elaborate concentrators based on nonimaging optics.[36][37][38]\n Different types of concentrators produce different peak temperatures and correspondingly varying thermodynamic efficiencies, due to differences in the way that they track the sun and focus light. New innovations in CSP technology are leading systems to become more and more cost-effective.[39][40]\n In 2023, Australia’s national science agency CSIRO tested a CSP arrangement in which tiny ceramic particles fall through the beam of concentrated solar energy, the ceramic particles capable of storing a greater amount of heat than molten salt, while not requiring a container that would diminish heat transfer.[41]\n A parabolic trough consists of a linear parabolic reflector that concentrates light onto a receiver positioned along the reflector's focal line. The receiver is a tube positioned at the longitudinal focal line of the parabolic mirror and filled with a working fluid. The reflector follows the sun during the daylight hours by tracking along a single axis. A working fluid (e.g. molten salt[42]) is heated to 150–350 °C (302–662 °F) as it flows through the receiver and is then used as a heat source for a power generation system.[43] Trough systems are the most developed CSP technology. The Solar Energy Generating Systems (SEGS) plants in California, the world's first commercial parabolic trough plants, Acciona's Nevada Solar One near Boulder City, Nevada, and Andasol, Europe's first commercial parabolic trough plant are representative, along with Plataforma Solar de Almería's SSPS-DCS test facilities in Spain.[44]\n The design encapsulates the solar thermal system within a greenhouse-like glasshouse. The glasshouse creates a protected environment to withstand the elements that can negatively impact reliability and efficiency of the solar thermal system.[45] Lightweight curved solar-reflecting mirrors are suspended from the ceiling of the glasshouse by wires. A single-axis tracking system positions the mirrors to retrieve the optimal amount of sunlight. The mirrors concentrate the sunlight and focus it on a network of stationary steel pipes, also suspended from the glasshouse structure.[46] Water is carried throughout the length of the pipe, which is boiled to generate steam when intense solar radiation is applied. Sheltering the mirrors from the wind allows them to achieve higher temperature rates and prevents dust from building up on the mirrors.[45]\n GlassPoint Solar, the company that created the Enclosed Trough design, states its technology can produce heat for Enhanced Oil Recovery (EOR) for about $5 per 290 kWh (1,000,000 BTU) in sunny regions, compared to between $10 and $12 for other conventional solar thermal technologies.[47]\n A solar power tower consists of an array of dual-axis tracking reflectors (heliostats) that concentrate sunlight on a central receiver atop a tower; the receiver contains a heat-transfer fluid, which can consist of water-steam or molten salt. Optically a solar power tower is the same as a circular Fresnel reflector. The working fluid in the receiver is heated to 500–1000 °C (773–1,273 K or 932–1,832 °F) and then used as a heat source for a power generation or energy storage system.[43] An advantage of the solar tower is the reflectors can be adjusted instead of the whole tower. Power-tower development is less advanced than trough systems, but they offer higher efficiency and better energy storage capability. Beam down tower application is also feasible with heliostats to heat the working fluid.[48]\n The Solar Two in Daggett, California and the CESA-1 in Plataforma Solar de Almeria Almeria, Spain, are the most representative demonstration plants. The Planta Solar 10 (PS10) in Sanlucar la Mayor, Spain, is the first commercial utility-scale solar power tower in the world. The 377 MW Ivanpah Solar Power Facility, located in the Mojave Desert, was the largest CSP facility in the world, and uses three power towers.[49] Ivanpah generated only 0.652 TWh (63%) of its energy from solar means, and the other 0.388 TWh (37%) was generated by burning natural gas.[50][51][52]\n Supercritical carbon dioxide can be used instead of steam as heat-transfer fluid for increased electricity production efficiency. However, because of the high temperatures in arid areas where solar power is usually located, it is impossible to cool down carbon dioxide below its critical temperature in the  compressor inlet. Therefore, supercritical carbon dioxide blends with higher critical temperature are currently in development.\n Fresnel reflectors are made of many thin, flat mirror strips to concentrate sunlight onto tubes through which working fluid is pumped. Flat mirrors allow more reflective surface in the same amount of space than a parabolic reflector, thus capturing more of the available sunlight, and they are much cheaper than parabolic reflectors.[53] Fresnel reflectors can be used in various size CSPs.[54][55]\n Fresnel reflectors are sometimes regarded as a technology with a worse output than other methods. The cost efficiency of this model is what causes some to use this instead of others with higher output ratings. Some new models of Fresnel reflectors with Ray Tracing capabilities have begun to be tested and have initially proved to yield higher output than the standard version.[56]\n A dish Stirling or dish engine system consists of a stand-alone parabolic reflector that concentrates light onto a receiver positioned at the reflector's focal point. The reflector tracks the Sun along two axes. The working fluid in the receiver is heated to 250–700 °C (482–1,292 °F) and then used by a Stirling engine to generate power.[43] Parabolic-dish systems provide high solar-to-electric efficiency (between 31% and 32%), and their modular nature provides scalability. The Stirling Energy Systems (SES), United Sun Systems (USS) and Science Applications International Corporation (SAIC) dishes at UNLV, and Australian National University's Big Dish in Canberra, Australia are representative of this technology. A world record for solar to electric efficiency was set at 31.25% by SES dishes at the National Solar Thermal Test Facility (NSTTF) in New Mexico on 31 January 2008, a cold, bright day.[57] According to its developer, Ripasso Energy, a Swedish firm, in 2015 its Dish Sterling system being tested in the Kalahari Desert in South Africa showed 34% efficiency.[58] The SES installation in Maricopa, Phoenix was the largest Stirling Dish power installation in the world until it was sold to United Sun Systems. Subsequently, larger parts of the installation have been moved to China as part of the huge energy demand.\n Heat from the sun can be used to provide steam used to make heavy oil less viscous and easier to pump. Solar power tower and parabolic troughs can be used to provide the steam which is used directly so no generators are required and no electricity is produced. Solar thermal enhanced oil recovery can extend the life of oilfields with very thick oil which would not otherwise be economical to pump.[59]\n In a CSP plant that includes storage, the solar energy is first used to heat the molten salt or synthetic oil which is stored providing thermal\/heat energy at high temperature in insulated tanks.[60][61] Later the hot molten salt (or oil) is used in a steam generator to produce steam to generate electricity by steam turbo generator as per requirement.[62] Thus solar energy which is available in daylight only is used to generate electricity round the clock on demand as a load following power plant or solar peaker plant.[63][64] The thermal storage capacity is indicated in hours of power generation at nameplate capacity. Unlike solar PV or CSP without storage, the power generation from solar thermal storage plants is dispatchable and self-sustainable similar to coal\/gas-fired power plants, but without the pollution.[65] CSP with thermal energy storage plants can also be used as cogeneration plants to supply both electricity and process steam round the clock. As of December 2018, CSP with thermal energy storage plants generation cost have ranged between 5 c € \/ kWh and 7 c € \/ kWh depending on good to medium solar radiation received at a location.[66] Unlike solar PV plants, CSP with thermal energy storage plants can also be used economically round the clock to produce only process steam replacing pollution emitting fossil fuels. CSP plant can also be integrated with solar PV for better synergy.[67][68][69]\n CSP with thermal storage systems are also available using Brayton cycle with air instead of steam for generating electricity and\/or steam round the clock. These CSP plants are equipped with gas turbine to generate electricity.[70] These are also small in capacity (<0.4 MW) with flexibility to install in few acres area.[70] Waste heat from the power plant can also be used for process steam generation and HVAC needs.[71] In case land availability is not a limitation, any number of these modules can be installed up to 1000 MW with RAMS and cost advantage since the per MW cost of these units are cheaper than bigger size solar thermal stations.[72]\n Centralized district heating round the clock is also feasible with concentrated solar thermal storage plants.[73]\n Carbon neutral synthetic fuel production using concentrated solar thermal energy at nearly 1500 °C temperature is technically feasible and will be commercially viable in the near future as the costs of CSP plants decline.[74] Also carbon neutral hydrogen can be produced with solar thermal energy (CSP) using Sulfur–iodine cycle, Hybrid sulfur cycle, Iron oxide cycle, Copper–chlorine cycle, Zinc–zinc oxide cycle, Cerium(IV) oxide–cerium(III) oxide cycle, etc.\n An early plant operated in Sicily at Adrano.  The US deployment of CSP plants started by 1984 with the SEGS plants. The last SEGS plant was completed in 1990. From 1991 to 2005, no CSP plants were built anywhere in the world. Global installed CSP-capacity increased nearly tenfold between 2004 and 2013 and grew at an average of 50 percent per year during the last five of those years, as the number of countries with installed CSP were growing [78]: 51   In 2013, worldwide installed capacity increased by 36% or nearly 0.9 gigawatt (GW) to more than 3.4 GW. The record for capacity installed was reached in 2014, corresponding to 925 MW, however, was followed by a decline  caused by policy changes, the global financial crisis, and the rapid decrease in price of the photovoltaic cells. Nevertheless, total capacity reached 6800 MW in 2021.[8]\n Spain accounted for almost one third of the world's capacity, at 2,300 MW, despite no new capacity entering commercial operation in the country since 2013.[77]\nThe United States follows with 1,740 MW. Interest is also notable in North Africa and the Middle East, as well as China and India. There is a notable trend towards developing countries and regions with high solar radiation with several large plants under construction in 2017.\n The global market was initially dominated by parabolic-trough plants, which accounted for 90% of CSP plants at one point.[83]\n Since about 2010, central power tower CSP has been favored in new plants due to its higher temperature operation – up to 565 °C (1,049 °F) vs. trough's maximum of 400 °C (752 °F) – which promises greater efficiency.\n Among the larger CSP projects are the Ivanpah Solar Power Facility (392 MW) in the United States, which uses solar power tower technology without thermal energy storage, and the Ouarzazate Solar Power Station in Morocco,[84] which combines trough and tower technologies for a total of 510 MW with several hours of energy storage.\n The efficiency of a concentrating solar power system will depend on the technology used to convert the solar power to electrical energy, the operating temperature of the receiver and the heat rejection, thermal losses in the system, and the presence or absence of other system losses; in addition to the conversion efficiency, the optical system which concentrates the sunlight will also add additional losses.\n Real-world systems claim a maximum conversion efficiency of 23-35% for \"power tower\" type systems, operating at temperatures from 250 to 565 °C, with the higher efficiency number assuming a combined cycle turbine. Dish Stirling systems, operating at temperatures of 550-750 °C, claim an efficiency of about 30%.[85] Due to variation in sun incidence during the day, the average conversion efficiency achieved is not equal to these maximum efficiencies, and the net annual solar-to- electricity efficiencies are 7-20% for pilot power tower systems, and 12-25% for demonstration-scale Stirling dish systems.[85]\n The maximum conversion efficiency of any thermal to electrical energy system is given by the Carnot efficiency, which represents a theoretical limit to the efficiency that can be achieved by any system, set by the laws of thermodynamics. Real-world systems do not achieve the Carnot efficiency.\n The conversion efficiency \n\n\n\nη\n\n\n{\\displaystyle \\eta }\n\n of the incident solar radiation into mechanical work depends on the thermal radiation properties of the solar receiver and on the heat engine (e.g. steam turbine). \nSolar irradiation is first converted into heat by the solar receiver with the efficiency \n\n\n\n\nη\n\nR\ne\nc\ne\ni\nv\ne\nr\n\n\n\n\n{\\displaystyle \\eta _{Receiver}}\n\n and subsequently the heat is converted into mechanical energy by the heat engine with the efficiency \n\n\n\n\nη\n\nm\ne\nc\nh\na\nn\ni\nc\na\nl\n\n\n\n\n{\\displaystyle \\eta _{mechanical}}\n\n, using Carnot's principle.[86][87] The mechanical energy is then converted into electrical energy by a generator.\nFor a solar receiver with a mechanical converter (e.g., a turbine), the overall conversion efficiency can be defined as follows:\n where \n\n\n\n\nη\n\n\no\np\nt\ni\nc\ns\n\n\n\n\n\n{\\displaystyle \\eta _{\\mathrm {optics} }}\n\n represents the fraction of incident light concentrated onto the receiver, \n\n\n\n\nη\n\n\nr\ne\nc\ne\ni\nv\ne\nr\n\n\n\n\n\n{\\displaystyle \\eta _{\\mathrm {receiver} }}\n\n the fraction of light incident on the receiver that is converted into heat energy, \n\n\n\n\nη\n\n\nm\ne\nc\nh\na\nn\ni\nc\na\nl\n\n\n\n\n\n{\\displaystyle \\eta _{\\mathrm {mechanical} }}\n\n the efficiency of conversion of heat energy into mechanical energy, and \n\n\n\n\nη\n\n\ng\ne\nn\ne\nr\na\nt\no\nr\n\n\n\n\n\n{\\displaystyle \\eta _{\\mathrm {generator} }}\n\n the efficiency of converting the mechanical energy into electrical power.\n \n\n\n\n\nη\n\n\nr\ne\nc\ne\ni\nv\ne\nr\n\n\n\n\n\n{\\displaystyle \\eta _{\\mathrm {receiver} }}\n\n is:\n The conversion efficiency \n\n\n\n\nη\n\n\nm\ne\nc\nh\na\nn\ni\nc\na\nl\n\n\n\n\n\n{\\displaystyle \\eta _{\\mathrm {mechanical} }}\n\n is at most the Carnot efficiency, which is determined by the temperature of the receiver \n\n\n\n\nT\n\nH\n\n\n\n\n{\\displaystyle T_{H}}\n\n and the temperature of the heat rejection (\"heat sink temperature\") \n\n\n\n\nT\n\n0\n\n\n\n\n{\\displaystyle T^{0}}\n\n,\n The real-world efficiencies of typical engines achieve 50% to at most 70% of the Carnot efficiency due to losses such as heat loss and windage in the moving parts.\n For a solar flux \n\n\n\nI\n\n\n{\\displaystyle I}\n\n (e.g. \n\n\n\nI\n=\n1000\n\n\nW\n\n\/\n\n\nm\n\n2\n\n\n\n\n\n{\\displaystyle I=1000\\,\\mathrm {W\/m^{2}} }\n\n) concentrated \n\n\n\nC\n\n\n{\\displaystyle C}\n\n times with an efficiency \n\n\n\n\nη\n\nO\np\nt\ni\nc\ns\n\n\n\n\n{\\displaystyle \\eta _{Optics}}\n\n on the system solar receiver with a collecting area \n\n\n\nA\n\n\n{\\displaystyle A}\n\n and an absorptivity \n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n:\n For simplicity's sake, one can assume that the losses are only radiative ones (a fair assumption for high temperatures), thus for a reradiating area A and an emissivity \n\n\n\nϵ\n\n\n{\\displaystyle \\epsilon }\n\n applying the Stefan–Boltzmann law yields:\n Simplifying these equations by considering perfect optics (\n\n\n\n\nη\n\n\nO\np\nt\ni\nc\ns\n\n\n\n\n\n{\\displaystyle \\eta _{\\mathrm {Optics} }}\n\n = 1) and without considering the ultimate conversion step into electricity by a generator, collecting and reradiating areas equal and maximum absorptivity and emissivity (\n\n\n\nα\n\n\n{\\displaystyle \\alpha }\n\n = 1, \n\n\n\nϵ\n\n\n{\\displaystyle \\epsilon }\n\n = 1) then substituting in the first equation gives\n \n The graph shows that the overall efficiency does not increase steadily with the receiver's temperature. Although the heat engine's efficiency (Carnot) increases with higher temperature, the receiver's efficiency does not. On the contrary, the receiver's efficiency is decreasing, as the amount of energy it cannot absorb (Qlost) grows by the fourth power as a function of temperature. Hence, there is a maximum reachable temperature. When the receiver efficiency is null (blue curve on the figure below), Tmax is:\n\n\n\n\n\nT\n\n\nm\na\nx\n\n\n\n=\n\n\n(\n\n\n\nI\nC\n\nσ\n\n\n)\n\n\n0.25\n\n\n\n\n{\\displaystyle T_{\\mathrm {max} }=\\left({\\frac {IC}{\\sigma }}\\right)^{0.25}}\n\n\n There is a temperature Topt for which the efficiency is maximum, i.e.. when the efficiency derivative relative to the receiver temperature is null:\n Consequently, this leads us to the following equation:\n Solving this equation numerically allows us to obtain the optimum process temperature according to the solar concentration ratio \n\n\n\nC\n\n\n{\\displaystyle C}\n\n (red curve on the figure below)\n \n Theoretical efficiencies aside, real-world experience of CSP reveals a 25%–60% shortfall in projected production, a good part of which is due to the practical Carnot cycle losses not included in the above analysis.\n Bulk power from CSP today is much more expensive than solar PV or Wind power, however when including energy storage CSP can be a cheaper alternative. As early as 2011, the rapid decline of the price of photovoltaic systems lead to projections that CSP will no longer be economically viable.[88] As of 2020, the least expensive utility-scale concentrated solar power stations in the United States and worldwide are five times more expensive than utility-scale photovoltaic power stations, with a projected minimum price of 7 cents per kilowatt-hour for the most advanced CSP stations against record lows of 1.32 cents per kWh[89] for utility-scale PV.[90] This five-fold price difference has been maintained since 2018.[91]\n Even though overall deployment of CSP remains limited in the early 2020s, the levelized cost of power from commercial scale plants has decreased significantly since the 2010s. With a learning rate estimated at around 20% cost reduction of every doubling in capacity [92] the cost were approaching the upper end of the fossil fuel cost range at the beginning of the 2020s driven by support schemes in several countries, including Spain, the US, Morocco, South Africa, China, and the UAE:\n CSP deployment has slowed down considerably as most of the above-mentioned markets have cancelled their support,[93] as the technology turned out to be more expensive on a per kWH basis than solar PV and wind power. CSP in combination with Thermal Energy Storage (TES) is expected by some to remain cheaper than PV with lithium batteries for storage durations above 4 hours per day,[94] while NREL expects that by 2030 PV with 10-hour storage lithium batteries will cost the same as PV with 4-hour storage used to cost in 2020.[95]\n Combining the affordability of PV and the dispatchability of CSP is a promising avenue for high capacity factor solar power at low cost. Few PV-CSP plants in China are hoping to operate profitably on the regional coal tariff of US$50 per MWh in the year 2021.[96]\n In 2008 Spain launched the first commercial scale CSP market in Europe. Until 2012, solar-thermal electricity generation was initially eligible for feed-in tariff payments (art. 2 RD 661\/2007) - leading to the creation of the largest CSP fleet in the world which at 2.3 GW of installed capacity contributes about 5TWh of power to the Spanish grid every year.[97]\nThe initial requirements for plants in the FiT were:\n The capacity limits for the different system types were re-defined during the review of the application conditions every quarter (art. 5 RD 1578\/2008, Annex III RD 1578\/2008). Prior to the end of an application period, the market caps specified for each system type are published on the website of the Ministry of Industry, Tourism and Trade (art. 5 RD 1578\/2008).[98] Because of cost concerns Spain has halted acceptance of new projects for the feed-in-tariff on 27 January 2012 [99][100] Already accepted projects were affected by a 6% \"solar-tax\" on feed-in-tariffs, effectively reducing the feed-in-tariff.[101]\n In this context, the Spanish Government enacted the Royal Decree-Law 9\/2013 [102] in 2013, aimed at the adoption of urgent measures to guarantee the economic and financial stability of the electric system, laying the foundations of the new Law 24\/2013 of the Spanish electricity sector.[103] This new retroactive legal-economic framework applied to all the renewable energy systems was developed in 2014 by the RD 413\/2014,[104] which abolished the former regulatory frameworks set by the RD 661\/2007 and the RD 1578\/2008 and defined a new remuneration scheme for these assets.\n After a lost decade for CSP in Europe, Spain announced in its National Energy and Climate Plan the intention of adding 5GW of CSP capacity between 2021 and 2030.[105] Towards this end bi-annual auctions of 200 MW of CSP capacity starting in October 2022 are expected, but details are not yet known.[106]\n Several CSP dishes have been set up in remote Aboriginal settlements in the Northern Territory: Hermannsburg, Yuendumu and Lajamanu.\n So far no commercial scale CSP project has been commissioned in Australia, but several projects were suggested. In 2017 now bankrupt American CSP developer SolarReserve got awarded a PPA to realize the 150MW Aurora Solar Thermal Power Project in South Australia at a record low rate of just AUD$0.08\/kWh or close to USD$0.06\/kWh.[107] Unfortunately the company failed to secure financing and the project got cancelled. Another promising application for CSP in Australia are mines that need 24\/7 electricity but often have no grid connection. Vast Solar a startup company aiming to commercialize a novel modular third generation CSP design [108][109] is looking to start construction of a 50MW combines CSP and PV facility in Mt. Isa of North-West Queensland in 2021.[110]\n At the federal level, under the Large-scale Renewable Energy Target (LRET), in operation under the Renewable Energy Electricity Act 2000, large-scale solar thermal electricity generation from accredited RET power stations may be entitled to create large-scale generation certificates (LGCs). These certificates can then be sold and transferred to liable entities (usually electricity retailers) to meet their obligations under this tradeable certificates scheme. However, as this legislation is technology neutral in its operation, it tends to favour more established RE technologies with a lower levelised cost of generation, such as large-scale onshore wind, rather than solar thermal and CSP.[111]\nAt State level, renewable energy feed-in laws typically are capped by maximum generation capacity in kWp, and are open only to micro or medium scale generation and in a number of instances are only open to solar PV (photovoltaic) generation. This means that larger scale CSP projects would not be eligible for payment for feed-in incentives in many of the State and Territory jurisdictions.\n In 2024, China is offering second generation CSP technology to compete with other on-demand electricity generation methods based on renewable or non-renewable fossil fuels without any  direct or indirect subsidies.[11] In the current 14th Five-Year Plan CSP projects are developed in several provinces alongside large GW sized solar PV and wind projects.[96][8]\n In 2016 China announced its intention to build a batch of 20 technologically diverse CSP demonstration projects in the context of the 13th Five-Year Plan, with the intention of building up an internationally competitive CSP industry.[112] Since the first plants were completed in 2018,  the generated electricity from the plants with thermal storage is supported with an administratively set FiT of RMB 1.5 per kWh.[113] At the end of 2020, China operated a total of 545 MW in 12 CSP plants,[114][115] seven plants (320 MW) are molten-salt towers; another two plants (150MW) use the proven Eurotrough 150 parabolic trough design,[116] three plants (75 MW) use linear Fresnel collectors. Plans to build a second batch of demonstration projects were never enacted and further technology specific support for CSP in the upcoming 14th Five-Year Plan is unknown. Federal support projects from the demonstration batch ran out at the end of 2021.[117]\n In March 2024, SECI announced that a RfQ for 500 MW tender would be called in the year 2024.[118]\n A study done by Greenpeace International, the European Solar Thermal Electricity Association, and the International Energy Agency's SolarPACES group investigated the potential and future of concentrated solar power. The study found that concentrated solar power could account for up to 25% of the world's energy needs by 2050. The increase in investment would be from €2 billion worldwide to €92.5 billion in that time period.[119]\nSpain is the leader in concentrated solar power technology, with more than 50 government-approved projects in the works. Also, it exports its technology, further increasing the technology's stake in energy worldwide. Because the technology works best with areas of high insolation (solar radiation), experts predict the biggest growth in places like Africa, Mexico, and the southwest United States. It indicates that the thermal storage systems based in nitrates (calcium, potassium, sodium,...) will make the CSP plants more and more profitable. The study examined three different outcomes for this technology: no increases in CSP technology, investment continuing as it has been in Spain and the US, and finally the true potential of CSP without any barriers on its growth. The findings of the third part are shown in the table below:\n Finally, the study acknowledged how technology for CSP was improving and how this would result in a drastic price decrease by 2050. It predicted a drop from the current range of €0.23–0.15\/kWh to €0.14–0.10\/kWh.[119]\n The European Union looked into developing a €400 billion (US$774 billion) network of solar power plants based in the Sahara region using CSP technology to be known as Desertec, to create \"a new carbon-free network linking Europe, the Middle East and North Africa\". The plan was backed mainly by German industrialists and predicted production of 15% of Europe's power by 2050. Morocco was a major partner in Desertec and as it has barely 1% of the electricity consumption of the EU, it could produce more than enough energy for the entire country with a large energy surplus to deliver to Europe.[120] Algeria has the biggest area of desert, and private Algerian firm Cevital signed up for Desertec.[120] With its wide desert (the highest CSP potential in the Mediterranean and Middle East regions ~ about 170 TWh\/year) and its strategic geographical location near Europe, Algeria is one of the key countries to ensure the success of Desertec project. Moreover, with the abundant natural-gas reserve in the Algerian desert, this will strengthen the technical potential of Algeria in acquiring Solar-Gas Hybrid Power Plants for 24-hour electricity generation. Most of the participants pulled out of the effort at the end of 2014.\n Experience with first-of-a-kind CSP plants in the USA was mixed. Solana in Arizona, and Ivanpah in California indicate large production shortfalls in electricity generation between 25% and 40% in the first years of operation. Producers blame clouds and stormy weather, but critics seem to think there are technological issues. These problems are causing utilities to pay inflated prices for wholesale electricity, and threaten the long-term viability of the technology. As photovoltaic costs continue to plummet, many think CSP has a limited future in utility-scale electricity production.[121] In other countries especially Spain and South Africa CSP plants have met their designed parameters [122]\n CSP has other uses than electricity. Researchers are investigating solar thermal reactors for the production of solar fuels, making solar a fully transportable form of energy in the future. These researchers use the solar heat of CSP as a catalyst for thermochemistry to break apart molecules of H2O, to create hydrogen (H2) from solar energy with no carbon emissions.[123] By splitting both H2O and CO2, other much-used hydrocarbons – for example, the jet fuel used to fly commercial airplanes – could also be created with solar energy rather than from fossil fuels.[124]\n Around the turn of the millennium up to about 2010, there have been several proposals for gigawatt size, very-large-scale solar power plants using CSP.[125] They include the Euro-Mediterranean Desertec proposal and Project Helios in Greece (10 GW), both now canceled. A 2003 study concluded that the world could generate 2,357,840 TWh each year from very large-scale solar power plants using 1% of each of the world's deserts. Total consumption worldwide was 15,223 TWh\/year[126] (in 2003). The gigawatt size projects would have been arrays of standard-sized single plants. In 2012, the BLM made available 97,921,069 acres (39,627,251 hectares) of land in the southwestern United States for solar projects, enough for between 10,000 and 20,000 GW.[127] The largest single plant in operation is the 510 MW Noor Solar Power Station. In 2022 the 700 MW CSP 4th phase of the 5GW Mohammed bin Rashid Al Maktoum Solar Park in Dubai will become the largest solar complex featuring CSP.\n The locations with highest direct irradiance are dry, at high altitude, and located in the tropics. These locations have a higher potential for CSP than areas with less sun.\n Abandoned opencast mines, moderate hill slopes and crater depressions may be advantageous in the case of power tower CSP as the power tower can be located on the ground integral with the molten salt storage tank.[128][129]\n CSP has a number of environmental effects, particularly on water use, land use and the use of hazardous materials.[130]\nWater is generally used for cooling and to clean mirrors. Some projects are looking into various approaches to reduce the water and cleaning agents used, including the use of barriers, non-stick coatings on mirrors, water misting systems, and others.[131]\n Concentrating solar power plants with wet-cooling systems have the highest water-consumption intensities of any conventional type of electric power plant; only fossil-fuel plants with carbon-capture and storage may have higher water intensities.[132] A 2013 study comparing various sources of electricity found that the median water consumption during operations of concentrating solar power plants with wet cooling was 3.1 cubic metres per megawatt-hour (810 US gal\/MWh) for power tower plants and 3.4 m3\/MWh (890 US gal\/MWh) for trough plants. This was higher than the operational water consumption (with cooling towers) for nuclear at 2.7 m3\/MWh (720 US gal\/MWh), coal at 2.0 m3\/MWh (530 US gal\/MWh), or natural gas at 0.79 m3\/MWh (210 US gal\/MWh).[133] A 2011 study by the National Renewable Energy Laboratory came to similar conclusions: for power plants with cooling towers, water consumption during operations was 3.27 m3\/MWh (865 US gal\/MWh)  for CSP trough, 2.98 m3\/MWh (786 US gal\/MWh) for CSP tower, 2.60 m3\/MWh (687 US gal\/MWh) for coal, 2.54 m3\/MWh (672 US gal\/MWh) for nuclear, and 0.75 m3\/MWh (198 US gal\/MWh) for natural gas.[134] The Solar Energy Industries Association noted that the Nevada Solar One trough CSP plant consumes 3.2 m3\/MWh (850 US gal\/MWh).[135] The issue of water consumption is heightened because CSP plants are often located in arid environments where water is scarce.\n In 2007, the US Congress directed the Department of Energy to report on ways to reduce water consumption by CSP. The subsequent report noted that dry cooling technology was available that, although more expensive to build and operate, could reduce water consumption by CSP by 91 to 95 percent. A hybrid wet\/dry cooling system could reduce water consumption by 32 to 58 percent.[136] A 2015 report by NREL noted that of the 24 operating CSP power plants in the US, 4 used dry cooling systems. The four dry-cooled systems were the three power plants at the Ivanpah Solar Power Facility near Barstow, California, and the Genesis Solar Energy Project in Riverside County, California. Of 15 CSP projects under construction or development in the US as of March 2015, 6 were wet systems, 7 were dry systems, 1 hybrid, and 1 unspecified.\n Although many older thermoelectric power plants with once-through cooling or cooling ponds use more water than CSP, meaning that more water passes through their systems, most of the cooling water returns to the water body available for other uses, and they consume less water by evaporation. For instance, the median coal power plant in the US with once-through cooling uses 138 m3\/MWh (36,350 US gal\/MWh), but only 0.95 m3\/MWh (250 US gal\/MWh) (less than one percent) is lost through evaporation.[137] Since the 1970s, the majority of US power plants have used recirculating systems such as cooling towers rather than once-through systems.[138]\n Insects can be attracted to the bright light caused by concentrated solar technology, and as a result birds that hunt them can be killed by being burned if they fly near the point where light is being focused. This can also affect raptors who hunt the birds.[139][140][141][142] Federal wildlife officials were quoted by opponents as calling the Ivanpah power towers \"mega traps\" for wildlife.[143][144][145]\n Some media sources have reported that concentrated solar power plants have injured or killed large numbers of birds due to intense heat from the concentrated sunrays.[146][147] Some of the claims may have been overstated or exaggerated.[148]\n According to rigorous reporting, in over six months, 133 songbirds were counted at Ivanpah.[149] By focusing no more than four mirrors on any one place in the air during standby, at Crescent Dunes Solar Energy Project, in three months, the death rate dropped to zero.[150]\n"}
{"key":"Renewable Energy","link":"https:\/\/en.wikipedia.org\/wiki\/Thermal_energy_storage#Molten_salt_technology","headline":"Thermal energy storage - Wikipedia","content":"\n Thermal energy storage (TES) is the storage of thermal energy for later reuse.  Employing widely different technologies, it allows surplus thermal energy to be stored for hours, days, or months.  Scale both of storage and use vary from small to large – from individual processes to district, town, or region. Usage examples are the balancing of energy demand between daytime and nighttime, storing summer heat for winter heating, or winter cold for summer cooling (Seasonal thermal energy storage). Storage media include water or ice-slush tanks, masses of native earth or bedrock accessed with heat exchangers by means of boreholes, deep aquifers contained between impermeable strata; shallow, lined pits filled with gravel and water and insulated at the top, as well as eutectic solutions and phase-change materials.[4][5]\n Other sources of thermal energy for storage include heat or cold produced with heat pumps from off-peak, lower cost electric power, a practice called peak shaving; heat from combined heat and power (CHP) power plants; heat produced by renewable electrical energy that exceeds grid demand and waste heat from industrial processes. Heat storage, both seasonal and short term, is considered an important means for cheaply balancing high shares of variable renewable electricity production and integration of electricity and heating sectors in energy systems almost or completely fed by renewable energy.[6][7][8][9]\n The different kinds of thermal energy storage can be divided into three separate categories: sensible heat, latent heat, and thermo-chemical heat storage. Each of these has different advantages and disadvantages that determine their applications.\n Sensible heat storage (SHS) is the most straightforward method. It simply means the temperature of some medium is either increased or decreased. This type of storage is the most commercially available out of the three; other techniques are less developed.\n The materials are generally inexpensive and safe. One of the cheapest, most commonly used options is a water tank, but materials such as molten salts or metals can be heated to higher temperatures and therefore offer a higher storage capacity. Energy can also be stored underground (UTES), either in an underground tank or in some kind of heat-transfer fluid (HTF) flowing through a system of pipes, either placed vertically in U-shapes (boreholes) or horizontally in trenches. Yet another system is known as a packed-bed (or pebble-bed) storage unit, in which some fluid, usually air, flows through a bed of loosely packed material (usually rock, pebbles or ceramic brick) to add or extract heat.\n A disadvantage of SHS is its dependence on the properties of the storage medium. Storage capacities are limited by the specific heat capacity of the storage material, and the system needs to be properly designed to ensure energy extraction at a constant temperature.[10]\n The sensible heat of molten salt is also used for storing solar energy at a high temperature,[11] termed molten-salt technology or molten salt energy storage (MSES). Molten salts can be employed as a thermal energy storage method to retain thermal energy. Presently, this is a commercially used technology to store the heat collected by concentrated solar power (e.g., from a solar tower or solar trough). The heat can later be converted into superheated steam to power conventional steam turbines and generate electricity at a later time. It was demonstrated in the Solar Two project from 1995 to 1999. Estimates in 2006 predicted an annual efficiency of 99%, a reference to the energy retained by storing heat before turning it into electricity, versus converting heat directly into electricity.[12][13][14] Various eutectic mixtures of different salts are used (e.g., sodium nitrate, potassium nitrate and calcium nitrate). Experience with such systems exists in non-solar applications in the chemical and metals industries as a heat-transport fluid.\n \nThe salt melts at 131 °C (268 °F). It is kept liquid at 288 °C (550 °F) in an insulated \"cold\" storage tank. The liquid salt is pumped through panels in a solar collector where the focused sun heats it to 566 °C (1,051 °F). It is then sent to a hot storage tank. With proper insulation of the tank the thermal energy can be usefully stored for up to a week.[15] When electricity is needed, the hot molten salt is pumped to a conventional steam-generator to produce superheated steam for driving a conventional turbine\/generator set as used in any coal, oil, or nuclear power plant. A 100-megawatt turbine would need a tank of about 9.1 metres (30 ft) tall and 24 metres (79 ft) in diameter to drive it for four hours by this design. A single tank with a divider plate to separate cold and hot molten salt is under development.[16] It is more economical by achieving 100% more heat storage per unit volume over the dual tanks system as the molten-salt storage tank is costly due to its complicated construction. Phase Change Material (PCMs) are also used in molten-salt energy storage,[17] while research on obtaining shape-stabilized PCMs using high porosity matrices is ongoing.[18]\n Most solar thermal power plants use this thermal energy storage concept. The Solana Generating Station in the U.S. can store 6 hours worth of generating capacity in molten salt. During the summer of 2013 the Gemasolar Thermosolar solar power-tower\/molten-salt plant in Spain achieved a first by continuously producing electricity 24 hours per day for 36 days.[19] The Cerro Dominador Solar Thermal Plant, inaugurated in June 2021, has 17.5 hours of heat storage.[20]\n A steam accumulator consists of an insulated steel pressure tank containing hot water and steam under pressure.  As a heat storage device, it is used to mediate heat production by a variable or steady source from a variable demand for heat. Steam accumulators may take on a significance for energy storage in solar thermal energy projects.\n Large stores are widely used in Nordic countries to store heat for several days, to decouple heat and power production and to help meet peak demands. Intersessional storage in caverns has been investigated and appears to be economical[21] and plays a significant role in heating in Finland. Energy producer Helen Oy estimates an 11.6 GWh capacity and 120 MW thermal output for its 260,000 m3 water cistern under Mustikkamaa (fully charged or discharged in 4 days at capacity), operating from 2021 to offset days of peak production\/demand;[22] while the 300,000 m3 rock caverns 50 m under sea level in Kruunuvuorenranta (near Laajasalo) were designated in 2018 to store heat in summer from warm seawater and release it in winter for district heating.[23]\n Solid or molten silicon offers much higher storage temperatures than salts with consequent greater capacity and efficiency. It is being researched as a possible more energy efficient storage technology. Silicon is able to store more than 1 MWh of energy per cubic meter at 1400 °C. An additional advantage is the relative abundance of silicon when compared to the salts used for the same purpose.[24][25]\n Another medium that can store thermal energy is molten (recycled) aluminum. This technology was developed by the Swedish company Azelio. The material is heated to 600 °C. When needed, the energy is transported to a Stirling engine using a heat-transfer fluid.\n Water has one of the highest thermal capacities at 4.2 kJ\/(kg⋅K) whereas concrete has about one third of that.  On the other hand, concrete can be heated to much higher temperatures (1200 °C) by for example electrical heating and therefore has a much higher overall volumetric capacity. Thus in the example below, an insulated cube of about 2.8 m3 would appear to provide sufficient storage for a single house to meet 50% of heating demand. This could, in principle, be used to store surplus wind or solar heat due to the ability of electrical heating to reach high temperatures. At the neighborhood level, the Wiggenhausen-Süd solar development at Friedrichshafen in southern Germany has received international attention. This features a 12,000 m3 (420,000 cu ft) reinforced concrete thermal store linked to 4,300 m2 (46,000 sq ft) of solar collectors, which will supply the 570 houses with around 50% of their heating and hot water. Siemens-Gamesa built a 130 MWh thermal storage near Hamburg with 750 °C in basalt and 1.5 MW electric output.[26][27] A similar system is scheduled for Sorø, Denmark, with 41–58% of the stored 18 MWh heat returned for the town's district heating, and 30–41% returned as electricity.[28]\n “Brick toaster” is a recently (August 2022) announced innovative heat reservoir operating at up to 1,500 °C (2,732 °F) that its maker, Titan Cement\/Rondo claims should be able cut global CO2 output by 15% over 15 years.[29]\n Because latent heat storage (LHS) is associated with a phase transition, the general term for the associated media is Phase-Change Material (PCM). During these transitions, heat can be added or extracted without affecting the material's temperature, giving it an advantage over SHS-technologies. Storage capacities are often higher as well.\n There are a multitude of PCMs available, including but not limited to salts, polymers, gels, paraffin waxes and metal alloys, each with different properties. This allows for a more target-oriented system design. As the process is isothermal at the PCM's melting point, the material can be picked to have the desired temperature range. Desirable qualities include high latent heat and thermal conductivity. Furthermore, the storage unit can be more compact if volume changes during the phase transition are small.\n PCMs are further subdivided into organic, inorganic and eutectic materials. Compared to organic PCMs, inorganic materials are less flammable, cheaper and more widely available. They also have higher storage capacity and thermal conductivity. Organic PCMs, on the other hand, are less corrosive and not as prone to phase-separation. Eutectic materials, as they are mixtures, are more easily adjusted to obtain specific properties, but have low latent and specific heat capacities.\n Another important factor in LHS is the encapsulation of the PCM. Some materials are more prone to erosion and leakage than others. The system must be carefully designed in order to avoid unnecessary loss of heat.[10]\n Miscibility gap alloys [30] rely on the phase change of a metallic material (see: latent heat) to store thermal energy.[31]\n Rather than pumping the liquid metal between tanks as in a molten-salt system, the metal is encapsulated in another metallic material that it cannot alloy with (immiscible).  Depending on the two materials selected (the phase changing material and the encapsulating material) storage densities can be between 0.2 and 2 MJ\/L.\n A working fluid, typically water or steam, is used to transfer the heat into and out of the system.  Thermal conductivity of miscibility gap alloys is often higher (up to 400 W\/(m⋅K)) than competing technologies[32][33] which means quicker \"charge\" and \"discharge\" of the thermal storage is possible.  The technology has not yet been implemented on a large scale.\n Several applications are being developed where ice is produced during off-peak periods and used for cooling at a later time. For example, air conditioning can be provided more economically by using low-cost electricity at night to freeze water into ice, then using the cooling capacity of ice in the afternoon to reduce the electricity needed to handle air conditioning demands. Thermal energy storage using ice makes use of the large heat of fusion of water. Historically, ice was transported from mountains to cities for use as a coolant. One metric ton of water (= one cubic meter) can store 334 million joules (MJ) or 317,000 BTUs (93 kWh). A relatively small storage facility can hold enough ice to cool a large building for a day or a week.\n In addition to using ice in direct cooling applications, it is also being used in heat pump-based heating systems.  In these applications, the phase change energy provides a very significant layer of thermal capacity that is near the bottom range of temperature that water source heat pumps can operate in.  This allows the system to ride out the heaviest heating load conditions and extends the timeframe by which the source energy elements can contribute heat back into the system.\n Cryogenic energy storage uses liquification of air or nitrogen as an energy store.\n A pilot cryogenic energy system that uses liquid air as the energy store, and low-grade waste heat to drive the thermal re-expansion of the air, operated at a power station in Slough, UK in 2010.[34]\n Thermo-chemical heat storage (TCS) involves some kind of reversible exotherm\/endotherm chemical reaction with thermo-chemical materials (TCM). Depending on the reactants, this method can allow for an even higher storage capacity than LHS.\n In one type of TCS, heat is applied to decompose certain molecules. The reaction products are then separated, and mixed again when required, resulting in a release of energy. Some examples are the decomposition of potassium oxide (over a range of 300–800 °C, with a heat decomposition of 2.1 MJ\/kg), lead oxide (300–350 °C, 0.26 MJ\/kg) and calcium hydroxide (above 450 °C, where the reaction rates can be increased by adding zinc or aluminum). The photochemical decomposition of nitrosyl chloride can also be used and, since it needs photons to occur, works especially well when paired with solar energy.[10]\n Adsorption processes also fall into this category. It can be used to not only store thermal energy, but also control air humidity. Zeolites (microporous crystalline alumina-silicates) and silica gels are well suited for this purpose. In hot, humid environments, this technology is often used in combination with lithium chloride to cool water.\n The low cost ($200\/ton) and high cycle rate (2,000×) of synthetic zeolites such as Linde 13X with water adsorbate has garnered much academic and commercial interest recently for use for thermal energy storage (TES), specifically of low-grade solar and waste heat. Several pilot projects have been funded in the EU from 2000 to the present (2020). The basic concept is to store solar thermal energy as chemical latent energy in the zeolite. Typically, hot dry air from flat plate solar collectors is made to flow through a bed of zeolite such that any water adsorbate present is driven off. Storage can be diurnal, weekly, monthly, or even seasonal depending on the volume of the zeolite and the area of the solar thermal panels. When heat is called for during the night, or sunless hours, or winter, humidified air flows through the zeolite. As the humidity is adsorbed by the zeolite, heat is released to the air and subsequently to the building space. This form of TES, with specific use of zeolites, was first taught by Guerra in 1978.[35] Advantages over molten salts and other high temperature TES include that (1) the temperature required is only the stagnation temperature typical of a solar flat plate thermal collector, and (2) as long as the zeolite is kept dry, the energy is stored indefinitely. Because of the low temperature, and because the energy is stored as latent heat of adsorption, thus eliminating the insulation requirements of a molten salt storage system, costs are significantly lower.\n One example of an experimental storage system based on chemical reaction energy is the salt hydrate technology. The system uses the reaction energy created when salts are hydrated or dehydrated. It works by storing heat in a container containing 50% sodium hydroxide (NaOH) solution. Heat (e.g. from using a solar collector) is stored by evaporating the water in an endothermic reaction. When water is added again, heat is released in an exothermic reaction at 50 °C (120 °F). Current systems operate at 60% efficiency. The system is especially advantageous for seasonal thermal energy storage, because the dried salt can be stored at room temperature for prolonged times, without energy loss. The containers with the dehydrated salt can even be transported to a different location. The system has a higher energy density than heat stored in water and the capacity of the system can be designed to store energy from a few months to years.[36]\n In 2013 the Dutch technology developer TNO presented the results of the MERITS project to store heat in a salt container. The heat, which can be derived from a solar collector on a rooftop, expels the water contained in the salt. When the water is added again, the heat is released, with almost no energy losses. A container with a few cubic meters of salt could store enough of this thermochemical energy to heat a house throughout the winter. In a temperate climate like that of the Netherlands, an average low-energy household requires about 6.7 GJ\/winter. To store this energy in water (at a temperature difference of 70 °C), 23 m3 insulated water storage would be needed, exceeding the storage abilities of most households. Using salt hydrate technology with a storage density of about 1 GJ\/m3, 4–8 m3 could be sufficient.[37]\n As of 2016, researchers in several countries are conducting experiments to determine the best type of salt, or salt mixture. Low pressure within the container seems favorable for the energy transport.[38] Especially promising are organic salts, so called ionic liquids. Compared to lithium halide-based sorbents they are less problematic in terms of limited global resources and compared to most other halides and sodium hydroxide (NaOH) they are less corrosive and not negatively affected by CO2 contaminations.[39]\n Storing energy in molecular bonds is being investigated. Energy densities equivalent to lithium-ion batteries have been achieved.[40] This has been done by a DSPEC (dys-sensitized photoelectrosythesis cell). This is a cell that can store energy that has been acquired by solar panels during the day for night-time (or even later) use. It is designed by taking an indication from, well known, natural photosynthesis.\n The DSPEC generates hydrogen fuel by making use of the acquired solar energy to split water molecules into its elements. As the result of this split, the hydrogen is isolated and the oxygen is released into the air. This sounds easier than it actually is. Four electrons of the water molecules need to be separated and transported elsewhere. Another difficult part is the process of merging the two separate hydrogen molecules.\n The DSPEC consists of two components: a molecule and a nanoparticle. The molecule is called a chromophore-catalyst assembly which absorbs sunlight and kick starts the catalyst. This catalyst separates the electrons and the water molecules. The nanoparticles are assembled into a thin layer and a single nanoparticle has many chromophore-catalyst on it. The function of this thin layer of nanoparticles is to transfer away the electrons which are separated from the water. This thin layer of nanoparticles is coated by a layer of titanium dioxide. With this coating, the electrons that come free can be transferred more quickly so that hydrogen could be made. This coating is, again, coated with a protective coating that strengthens the connection between the chromophore-catalyst and the nanoparticle.\n Using this method, the solar energy acquired from the solar panels is converted into fuel (hydrogen) without releasing the so-called greenhouse gasses. This fuel can be stored into a fuel cell and, at a later time, used to generate electricity.[41]\n Another promising way to store solar energy for electricity and heat production is a so-called molecular solar thermal system (MOST). With this approach a molecule is converted by photoisomerization into a higher-energy isomer. Photoisomerization is a process in which one (cis trans) isomer is converted into another by light (solar energy). This isomer is capable of storing the solar energy until the energy is released by a heat trigger or catalyst (then, the isomer is converted into its original isomer). A promising candidate for such a MOST is Norbornadiene (NBD). This is because there is a high energy difference between the NBD and the quadricyclane (QC) photoisomer. This energy difference is approximately 96 kJ\/mol. It is also known that for such systems, the donor-acceptor substitutions provide an effective means for red shifting the longest-wavelength absorption. This improves the solar spectrum match.\n A crucial challenge for a useful MOST system is to acquire a satisfactory high energy storage density (if possible, higher than 300 kJ\/kg). Another challenge of a MOST system is that light can be harvested in the visible region. The functionalization of the NBD with the donor and acceptor units is used to adjust this absorption maxima. However, this positive effect on the solar absorption is compensated by a higher molecular weight. This implies a lower energy density. This positive effect on the solar absorption has another downside. Namely, that the energy storage time is lowered when the absorption is redshifted. A possible solution to overcome this anti-correlation between the energy density and the red shifting is to couple one chromophore unit to several photo switches. In this case, it is advantageous to form so called dimers or trimers. The NBD share a common donor and\/or acceptor.\n Kasper Moth-Poulsen and his team tried to engineer the stability of the high energy photo isomer by having two electronically coupled photo switches with separate barriers for thermal conversion.[42] By doing so, a blue shift occurred after the first isomerization (NBD-NBD to QC-NBD). This led to a higher energy of isomerization of the second switching event (QC-NBD to QC-QC). Another advantage of this system, by sharing a donor, is that the molecular weight per norbornadiene unit is reduced. This leads to an increase of the energy density.\n Eventually, this system could reach a quantum yield of photoconversion up 94% per NBD unit. A quantum yield is a measure of the efficiency of photon emission. With this system the measured energy densities reached up to 559 kJ\/kg (exceeding the target of 300 kJ\/kg). So, the potential of the molecular photo switches is enormous—not only for solar thermal energy storage but for other applications as well.[42]\n In 2022, researchers reported combining the MOST with a chip-sized thermoelectric generator to generate electricity from it. The system can reportedly store solar energy for up to 18 years and may be an option for renewable energy storage.[43][44]\n Storage heaters are commonplace in European homes with time-of-use metering (traditionally using cheaper electricity at nighttime). They consist of high-density ceramic bricks or feolite blocks heated to a high temperature with electricity and may or may not have good insulation and controls to release heat over a number of hours. Some advice not to use them in areas with young children or where there is an increased risk of fires due to poor housekeeping, both due to the high temperatures involved.[45][46]\n With the rise of wind and solar power (and other renewable energies) providing an ever increasing share of energy input into the electricity grids in some countries, the use of larger scale electric energy storage is being explored by several commercial companies. Ideally, the utilisation of surplus renewable energy is transformed into high temperature high grade heat in highly insulated heat stores, for release later when needed. An emerging technology is the use of vacuum super insulated (VSI) heat stores.[47] The use of electricity to generate heat, and not say direct heat from solar thermal collectors, means that very high temperatures can be realised, potentially allowing for inter seasonal heat transfer—storing high grade heat in summer from surplus photovoltaics generation into heat stored for the following winter with relatively minimal standing losses.\n Solar energy is an application of thermal energy storage. Most practical solar thermal storage systems provide storage from a few hours to a day's worth of energy. However, a growing number of facilities use seasonal thermal energy storage (STES), enabling solar energy to be stored in summer to heat space during winter.[48][49][50] In 2017 Drake Landing Solar Community in Alberta, Canada, achieved a year-round 97% solar heating fraction, a world record made possible by incorporating STES.[48][51]\n The combined use of latent heat and sensible heat are possible with high temperature solar thermal input. Various eutectic metal mixtures, such as aluminum and silicon (AlSi12) offer a high melting point suited to efficient steam generation,[52] while high alumina cement-based materials offer good storage capabilities.[53]\n In pumped-heat electricity storage (PHES), a reversible heat-pump system is used to store energy as a temperature difference between two heat stores.[54][55][56]\n Isentropic systems involve two insulated containers filled, for example, with crushed rock or gravel: a hot vessel storing thermal energy at high temperature\/pressure, and a cold vessel storing thermal energy at low temperature\/pressure. The vessels are connected at top and bottom by pipes and the whole system is filled with an inert gas such as argon.[57]\n While charging, the system can use off-peak electricity to work as a heat pump. One prototype used argon at ambient temperature and pressure from the top of the cold store is compressed adiabatically, to a pressure of, for example, 12 bar, heating it to around 500 °C (900 °F). The compressed gas is transferred to the top of the hot vessel where it percolates down through the gravel, transferring heat to the rock and cooling to ambient temperature. The cooled, but still pressurized, gas emerging at the bottom of the vessel is then adiabatically expanded to 1 bar, which lowers its temperature to −150 °C. The cold gas is then passed up through the cold vessel where it cools the rock while warming to its initial condition.\n The energy is recovered as electricity by reversing the cycle. The hot gas from the hot vessel is expanded to drive a generator and then supplied to the cold store. The cooled gas retrieved from the bottom of the cold store is compressed which heats the gas to ambient temperature. The gas is then transferred to the bottom of the hot vessel to be reheated.\n The compression and expansion processes are provided by a specially designed reciprocating machine using sliding valves. Surplus heat generated by inefficiencies in the process is shed to the environment through heat exchangers during the discharging cycle.[54][57]\n The developer claimed that a round trip efficiency of 72–80% was achievable.[54][57] This compares to >80% achievable with pumped hydro energy storage.[55]\n Another proposed system uses turbomachinery and is capable of operating at much higher power levels.[56] Use of phase change material as heat storage material could enhance performance.[17]\n  Renewable energy portal\n"}
{"key":"Renewable Energy","link":"https:\/\/en.wikipedia.org\/wiki\/Wind_energy","headline":"Wind power - Wikipedia","content":"\n Wind power is the use of wind energy to generate useful work. Historically, wind power was used by sails, windmills and windpumps, but today it is mostly used to generate electricity. This article deals only with wind power for electricity generation.\nToday, wind power is generated almost completely with wind turbines, generally grouped into wind farms and connected to the electrical grid.\n In 2022, wind supplied over 2000 TWh of electricity, which was over 7% of world electricity[1]: 58  and about 2% of world energy.[2][3] With about 100 GW added during 2021, mostly in China and the United States, global installed wind power capacity exceeded 800 GW.[4][3][5] To help meet the Paris Agreement goals to limit climate change, analysts say it should expand much faster - by over 1% of electricity generation per year.[6]\n Wind power is considered a sustainable, renewable energy source, and has a much smaller impact on the environment compared to burning fossil fuels. \nWind power is variable, so it needs energy storage or other dispatchable generation energy sources to attain a reliable supply of electricity. \nLand-based (onshore) wind farms have a greater visual impact on the landscape than most other power stations per energy produced.[7][8] Wind farms sited offshore have less visual impact and have higher capacity factors, although they are generally more expensive.[4] Offshore wind power currently has a share of about 10% of new installations.[9]\n Wind power is one of the lowest-cost electricity sources per unit of energy produced. \nIn many locations, new onshore wind farms are cheaper than new coal or gas plants.[10]\n Regions in the higher northern and southern latitudes have the highest potential for wind power.[11] In most regions, wind power generation is higher in nighttime, and in winter when solar power output is low. For this reason, combinations of wind and solar power are suitable in many countries.[12]\n Wind is air movement in the Earth's atmosphere. In a unit of time, say 1 second, the volume of air that had passed an area \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is \n\n\n\nA\nv\n\n\n{\\displaystyle Av}\n\n. If the air density is \n\n\n\nρ\n\n\n{\\displaystyle \\rho }\n\n , the mass of this volume of air is \n\n\n\nM\n=\nρ\nA\nv\n\n\n{\\displaystyle M=\\rho Av}\n\n, and the power transfer, or energy transfer per second is \n\n\n\nP\n=\n\n\n\n1\n2\n\n\n\nM\n\nv\n\n2\n\n\n=\n\n\n\n1\n2\n\n\n\nρ\nA\n\nv\n\n3\n\n\n\n\n{\\displaystyle P={\\tfrac {1}{2}}Mv^{2}={\\tfrac {1}{2}}\\rho Av^{3}}\n\n. Wind power is thus proportional to the third power of the wind speed; the available power increases eightfold when the wind speed doubles. Change of wind speed by a factor of 2.1544 increases the wind power by one order of magnitude (multiply by 10).\n The global wind kinetic energy averaged approximately 1.50 MJ\/m2 over the period from 1979 to 2010, 1.31 MJ\/m2 in the Northern Hemisphere with 1.70 MJ\/m2 in the Southern Hemisphere. The atmosphere acts as a thermal engine, absorbing heat at higher temperatures, releasing heat at lower temperatures. The process is responsible for the production of wind kinetic energy at a rate of 2.46 W\/m2 thus sustaining the circulation of the atmosphere against friction.[15]\n Through wind resource assessment, it is possible to estimate wind power potential globally, by country or region, or for a specific site. The Global Wind Atlas provided by the Technical University of Denmark in partnership with the World Bank provides a global assessment of wind power potential.[13][16][17]\nUnlike 'static' wind resource atlases which average estimates of wind speed and power density across multiple years, tools such as Renewables.ninja provide time-varying simulations of wind speed and power output from different wind turbine models at an hourly resolution.[18] More detailed, site-specific assessments of wind resource potential can be obtained from specialist commercial providers, and many of the larger wind developers have in-house modeling capabilities.\n The total amount of economically extractable power available from the wind is considerably more than present human power use from all sources.[19] The strength of wind varies, and an average value for a given location does not alone indicate the amount of energy a wind turbine could produce there.\n To assess prospective wind power sites, a probability distribution function is often fit to the observed wind speed data.[20] Different locations will have different wind speed distributions. The Weibull model closely mirrors the actual distribution of hourly\/ten-minute wind speeds at many locations. The Weibull factor is often close to 2 and therefore a Rayleigh distribution can be used as a less accurate, but simpler model.[21]\n A wind farm is a group of wind turbines in the same location. A large wind farm may consist of several hundred individual wind turbines distributed over an extended area. The land between the turbines may be used for agricultural or other purposes. A wind farm may also be located offshore. Almost all large wind turbines have the same design — a horizontal axis wind turbine having an upwind rotor with 3 blades, attached to a nacelle on top of a tall tubular tower.\n In a wind farm, individual turbines are interconnected with a medium voltage (often 34.5 kV) power collection system[26] and communications network. In general, a distance of 7D (7 times the rotor diameter of the wind turbine) is set between each turbine in a fully developed wind farm.[27] At a substation, this medium-voltage electric current is increased in voltage with a transformer for connection to the high voltage electric power transmission system.[28]\n Most modern turbines use variable speed generators combined with either a partial or full-scale power converter between the turbine generator and the collector system, which generally have more desirable properties for grid interconnection and have low voltage ride through-capabilities.[29] Modern turbines use either doubly fed electric machines with partial-scale converters or squirrel-cage induction generators or synchronous generators (both permanently and electrically excited) with full-scale converters.[30] Black start is possible[31] and is being further developed for places (such as Iowa) which generate most of their electricity from wind.[32]\n Transmission system operators will supply a wind farm developer with a grid code to specify the requirements for interconnection to the transmission grid. This will include the power factor, the constancy of frequency, and the dynamic behaviour of the wind farm turbines during a system fault.[33][34]\n Offshore wind power is wind farms in large bodies of water, usually the sea. These installations can use the more frequent and powerful winds that are available in these locations and have less visual impact on the landscape than land-based projects. However, the construction and maintenance costs are considerably higher.[36][37]\n As of November 2021, the Hornsea Wind Farm in the United Kingdom is the largest offshore wind farm in the world at 1,218 MW.[38]\n Near offshore wind farms may be connected by AC and far offshore by HVDC.[39]\n Wind power resources are not always located near to high population density. As transmission lines become longer, the losses associated with power transmission increase, as modes of losses at lower lengths are exacerbated and new modes of losses are no longer negligible as the length is increased; making it harder to transport large loads over large distances.[40]\n When the transmission capacity does not meet the generation capacity, wind farms are forced to produce below their full potential or stop running altogether, in a process known as curtailment. While this leads to potential renewable generation left untapped, it prevents possible grid overload or risk to reliable service.[41]\n One of the biggest current challenges to wind power grid integration in some countries is the necessity of developing new transmission lines to carry power from wind farms, usually in remote lowly populated areas due to availability of wind, to high load locations, usually on the coasts where population density is higher.[42] Any existing transmission lines in remote locations may not have been designed for the transport of large amounts of energy.[43] In particular geographic regions, peak wind speeds may not coincide with peak demand for electrical power, whether offshore or onshore. A possible future option may be to interconnect widely dispersed geographic areas with an HVDC super grid.[44]\n In 2020, wind supplied almost 1600 TWh of electricity, which was over 5% of worldwide electrical generation and about 2% of energy consumption.[2][3] With over 100 GW added during 2020, mostly in China, global installed wind power capacity reached more than 730 GW.[4][3] But to help meet the Paris Agreement's goals to limit climate change, analysts say it should expand much faster - by over 1% of electricity generation per year.[6] Expansion of wind power is being hindered by fossil fuel subsidies.[48][49][50]\n The actual amount of electric power that wind can generate is calculated by multiplying the nameplate capacity by the capacity factor, which varies according to equipment and location. Estimates of the capacity factors for wind installations are in the range of 35% to 44%.[51]\n Since wind speed is not constant, a wind farm's annual energy production is never as much as the sum of the generator nameplate ratings multiplied by the total hours in a year. The ratio of actual productivity in a year to this theoretical maximum is called the capacity factor. Online data is available for some locations, and the capacity factor can be calculated from the yearly output.[52][53]\n Wind energy penetration is the fraction of energy produced by wind compared with the total generation. Wind power's share of worldwide electricity usage in 2021 was almost 7%,[55] up from 3.5% in 2015.[56][57]\n There is no generally accepted maximum level of wind penetration. The limit for a particular grid will depend on the existing generating plants, pricing mechanisms, capacity for energy storage, demand management, and other factors. An interconnected electric power grid will already include reserve generating and transmission capacity to allow for equipment failures. This reserve capacity can also serve to compensate for the varying power generation produced by wind stations. Studies have indicated that 20% of the total annual electrical energy consumption may be incorporated with minimal difficulty.[58] These studies have been for locations with geographically dispersed wind farms, some degree of dispatchable energy or hydropower with storage capacity, demand management, and interconnected to a large grid area enabling the export of electric power when needed. Electrical utilities continue to study the effects of large-scale penetration of wind generation on system stability.[59]\n A wind energy penetration figure can be specified for different duration of time but is often quoted annually. To generate almost all electricity from wind annually requires substantial interconnection to other systems, for example some wind power in Scotland is sent to the rest of the British grid.[60] On a monthly, weekly, daily, or hourly basis—or less—wind might supply as much as or more than 100% of current use, with the rest stored, exported or curtailed. The seasonal industry might then take advantage of high wind and low usage times such as at night when wind output can exceed normal demand. Such industry might include the production of silicon, aluminum,[61] steel, or natural gas, and hydrogen, and using future long-term storage to facilitate 100% energy from variable renewable energy.[62][63][better source needed] Homes and businesses can also be programmed to vary electricity demand,[64][65] for example by remotely turning up water heater thermostats.[66]\n Wind power is variable, and during low wind periods, it may need to be replaced by other power sources. Transmission networks presently cope with outages of other generation plants and daily changes in electrical demand, but the variability of intermittent power sources such as wind power is more frequent than those of conventional power generation plants which, when scheduled to be operating, may be able to deliver their nameplate capacity around 95% of the time.\n Electric power generated from wind power can be highly variable at several different timescales: hourly, daily, or seasonally. Annual variation also exists but is not as significant.[citation needed] Because instantaneous electrical generation and consumption must remain in balance to maintain grid stability, this variability can present substantial challenges to incorporating large amounts of wind power into a grid system. Intermittency and the non-dispatchable nature of wind energy production can raise costs for regulation, incremental operating reserve, and (at high penetration levels) could require an increase in the already existing energy demand management, load shedding, storage solutions, or system interconnection with HVDC cables.\n Fluctuations in load and allowance for the failure of large fossil-fuel generating units require operating reserve capacity, which can be increased to compensate for the variability of wind generation.\n Utility-scale batteries are often used to balance hourly and shorter timescale variation,[67][68] but car batteries may gain ground from the mid-2020s.[69] Wind power advocates argue that periods of low wind can be dealt with by simply restarting existing power stations that have been held in readiness, or interlinking with HVDC.[70]\n The combination of diversifying variable renewables by type and location, forecasting their variation, and integrating them with dispatchable renewables, flexible fueled generators, and demand response can create a power system that has the potential to meet power supply needs reliably. Integrating ever-higher levels of renewables is being successfully demonstrated in the real world.[71]\n Solar power tends to be complementary to wind.[73][74] On daily to weekly timescales, high-pressure areas tend to bring clear skies and low surface winds, whereas low-pressure areas tend to be windier and cloudier. On seasonal timescales, solar energy peaks in summer, whereas in many areas wind energy is lower in summer and higher in winter.[A][75] Thus the seasonal variation of wind and solar power tend to cancel each other somewhat.[72] Wind hybrid power systems are becoming more popular.[76]\n For any particular generator, there is an 80% chance that wind output will change less than 10% in an hour and a 40% chance that it will change 10% or more in 5 hours.[77]\n In summer 2021, wind power in the United Kingdom fell due to the lowest winds in seventy years,[78] In the future, smoothing peaks by producing green hydrogen may help when wind has a larger share of generation.[79]\n While the output from a single turbine can vary greatly and rapidly as local wind speeds vary, as more turbines are connected over larger and larger areas the average power output becomes less variable and more predictable.[29][80] Weather forecasting permits the electric-power network to be readied for the predictable variations in production that occur.[81]\n It is thought that the most reliable low-carbon electricity systems will include a large share of wind power.[82]\n Typically, conventional hydroelectricity complements wind power very well. When the wind is blowing strongly, nearby hydroelectric stations can temporarily hold back their water. When the wind drops they can, provided they have the generation capacity, rapidly increase production to compensate. This gives a very even overall power supply and virtually no loss of energy and uses no more water.\n Alternatively, where a suitable head of water is not available, pumped-storage hydroelectricity or other forms of grid energy storage such as compressed air energy storage and thermal energy storage can store energy developed by high-wind periods and release it when needed. The type of storage needed depends on the wind penetration level – low penetration requires daily storage, and high penetration requires both short- and long-term storage – as long as a month or more.[citation needed] Stored energy increases the economic value of wind energy since it can be shifted to displace higher-cost generation during peak demand periods. The potential revenue from this arbitrage can offset the cost and losses of storage. Although pumped-storage power systems are only about 75% efficient and have high installation costs, their low running costs and ability to reduce the required electrical base-load can save both fuel and total electrical generation costs.[83][84]\n The energy needed to build a wind farm divided into the total output over its life, Energy Return on Energy Invested, of wind power varies, but averages about 20–25.[85][86] Thus, the energy payback time is typically around a year.\n Onshore wind is an inexpensive source of electric power, cheaper than coal plants and new gas plants.[10] According to BusinessGreen, wind turbines reached grid parity (the point at which the cost of wind power matches traditional sources) in some areas of Europe in the mid-2000s, and in the US around the same time. Falling prices continue to drive the Levelized cost down and it has been suggested that it has reached general grid parity in Europe in 2010, and will reach the same point in the US around 2016 due to an expected reduction in capital costs of about 12%.[88][needs update] In 2021, the CEO of Siemens Gamesa warned that increased demand for low-cost wind turbines combined with high input costs and high costs of steel result in increased pressure on the manufacturers and decreasing profit margins.[89]\n Northern Eurasia, Canada, some parts of the United States, and Patagonia in Argentina are the best areas for onshore wind: whereas in other parts of the world solar power, or a combination of wind and solar, tend to be cheaper.[90]: 8 \n Wind power is capital intensive but has no fuel costs.[91] The price of wind power is therefore much more stable than the volatile prices of fossil fuel sources.[92] However, the estimated average cost per unit of electric power must incorporate the cost of construction of the turbine and transmission facilities, borrowed funds, return to investors (including the cost of risk), estimated annual production, and other components, averaged over the projected useful life of the equipment, which may be more than 20 years. Energy cost estimates are highly dependent on these assumptions so published cost figures can differ substantially.\n The presence of wind energy, even when subsidized, can reduce costs for consumers (€5 billion\/yr in Germany) by reducing the marginal price, by minimizing the use of expensive peaking power plants.[93]\n The cost has decreased as wind turbine technology has improved. There are now longer and lighter wind turbine blades, improvements in turbine performance, and increased power generation efficiency. Also, wind project capital expenditure costs and maintenance costs have continued to decline.[94]\n In 2021, a Lazard study of unsubsidized electricity said that wind power levelized cost of electricity continues to fall but more slowly than before. The study estimated new wind-generated electricity cost from $26 to $50\/MWh, compared to new gas power from $45 to $74\/MWh. The median cost of fully deprecated existing coal power was $42\/MWh, nuclear $29\/MWh and gas $24\/MWh. The study estimated offshore wind at around $83\/MWh. Compound annual growth rate was 4% per year from 2016 to 2021, compared to 10% per year from 2009 to 2021.[10]\n Turbine prices have fallen significantly in recent years due to tougher competitive conditions such as the increased use of energy auctions, and the elimination of subsidies in many markets.[95] As of 2021, subsidies are still often given to offshore wind. But they are generally no longer necessary for onshore wind in countries with even a very low carbon price such as China, provided there are no competing fossil fuel subsidies.[96]\n Secondary market forces provide incentives for businesses to use wind-generated power, even if there is a premium price for the electricity. For example, socially responsible manufacturers pay utility companies a premium that goes to subsidize and build new wind power infrastructure. Companies use wind-generated power, and in return, they can claim that they are undertaking strong \"green\" efforts.[97] Wind projects provide local taxes, or payments in place of taxes and strengthen the economy of rural communities by providing income to farmers with wind turbines on their land.[98][99]\n The wind energy sector can also produce jobs during the construction and operating phase.[100] Jobs include the manufacturing of wind turbines and the construction process, which includes transporting, installing, and then maintaining the turbines. An estimated 1.25 million people were employed in wind power in 2020.[101]\n Small-scale wind power is the name given to wind generation systems with the capacity to produce up to 50 kW of electrical power.[102] Isolated communities, that may otherwise rely on diesel generators, may use wind turbines as an alternative. Individuals may purchase these systems to reduce or eliminate their dependence on grid electric power for economic reasons, or to reduce their carbon footprint. Wind turbines have been used for household electric power generation in conjunction with battery storage over many decades in remote areas.[103]\n Examples of small-scale wind power projects in an urban setting can be found in New York City, where, since 2009, several building projects have capped their roofs with Gorlov-type helical wind turbines. Although the energy they generate is small compared to the buildings' overall consumption, they help to reinforce the building's 'green' credentials in ways that \"showing people your high-tech boiler\" cannot, with some of the projects also receiving the direct support of the New York State Energy Research and Development Authority.[104]\n Grid-connected domestic wind turbines may use grid energy storage, thus replacing purchased electric power with locally produced power when available. The surplus power produced by domestic microgenerators can, in some jurisdictions, be fed into the network and sold to the utility company, producing a retail credit for the microgenerators' owners to offset their energy costs.[105]\n Off-grid system users can either adapt to intermittent power or use batteries, photovoltaic, or diesel systems to supplement the wind turbine.[106] Equipment such as parking meters, traffic warning signs, street lighting, or wireless Internet gateways may be powered by a small wind turbine, possibly combined with a photovoltaic system, that charges a small battery replacing the need for a connection to the power grid.[107]\n Airborne wind turbines, such as kites, can be used in places at risk of hurricanes, as they can be taken down in advance.[108]\n The environmental impact of electricity generation from wind power is minor when compared to that of fossil fuel power.[110] Wind turbines have some of the lowest life-cycle greenhouse-gas emissions of energy sources: far less greenhouse gas is emitted than for the average unit of electricity, so wind power helps limit climate change.[111] Use of engineered wood may allow carbon negative wind power.[112] Wind power consumes no fuel, and emits no local air pollution, unlike fossil fuel power sources.\n Onshore wind farms can have a significant visual impact.[113] Due to a very low surface power density and spacing requirements, wind farms typically need to be spread over more land than other power stations.[7][114] Their network of turbines, access roads, transmission lines, and substations can result in \"energy sprawl\";[8] although land between the turbines and roads can still be used for agriculture.[115][116] Some wind farms are opposed for potentially spoiling protected scenic areas, archaeological landscapes and heritage sites.[117][118][119] A report by the Mountaineering Council of Scotland concluded that wind farms harmed tourism in areas known for natural landscapes and panoramic views.[120]\n Habitat loss and fragmentation are the greatest potential impacts on wildlife of onshore wind farms,[8] but the worldwide ecological impact is minimal.[110] Thousands of birds and bats, including rare species, have been killed by wind turbine blades,[121] though wind turbines are responsible for far fewer bird deaths than fossil-fueled power stations.[122] This can be mitigated with proper wildlife monitoring.[123]\n Many wind turbine blades are made of fiberglass, and have a lifetime of 20 years.[124] Blades are hollow: some blades are crushed to reduce their volume and then landfilled.[125] However, as they can take a lot of weight they can be made into long lasting small bridges for walkers or cyclists.[126] Blade end-of-life is complicated,[127] and blades manufactured in the 2020s are more likely to be designed to be completely recyclable.[128]\n Wind turbines also generate noise. At a distance of 300 metres (980 ft), this may be around 45 dB, which is slightly louder than a refrigerator. At 1.5 km (1 mi), they become inaudible.[129][130] There are anecdotal reports of negative health effects on people who live very close to wind turbines.[131] Peer-reviewed research has generally not supported these claims.[132][133][134]\n Although wind turbines with fixed bases are a mature technology and new installations are generally no longer subsidized,[135][136] floating wind turbines are a relatively new technology so some governments subsidize them, for example to use deeper waters.[137]\n Fossil fuel subsidies by some governments are slowing the growth of renewables.[138]\n Permitting of wind farms can take years and some governments are trying to speed up - the wind industry says this will help limit climate change and increase energy security[139] - sometimes groups such as fishers resist this[140] but governments say that rules protecting biodiversity will still be followed.[141]\n Surveys of public attitudes across Europe and in many other countries show strong public support for wind power.[143][144][145] Bakker et al. (2012) found in their study that residents who did not want turbines built near them suffered significantly more stress than those who \"benefited economically from wind turbines\".[146]\n Although wind power is a popular form of energy generation, onshore or near offshore wind farms are sometimes opposed for their impact on the landscape (especially scenic areas, heritage areas and archaeological landscapes), as well as noise, and impact on tourism.[147][148]\n In other cases, there is direct community ownership of wind farms. The hundreds of thousands of people who have become involved in Germany's small and medium-sized wind farms demonstrate such support there.[149]\n A 2010 Harris Poll found strong support for wind power in Germany, other European countries, and the United States.[143][144][150]\n Public support in the United States has decreased from 75% in 2020 to 62% in 2021, with the Democrat Party supporting the use of wind energy twice as much as the Republican Party.[151] President Biden has signed an executive order to begin building large scale wind farms.[152]\n In China, Shen et al. (2019) found that Chinese city-dwellers may be resistant to building wind turbines in urban areas, with a surprisingly high proportion of people citing an unfounded fear of radiation as driving their concerns.[153] Also, the study finds that like their counterparts in OECD countries, urban Chinese respondents are sensitive to direct costs and wildlife externalities. Distributing relevant information about turbines to the public may alleviate resistance.\n Many wind power companies work with local communities to reduce environmental and other concerns associated with particular wind farms.[156][157][158]\nIn other cases there is direct community ownership of wind farm projects. Appropriate government consultation, planning and approval procedures also help to minimize environmental risks.[143][159][160]\nSome may still object to wind farms[161] but many say their concerns should be weighed against the need to address the threats posed by air pollution,[162][111] climate change[163] and the opinions of the broader community.[164]\n In the US, wind power projects are reported to boost local tax bases, helping to pay for schools, roads, and hospitals, and to revitalize the economies of rural communities by providing steady income to farmers and other landowners.[98]\n In the UK, both the National Trust and the Campaign to Protect Rural England have expressed concerns about the effects on the rural landscape caused by inappropriately sited wind turbines and wind farms.[165][166]\n Some wind farms have become tourist attractions. The Whitelee Wind Farm Visitor Centre has an exhibition room, a learning hub, a café with a viewing deck and also a shop. It is run by the Glasgow Science Centre.[167]\n In Denmark, a loss-of-value scheme gives people the right to claim compensation for loss of value of their property if it is caused by proximity to a wind turbine. The loss must be at least 1% of the property's value.[168]\n Despite this general support for the concept of wind power in the public at large, local opposition often exists and has delayed or aborted a number of projects.[169][170][171]\nAs well as concerns about the landscape, there are concerns that some installations can produce excessive sound and vibration levels leading to a decrease in property values.[172] A study of 50,000 home sales near wind turbines found no statistical evidence that prices were affected.[173]\n While aesthetic issues are subjective and some find wind farms pleasant and optimistic, or symbols of energy independence and local prosperity, protest groups are often formed to attempt to block some wind power stations for various reasons.[161][174][175]\n Some opposition to wind farms is dismissed as NIMBYism,[176] but research carried out in 2009 found that there is little evidence to support the belief that residents only object to wind farms because of a \"Not in my Back Yard\" attitude.[177]\n Wind cannot be cut off unlike oil and gas so can contribute to energy security.[178]\n Wind turbines are devices that convert the wind's kinetic energy into electrical power. The result of over a millennium of windmill development and modern engineering, today's wind turbines are manufactured in a wide range of horizontal axis and vertical axis types. The smallest turbines are used for applications such as battery charging for auxiliary power. Slightly larger turbines can be used for making small contributions to a domestic power supply while selling unused power back to the utility supplier via the electrical grid. Arrays of large turbines, known as wind farms, have become an increasingly important source of renewable energy and are used in many countries as part of a strategy to reduce their reliance on fossil fuels.\n Wind turbine design is the process of defining the form and specifications of a wind turbine to extract energy from the wind.[179]\nA wind turbine installation consists of the necessary systems needed to capture the wind's energy, point the turbine into the wind, convert mechanical rotation into electrical power, and other systems to start, stop, and control the turbine.\n In 1919, the German physicist Albert Betz showed that for a hypothetical ideal wind-energy extraction machine, the fundamental laws of conservation of mass and energy allowed no more than 16\/27 (59%) of the kinetic energy of the wind to be captured. This Betz limit can be approached in modern turbine designs, which may reach 70 to 80% of the theoretical Betz limit.[180][181]\n The aerodynamics of a wind turbine are not straightforward. The airflow at the blades is not the same as the airflow far away from the turbine. The very nature of how energy is extracted from the air also causes air to be deflected by the turbine. This affects the objects or other turbines downstream, which is known as \"wake effect\". Also, the aerodynamics of a wind turbine at the rotor surface exhibit phenomena that are rarely seen in other aerodynamic fields. The shape and dimensions of the blades of the wind turbine are determined by the aerodynamic performance required to efficiently extract energy from the wind, and by the strength required to resist the forces on the blade.[182]\n In addition to the aerodynamic design of the blades, the design of a complete wind power system must also address the design of the installation's rotor hub, nacelle, tower structure, generator, controls, and foundation.[183]\n Wind power has been used as long as humans have put sails into the wind. King Hammurabi's Codex (reign 1792 - 1750 BC) already mentioned windmills for generating mechanical energy.[184] Wind-powered machines used to grind grain and pump water, the windmill and wind pump, were developed in what is now Iran, Afghanistan, and Pakistan by the 9th century.[185][186] Wind power was widely available and not confined to the banks of fast-flowing streams, or later, requiring sources of fuel. Wind-powered pumps drained the polders of the Netherlands, and in arid regions such as the American mid-west or the Australian outback, wind pumps provided water for livestock and steam engines.\n The first windmill used for the production of electric power was built in Scotland in July 1887 by Prof James Blyth of Anderson's College, Glasgow (the precursor of Strathclyde University).[187] Blyth's 10 metres (33 ft) high cloth-sailed wind turbine was installed in the garden of his holiday cottage at Marykirk in Kincardineshire, and was used to charge accumulators developed by the Frenchman Camille Alphonse Faure, to power the lighting in the cottage,[187] thus making it the first house in the world to have its electric power supplied by wind power.[188] Blyth offered the surplus electric power to the people of Marykirk for lighting the main street, however, they turned down the offer as they thought electric power was \"the work of the devil.\"[187] Although he later built a wind turbine to supply emergency power to the local Lunatic Asylum, Infirmary, and Dispensary of Montrose, the invention never really caught on as the technology was not considered to be economically viable.[187]\n Across the Atlantic, in Cleveland, Ohio, a larger and heavily engineered machine was designed and constructed in the winter of 1887–1888 by Charles F. Brush.[189] This was built by his engineering company at his home and operated from 1886 until 1900.[190] The Brush wind turbine had a rotor 17 metres (56 ft) in diameter and was mounted on an 18 metres (59 ft) tower. Although large by today's standards, the machine was only rated at 12 kW. The connected dynamo was used either to charge a bank of batteries or to operate up to 100 incandescent light bulbs, three arc lamps, and various motors in Brush's laboratory.[191]\nWith the development of electric power, wind power found new applications in lighting buildings remote from centrally generated power. Throughout the 20th century parallel paths developed small wind stations suitable for farms or residences. \nFrom 1932 many isolated properties in Australia ran their lighting and electric fans from batteries, charged by a \"Freelite\" wind-driven generator, producing 100 watts of electrical power from as little wind speed as 10 miles per hour (16 km\/h).[192]\n The 1973 oil crisis triggered the investigation in Denmark and the United States that led to larger utility-scale wind generators that could be connected to electric power grids for remote use of power. By 2008, the U.S. installed capacity had reached 25.4 gigawatts, and by 2012 the installed capacity was 60 gigawatts.[193] Today, wind-powered generators operate in every size range between tiny stations for battery charging at isolated residences, up to gigawatt-sized offshore wind farms that provide electric power to national electrical networks. The European Union is working to augment these prospects.[194]\n"}
{"key":"Football","link":"https:\/\/en.wikipedia.org\/wiki\/Football","headline":"Football - Wikipedia","content":"\n Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.  Unqualified, the word football normally means the form of football that is the most popular where the word is used. Sports commonly called football include association football (known as soccer in Australia, Canada, South Africa, the United States, and sometimes in Ireland and New Zealand); Australian rules football; Gaelic football; gridiron football (specifically American football, Arena football, or Canadian football); International rules football; rugby league football; and rugby union football.[1] These various forms of football share, to varying degrees, common origins and are known as \"football codes\".\n There are a number of references to traditional, ancient, or prehistoric ball games played in many different parts of the world.[2][3][4] Contemporary codes of football can be traced back to the codification of these games at English public schools during the 19th century, itself an outgrowth of medieval football.[5][6] The expansion and cultural power of the British Empire allowed these rules of football to spread to areas of British influence outside the directly controlled Empire.[7] By the end of the 19th century, distinct regional codes were already developing: Gaelic football, for example, deliberately incorporated the rules of local traditional football games in order to maintain their heritage.[8] In 1888, the Football League was founded in England, becoming the first of many professional football associations. During the 20th century, several of the various kinds of football grew to become some of the most popular team sports in the world.[9]\n The various codes of football share certain common elements and can be grouped into two main classes of football: carrying codes like American football, Canadian football, Australian football, rugby union and rugby league, where the ball is moved about the field while being held in the hands or thrown, and kicking codes such as association football and Gaelic football, where the ball is moved primarily with the feet, and where handling is strictly limited.[10]\n Common rules among the sports include:[11]\n In all codes, common skills include passing, tackling, evasion of tackles, catching and kicking.[10] In most codes, there are rules restricting the movement of players offside, and players scoring a goal must put the ball either under or over a crossbar between the goalposts.\n There are conflicting explanations of the origin of the word \"football\".  It is widely assumed that the word \"football\" (or the phrase \"foot ball\") refers to the action of the foot kicking a ball.[12] There is an alternative explanation, which is that football originally referred to a variety of games in medieval Europe that were played on foot.[13] There is no conclusive evidence for either explanation.\n The Chinese competitive game cuju (蹴鞠) resembles modern association football.[14] It existed during the Han dynasty and possibly the Qin dynasty, in the second and third centuries BC, attested by descriptions in a military manual.[15][16] The Japanese version of cuju is kemari (蹴鞠), and was developed during the Asuka period.[17] This is known to have been played within the Japanese imperial court in Kyoto from about 600 AD. In kemari, several people stand in a circle and kick a ball to each other, trying not to let the ball drop to the ground (much like keepie uppie).\n The Ancient Greeks and Romans are known to have played many ball games, some of which involved the use of the feet. The Roman game harpastum is believed to have been adapted from a Greek team game known as \"ἐπίσκυρος\" (Episkyros)[18][19] or \"φαινίνδα\" (phaininda),[20] which is mentioned by a Greek playwright, Antiphanes (388–311 BC) and later referred to by the Christian theologian Clement of Alexandria (c. 150 – c. 215 AD). These games appear to have resembled rugby football.[21][22][23][24][25] The Roman politician Cicero (106–43 BC) describes the case of a man who was killed whilst having a shave when a ball was kicked into a barber's shop. Roman ball games already knew the air-filled ball, the follis.[26][27] Episkyros is described as an early form of football by FIFA.[28]\n There are a number of references to traditional, ancient, or prehistoric ball games, played by indigenous peoples in many different parts of the world. For example, in 1586, men from a ship commanded by an English explorer named John Davis went ashore to play a form of football with Inuit in Greenland.[29] There are later accounts of an Inuit game played on ice, called Aqsaqtuk. Each match began with two teams facing each other in parallel lines, before attempting to kick the ball through each other team's line and then at a goal. In 1610, William Strachey, a colonist at Jamestown, Virginia recorded a game played by Native Americans, called Pahsaheman.[citation needed] Pasuckuakohowog, a game similar to modern-day association football played amongst Amerindians, was also reported as early as the 17th century.\n Games played in Mesoamerica with rubber balls by indigenous peoples are also well-documented as existing since before this time, but these had more similarities to basketball or volleyball, and no links have been found between such games and modern football sports. Northeastern American Indians, especially the Iroquois Confederation, played a game which made use of net racquets to throw and catch a small ball; however, although it is a ball-goal foot game, lacrosse (as its modern descendant is called) is likewise not usually classed as a form of \"football\".[citation needed]\n On the Australian continent several tribes of indigenous people played kicking and catching games with stuffed balls which have been generalised by historians as Marn Grook (Djab Wurrung for \"game ball\"). The earliest historical account is an anecdote from the 1878 book by Robert Brough-Smyth, The Aborigines of Victoria, in which a man called Richard Thomas is quoted as saying, in about 1841 in Victoria, Australia, that he had witnessed Aboriginal people playing the game: \"Mr Thomas describes how the foremost player will drop kick a ball made from the skin of a possum and how other players leap into the air in order to catch it.\" Some historians have theorised that Marn Grook was one of the origins of Australian rules football.\n The Māori in New Zealand played a game called Kī-o-rahi consisting of teams of seven players play on a circular field divided into zones, and score points by touching the 'pou' (boundary markers) and hitting a central 'tupu' or target.[citation needed]\n These games and others may well go far back into antiquity. However, the main sources of modern football codes appear to lie in western Europe, especially England.\n Mahmud al-Kashgari in his Dīwān Lughāt al-Turk, described a game called \"tepuk\" among Turks in Central and East Asia. In the game, people try to attack each other's castle by kicking a ball made of sheep leather.[30]\n The Middle Ages saw a huge rise in popularity of annual Shrovetide football matches throughout Europe, particularly in England. An early reference to a ball game played in Britain comes from the 9th-century Historia Brittonum, attributed to Nennius, which describes \"a party of boys ... playing at ball\".[31] References to a ball game played in northern France known as La Soule or Choule, in which the ball was propelled by hands, feet, and sticks,[32] date from the 12th century.[33]\n The early forms of football played in England, sometimes referred to as \"mob football\", would be played in towns or between neighbouring villages, involving an unlimited number of players on opposing teams who would clash en masse,[34] struggling to move an item, such as inflated animal's bladder[35] to particular geographical points, such as their opponents' church, with play taking place in the open space between neighbouring parishes.[36] The game was played primarily during significant religious festivals, such as Shrovetide, Christmas, or Easter,[35] and Shrovetide games have survived into the modern era in a number of English towns (see below).\n The first detailed description of what was almost certainly football in England was given by William FitzStephen in about 1174–1183. He described the activities of London youths during the annual festival of Shrove Tuesday:\n After lunch all the youth of the city go out into the fields to take part in a ball game. The students of each school have their own ball; the workers from each city craft are also carrying their balls. Older citizens, fathers, and wealthy citizens come on horseback to watch their juniors competing, and to relive their own youth vicariously: you can see their inner passions aroused as they watch the action and get caught up in the fun being had by the carefree adolescents.[37] Most of the very early references to the game speak simply of \"ball play\" or \"playing at ball\". This reinforces the idea that the games played at the time did not necessarily involve a ball being kicked.\n An early reference to a ball game that was probably football comes from 1280 at Ulgham, Northumberland, England:  \"Henry... while playing at ball.. ran against David\".[38] Football was played in Ireland in 1308, with a documented reference to John McCrocan, a spectator at a \"football game\" at Newcastle, County Down being charged with accidentally stabbing a player named William Bernard.[39] Another reference to a football game comes in 1321 at Shouldham, Norfolk, England: \"[d]uring the game at ball as he kicked the ball, a lay friend of his... ran against him and wounded himself\".[38]\n In 1314, Nicholas de Farndone, Lord Mayor of the City of London issued a decree banning football in the French used by the English upper classes at the time. A translation reads: \"[f]orasmuch as there is great noise in the city caused by hustling over large foot balls [rageries de grosses pelotes de pee][40] in the fields of the public from which many evils might arise which God forbid: we command and forbid on behalf of the king, on pain of imprisonment, such game to be used in the city in the future.\" This is the earliest reference to football.\n In 1363, King Edward III of England issued a proclamation banning \"...handball, football, or hockey; coursing and cock-fighting, or other such idle games\",[41] showing that \"football\" – whatever its exact form in this case – was being differentiated from games involving other parts of the body, such as handball.\n A game known as \"football\" was played in Scotland as early as the 15th century: it was prohibited by the Football Act 1424 and although the law fell into disuse it was not repealed until 1906. There is evidence for schoolboys playing a \"football\" ball game in Aberdeen in 1633 (some references cite 1636) which is notable as an early allusion to what some have considered to be passing the ball. The word \"pass\" in the most recent translation is derived from \"huc percute\" (strike it here) and later \"repercute pilam\" (strike the ball again) in the original Latin. It is not certain that the ball was being struck between members of the same team. The original word translated as \"goal\" is \"metum\", literally meaning the \"pillar at each end of the circus course\" in a Roman chariot race. There is a reference to \"get hold of the ball before [another player] does\" (Praeripe illi pilam si possis agere) suggesting that handling of the ball was allowed. One sentence states in the original 1930 translation \"Throw yourself against him\" (Age, objice te illi).\n King Henry IV of England also presented one of the earliest documented uses of the English word \"football\", in 1409, when he issued a proclamation forbidding the levying of money for \"foteball\".[38][42]\n There is also an account in Latin from the end of the 15th century of football being played at Caunton, Nottinghamshire. This is the first description of a \"kicking game\" and the first description of dribbling: \"[t]he game at which they had met for common recreation is called by some the foot-ball game. It is one in which young men, in country sport, propel a huge ball not by throwing it into the air but by striking it and rolling it along the ground, and that not with their hands but with their feet... kicking in opposite directions.\" The chronicler gives the earliest reference to a football pitch, stating that: \"[t]he boundaries have been marked and the game had started.[38]\n Other firsts in the medieval and early modern eras:\n In the 16th century, the city of Florence celebrated the period between Epiphany and Lent by playing a game which today is known as \"calcio storico\" (\"historic kickball\") in the Piazza Santa Croce.[46] The young aristocrats of the city would dress up in fine silk costumes and embroil themselves in a violent form of football. For example, calcio players could punch, shoulder charge, and kick opponents. Blows below the belt were allowed. The game is said to have originated as a military training exercise. In 1580, Count Giovanni de' Bardi di Vernio wrote Discorso sopra 'l giuoco del Calcio Fiorentino. This is sometimes said to be the earliest code of rules for any football game. The game was not played after January 1739 (until it was revived in May 1930).\n There have been many attempts to ban football, from the middle ages through to the modern day. The first such law was passed in England in 1314; it was followed by more than 30 in England alone between 1314 and 1667.[47]: 6  Women were banned from playing at English and Scottish Football League grounds in 1921, a ban that was only lifted in the 1970s. Female footballers still face similar problems in some parts of the world.\n American football also faced pressures to ban the sport.  The game played in the 19th century resembled mob football that developed in medieval Europe, including a version popular on university campuses known as old division football, and several municipalities banned its play in the mid-19th century.[48][49] By the 20th century, the game had evolved to a more rugby style game.  In 1905, there were calls to ban American football in the U.S. due to its violence; a meeting that year was hosted by American president Theodore Roosevelt led to sweeping rules changes that caused the sport to diverge significantly from its rugby roots to become more like the sport as it is played today.[50]\n While football continued to be played in various forms throughout Britain, its public schools (equivalent to private schools in other countries) are widely credited with four key achievements in the creation of modern football codes. First of all, the evidence suggests that they were important in taking football away from its \"mob\" form and turning it into an organised team sport. Second, many early descriptions of football and references to it were recorded by people who had studied at these schools. Third, it was teachers, students, and former students from these schools who first codified football games, to enable matches to be played between schools. Finally, it was at English public schools that the division between \"kicking\" and \"running\" (or \"carrying\") games first became clear.\n The earliest evidence that games resembling football were being played at English public schools – mainly attended by boys from the upper, upper-middle and professional classes – comes from the Vulgaria by William Herman in 1519. Herman had been headmaster at Eton and Winchester colleges and his Latin textbook includes a translation exercise with the phrase \"We wyll playe with a ball full of wynde\".[51]\n Richard Mulcaster, a student at Eton College in the early 16th century and later headmaster at other English schools, has been described as \"the greatest sixteenth Century advocate of football\".[52] Among his contributions are the earliest evidence of organised team football. Mulcaster's writings refer to teams (\"sides\" and \"parties\"), positions (\"standings\"), a referee (\"judge over the parties\") and a coach \"(trayning maister)\". Mulcaster's \"footeball\" had evolved from the disordered and violent forms of traditional football:\n [s]ome smaller number with such overlooking, sorted into sides and standings, not meeting with their bodies so boisterously to trie their strength: nor shouldring or shuffing one an other so barbarously ... may use footeball for as much good to the body, by the chiefe use of the legges.[53] In 1633, David Wedderburn, a teacher from Aberdeen, mentioned elements of modern football games in a short Latin textbook called Vocabula. Wedderburn refers to what has been translated into modern English as \"keeping goal\" and makes an allusion to passing the ball (\"strike it here\"). There is a reference to \"get hold of the ball\", suggesting that some handling was allowed. It is clear that the tackles allowed included the charging and holding of opposing players (\"drive that man back\").[54]\n A more detailed description of football is given in Francis Willughby's Book of Games, written in about 1660.[55] Willughby, who had studied at Bishop Vesey's Grammar School, Sutton Coldfield, is the first to describe goals and a distinct playing field: \"a close that has a gate at either end. The gates are called Goals.\" His book includes a diagram illustrating a football field. He also mentions tactics (\"leaving some of their best players to guard the goal\"); scoring (\"they that can strike the ball through their opponents' goal first win\") and the way teams were selected (\"the players being equally divided according to their strength and nimbleness\"). He is the first to describe a \"law\" of football: \"they must not strike [an opponent's leg] higher than the ball\".[56][57]\n English public schools were the first to codify football games. In particular, they devised the first offside rules, during the late 18th century.[58] In the earliest manifestations of these rules, players were \"off their side\" if they simply stood between the ball and the goal which was their objective. Players were not allowed to pass the ball forward, either by foot or by hand. They could only dribble with their feet, or advance the ball in a scrum or similar formation. However, offside laws began to diverge and develop differently at each school, as is shown by the rules of football from Winchester, Rugby, Harrow and Cheltenham, during between 1810 and 1850.[58] The first known codes – in the sense of a set of rules – were those of Eton in 1815[59] and Aldenham in 1825.[59])\n During the early 19th century, most working-class people in Britain had to work six days a week, often for over twelve hours a day. They had neither the time nor the inclination to engage in sport for recreation and, at the time, many children were part of the labour force. Feast day football played on the streets was in decline. Public school boys, who enjoyed some freedom from work, became the inventors of organised football games with formal codes of rules.\n Football was adopted by a number of public schools as a way of encouraging competitiveness and keeping youths fit. Each school drafted its own rules, which varied widely between different schools and were changed over time with each new intake of pupils. Two schools of thought developed regarding rules. Some schools favoured a game in which the ball could be carried (as at Rugby, Marlborough and Cheltenham), while others preferred a game where kicking and dribbling the ball was promoted (as at Eton, Harrow, Westminster and Charterhouse). The division into these two camps was partly the result of circumstances in which the games were played. For example, Charterhouse and Westminster at the time had restricted playing areas; the boys were confined to playing their ball game within the school cloisters, making it difficult for them to adopt rough and tumble running games.[citation needed]\n William Webb Ellis, a pupil at Rugby School, is said to have \"with a fine disregard for the rules of football, as played in his time [emphasis added], first took the ball in his arms and ran with it, thus creating the distinctive feature of the rugby game.\" in 1823. This act is usually said to be the beginning of Rugby football, but there is little evidence that it occurred, and most sports historians believe the story to be apocryphal. The act of 'taking the ball in his arms' is often misinterpreted as 'picking the ball up' as it is widely believed that Webb Ellis' 'crime' was handling the ball, as in modern association football, however handling the ball at the time was often permitted and in some cases compulsory,[60] the rule for which Webb Ellis showed disregard was running forward with it as the rules of his time only allowed a player to retreat backwards or kick forwards.\n The boom in rail transport in Britain during the 1840s meant that people were able to travel farther and with less inconvenience than they ever had before. Inter-school sporting competitions became possible. However, it was difficult for schools to play each other at football, as each school played by its own rules. The solution to this problem was usually that the match be divided into two-halves, one half played by the rules of the host \"home\" school, and the other half by the visiting \"away\" school.\n The modern rules of many football codes were formulated during the mid- or late- 19th century. This also applies to other sports such as lawn bowls, lawn tennis, etc. The major impetus for this was the patenting of the world's first lawnmower in 1830. This allowed for the preparation of modern ovals, playing fields, pitches, grass courts, etc.[61]\n Apart from Rugby football, the public school codes have barely been played beyond the confines of each school's playing fields. However, many of them are still played at the schools which created them (see Surviving UK school games below).\n Public schools' dominance of sports in the UK began to wane after the Factory Act of 1850, which significantly increased the recreation time available to working class children. Before 1850, many British children had to work six days a week, for more than twelve hours a day. From 1850, they could not work before 6 a.m. (7 a.m. in winter) or after 6 p.m. on weekdays (7 p.m. in winter); on Saturdays they had to cease work at 2 pm. These changes meant that working class children had more time for games, including various forms of football.\n The earliest known matches between public schools are as follows:\n Sports clubs dedicated to playing football began in the 18th century, for example London's Gymnastic Society which was founded in the mid-18th century and ceased playing matches in 1796.[65][63]\n The first documented club to bear in the title a reference to being a 'football club' were called \"The Foot-Ball Club\" who were located in Edinburgh, Scotland, during the period 1824–41.[66][67] The club forbade tripping but allowed pushing and holding and the picking up of the ball.[67]\n In 1845, three boys at Rugby school were tasked with codifying the rules then being used at the school. These were the first set of written rules (or code) for any form of football.[68] This further assisted the spread of the Rugby game.\n The earliest known matches involving non-public school clubs or institutions are as follows:\n One of the longest running football fixture is the Cordner-Eggleston Cup, contested between Melbourne Grammar School and Scotch College, Melbourne every year since 1858. It is believed by many to also be the first match of Australian rules football, although it was played under experimental rules in its first year. The first football trophy tournament was the Caledonian Challenge Cup, donated by the Royal Caledonian Society of Melbourne, played in 1861 under the Melbourne Rules.[80] The oldest football league is a rugby football competition, the United Hospitals Challenge Cup (1874), while the oldest rugby trophy is the Yorkshire Cup, contested since 1878. The South Australian Football Association (30 April 1877) is the oldest surviving Australian rules football competition. The oldest surviving soccer trophy is the Youdan Cup (1867) and the oldest national football competition is the English FA Cup (1871). The Football League (1888) is recognised as the longest running association football league. The first international football match took place between sides representing England and Scotland on 5 March 1870 at the Oval under the authority of the FA. The first rugby international took place in 1871.\n In Europe, early footballs were made out of animal bladders, more specifically pig's bladders, which were inflated. Later leather coverings were introduced to allow the balls to keep their shape.[81] However, in 1851, Richard Lindon and William Gilbert, both shoemakers from the town of Rugby (near the school), exhibited both round and oval-shaped balls at the Great Exhibition in London. Richard Lindon's wife is said to have died of lung disease caused by blowing up pig's bladders.[a] Lindon also won medals for the invention of the \"Rubber inflatable Bladder\" and the \"Brass Hand Pump\".\n In 1855, the U.S. inventor Charles Goodyear – who had patented vulcanised rubber – exhibited a spherical football, with an exterior of vulcanised rubber panels, at the Paris Exhibition Universelle. The ball was to prove popular in early forms of football in the U.S.[82]\n The iconic ball with a regular pattern of hexagons and pentagons (see truncated icosahedron) did not become popular until the 1960s, and was first used in the World Cup in 1970.\n The earliest reference to a game of football involving players passing the ball and attempting to score past a goalkeeper was written in 1633 by David Wedderburn, a poet and teacher in Aberdeen, Scotland.[83] Nevertheless, the original text does not state whether the allusion to passing as 'kick the ball back' ('repercute pilam') was in a forward or backward direction or between members of the same opposing teams (as was usual at this time).[84]\n \"Scientific\" football is first recorded in 1839 from Lancashire[85] and in the modern game in rugby football from 1862[86] and from Sheffield FC as early as 1865.[87][88] The first side to play a passing combination game was the Royal Engineers AFC in 1869\/70.[89][90] By 1869 they were \"work[ing] well together\", \"backing up\" and benefiting from \"cooperation\".[91] By 1870 the Engineers were passing the ball: \"Lieut. Creswell, who having brought the ball up the side then kicked it into the middle to another of his side, who kicked it through the posts the minute before time was called\".[92] Passing was a regular feature of their style.[93] By early 1872 the Engineers were the first football team renowned for \"play[ing] beautifully together\".[94] A double pass is first reported from Derby school against Nottingham Forest in March 1872, the first of which is irrefutably a short pass: \"Mr Absey dribbling the ball half the length of the field delivered it to Wallis, who kicking it cleverly in front of the goal, sent it to the captain who drove it at once between the Nottingham posts\".[95] The first side to have perfected the modern formation was Cambridge University AFC;[96][97][98] they also introduced the 2–3–5 \"pyramid\" formation.[99][100]\n Rugby football was thought to have been started about 1845 at Rugby School in Rugby, Warwickshire, England although forms of football in which the ball was carried and tossed date to medieval times. In Britain, by 1870, there were 49 clubs playing variations of the Rugby school game.[101] There were also \"rugby\" clubs in Ireland, Australia, Canada and New Zealand. However, there was no generally accepted set of rules for rugby until 1871, when 21 clubs from London came together to form the Rugby Football Union (RFU). The first official RFU rules were adopted in June 1871.[102] These rules allowed passing the ball. They also included the try, where touching the ball over the line allowed an attempt at goal, though drop-goals from marks and general play, and penalty conversions were still the main form of contest. Regardless of any form of football, the first international match between the national team of England and Scotland took place at Raeburn Place on 27 March 1871.\n Rugby football split into Rugby union, Rugby league, American football, and Canadian football. Tom Wills played Rugby football in England before founding Australian rules football.\n During the nineteenth century, several codifications of the rules of football were made at the University of Cambridge, in order to enable students from different public schools to play each other. The Cambridge Rules of 1863 influenced the decision of the Football Association to ban Rugby-style carrying of the ball in its own first set of laws.[103]\n By the late 1850s, many football clubs had been formed throughout the English-speaking world, to play various codes of football. Sheffield Football Club, founded in 1857 in the English city of Sheffield by Nathaniel Creswick and William Prest, was later recognised as the world's oldest club playing association football.[104]\nHowever, the club initially played its own code of football: the Sheffield rules. The code was largely independent of the public school rules, the most significant difference being the lack of an offside rule.\n The code was responsible for many innovations that later spread to association football. These included free kicks, corner kicks, handball, throw-ins and the crossbar.[105] By the 1870s they became the dominant code in the north and midlands of England. At this time, a series of rule changes by both the London and Sheffield FAs gradually eroded the differences between the two games until the adoption of a common code in 1877.\n There is archival evidence of \"foot-ball\" games being played in various parts of Australia throughout the first half of the 19th century. The origins of an organised game of football known today as Australian rules football can be traced back to 1858 in Melbourne, the capital city of Victoria.\n In July 1858, Tom Wills, an Australian-born cricketer educated at Rugby School in England, wrote a letter to Bell's Life in Victoria & Sporting Chronicle, calling for a \"foot-ball club\" with a \"code of laws\" to keep cricketers fit during winter.[106] This is considered by historians to be a defining moment in the creation of Australian rules football. Through publicity and personal contacts Wills was able to co-ordinate football matches in Melbourne that experimented with various rules,[107] the first of which was played on 31 July 1858. One week later, Wills umpired a schoolboys match between Melbourne Grammar School and Scotch College. Following these matches, organised football in Melbourne rapidly increased in popularity.\n Wills and others involved in these early matches formed the Melbourne Football Club (the oldest surviving Australian football club) on 14 May 1859. Club members Wills, William Hammersley, J. B. Thompson and Thomas H. Smith met with the intention of forming a set of rules that would be widely adopted by other clubs. The committee debated rules used in English public school games; Wills pushed for various rugby football rules he learnt during his schooling. The first rules share similarities with these games, and were shaped to suit to Australian conditions. H. C. A. Harrison, a seminal figure in Australian football, recalled that his cousin Wills wanted \"a game of our own\".[108] The code was distinctive in the prevalence of the mark, free kick, tackling, lack of an offside rule and that players were specifically penalised for throwing the ball.\n The Melbourne football rules were widely distributed and gradually adopted by the other Victorian clubs. The rules were updated several times during the 1860s to accommodate the rules of other influential Victorian football clubs. A significant redraft in 1866 by H. C. A. Harrison's committee accommodated the Geelong Football Club's rules, making the game then known as \"Victorian Rules\" increasingly distinct from other codes. It soon adopted cricket fields and an oval ball, used specialised goal and behind posts, and featured bouncing the ball while running and spectacular high marking. The game spread quickly to other Australian colonies. Outside its heartland in southern Australia, the code experienced a significant period of decline following World War I but has since grown throughout Australia and in other parts of the world, and the Australian Football League emerged as the dominant professional competition.\n During the early 1860s, there were increasing attempts in England to unify and reconcile the various public school games. In 1862, J. C. Thring, who had been one of the driving forces behind the original Cambridge Rules, was a master at Uppingham School, and he issued his own rules of what he called \"The Simplest Game\" (these are also known as the Uppingham Rules). In early October 1863, another new revised version of the Cambridge Rules was drawn up by a seven member committee representing former pupils from Harrow, Shrewsbury, Eton, Rugby, Marlborough and Westminster.\n At the Freemasons' Tavern, Great Queen Street, London on the evening of 26 October 1863, representatives of several football clubs in the London Metropolitan area met for the inaugural meeting of the Football Association (FA). The aim of the association was to establish a single unifying code and regulate the playing of the game among its members. Following the first meeting, the public schools were invited to join the association. All of them declined, except Charterhouse and Uppingham. In total, six meetings of the FA were held between October and December 1863. After the third meeting, a draft set of rules were published. However, at the beginning of the fourth meeting, attention was drawn to the recently published Cambridge Rules of 1863. The Cambridge rules differed from the draft FA rules in two significant areas; namely running with (carrying) the ball and hacking (kicking opposing players in the shins). The two contentious FA rules were as follows:\n IX. A player shall be entitled to run with the ball towards his adversaries' goal if he makes a fair catch, or catches the ball on the first bound; but in case of a fair catch, if he makes his mark he shall not run.\nX. If any player shall run with the ball towards his adversaries' goal, any player on the opposite side shall be at liberty to charge, hold, trip or hack him, or to wrest the ball from him, but no player shall be held and hacked at the same time.[109] At the fifth meeting it was proposed that these two rules be removed. Most of the delegates supported this, but F. M. Campbell, the representative from Blackheath and the first FA treasurer, objected. He said: \"hacking is the true football\". However, the motion to ban running with the ball in hand and hacking was carried and Blackheath withdrew from the FA. After the final meeting on 8 December, the FA published the \"Laws of the Game\", the first comprehensive set of rules for the game later known as association football. The term \"soccer\", in use since the late 19th century, derives from an Oxford University abbreviation of \"association\".[110]\n The first FA rules still contained elements that are no longer part of association football, but which are still recognisable in other games (such as Australian football and rugby football): for instance, a player could make a fair catch and claim a mark, which entitled him to a free kick; and if a player touched the ball behind the opponents' goal line, his side was entitled to a free kick at goal, from 15 yards (13.5 metres) in front of the goal line.\n As was the case in Britain, by the early 19th century, North American schools and universities played their own local games, between sides made up of students. For example, students at Dartmouth College in New Hampshire played a game called Old division football, a variant of the association football codes, as early as the 1820s.[49] They remained largely \"mob football\" style games, with huge numbers of players attempting to advance the ball into a goal area, often by any means necessary. Rules were simple, violence and injury were common.[48] The violence of these mob-style games led to widespread protests and a decision to abandon them. Yale University, under pressure from the city of New Haven, banned the play of all forms of football in 1860, while Harvard University followed suit in 1861.[48] In its place, two general types of football evolved: \"kicking\" games and \"running\" (or \"carrying\") games. A hybrid of the two, known as the \"Boston game\", was played by a group known as the Oneida Football Club. The club, considered by some historians as the first formal football club in the United States, was formed in 1862 by schoolboys who played the Boston game on Boston Common.[48][111] The game began to return to American college campuses by the late 1860s. The universities of Yale, Princeton (then known as the College of New Jersey), Rutgers, and Brown all began playing \"kicking\" games during this time. In 1867, Princeton used rules based on those of the English Football Association.[48]\n In Canada, the first documented football match was a practice game played on 9 November 1861, at University College, University of Toronto (approximately 400 yards west of Queen's Park). One of the participants in the game involving University of Toronto students was (Sir) William Mulock, later Chancellor of the school.[113] In 1864, at Trinity College, Toronto, F. Barlow Cumberland, Frederick A. Bethune, and Christopher Gwynn, one of the founders of Milton, Massachusetts, devised rules based on rugby football.[113] A \"running game\", resembling rugby football, was then taken up by the Montreal Football Club in Canada in 1868.[114]\n On 6 November 1869, Rutgers faced Princeton in a game that was played with a round ball and, like all early games, used improvised rules. It is usually regarded as the first game of American intercollegiate football.[48][115]\n Modern North American football grew out of a match between McGill University of Montreal and Harvard University in 1874. During the game, the two teams alternated between the rugby-based rules used by McGill and the Boston Game rules used by Harvard.[116][117][118] Within a few years, Harvard had both adopted McGill's rules and persuaded other U.S. university teams to do the same. On 23 November 1876, representatives from Harvard, Yale, Princeton, and Columbia met at the Massasoit Convention in Springfield, Massachusetts, agreeing to adopt most of the Rugby Football Union rules, with some variations.[119]\n In 1880, Yale coach Walter Camp, who had become a fixture at the Massasoit House conventions where the rules were debated and changed, devised a number of major innovations. Camp's two most important rule changes that diverged the American game from rugby were replacing the scrummage with the line of scrimmage and the establishment of the down-and-distance rules.[119] American football still however remained a violent sport where collisions often led to serious injuries and sometimes even death.[120] This led U.S. President Theodore Roosevelt to hold a meeting with football representatives from Harvard, Yale, and Princeton on 9 October 1905, urging them to make drastic changes.[121] One rule change introduced in 1906, devised to open up the game and reduce injury, was the introduction of the legal forward pass. Though it was underutilised for years, this proved to be one of the most important rule changes in the establishment of the modern game.[122]\n Over the years, Canada absorbed some of the developments in American football in an effort to distinguish it from a more rugby-oriented game. In 1903, the Ontario Rugby Football Union adopted the Burnside rules, which implemented the line of scrimmage and down-and-distance system from American football, among others.[123] Canadian football then implemented the legal forward pass in 1929.[124] American and Canadian football remain different codes, stemming from rule changes that the American side of the border adopted but the Canadian side has not.\n In the mid-19th century, various traditional football games, referred to collectively as caid, remained popular in Ireland, especially in County Kerry. One observer, Father W. Ferris, described two main forms of caid during this period: the \"field game\" in which the object was to put the ball through arch-like goals, formed from the boughs of two trees; and the epic \"cross-country game\" which took up most of the daylight hours of a Sunday on which it was played, and was won by one team taking the ball across a parish boundary. \"Wrestling\", \"holding\" opposing players, and carrying the ball were all allowed.\n By the 1870s, rugby and association football had started to become popular in Ireland. Trinity College Dublin was an early stronghold of rugby (see the Developments in the 1850s section above). The rules of the English FA were being distributed widely. Traditional forms of caid had begun to give way to a \"rough-and-tumble game\" which allowed tripping.\n There was no serious attempt to unify and codify Irish varieties of football, until the establishment of the Gaelic Athletic Association (GAA) in 1884. The GAA sought to promote traditional Irish sports, such as hurling and to reject imported games like rugby and association football. The first Gaelic football rules were drawn up by Maurice Davin and published in the United Ireland magazine on 7 February 1887.[125] Davin's rules showed the influence of games such as hurling and a desire to formalise a distinctly Irish code of football. The prime example of this differentiation was the lack of an offside rule (an attribute which, for many years, was shared only by other Irish games like hurling, and by Australian rules football).\n The International Rugby Football Board (IRFB) was founded in 1886,[126] but rifts were beginning to emerge in the code. Professionalism had already begun to creep into the various codes of football.\n In England, by the 1890s, a long-standing Rugby Football Union ban on professional players was causing regional tensions within rugby football, as many players in northern England were working class and could not afford to take time off to train, travel, play and recover from injuries. This was not very different from what had occurred ten years earlier in soccer in Northern England but the authorities reacted very differently in the RFU, attempting to alienate the working class support in Northern England. In 1895, following a dispute about a player being paid broken time payments, which replaced wages lost as a result of playing rugby, representatives of the northern clubs met in Huddersfield to form the Northern Rugby Football Union (NRFU). The new body initially permitted only various types of player wage replacements. However, within two years, NRFU players could be paid, but they were required to have a job outside sport.\n The demands of a professional league dictated that rugby had to become a better \"spectator\" sport. Within a few years the NRFU rules had started to diverge from the RFU, most notably with the abolition of the line-out. This was followed by the replacement of the ruck with the \"play-the-ball ruck\", which allowed a two-player ruck contest between the tackler at marker and the player tackled. Mauls were stopped once the ball carrier was held, being replaced by a play-the ball-ruck. The separate Lancashire and Yorkshire competitions of the NRFU merged in 1901, forming the Northern Rugby League, the first time the name rugby league was used officially in England.\n Over time, the RFU form of rugby, played by clubs which remained members of national federations affiliated to the IRFB, became known as rugby union.\n The need for a single body to oversee association football had become apparent by the beginning of the 20th century, with the increasing popularity of international fixtures. The English Football Association had chaired many discussions on setting up an international body, but was perceived as making no progress. It fell to associations from seven other European countries: France, Belgium, Denmark, Netherlands, Spain, Sweden, and Switzerland, to form an international association. The Fédération Internationale de Football Association (FIFA) was founded in Paris on 21 May 1904.[127] Its first president was Robert Guérin.[127] The French name and acronym has remained, even outside French-speaking countries.\n Rugby league rules diverged significantly from rugby union in 1906, with the reduction of the team from 15 to 13 players. In 1907, a New Zealand professional rugby team toured Australia and Britain, receiving an enthusiastic response, and professional rugby leagues were launched in Australia the following year. However, the rules of professional games varied from one country to another, and negotiations between various national bodies were required to fix the exact rules for each international match. This situation endured until 1948, when at the instigation of the French league, the Rugby League International Federation (RLIF) was formed at a meeting in Bordeaux.\n During the second half of the 20th century, the rules changed further. In 1966, rugby league officials borrowed the American football concept of downs: a team was allowed to retain possession of the ball for four tackles (rugby union retains the original rule that a player who is tackled and brought to the ground must release the ball immediately).  The maximum number of tackles was later increased to six (in 1971), and in rugby league this became known as the six tackle rule.\n With the advent of full-time professionals in the early 1990s, and the consequent speeding up of the game, the five-metre off-side distance between the two teams became 10 metres, and the replacement rule was superseded by various interchange rules, among other changes.\n The laws of rugby union also changed during the 20th century, although less significantly than those of rugby league. In particular, goals from marks were abolished, kicks directly into touch from outside the 22-metre line were penalised, new laws were put in place to determine who had possession following an inconclusive ruck or maul, and the lifting of players in line-outs was legalised.\n In 1995, rugby union became an \"open\" game, that is one which allowed professional players.[128] Although the original dispute between the two codes has now disappeared – and despite the fact that officials from both forms of rugby football have sometimes mentioned the possibility of re-unification – the rules of both codes and their culture have diverged to such an extent that such an event is unlikely in the foreseeable future.\n The word football, when used in reference to a specific game can mean any one of those described above. Because of this, much controversy has occurred over the term football, primarily because it is used in different ways in different parts of the English-speaking world. Most often, the word \"football\" is used to refer to the code of football that is considered dominant within a particular region (which is association football in most countries). So, effectively, what the word \"football\" means usually depends on where one says it.\n In each of the United Kingdom, the United States, and Canada, one football code is known solely as \"football\", while the others generally require a qualifier. In New Zealand, \"football\" historically referred to rugby union, but more recently may be used unqualified to refer to association football. The sport meant by the word \"football\" in Australia is either Australian rules football or rugby league, depending on local popularity (which largely conforms to the Barassi Line). In francophone Quebec, where Canadian football is more popular, the Canadian code is known as le football while American football is known as le football américain and association football is known as le soccer.[129]\n Of the 45 national FIFA (Fédération Internationale de Football Association) affiliates in which English is an official or primary language, most currently use Football in their organisations' official names; the FIFA affiliates in Canada and the United States use Soccer in their names. A few FIFA affiliates have recently \"normalised\" to using \"Football\", including:\n Several of the football codes are the most popular team sports in the world.[9] Globally, association football is played by over 250 million players in over 200 nations,[134] and has the highest television audience in sport,[135] making it the most popular in the world.[136] American football, with 1.1 million high school football players and nearly 70,000 college football players, is the most popular sport in the United States,[137][138] with the annual Super Bowl game accounting for nine of the top ten of the most watched broadcasts in U.S. television history.[139] The NFL has the highest average attendance (67,591) of any professional sports league in the world and has the highest revenue[140] out of any single professional sports league.[141] Thus, the best association football and American football players are among the highest paid athletes in the world.[142][143][144]\n Australian rules football has the highest spectator attendance of all sports in Australia.[145][146] Similarly, Gaelic football is the most popular sport in Ireland in terms of match attendance,[147] and the All-Ireland Football Final is the most watched event of that nation's sporting year.[148]\n Rugby union is the most popular sport in New Zealand, Samoa, Tonga, and Fiji.[149] It is also the fastest growing sport in the U.S.,[150][151][152][153] with college rugby being the fastest growing[clarification needed][154][155] college sport in that country.[156][dubious  – discuss]\n These codes have in common the prohibition of the use of hands (by all players except the goalkeeper, though outfield players can \"throw-in\" the ball when it goes out of play), unlike other codes where carrying or handling the ball by all players is allowed\n The hockey game bandy has rules partly based on the association football rules and is sometimes nicknamed as 'winter football'.\n There are also motorsport variations of the game.\n These codes have in common the ability of players to carry the ball with their hands, and to throw it to teammates, unlike association football where the use of hands during play is prohibited by anyone except the goalkeeper.  They also feature various methods of scoring based upon whether the ball is carried into the goal area, or kicked above the goalposts.\n These codes have in common the absence of an offside rule, the prohibition of continuous carrying of the ball (requiring a periodic bounce or solo (toe-kick), depending on the code) while running, handpassing by punching or tapping the ball rather than throwing it, and other traditions.\n Games still played at UK public (private) schools:\n Although similar to football and volleyball in some aspects, Sepak takraw has ancient origins and cannot be considered a hybrid game.\n"}
{"key":"Football","link":"https:\/\/en.wikipedia.org\/wiki\/Football","headline":"Football - Wikipedia","content":"\n Football is a family of team sports that involve, to varying degrees, kicking a ball to score a goal.  Unqualified, the word football normally means the form of football that is the most popular where the word is used. Sports commonly called football include association football (known as soccer in Australia, Canada, South Africa, the United States, and sometimes in Ireland and New Zealand); Australian rules football; Gaelic football; gridiron football (specifically American football, Arena football, or Canadian football); International rules football; rugby league football; and rugby union football.[1] These various forms of football share, to varying degrees, common origins and are known as \"football codes\".\n There are a number of references to traditional, ancient, or prehistoric ball games played in many different parts of the world.[2][3][4] Contemporary codes of football can be traced back to the codification of these games at English public schools during the 19th century, itself an outgrowth of medieval football.[5][6] The expansion and cultural power of the British Empire allowed these rules of football to spread to areas of British influence outside the directly controlled Empire.[7] By the end of the 19th century, distinct regional codes were already developing: Gaelic football, for example, deliberately incorporated the rules of local traditional football games in order to maintain their heritage.[8] In 1888, the Football League was founded in England, becoming the first of many professional football associations. During the 20th century, several of the various kinds of football grew to become some of the most popular team sports in the world.[9]\n The various codes of football share certain common elements and can be grouped into two main classes of football: carrying codes like American football, Canadian football, Australian football, rugby union and rugby league, where the ball is moved about the field while being held in the hands or thrown, and kicking codes such as association football and Gaelic football, where the ball is moved primarily with the feet, and where handling is strictly limited.[10]\n Common rules among the sports include:[11]\n In all codes, common skills include passing, tackling, evasion of tackles, catching and kicking.[10] In most codes, there are rules restricting the movement of players offside, and players scoring a goal must put the ball either under or over a crossbar between the goalposts.\n There are conflicting explanations of the origin of the word \"football\".  It is widely assumed that the word \"football\" (or the phrase \"foot ball\") refers to the action of the foot kicking a ball.[12] There is an alternative explanation, which is that football originally referred to a variety of games in medieval Europe that were played on foot.[13] There is no conclusive evidence for either explanation.\n The Chinese competitive game cuju (蹴鞠) resembles modern association football.[14] It existed during the Han dynasty and possibly the Qin dynasty, in the second and third centuries BC, attested by descriptions in a military manual.[15][16] The Japanese version of cuju is kemari (蹴鞠), and was developed during the Asuka period.[17] This is known to have been played within the Japanese imperial court in Kyoto from about 600 AD. In kemari, several people stand in a circle and kick a ball to each other, trying not to let the ball drop to the ground (much like keepie uppie).\n The Ancient Greeks and Romans are known to have played many ball games, some of which involved the use of the feet. The Roman game harpastum is believed to have been adapted from a Greek team game known as \"ἐπίσκυρος\" (Episkyros)[18][19] or \"φαινίνδα\" (phaininda),[20] which is mentioned by a Greek playwright, Antiphanes (388–311 BC) and later referred to by the Christian theologian Clement of Alexandria (c. 150 – c. 215 AD). These games appear to have resembled rugby football.[21][22][23][24][25] The Roman politician Cicero (106–43 BC) describes the case of a man who was killed whilst having a shave when a ball was kicked into a barber's shop. Roman ball games already knew the air-filled ball, the follis.[26][27] Episkyros is described as an early form of football by FIFA.[28]\n There are a number of references to traditional, ancient, or prehistoric ball games, played by indigenous peoples in many different parts of the world. For example, in 1586, men from a ship commanded by an English explorer named John Davis went ashore to play a form of football with Inuit in Greenland.[29] There are later accounts of an Inuit game played on ice, called Aqsaqtuk. Each match began with two teams facing each other in parallel lines, before attempting to kick the ball through each other team's line and then at a goal. In 1610, William Strachey, a colonist at Jamestown, Virginia recorded a game played by Native Americans, called Pahsaheman.[citation needed] Pasuckuakohowog, a game similar to modern-day association football played amongst Amerindians, was also reported as early as the 17th century.\n Games played in Mesoamerica with rubber balls by indigenous peoples are also well-documented as existing since before this time, but these had more similarities to basketball or volleyball, and no links have been found between such games and modern football sports. Northeastern American Indians, especially the Iroquois Confederation, played a game which made use of net racquets to throw and catch a small ball; however, although it is a ball-goal foot game, lacrosse (as its modern descendant is called) is likewise not usually classed as a form of \"football\".[citation needed]\n On the Australian continent several tribes of indigenous people played kicking and catching games with stuffed balls which have been generalised by historians as Marn Grook (Djab Wurrung for \"game ball\"). The earliest historical account is an anecdote from the 1878 book by Robert Brough-Smyth, The Aborigines of Victoria, in which a man called Richard Thomas is quoted as saying, in about 1841 in Victoria, Australia, that he had witnessed Aboriginal people playing the game: \"Mr Thomas describes how the foremost player will drop kick a ball made from the skin of a possum and how other players leap into the air in order to catch it.\" Some historians have theorised that Marn Grook was one of the origins of Australian rules football.\n The Māori in New Zealand played a game called Kī-o-rahi consisting of teams of seven players play on a circular field divided into zones, and score points by touching the 'pou' (boundary markers) and hitting a central 'tupu' or target.[citation needed]\n These games and others may well go far back into antiquity. However, the main sources of modern football codes appear to lie in western Europe, especially England.\n Mahmud al-Kashgari in his Dīwān Lughāt al-Turk, described a game called \"tepuk\" among Turks in Central and East Asia. In the game, people try to attack each other's castle by kicking a ball made of sheep leather.[30]\n The Middle Ages saw a huge rise in popularity of annual Shrovetide football matches throughout Europe, particularly in England. An early reference to a ball game played in Britain comes from the 9th-century Historia Brittonum, attributed to Nennius, which describes \"a party of boys ... playing at ball\".[31] References to a ball game played in northern France known as La Soule or Choule, in which the ball was propelled by hands, feet, and sticks,[32] date from the 12th century.[33]\n The early forms of football played in England, sometimes referred to as \"mob football\", would be played in towns or between neighbouring villages, involving an unlimited number of players on opposing teams who would clash en masse,[34] struggling to move an item, such as inflated animal's bladder[35] to particular geographical points, such as their opponents' church, with play taking place in the open space between neighbouring parishes.[36] The game was played primarily during significant religious festivals, such as Shrovetide, Christmas, or Easter,[35] and Shrovetide games have survived into the modern era in a number of English towns (see below).\n The first detailed description of what was almost certainly football in England was given by William FitzStephen in about 1174–1183. He described the activities of London youths during the annual festival of Shrove Tuesday:\n After lunch all the youth of the city go out into the fields to take part in a ball game. The students of each school have their own ball; the workers from each city craft are also carrying their balls. Older citizens, fathers, and wealthy citizens come on horseback to watch their juniors competing, and to relive their own youth vicariously: you can see their inner passions aroused as they watch the action and get caught up in the fun being had by the carefree adolescents.[37] Most of the very early references to the game speak simply of \"ball play\" or \"playing at ball\". This reinforces the idea that the games played at the time did not necessarily involve a ball being kicked.\n An early reference to a ball game that was probably football comes from 1280 at Ulgham, Northumberland, England:  \"Henry... while playing at ball.. ran against David\".[38] Football was played in Ireland in 1308, with a documented reference to John McCrocan, a spectator at a \"football game\" at Newcastle, County Down being charged with accidentally stabbing a player named William Bernard.[39] Another reference to a football game comes in 1321 at Shouldham, Norfolk, England: \"[d]uring the game at ball as he kicked the ball, a lay friend of his... ran against him and wounded himself\".[38]\n In 1314, Nicholas de Farndone, Lord Mayor of the City of London issued a decree banning football in the French used by the English upper classes at the time. A translation reads: \"[f]orasmuch as there is great noise in the city caused by hustling over large foot balls [rageries de grosses pelotes de pee][40] in the fields of the public from which many evils might arise which God forbid: we command and forbid on behalf of the king, on pain of imprisonment, such game to be used in the city in the future.\" This is the earliest reference to football.\n In 1363, King Edward III of England issued a proclamation banning \"...handball, football, or hockey; coursing and cock-fighting, or other such idle games\",[41] showing that \"football\" – whatever its exact form in this case – was being differentiated from games involving other parts of the body, such as handball.\n A game known as \"football\" was played in Scotland as early as the 15th century: it was prohibited by the Football Act 1424 and although the law fell into disuse it was not repealed until 1906. There is evidence for schoolboys playing a \"football\" ball game in Aberdeen in 1633 (some references cite 1636) which is notable as an early allusion to what some have considered to be passing the ball. The word \"pass\" in the most recent translation is derived from \"huc percute\" (strike it here) and later \"repercute pilam\" (strike the ball again) in the original Latin. It is not certain that the ball was being struck between members of the same team. The original word translated as \"goal\" is \"metum\", literally meaning the \"pillar at each end of the circus course\" in a Roman chariot race. There is a reference to \"get hold of the ball before [another player] does\" (Praeripe illi pilam si possis agere) suggesting that handling of the ball was allowed. One sentence states in the original 1930 translation \"Throw yourself against him\" (Age, objice te illi).\n King Henry IV of England also presented one of the earliest documented uses of the English word \"football\", in 1409, when he issued a proclamation forbidding the levying of money for \"foteball\".[38][42]\n There is also an account in Latin from the end of the 15th century of football being played at Caunton, Nottinghamshire. This is the first description of a \"kicking game\" and the first description of dribbling: \"[t]he game at which they had met for common recreation is called by some the foot-ball game. It is one in which young men, in country sport, propel a huge ball not by throwing it into the air but by striking it and rolling it along the ground, and that not with their hands but with their feet... kicking in opposite directions.\" The chronicler gives the earliest reference to a football pitch, stating that: \"[t]he boundaries have been marked and the game had started.[38]\n Other firsts in the medieval and early modern eras:\n In the 16th century, the city of Florence celebrated the period between Epiphany and Lent by playing a game which today is known as \"calcio storico\" (\"historic kickball\") in the Piazza Santa Croce.[46] The young aristocrats of the city would dress up in fine silk costumes and embroil themselves in a violent form of football. For example, calcio players could punch, shoulder charge, and kick opponents. Blows below the belt were allowed. The game is said to have originated as a military training exercise. In 1580, Count Giovanni de' Bardi di Vernio wrote Discorso sopra 'l giuoco del Calcio Fiorentino. This is sometimes said to be the earliest code of rules for any football game. The game was not played after January 1739 (until it was revived in May 1930).\n There have been many attempts to ban football, from the middle ages through to the modern day. The first such law was passed in England in 1314; it was followed by more than 30 in England alone between 1314 and 1667.[47]: 6  Women were banned from playing at English and Scottish Football League grounds in 1921, a ban that was only lifted in the 1970s. Female footballers still face similar problems in some parts of the world.\n American football also faced pressures to ban the sport.  The game played in the 19th century resembled mob football that developed in medieval Europe, including a version popular on university campuses known as old division football, and several municipalities banned its play in the mid-19th century.[48][49] By the 20th century, the game had evolved to a more rugby style game.  In 1905, there were calls to ban American football in the U.S. due to its violence; a meeting that year was hosted by American president Theodore Roosevelt led to sweeping rules changes that caused the sport to diverge significantly from its rugby roots to become more like the sport as it is played today.[50]\n While football continued to be played in various forms throughout Britain, its public schools (equivalent to private schools in other countries) are widely credited with four key achievements in the creation of modern football codes. First of all, the evidence suggests that they were important in taking football away from its \"mob\" form and turning it into an organised team sport. Second, many early descriptions of football and references to it were recorded by people who had studied at these schools. Third, it was teachers, students, and former students from these schools who first codified football games, to enable matches to be played between schools. Finally, it was at English public schools that the division between \"kicking\" and \"running\" (or \"carrying\") games first became clear.\n The earliest evidence that games resembling football were being played at English public schools – mainly attended by boys from the upper, upper-middle and professional classes – comes from the Vulgaria by William Herman in 1519. Herman had been headmaster at Eton and Winchester colleges and his Latin textbook includes a translation exercise with the phrase \"We wyll playe with a ball full of wynde\".[51]\n Richard Mulcaster, a student at Eton College in the early 16th century and later headmaster at other English schools, has been described as \"the greatest sixteenth Century advocate of football\".[52] Among his contributions are the earliest evidence of organised team football. Mulcaster's writings refer to teams (\"sides\" and \"parties\"), positions (\"standings\"), a referee (\"judge over the parties\") and a coach \"(trayning maister)\". Mulcaster's \"footeball\" had evolved from the disordered and violent forms of traditional football:\n [s]ome smaller number with such overlooking, sorted into sides and standings, not meeting with their bodies so boisterously to trie their strength: nor shouldring or shuffing one an other so barbarously ... may use footeball for as much good to the body, by the chiefe use of the legges.[53] In 1633, David Wedderburn, a teacher from Aberdeen, mentioned elements of modern football games in a short Latin textbook called Vocabula. Wedderburn refers to what has been translated into modern English as \"keeping goal\" and makes an allusion to passing the ball (\"strike it here\"). There is a reference to \"get hold of the ball\", suggesting that some handling was allowed. It is clear that the tackles allowed included the charging and holding of opposing players (\"drive that man back\").[54]\n A more detailed description of football is given in Francis Willughby's Book of Games, written in about 1660.[55] Willughby, who had studied at Bishop Vesey's Grammar School, Sutton Coldfield, is the first to describe goals and a distinct playing field: \"a close that has a gate at either end. The gates are called Goals.\" His book includes a diagram illustrating a football field. He also mentions tactics (\"leaving some of their best players to guard the goal\"); scoring (\"they that can strike the ball through their opponents' goal first win\") and the way teams were selected (\"the players being equally divided according to their strength and nimbleness\"). He is the first to describe a \"law\" of football: \"they must not strike [an opponent's leg] higher than the ball\".[56][57]\n English public schools were the first to codify football games. In particular, they devised the first offside rules, during the late 18th century.[58] In the earliest manifestations of these rules, players were \"off their side\" if they simply stood between the ball and the goal which was their objective. Players were not allowed to pass the ball forward, either by foot or by hand. They could only dribble with their feet, or advance the ball in a scrum or similar formation. However, offside laws began to diverge and develop differently at each school, as is shown by the rules of football from Winchester, Rugby, Harrow and Cheltenham, during between 1810 and 1850.[58] The first known codes – in the sense of a set of rules – were those of Eton in 1815[59] and Aldenham in 1825.[59])\n During the early 19th century, most working-class people in Britain had to work six days a week, often for over twelve hours a day. They had neither the time nor the inclination to engage in sport for recreation and, at the time, many children were part of the labour force. Feast day football played on the streets was in decline. Public school boys, who enjoyed some freedom from work, became the inventors of organised football games with formal codes of rules.\n Football was adopted by a number of public schools as a way of encouraging competitiveness and keeping youths fit. Each school drafted its own rules, which varied widely between different schools and were changed over time with each new intake of pupils. Two schools of thought developed regarding rules. Some schools favoured a game in which the ball could be carried (as at Rugby, Marlborough and Cheltenham), while others preferred a game where kicking and dribbling the ball was promoted (as at Eton, Harrow, Westminster and Charterhouse). The division into these two camps was partly the result of circumstances in which the games were played. For example, Charterhouse and Westminster at the time had restricted playing areas; the boys were confined to playing their ball game within the school cloisters, making it difficult for them to adopt rough and tumble running games.[citation needed]\n William Webb Ellis, a pupil at Rugby School, is said to have \"with a fine disregard for the rules of football, as played in his time [emphasis added], first took the ball in his arms and ran with it, thus creating the distinctive feature of the rugby game.\" in 1823. This act is usually said to be the beginning of Rugby football, but there is little evidence that it occurred, and most sports historians believe the story to be apocryphal. The act of 'taking the ball in his arms' is often misinterpreted as 'picking the ball up' as it is widely believed that Webb Ellis' 'crime' was handling the ball, as in modern association football, however handling the ball at the time was often permitted and in some cases compulsory,[60] the rule for which Webb Ellis showed disregard was running forward with it as the rules of his time only allowed a player to retreat backwards or kick forwards.\n The boom in rail transport in Britain during the 1840s meant that people were able to travel farther and with less inconvenience than they ever had before. Inter-school sporting competitions became possible. However, it was difficult for schools to play each other at football, as each school played by its own rules. The solution to this problem was usually that the match be divided into two-halves, one half played by the rules of the host \"home\" school, and the other half by the visiting \"away\" school.\n The modern rules of many football codes were formulated during the mid- or late- 19th century. This also applies to other sports such as lawn bowls, lawn tennis, etc. The major impetus for this was the patenting of the world's first lawnmower in 1830. This allowed for the preparation of modern ovals, playing fields, pitches, grass courts, etc.[61]\n Apart from Rugby football, the public school codes have barely been played beyond the confines of each school's playing fields. However, many of them are still played at the schools which created them (see Surviving UK school games below).\n Public schools' dominance of sports in the UK began to wane after the Factory Act of 1850, which significantly increased the recreation time available to working class children. Before 1850, many British children had to work six days a week, for more than twelve hours a day. From 1850, they could not work before 6 a.m. (7 a.m. in winter) or after 6 p.m. on weekdays (7 p.m. in winter); on Saturdays they had to cease work at 2 pm. These changes meant that working class children had more time for games, including various forms of football.\n The earliest known matches between public schools are as follows:\n Sports clubs dedicated to playing football began in the 18th century, for example London's Gymnastic Society which was founded in the mid-18th century and ceased playing matches in 1796.[65][63]\n The first documented club to bear in the title a reference to being a 'football club' were called \"The Foot-Ball Club\" who were located in Edinburgh, Scotland, during the period 1824–41.[66][67] The club forbade tripping but allowed pushing and holding and the picking up of the ball.[67]\n In 1845, three boys at Rugby school were tasked with codifying the rules then being used at the school. These were the first set of written rules (or code) for any form of football.[68] This further assisted the spread of the Rugby game.\n The earliest known matches involving non-public school clubs or institutions are as follows:\n One of the longest running football fixture is the Cordner-Eggleston Cup, contested between Melbourne Grammar School and Scotch College, Melbourne every year since 1858. It is believed by many to also be the first match of Australian rules football, although it was played under experimental rules in its first year. The first football trophy tournament was the Caledonian Challenge Cup, donated by the Royal Caledonian Society of Melbourne, played in 1861 under the Melbourne Rules.[80] The oldest football league is a rugby football competition, the United Hospitals Challenge Cup (1874), while the oldest rugby trophy is the Yorkshire Cup, contested since 1878. The South Australian Football Association (30 April 1877) is the oldest surviving Australian rules football competition. The oldest surviving soccer trophy is the Youdan Cup (1867) and the oldest national football competition is the English FA Cup (1871). The Football League (1888) is recognised as the longest running association football league. The first international football match took place between sides representing England and Scotland on 5 March 1870 at the Oval under the authority of the FA. The first rugby international took place in 1871.\n In Europe, early footballs were made out of animal bladders, more specifically pig's bladders, which were inflated. Later leather coverings were introduced to allow the balls to keep their shape.[81] However, in 1851, Richard Lindon and William Gilbert, both shoemakers from the town of Rugby (near the school), exhibited both round and oval-shaped balls at the Great Exhibition in London. Richard Lindon's wife is said to have died of lung disease caused by blowing up pig's bladders.[a] Lindon also won medals for the invention of the \"Rubber inflatable Bladder\" and the \"Brass Hand Pump\".\n In 1855, the U.S. inventor Charles Goodyear – who had patented vulcanised rubber – exhibited a spherical football, with an exterior of vulcanised rubber panels, at the Paris Exhibition Universelle. The ball was to prove popular in early forms of football in the U.S.[82]\n The iconic ball with a regular pattern of hexagons and pentagons (see truncated icosahedron) did not become popular until the 1960s, and was first used in the World Cup in 1970.\n The earliest reference to a game of football involving players passing the ball and attempting to score past a goalkeeper was written in 1633 by David Wedderburn, a poet and teacher in Aberdeen, Scotland.[83] Nevertheless, the original text does not state whether the allusion to passing as 'kick the ball back' ('repercute pilam') was in a forward or backward direction or between members of the same opposing teams (as was usual at this time).[84]\n \"Scientific\" football is first recorded in 1839 from Lancashire[85] and in the modern game in rugby football from 1862[86] and from Sheffield FC as early as 1865.[87][88] The first side to play a passing combination game was the Royal Engineers AFC in 1869\/70.[89][90] By 1869 they were \"work[ing] well together\", \"backing up\" and benefiting from \"cooperation\".[91] By 1870 the Engineers were passing the ball: \"Lieut. Creswell, who having brought the ball up the side then kicked it into the middle to another of his side, who kicked it through the posts the minute before time was called\".[92] Passing was a regular feature of their style.[93] By early 1872 the Engineers were the first football team renowned for \"play[ing] beautifully together\".[94] A double pass is first reported from Derby school against Nottingham Forest in March 1872, the first of which is irrefutably a short pass: \"Mr Absey dribbling the ball half the length of the field delivered it to Wallis, who kicking it cleverly in front of the goal, sent it to the captain who drove it at once between the Nottingham posts\".[95] The first side to have perfected the modern formation was Cambridge University AFC;[96][97][98] they also introduced the 2–3–5 \"pyramid\" formation.[99][100]\n Rugby football was thought to have been started about 1845 at Rugby School in Rugby, Warwickshire, England although forms of football in which the ball was carried and tossed date to medieval times. In Britain, by 1870, there were 49 clubs playing variations of the Rugby school game.[101] There were also \"rugby\" clubs in Ireland, Australia, Canada and New Zealand. However, there was no generally accepted set of rules for rugby until 1871, when 21 clubs from London came together to form the Rugby Football Union (RFU). The first official RFU rules were adopted in June 1871.[102] These rules allowed passing the ball. They also included the try, where touching the ball over the line allowed an attempt at goal, though drop-goals from marks and general play, and penalty conversions were still the main form of contest. Regardless of any form of football, the first international match between the national team of England and Scotland took place at Raeburn Place on 27 March 1871.\n Rugby football split into Rugby union, Rugby league, American football, and Canadian football. Tom Wills played Rugby football in England before founding Australian rules football.\n During the nineteenth century, several codifications of the rules of football were made at the University of Cambridge, in order to enable students from different public schools to play each other. The Cambridge Rules of 1863 influenced the decision of the Football Association to ban Rugby-style carrying of the ball in its own first set of laws.[103]\n By the late 1850s, many football clubs had been formed throughout the English-speaking world, to play various codes of football. Sheffield Football Club, founded in 1857 in the English city of Sheffield by Nathaniel Creswick and William Prest, was later recognised as the world's oldest club playing association football.[104]\nHowever, the club initially played its own code of football: the Sheffield rules. The code was largely independent of the public school rules, the most significant difference being the lack of an offside rule.\n The code was responsible for many innovations that later spread to association football. These included free kicks, corner kicks, handball, throw-ins and the crossbar.[105] By the 1870s they became the dominant code in the north and midlands of England. At this time, a series of rule changes by both the London and Sheffield FAs gradually eroded the differences between the two games until the adoption of a common code in 1877.\n There is archival evidence of \"foot-ball\" games being played in various parts of Australia throughout the first half of the 19th century. The origins of an organised game of football known today as Australian rules football can be traced back to 1858 in Melbourne, the capital city of Victoria.\n In July 1858, Tom Wills, an Australian-born cricketer educated at Rugby School in England, wrote a letter to Bell's Life in Victoria & Sporting Chronicle, calling for a \"foot-ball club\" with a \"code of laws\" to keep cricketers fit during winter.[106] This is considered by historians to be a defining moment in the creation of Australian rules football. Through publicity and personal contacts Wills was able to co-ordinate football matches in Melbourne that experimented with various rules,[107] the first of which was played on 31 July 1858. One week later, Wills umpired a schoolboys match between Melbourne Grammar School and Scotch College. Following these matches, organised football in Melbourne rapidly increased in popularity.\n Wills and others involved in these early matches formed the Melbourne Football Club (the oldest surviving Australian football club) on 14 May 1859. Club members Wills, William Hammersley, J. B. Thompson and Thomas H. Smith met with the intention of forming a set of rules that would be widely adopted by other clubs. The committee debated rules used in English public school games; Wills pushed for various rugby football rules he learnt during his schooling. The first rules share similarities with these games, and were shaped to suit to Australian conditions. H. C. A. Harrison, a seminal figure in Australian football, recalled that his cousin Wills wanted \"a game of our own\".[108] The code was distinctive in the prevalence of the mark, free kick, tackling, lack of an offside rule and that players were specifically penalised for throwing the ball.\n The Melbourne football rules were widely distributed and gradually adopted by the other Victorian clubs. The rules were updated several times during the 1860s to accommodate the rules of other influential Victorian football clubs. A significant redraft in 1866 by H. C. A. Harrison's committee accommodated the Geelong Football Club's rules, making the game then known as \"Victorian Rules\" increasingly distinct from other codes. It soon adopted cricket fields and an oval ball, used specialised goal and behind posts, and featured bouncing the ball while running and spectacular high marking. The game spread quickly to other Australian colonies. Outside its heartland in southern Australia, the code experienced a significant period of decline following World War I but has since grown throughout Australia and in other parts of the world, and the Australian Football League emerged as the dominant professional competition.\n During the early 1860s, there were increasing attempts in England to unify and reconcile the various public school games. In 1862, J. C. Thring, who had been one of the driving forces behind the original Cambridge Rules, was a master at Uppingham School, and he issued his own rules of what he called \"The Simplest Game\" (these are also known as the Uppingham Rules). In early October 1863, another new revised version of the Cambridge Rules was drawn up by a seven member committee representing former pupils from Harrow, Shrewsbury, Eton, Rugby, Marlborough and Westminster.\n At the Freemasons' Tavern, Great Queen Street, London on the evening of 26 October 1863, representatives of several football clubs in the London Metropolitan area met for the inaugural meeting of the Football Association (FA). The aim of the association was to establish a single unifying code and regulate the playing of the game among its members. Following the first meeting, the public schools were invited to join the association. All of them declined, except Charterhouse and Uppingham. In total, six meetings of the FA were held between October and December 1863. After the third meeting, a draft set of rules were published. However, at the beginning of the fourth meeting, attention was drawn to the recently published Cambridge Rules of 1863. The Cambridge rules differed from the draft FA rules in two significant areas; namely running with (carrying) the ball and hacking (kicking opposing players in the shins). The two contentious FA rules were as follows:\n IX. A player shall be entitled to run with the ball towards his adversaries' goal if he makes a fair catch, or catches the ball on the first bound; but in case of a fair catch, if he makes his mark he shall not run.\nX. If any player shall run with the ball towards his adversaries' goal, any player on the opposite side shall be at liberty to charge, hold, trip or hack him, or to wrest the ball from him, but no player shall be held and hacked at the same time.[109] At the fifth meeting it was proposed that these two rules be removed. Most of the delegates supported this, but F. M. Campbell, the representative from Blackheath and the first FA treasurer, objected. He said: \"hacking is the true football\". However, the motion to ban running with the ball in hand and hacking was carried and Blackheath withdrew from the FA. After the final meeting on 8 December, the FA published the \"Laws of the Game\", the first comprehensive set of rules for the game later known as association football. The term \"soccer\", in use since the late 19th century, derives from an Oxford University abbreviation of \"association\".[110]\n The first FA rules still contained elements that are no longer part of association football, but which are still recognisable in other games (such as Australian football and rugby football): for instance, a player could make a fair catch and claim a mark, which entitled him to a free kick; and if a player touched the ball behind the opponents' goal line, his side was entitled to a free kick at goal, from 15 yards (13.5 metres) in front of the goal line.\n As was the case in Britain, by the early 19th century, North American schools and universities played their own local games, between sides made up of students. For example, students at Dartmouth College in New Hampshire played a game called Old division football, a variant of the association football codes, as early as the 1820s.[49] They remained largely \"mob football\" style games, with huge numbers of players attempting to advance the ball into a goal area, often by any means necessary. Rules were simple, violence and injury were common.[48] The violence of these mob-style games led to widespread protests and a decision to abandon them. Yale University, under pressure from the city of New Haven, banned the play of all forms of football in 1860, while Harvard University followed suit in 1861.[48] In its place, two general types of football evolved: \"kicking\" games and \"running\" (or \"carrying\") games. A hybrid of the two, known as the \"Boston game\", was played by a group known as the Oneida Football Club. The club, considered by some historians as the first formal football club in the United States, was formed in 1862 by schoolboys who played the Boston game on Boston Common.[48][111] The game began to return to American college campuses by the late 1860s. The universities of Yale, Princeton (then known as the College of New Jersey), Rutgers, and Brown all began playing \"kicking\" games during this time. In 1867, Princeton used rules based on those of the English Football Association.[48]\n In Canada, the first documented football match was a practice game played on 9 November 1861, at University College, University of Toronto (approximately 400 yards west of Queen's Park). One of the participants in the game involving University of Toronto students was (Sir) William Mulock, later Chancellor of the school.[113] In 1864, at Trinity College, Toronto, F. Barlow Cumberland, Frederick A. Bethune, and Christopher Gwynn, one of the founders of Milton, Massachusetts, devised rules based on rugby football.[113] A \"running game\", resembling rugby football, was then taken up by the Montreal Football Club in Canada in 1868.[114]\n On 6 November 1869, Rutgers faced Princeton in a game that was played with a round ball and, like all early games, used improvised rules. It is usually regarded as the first game of American intercollegiate football.[48][115]\n Modern North American football grew out of a match between McGill University of Montreal and Harvard University in 1874. During the game, the two teams alternated between the rugby-based rules used by McGill and the Boston Game rules used by Harvard.[116][117][118] Within a few years, Harvard had both adopted McGill's rules and persuaded other U.S. university teams to do the same. On 23 November 1876, representatives from Harvard, Yale, Princeton, and Columbia met at the Massasoit Convention in Springfield, Massachusetts, agreeing to adopt most of the Rugby Football Union rules, with some variations.[119]\n In 1880, Yale coach Walter Camp, who had become a fixture at the Massasoit House conventions where the rules were debated and changed, devised a number of major innovations. Camp's two most important rule changes that diverged the American game from rugby were replacing the scrummage with the line of scrimmage and the establishment of the down-and-distance rules.[119] American football still however remained a violent sport where collisions often led to serious injuries and sometimes even death.[120] This led U.S. President Theodore Roosevelt to hold a meeting with football representatives from Harvard, Yale, and Princeton on 9 October 1905, urging them to make drastic changes.[121] One rule change introduced in 1906, devised to open up the game and reduce injury, was the introduction of the legal forward pass. Though it was underutilised for years, this proved to be one of the most important rule changes in the establishment of the modern game.[122]\n Over the years, Canada absorbed some of the developments in American football in an effort to distinguish it from a more rugby-oriented game. In 1903, the Ontario Rugby Football Union adopted the Burnside rules, which implemented the line of scrimmage and down-and-distance system from American football, among others.[123] Canadian football then implemented the legal forward pass in 1929.[124] American and Canadian football remain different codes, stemming from rule changes that the American side of the border adopted but the Canadian side has not.\n In the mid-19th century, various traditional football games, referred to collectively as caid, remained popular in Ireland, especially in County Kerry. One observer, Father W. Ferris, described two main forms of caid during this period: the \"field game\" in which the object was to put the ball through arch-like goals, formed from the boughs of two trees; and the epic \"cross-country game\" which took up most of the daylight hours of a Sunday on which it was played, and was won by one team taking the ball across a parish boundary. \"Wrestling\", \"holding\" opposing players, and carrying the ball were all allowed.\n By the 1870s, rugby and association football had started to become popular in Ireland. Trinity College Dublin was an early stronghold of rugby (see the Developments in the 1850s section above). The rules of the English FA were being distributed widely. Traditional forms of caid had begun to give way to a \"rough-and-tumble game\" which allowed tripping.\n There was no serious attempt to unify and codify Irish varieties of football, until the establishment of the Gaelic Athletic Association (GAA) in 1884. The GAA sought to promote traditional Irish sports, such as hurling and to reject imported games like rugby and association football. The first Gaelic football rules were drawn up by Maurice Davin and published in the United Ireland magazine on 7 February 1887.[125] Davin's rules showed the influence of games such as hurling and a desire to formalise a distinctly Irish code of football. The prime example of this differentiation was the lack of an offside rule (an attribute which, for many years, was shared only by other Irish games like hurling, and by Australian rules football).\n The International Rugby Football Board (IRFB) was founded in 1886,[126] but rifts were beginning to emerge in the code. Professionalism had already begun to creep into the various codes of football.\n In England, by the 1890s, a long-standing Rugby Football Union ban on professional players was causing regional tensions within rugby football, as many players in northern England were working class and could not afford to take time off to train, travel, play and recover from injuries. This was not very different from what had occurred ten years earlier in soccer in Northern England but the authorities reacted very differently in the RFU, attempting to alienate the working class support in Northern England. In 1895, following a dispute about a player being paid broken time payments, which replaced wages lost as a result of playing rugby, representatives of the northern clubs met in Huddersfield to form the Northern Rugby Football Union (NRFU). The new body initially permitted only various types of player wage replacements. However, within two years, NRFU players could be paid, but they were required to have a job outside sport.\n The demands of a professional league dictated that rugby had to become a better \"spectator\" sport. Within a few years the NRFU rules had started to diverge from the RFU, most notably with the abolition of the line-out. This was followed by the replacement of the ruck with the \"play-the-ball ruck\", which allowed a two-player ruck contest between the tackler at marker and the player tackled. Mauls were stopped once the ball carrier was held, being replaced by a play-the ball-ruck. The separate Lancashire and Yorkshire competitions of the NRFU merged in 1901, forming the Northern Rugby League, the first time the name rugby league was used officially in England.\n Over time, the RFU form of rugby, played by clubs which remained members of national federations affiliated to the IRFB, became known as rugby union.\n The need for a single body to oversee association football had become apparent by the beginning of the 20th century, with the increasing popularity of international fixtures. The English Football Association had chaired many discussions on setting up an international body, but was perceived as making no progress. It fell to associations from seven other European countries: France, Belgium, Denmark, Netherlands, Spain, Sweden, and Switzerland, to form an international association. The Fédération Internationale de Football Association (FIFA) was founded in Paris on 21 May 1904.[127] Its first president was Robert Guérin.[127] The French name and acronym has remained, even outside French-speaking countries.\n Rugby league rules diverged significantly from rugby union in 1906, with the reduction of the team from 15 to 13 players. In 1907, a New Zealand professional rugby team toured Australia and Britain, receiving an enthusiastic response, and professional rugby leagues were launched in Australia the following year. However, the rules of professional games varied from one country to another, and negotiations between various national bodies were required to fix the exact rules for each international match. This situation endured until 1948, when at the instigation of the French league, the Rugby League International Federation (RLIF) was formed at a meeting in Bordeaux.\n During the second half of the 20th century, the rules changed further. In 1966, rugby league officials borrowed the American football concept of downs: a team was allowed to retain possession of the ball for four tackles (rugby union retains the original rule that a player who is tackled and brought to the ground must release the ball immediately).  The maximum number of tackles was later increased to six (in 1971), and in rugby league this became known as the six tackle rule.\n With the advent of full-time professionals in the early 1990s, and the consequent speeding up of the game, the five-metre off-side distance between the two teams became 10 metres, and the replacement rule was superseded by various interchange rules, among other changes.\n The laws of rugby union also changed during the 20th century, although less significantly than those of rugby league. In particular, goals from marks were abolished, kicks directly into touch from outside the 22-metre line were penalised, new laws were put in place to determine who had possession following an inconclusive ruck or maul, and the lifting of players in line-outs was legalised.\n In 1995, rugby union became an \"open\" game, that is one which allowed professional players.[128] Although the original dispute between the two codes has now disappeared – and despite the fact that officials from both forms of rugby football have sometimes mentioned the possibility of re-unification – the rules of both codes and their culture have diverged to such an extent that such an event is unlikely in the foreseeable future.\n The word football, when used in reference to a specific game can mean any one of those described above. Because of this, much controversy has occurred over the term football, primarily because it is used in different ways in different parts of the English-speaking world. Most often, the word \"football\" is used to refer to the code of football that is considered dominant within a particular region (which is association football in most countries). So, effectively, what the word \"football\" means usually depends on where one says it.\n In each of the United Kingdom, the United States, and Canada, one football code is known solely as \"football\", while the others generally require a qualifier. In New Zealand, \"football\" historically referred to rugby union, but more recently may be used unqualified to refer to association football. The sport meant by the word \"football\" in Australia is either Australian rules football or rugby league, depending on local popularity (which largely conforms to the Barassi Line). In francophone Quebec, where Canadian football is more popular, the Canadian code is known as le football while American football is known as le football américain and association football is known as le soccer.[129]\n Of the 45 national FIFA (Fédération Internationale de Football Association) affiliates in which English is an official or primary language, most currently use Football in their organisations' official names; the FIFA affiliates in Canada and the United States use Soccer in their names. A few FIFA affiliates have recently \"normalised\" to using \"Football\", including:\n Several of the football codes are the most popular team sports in the world.[9] Globally, association football is played by over 250 million players in over 200 nations,[134] and has the highest television audience in sport,[135] making it the most popular in the world.[136] American football, with 1.1 million high school football players and nearly 70,000 college football players, is the most popular sport in the United States,[137][138] with the annual Super Bowl game accounting for nine of the top ten of the most watched broadcasts in U.S. television history.[139] The NFL has the highest average attendance (67,591) of any professional sports league in the world and has the highest revenue[140] out of any single professional sports league.[141] Thus, the best association football and American football players are among the highest paid athletes in the world.[142][143][144]\n Australian rules football has the highest spectator attendance of all sports in Australia.[145][146] Similarly, Gaelic football is the most popular sport in Ireland in terms of match attendance,[147] and the All-Ireland Football Final is the most watched event of that nation's sporting year.[148]\n Rugby union is the most popular sport in New Zealand, Samoa, Tonga, and Fiji.[149] It is also the fastest growing sport in the U.S.,[150][151][152][153] with college rugby being the fastest growing[clarification needed][154][155] college sport in that country.[156][dubious  – discuss]\n These codes have in common the prohibition of the use of hands (by all players except the goalkeeper, though outfield players can \"throw-in\" the ball when it goes out of play), unlike other codes where carrying or handling the ball by all players is allowed\n The hockey game bandy has rules partly based on the association football rules and is sometimes nicknamed as 'winter football'.\n There are also motorsport variations of the game.\n These codes have in common the ability of players to carry the ball with their hands, and to throw it to teammates, unlike association football where the use of hands during play is prohibited by anyone except the goalkeeper.  They also feature various methods of scoring based upon whether the ball is carried into the goal area, or kicked above the goalposts.\n These codes have in common the absence of an offside rule, the prohibition of continuous carrying of the ball (requiring a periodic bounce or solo (toe-kick), depending on the code) while running, handpassing by punching or tapping the ball rather than throwing it, and other traditions.\n Games still played at UK public (private) schools:\n Although similar to football and volleyball in some aspects, Sepak takraw has ancient origins and cannot be considered a hybrid game.\n"}
{"key":"Football","link":"https:\/\/en.wikipedia.org\/wiki\/American_football","headline":"American football - Wikipedia","content":"\n American football (referred to simply as football in the United States and Canada), also known as gridiron football,[nb 1] is a team sport played by two teams of eleven players on a rectangular field with goalposts at each end. The offense, the team with possession of the oval-shaped football, attempts to advance down the field by running with the ball or throwing it, while the defense, the team without possession of the ball, aims to stop the offense's advance and to take control of the ball for themselves. The offense must advance at least ten yards in four downs or plays; if they fail, they turn over the football to the defense, but if they succeed, they are given a new set of four downs to continue the drive. A game is won by the team with the higher number of points, which are scored primarily by advancing the ball into the opposing team's end zone for a touchdown or kicking the ball through the opponent's goalposts for a field goal.\n American football evolved in the United States, originating from the sports of soccer and rugby. The first American football match was played on November 6, 1869, between two college teams, Rutgers and Princeton, using rules based on the rules of soccer at the time. A set of rule changes drawn up from 1880 onward by Walter Camp, the \"Father of American Football\", established the snap, the line of scrimmage, eleven-player teams, and the concept of downs. Later rule changes legalized the forward pass, created the neutral zone, and specified the size and shape of the football. The sport is closely related to Canadian football, which evolved in parallel with and at the same time as the American game, although its rules were developed independently from those of Camp. Most of the features that distinguish American football from rugby and soccer are also present in Canadian football. The two sports are considered the primary variants of gridiron football.\n American football is the most popular sport in the United States in terms of broadcast viewership audience. The most popular forms of the game are professional and college football, with the other major levels being high-school and youth football. As of 2022[update], nearly 1.04 million high-school athletes play the sport in the U.S., with another 81,000 college athletes in the NCAA and the NAIA.[3] The National Football League (NFL) has the highest average attendance of any professional sports league in the world. Its championship game, the Super Bowl, ranks among the most-watched club sporting events globally. In 2022, the league had an annual revenue of around $18.6 billion,[4] making it the most valuable sports league in the world.[5] Other professional and amateur leagues exist worldwide, but the sport does not have the international popularity of other American sports like baseball or basketball; the sport maintains a growing following in the rest of North America, Europe, Brazil, and Japan.\n In the United States, American football is referred to as \"football\".[6] The term \"football\" was officially established in the rulebook for the 1876 college football season, when the sport first shifted from soccer-style rules to rugby-style rules. Although it could easily have been called \"rugby\" at this point, Harvard, one of the primary proponents of the rugby-style game, compromised and did not request the name of the sport be changed to \"rugby\".[7] The terms \"gridiron\" or \"American football\" are favored in English-speaking countries where other types of football are popular, such as the United Kingdom, Ireland, New Zealand, and Australia.[8][9]\n American football evolved from the sports of rugby and soccer. Rugby, like American football, is a sport in which two competing teams vie for control of a ball, which can be kicked through a set of goalposts or run into the opponent's goal area to score points.[10]\n What is considered to be the first American football game was played on November 6, 1869, between Rutgers and Princeton, two college teams. They consisted of 25 players per team and used a round ball that could not be picked up or carried. It could, however, be kicked or batted with the feet, hands, head, or sides, with the objective being to advance it into the opponent's goal. Rutgers won the game 6–4.[11][12] Collegiate play continued for several years with matches played using the rules of the host school. Representatives of Yale, Columbia, Princeton and Rutgers met on October 19, 1873, to create a standard set of rules for use by all schools. Teams were set at 20 players each, and fields of 400 by 250 feet (122 m × 76 m) were specified. Harvard abstained from the conference, as they favored a rugby-style game that allowed running with the ball.[12] After playing McGill University using both American (known as \"the Boston game\") for the first game and Canadian (rugby) rules for the second one,[12][13] the Harvard players preferred the Canadian style of having only 11 men on the field, running the ball without having to be chased by an opponent, the forward pass, tackling, and using an oblong instead of a round ball.[14][15]\n An 1875 Harvard–Yale game played under rugby-style rules was observed by two Princeton athletes who were impressed by it. They introduced the sport to Princeton, a feat the Professional Football Researchers Association compared to \"selling refrigerators to Eskimos\".[12] Princeton, Harvard, Yale, and Columbia then agreed to intercollegiate play using a form of rugby union rules with a modified scoring system.[16] These schools formed the Intercollegiate Football Association, although Yale did not join until 1879. Yale player Walter Camp, now regarded as the \"Father of American Football\",[16][17] secured rule changes in 1880 that reduced the size of each team from 15 to 11 players and instituted the snap to replace the chaotic and inconsistent scrum.[16] While the game between Rutgers and Princeton is commonly considered the first American football game, several years prior in 1862, the Oneida Football Club formed as the oldest known football club in the United States. The team consisted of graduates of Boston's elite preparatory schools and played from 1862 to 1865.[18]\n The introduction of the snap resulted in an unexpected consequence. Before the snap, the strategy had been to punt if a scrum resulted in bad field position. However, a group of Princeton players realized that, as the snap was uncontested, they could now hold the ball indefinitely to prevent their opponent from scoring. In 1881, in a game between Yale and Princeton, both teams used this strategy to maintain their undefeated records. Each team held the ball, gaining no ground, for an entire half, resulting in a 0–0 tie. This \"block game\" proved extremely unpopular with both teams' spectators and fans.[16]\n A rule change was necessary to prevent this strategy from taking hold, and a reversion to the scrum was considered. However, Camp successfully proposed a rule in 1882 that limited each team to three downs, or tackles, to advance the ball 5 yards (4.6 m). Failure to advance the ball the required distance within those three downs would result in control of the ball being forfeited to the other team. This change effectively made American football a separate sport from rugby, and the resulting five-yard lines added to the field to measure distances made it resemble a gridiron in appearance. Other major rule changes included a reduction of the field size to 110 by 53+1⁄3 yards (100.6 m × 48.8 m) and the adoption of a scoring system that awarded four points for a touchdown, two for a safety and a goal following a touchdown, and five for a goal from the field. Additionally, tackling below the waist was legalized,[16] and a static line of scrimmage was instituted.[19]\n Despite these new rules, football remained a violent sport. Dangerous mass-formations like the flying wedge resulted in serious injuries and deaths.[20] A 1905 peak of 19 fatalities nationwide resulted in a threat by President Theodore Roosevelt to abolish the game unless major changes were made.[21] In response, 62 colleges and universities met in New York City to discuss rule changes on December 28, 1905. These proceedings resulted in the formation of the Intercollegiate Athletic Association of the United States, later renamed the National Collegiate Athletic Association (NCAA).[22]\n The legal forward pass was introduced in 1906, although its effect was initially minimal due to the restrictions placed on its use. The idea of a 40-yard-wider field was opposed by Harvard due to the size of the new Harvard Stadium.[23] Other rule changes introduced that year included the reduction of playing time from 70 to 60 minutes and an increase of the distance required for a first down from 5 to 10 yards (4.6 to 9.1 m). To reduce infighting and dirty play between teams, the neutral zone was created along the width of the football before the snap.[24] Scoring was also adjusted: points awarded for field goals were reduced to three in 1909[17] and points for touchdowns were raised to six in 1912.[25] Also in 1912, the field was shortened to 100 yards (91 m) long, two 10-yard-long (9.1 m) end zones were created, and teams were given four downs instead of three to advance the ball 10 yards (9.1 m).[26][27] The roughing the passer penalty was implemented in 1914, and eligible players were first allowed to catch the ball anywhere on the field in 1918.[28]\n On November 12, 1892, Pudge Heffelfinger was paid $500 (equivalent to $16,285 in 2022) to play a game for the Allegheny Athletic Association in a match against the Pittsburgh Athletic Club. This is the first recorded instance of a player being paid to participate in a game of American football, although many athletic clubs in the 1880s offered indirect benefits, such as helping players attain employment, giving out trophies or watches that players could pawn for money, or paying double in expense money. Despite these extra benefits, the game had a strict sense of amateurism at the time, and direct payment to players was frowned upon, if not prohibited outright.[29]\n Over time, professional play became increasingly common, and with it came rising salaries and unpredictable player movement, as well as the illegal payment of college players who were still in school. The National Football League (NFL), a group of professional teams that was originally established in 1920 as the American Professional Football Association, aimed to solve these problems. This new league's stated goals included an end to bidding wars over players, prevention of the use of college players, and abolition of the practice of paying players to leave another team.[30] By 1922, the NFL had established itself as America's premier professional football league.[31]\n The dominant form of football at the time was played at the collegiate level. The upstart NFL received a boost to its legitimacy in 1925, however, when an NFL team, the Pottsville Maroons, defeated a team of Notre Dame all-stars in an exhibition game.[32] A greater emphasis on the passing game helped professional football to distinguish itself further from the college game during the late 1930s.[30] Football, in general, became increasingly popular following the 1958 NFL Championship game, a match between the Baltimore Colts and the New York Giants that is still referred to as the \"Greatest Game Ever Played\". The game, a 23–17 overtime victory by the Colts, was seen by millions of television viewers and had a major influence on the popularity of the sport. This, along with the innovations introduced by the new American Football League (AFL) in the early 1960s, helped football to become the most popular sport in the United States by the mid-1960s.[33]\n The rival AFL arose in 1960 and challenged the NFL's dominance. The AFL began in relative obscurity but eventually thrived, with an initial television contract with the ABC television network. The AFL's existence forced the conservative NFL to expand to Dallas and Minnesota in an attempt to destroy the new league. Meanwhile, the AFL introduced many new features to professional football in the United States: official time was kept on a scoreboard clock rather than on a watch in the referee's pocket, as the NFL did; optional two-point conversions by pass or run after touchdowns; names on the jerseys of players; and several others, including expansion of the role of minority players, actively recruited by the league in contrast to the NFL. The AFL also signed several star college players who had also been drafted by NFL teams. Competition for players heated up in 1965, when the AFL New York Jets signed rookie Joe Namath to a then-record $437,000 contract (equivalent to $3.11 million in 2022[34]). A five-year, $40 million  NBC television contract followed, which helped to sustain the young league. The bidding war for players ended in 1966 when NFL owners approached the AFL regarding a merger, and the two leagues agreed on one that took full effect in 1970. This agreement provided for a common draft that would take place each year, and it instituted an annual World Championship game to be played between the champions of each league. This championship game began play at the end of the 1966 season. Once the merger was completed, it was no longer a championship game between two leagues and reverted to the NFL championship game, which came to be known as the Super Bowl.[35]\n College football maintained a tradition of postseason bowl games. Each bowl game was associated with a particular conference and earning a spot in a bowl game was the reward for winning a conference. This arrangement was profitable, but it tended to prevent the two top-ranked teams from meeting in a true national championship game, as they would normally be committed to the bowl games of their respective conferences. Several systems have been used since 1992 to determine a national champion of college football. The first was the Bowl Coalition, in place from 1992 to 1994. This was replaced in 1995 by the Bowl Alliance, which gave way in 1997 to the Bowl Championship Series (BCS).[36] The BCS arrangement proved to be controversial, and was replaced in 2014 by the College Football Playoff (CFP).[37][38]\n A football game is played between two teams of 11 players each.[39][40][41] Playing with more on the field is punishable by a penalty.[39][42][43] Teams may substitute any number of their players between downs;[44][45][46] this \"platoon\" system replaced the original system, which featured limited substitution rules, and has resulted in teams utilizing specialized offensive, defensive and special teams units.[47] The number of players allowed on an active roster varies by league; the NFL has a 53-man roster,[48] while NCAA Division I allows teams to have 63 scholarship players in the FCS and 85 scholarship players in the FBS, respectively.[49]\n Individual players in a football game must be designated with a uniform number between 1 and 99, though some teams may \"retire\" certain numbers, making them unavailable to players. NFL teams are required to number their players by a league-approved numbering system, and any exceptions must be approved by the commissioner.[39] NCAA and NFHS teams are \"strongly advised\" to number their offensive players according to a league-suggested numbering scheme.[50][51]\n Although the sport is played almost exclusively by men, women are eligible to play in high school, college, and professional football. No woman has ever played in the NFL, but women have played in high school and college football games.[52] In 2018, 1,100 of the 225,000 players in Pop Warner Little Scholars youth football were girls, and around 11% of the 5.5 million Americans who report playing tackle football are female according to the Sports and Fitness Industry Association.[53]\n The role of the offensive unit is to advance the football down the field with the ultimate goal of scoring a touchdown.[54]\n The offensive team must line up in a legal formation before they can snap the ball. An offensive formation is considered illegal if there are more than four players in the backfield or fewer than five players numbered 50–79 on the offensive line.[40][55][56] Players can line up temporarily in a position whose eligibility is different from what their number permits as long as they report the change immediately to the referee, who then informs the defensive team of the change.[57] Neither team's players, except the center (C), are allowed to line up in or cross the neutral zone until the ball is snapped. Interior offensive linemen are not allowed to move until the snap of the ball.[58]\n The main backfield positions are the quarterback (QB), halfback\/tailback (HB\/TB), and fullback (FB). The quarterback is the leader of the offense. Either the quarterback or a coach calls the plays. Quarterbacks typically inform the rest of the offense of the play in the huddle before the team lines up. The quarterback lines up behind the center to take the snap and then hands the ball off, throws it, or runs with it.[54]\n The primary role of the halfback, also known as the running back or tailback, is to carry the ball on running plays. Halfbacks may also serve as receivers. Fullbacks tend to be larger than halfbacks and function primarily as blockers, but they are sometimes used as runners in short-yardage or goal-line situations.[59] They are seldom used as receivers.[60]\n The offensive line (OL) consists of several players whose primary function is to block members of the defensive line from tackling the ball carrier on running plays or sacking the quarterback on passing plays.[59] The leader of the offensive line is the center, who is responsible for snapping the ball to the quarterback, blocking,[59] and for making sure that the other linemen do their jobs during the play.[61] On either side of the center are the guards (G), while tackles (T) line up outside the guards.\n The principal receivers are the wide receivers (WR) and the tight ends (TE).[62] Wide receivers line up on or near the line of scrimmage, split outside the line. The main goal of the wide receiver is to catch passes thrown by the quarterback,[59] but they may also function as decoys or as blockers during running plays. Tight ends line up outside the tackles and function both as receivers and as blockers.[59]\n The role of the defense is to prevent the offense from scoring by tackling the ball carrier or by forcing turnovers (interceptions or fumbles).[54]\n The defensive line (DL) consists of defensive ends (DE) and defensive tackles (DT). Defensive ends line up on the ends of the line, while defensive tackles line up inside, between the defensive ends. The primary responsibilities of defensive ends and defensive tackles are to stop running plays on the outside and inside, respectively, to pressure the quarterback on passing plays, and to occupy the line so that the linebackers can break through.[59]\n Linebackers line up behind the defensive line but in front of the defensive backfield. They are divided into two types: middle linebackers (MLB) and outside linebackers (OLB). Linebackers tend to serve as the defensive leaders and call the defensive plays, given their vantage point of the offensive backfield. Their roles include defending the run, pressuring the quarterback, and tackling backs, wide receivers, and tight ends in the passing game.[63]\n The defensive backfield, often called the secondary, consists of cornerbacks (CB) and safeties (S). Safeties are themselves divided into free safeties (FS) and strong safeties (SS).[59] Cornerbacks line up outside the defensive formation, typically opposite a receiver to be able to cover them. Safeties line up between the cornerbacks but farther back in the secondary. Safeties tend to be viewed as \"the last line of defense\" and are responsible for stopping deep passing plays as well as breakout running plays.[59]\n The special teams unit is responsible for all kicking plays. The special teams unit of the team in control of the ball tries to execute field goal (FG) attempts, punts, and kickoffs, while the opposing team's unit will aim to block or return them.[54]\n Three positions are specific to the field goal and PAT (point-after-touchdown) unit: the placekicker (K or PK), holder (H), and long snapper (LS). The long snapper's job is to snap the football to the holder, who will catch and position it for the placekicker. There is not usually a holder on kickoffs, because the ball is kicked off a tee; however, a holder may be used in certain situations, such as if wind is preventing the ball from remaining upright on the tee. The player on the receiving team who catches the ball is known as the kickoff returner (KR).[64]\n The positions specific to punt plays are the punter (P), long snapper, upback, and gunner. The long snapper snaps the football directly to the punter, who then drops and kicks it before it hits the ground. Gunners line up split outside the line and race down the field, aiming to tackle the punt returner (PR)—the player who catches the punt. Upbacks line up a short distance behind the line of scrimmage, providing additional protection to the punter.[65]\n In football, the winner is the team that has scored more points at the end of the game. There are multiple ways to score in a football game. The touchdown (TD), worth six points, is the most valuable scoring play in American football. A touchdown is scored when a live ball is advanced into, caught, or recovered in the opposing team's end zone.[54] The scoring team then attempts a try, more commonly known as the point(s)-after-touchdown (PAT) or conversion, which is a single scoring opportunity. This is generally attempted from the two- or three-yard line, depending on the level of play. If the PAT is scored by a place kick or drop kick through the goal posts, it is worth one point, typically called the extra point. If the PAT is scored by what would normally be a touchdown, it is worth two points; this is known as a two-point conversion. In general, the extra point is almost always successful, while the two-point conversion is a much riskier play with a higher probability of failure; accordingly, extra point attempts are far more common than two-point conversion attempts.[66]\n A field goal (FG), worth three points, is scored when the ball is place kicked or drop kicked through the uprights and over the crossbars of the defense's goalposts.[67][68][69] In practice, almost all field goal attempts are done via place kick. While drop kicks were common in the early days of the sport, the shape of modern footballs makes it difficult to reliably drop kick the ball. The last successful scoring play by drop kick in the NFL was accomplished in 2006; prior to that, the last successful drop kick had been made in 1941.[70] After a PAT attempt or successful field goal, the scoring team must kick the ball off to the other team.[71]\n A safety is scored when the ball carrier is tackled in his own end zone. Safeties are worth two points, which are awarded to the defense.[54] In addition, the team that conceded the safety must kick the ball to the scoring team via a free kick.[72]\n Football games are played on a rectangular field that measures 120 yards (110 m) long and 53+1⁄3 yards (48.8 m) wide. Lines marked along the ends and sides of the field are known as the end lines and sidelines. Goal lines are marked 10 yards (9.1 m) inward from each end line.[73][74][75]\n Weighted pylons are placed the sidelines on the inside corner of the intersections with the goal lines and end lines. White markings on the field identify the distance from the end zone. Inbound lines, or hash marks, are short parallel lines that mark off 1-yard (0.91 m) increments. Yard lines, which can run the width of the field, are marked every 5 yards (4.6 m). A one-yard-wide line is placed at each end of the field; this line is marked at the center of the two-yard line in professional play and at the three-yard line in college play. Numerals that display the distance from the closest goal line in yards are placed on both sides of the field every ten yards.[73][74][75]\n Goalposts are located at the center of the plane of the two end lines. The crossbar of these posts is 10 feet (3.0 m) above the ground, with vertical uprights at the end of the crossbar 18 feet 6 inches (5.64 m) apart for professional and collegiate play, and 23 feet 4 inches (7.11 m) apart for high school play.[76][77][78] The uprights extend vertically 35 feet (11 m) on professional fields, a minimum of 10 yards (9.1 m) on college fields, and a minimum of 10 feet (3.0 m) on high school fields. Goal posts are padded at the base, and orange ribbons are normally placed at the tip of each upright as indicators of wind strength and direction.[76][77][78]\n The football itself is a prolate spheroid leather ball, similar to the balls used in rugby or Australian rules football.[79] To contain the compressed air within it, a pig's bladder was commonly used before the advent of artificial rubber inside the leather outer shell to sustain crushing forces.[80][81] At all levels of play, the football is inflated to 12+1⁄2 to 13+1⁄2 pounds per square inch (86 to 93 kPa), or just under one atmosphere, and weighs 14 to 15 ounces (400 to 430 g);[78][82][83] beyond that, the exact dimensions vary slightly. In professional play the ball has a long axis of 11 to 11+1⁄4 inches (28 to 29 cm), a long circumference of 28 to 28+1⁄2 inches (71 to 72 cm), and a short circumference of 21 to 21+1⁄4 inches (53 to 54 cm).[84] In college and high school play the ball has a long axis of 10+7⁄8 to 11+7⁄16 inches (27.6 to 29.1 cm), a long circumference of 27+3⁄4 to 28+1⁄2 inches (70 to 72 cm), and a short circumference of 20+3⁄4 to 21+1⁄4 inches (53 to 54 cm).[78][82]\n Football games last for a total of 60 minutes in professional and college play and are divided into two halves of 30 minutes and four quarters of 15 minutes.[85][86] High school football games are 48 minutes in length with two halves of 24 minutes and four quarters of 12 minutes.[87] The two halves are separated by a halftime period, and the first and third quarters are followed by a short break.[85][86][88] Before the game starts, the referee and each team's captain meet at midfield for a coin toss. The visiting team can call either \"heads\" or \"tails\"; the winner of the toss chooses whether to receive or kick off the ball or which goal they wish to defend. They can defer their choice until the second half. Unless the winning team decides to defer, the losing team chooses the option the winning team did not select—to receive, kick, or select a goal to defend to begin the second half. Most teams choose to receive or defer, because choosing to kick the ball to start the game allows the other team to choose which goal to defend.[89] Teams switch goals following the first and third quarters.[90] If a down is in progress when a quarter ends, play continues until the down is completed.[91][92][93] If certain fouls are committed during play while time has expired, the quarter may be extended through an untimed down.[94]\n Games last longer than their defined length due to play stoppages—the average NFL game lasts slightly over three hours.[95] Time in a football game is measured by the game clock. An operator is responsible for starting, stopping and operating the game clock based on the direction of the appropriate official.[85][96] A separate play clock is used to show the amount of time within which the offense must initiate a play. The play clock is set to 25 seconds after certain administrative stoppages in play and to 40 seconds when play is proceeding without such stoppages. If the offense fails to start a play before the play clock reads \"00\", a delay of game foul is called on the offense.[91][97][98]\n There are two main ways the offense can advance the ball: running and passing. In a typical play, the center passes the ball backwards and between their legs to the quarterback in a process known as the snap. The quarterback then either hands the ball off to a running back, throws the ball, or runs with it. The play ends when the player with the ball is tackled or goes out-of-bounds or a pass hits the ground without a player having caught it. A forward pass can be legally attempted only if the passer is behind the line of scrimmage; only one forward pass can be attempted per down.[71] As in rugby, players can also pass the ball backwards at any point during a play.[99] In the NFL, a down also ends immediately if the runner's helmet comes off.[100]\n The offense is given a series of four plays, known as downs. If the offense advances ten or more yards in the four downs, they are awarded a new set of four downs. If they fail to advance ten yards, possession of the football is turned over to the defense. In most situations, if the offense reaches their fourth down they will punt the ball to the other team, which forces them to begin their drive from farther down the field; if they are in field goal range, they might attempt to score a field goal instead.[71] A group of officials, the chain crew, keeps track of both the downs and the distance measurements.[101] On television, a yellow line is electronically superimposed on the field to show the first down line to the viewing audience.[102]\n There are two categories of kicks in football: scrimmage kicks, which can be executed by the offensive team on any down from behind or on the line of scrimmage,[105][106][107] and free kicks.[108][109][110] The free kicks are the kickoff, which starts the first and third quarters and overtime and follows a try attempt or a successful field goal; the safety kick follows a safety.[106][111][112]\n On a kickoff, the ball is placed at the 35-yard line of the kicking team in professional and college play and at the 40-yard line in high school play. The ball may be drop kicked or place kicked. If a place kick is chosen, the ball can be placed on the ground or a tee; a holder may be used in either case. On a safety kick, the kicking team kicks the ball from their own 20-yard line. They can punt, drop kick or place kick the ball, but a tee may not be used in professional play. Any member of the receiving team may catch or advance the ball. The ball may be recovered by the kicking team once it has gone at least ten yards and has touched the ground or has been touched by any member of the receiving team.[113][114][115]\n The three types of scrimmage kicks are place kicks, drop kicks, and punts. Only place kicks and drop kicks can score points.[67][68][69] The place kick is the standard method used to score points,[103] because the pointy shape of the football makes it difficult to reliably drop kick.[103][104] Once the ball has been kicked from a scrimmage kick, it can be advanced by the kicking team only if it is caught or recovered behind the line of scrimmage. If it is touched or recovered by the kicking team beyond this line, it becomes dead at the spot where it was touched.[116][117][118] The kicking team is prohibited from interfering with the receiver's opportunity to catch the ball. The receiving team has the option of signaling for a fair catch, which prohibits the defense from blocking into or tackling the receiver. The play ends as soon as the ball is caught, and the ball may not be advanced.[119][120][121]\n Officials are responsible for enforcing game rules and monitoring the clock. All officials carry a whistle and wear black-and-white striped shirts and black hats except for the referee, whose hat is white. Each carries a weighted yellow flag that is thrown to the ground to signal that a foul has been called. An official who spots multiple fouls will throw their hat as a secondary signal.[122] Women can serve as officials; Sarah Thomas became the NFL's first female official in 2015.[123] The seven officials (of a standard seven-man crew; lower levels of play up to the college level use fewer officials) on the field are each tasked with a different set of responsibilities:[122]\n Another set of officials, the chain crew, are responsible for moving the chains. The chains, consisting of two large sticks with a 10-yard-long chain between them, are used to measure for a first down. The chain crew stays on the sidelines during the game, but if requested by the officials they will briefly bring the chains on to the field to measure. A typical chain crew will have at least three people—two members of the chain crew will hold either of the two sticks, while a third will hold the down marker. The down marker, a large stick with a dial on it, is flipped after each play to indicate the current down and is typically moved to the approximate spot of the ball. The chain crew system has been used for over 100 years and is considered an accurate measure of distance, rarely subject to criticism from either side.[101]\n Football is a full-contact sport, and injuries are relatively common. Most injuries occur during training sessions, particularly ones that involve contact between players.[124] To try to prevent injuries, players are required to wear a set of equipment. At a minimum players must wear a football helmet and a set of shoulder pads, but individual leagues may require additional padding such as thigh pads and guards, knee pads, chest protectors, and mouthguards.[125][126][127] Most injuries occur in the lower extremities, particularly in the knee, but a significant number also affect the upper extremities. The most common types of injuries are strains, sprains, bruises, fractures, dislocations, and concussions.[124]\n Repeated concussions (and possibly sub-concussive head impacts[128]) can increase a person's risk in later life for CTE (chronic traumatic encephalopathy) and mental health issues such as dementia, Parkinson's disease, and depression.[129] Concussions are often caused by helmet-to-helmet or upper-body contact between opposing players, although helmets have prevented more serious injuries such as skull fractures.[130] Various programs are aiming to reduce concussions by reducing the frequency of helmet-to-helmet hits; USA Football's \"Heads Up Football\" program aims to reduce concussions in youth football by teaching coaches and players about the signs of a concussion, the proper way to wear football equipment and ensure it fits, and proper tackling methods that avoid helmet-to-helmet contact.[131] However, a study in the Orthopaedic Journal of Sports Medicine found that Heads Up Football was ineffective; the same study noted that more extensive reforms implemented by Pop Warner Little Scholars and its member teams were effective in significantly reducing concussion rates.[132]\n A 2018 study performed by the VA Boston Healthcare System and the Boston University School of Medicine found that tackle football before age 12 was correlated with earlier onset of symptoms of CTE, but not with symptom severity. More specifically, each year a player played tackle football under age 12 predicted earlier onset of cognitive, behavioral, and mood problems by an average of two and a half years.[133][134][135]\n The National Football League (NFL) and the National Collegiate Athletic Association (NCAA) are the most popular football leagues in the United States.[136] The National Football League was founded in 1920[137] and has since become the largest and most popular sport in the United States.[138] The NFL has the highest average attendance of any sporting league in the world, with an average attendance of 66,960 during the 2011 NFL season.[139] The NFL championship game is called the Super Bowl, and is among the biggest events in club sports worldwide.[140] It is played between the champions of the National Football Conference (NFC) and the American Football Conference (AFC), and its winner is awarded the Vince Lombardi Trophy.[141]\n College football is the third-most popular sport in the United States, behind professional baseball and professional football.[142] The NCAA, the largest collegiate organization, is divided into three Divisions: Division I, Division II and Division III.[143] Division I football is further divided into two subdivisions: the Football Bowl Subdivision (FBS) and the Football Championship Subdivision (FCS).[144] The champions of each level of play are determined through NCAA-sanctioned playoff systems; while the champion of Division I-FBS was historically determined by various polls and ranking systems, the subdivision adopted a four-team playoff system in 2014.[145]\n High school football is the most popular sport in the United States played by boys; over 1.1 million boys participated in the sport from 2007 to 2008 according to a survey by the National Federation of State High School Associations (NFHS). There is a stark contrast in youth football participation between boys and girls. Only one youth football league exists in the United States for girls, the GFL. The NFHS is the largest organization for high school football, with member associations in all 50 states as well as the District of Columbia. USA Football is the governing body for youth and amateur football,[146] and Pop Warner Little Scholars is the largest organization for youth football.[147]\n The most successful league to directly compete with the NFL was the American Football League (AFL), which existed from 1960 to 1969. The AFL became a significant rival in 1964 before signing a five-year, US$36 million television deal with NBC. AFL teams began signing NFL players to contracts, and the league's popularity grew to challenge that of the NFL. The two leagues merged in the 1970 season, and all the AFL teams joined the NFL. An earlier league, the All-America Football Conference (AAFC), was in play from 1946 to 1949. After it had dissolved, two AAFC teams, the Cleveland Browns and the San Francisco 49ers, became members of the NFL; another member, the Baltimore Colts joined the league, but folded after just a year in the NFL.[148]\n Other attempts to start rival leagues since the AFL merged with the NFL in 1970 have been far less successful, as professional football salaries and the NFL's television contracts began to escalate out of the reach of competitors and the NFL covered more of the larger cities. The World Football League (WFL) played for two seasons, in 1974 and 1975, but faced such severe monetary issues it could not pay its players. In its second and final season the WFL attempted to establish a stable credit rating, but the league disbanded before the season could be completed.[149] The United States Football League (USFL) operated for three seasons from 1983 to 1985. Originally not intended as a rival league, the entry of owners who sought marquee talent and to challenge the NFL led to an escalation in salaries and ensuing financial losses. A subsequent US$1.5 billion antitrust lawsuit against the NFL was successful in court, but the league was awarded only $1 in damages, which was automatically tripled to $3 under antitrust law.[150]\n The original XFL, created in 2001 by Vince McMahon, lasted for only one season. Despite television contracts with NBC and UPN, and high expectations, the XFL suffered from low quality of play and poor reception for its use of tawdry professional wrestling gimmicks, which caused initially high ratings and attendance to collapse.[151] The XFL was rebooted in 2020.[152] However, after only five weeks of play, the league's operations slowly came to a close due to the ongoing COVID-19 pandemic,[153] and filed for bankruptcy on April 13.[154] The United Football League (UFL) began in 2009 but folded after suspending its 2012 season amid declining interest and lack of major television coverage.[155] The Alliance of American Football lasted less than one season, unable to keep investors.[156]\n American football leagues exist throughout the world, but the game has yet to achieve the international success and popularity of baseball and basketball.[157] It is not an Olympic sport, but it was a demonstration sport at the 1932 Summer Olympics.[158] At the international level, Canada, Mexico, and Japan are considered to be second-tier, while Austria, Germany, and France would rank among a third tier. These countries rank far below the United States, which is dominant at the international level.[159]\n NFL Europa, the developmental league of the NFL, operated from 1991 to 1992 and then from 1995 to 2007. At the time of its closure, NFL Europa had five teams based in Germany and one in the Netherlands.[160] In Germany, the German Football League (GFL) has 16 teams and has operated for over 40 seasons, with the league's championship game, the German Bowl, closing out each season. The league operates in a promotion and relegation structure with German Football League 2 (GFL2), which also has 16 teams.[161] The BIG6 European Football League functions as a continental championship for Europe. The competition is contested between the top six European teams.[161]\n The United Kingdom also operated several teams within NFL Europe during the League's tenure.[162] The resulting rise in popularity of the sport brought the NFL back to the country in 2007 where they now hold the NFL International Series in London, currently consisting of four regular season games.[163][164] The continuing interest and growth in both the sport and the series has led to the possible formation of a potential NFL franchise in London[165][166][167]\n An American football league system already exists within the UK, the BAFANL, which has run under various guises since 1983. It currently has 70 teams operating across the tiers of contact football in which teams aim to earn promotion to the Division above, with the Premier Division teams competing to win the Britbowl, the annual British Football Bowl game that has been played since 1985.[168][169][170] In 2007, the British Universities American Football League was formed. From 2008, the BUAFL was officially associated with the National Football League (NFL), through its partner organization NFL UK.[171] In 2012, BUAFL's league and teams were absorbed into BUCS after American football became an official BUCS sport.[172] Over the period 2007 to 2014, the BUAFL grew from 42 teams and 2,460 participants to 75 teams and over 4,100 people involved.[173]\n American football federations are present in Africa, the Americas, Asia, Europe, and Oceania; a total of 75 national football federations exist as of 2023[update].[174] The International Federation of American Football (IFAF), an international governing body composed of continental federations, runs tournaments such as the IFAF World Championship, the IFAF Women's World Championship, the IFAF U-19 World Championship, and the Flag Football World Championship. The IFAF also organizes the annual International Bowl game.[175] The IFAF has received provisional recognition from the International Olympic Committee (IOC).[176] Several major obstacles hinder the IFAF goal of achieving status as an Olympic sport. These include the predominant participation of men in international play and the short three-week Olympic schedule. Large team sizes are an additional difficulty, due to the Olympics' set limit of 10,500 athletes and coaches. American football also has an issue with a lack of global visibility. Nigel Melville, the CEO of USA Rugby, noted that \"American football is recognized globally as a sport, but it's not played globally.\" To solve these concerns, major effort has been put into promoting flag football, a modified version of American football, at the international level.[159] Flag football has been shortlisted for appearance at the 2028 Summer Olympics, pending final approval by the International Olympic Committee.[177]\n \"Baseball is still called the national pastime, but football is by far the more popular sport in American society\", according to ESPN.com's Sean McAdam.[178] In a 2014 poll conducted by Harris Interactive, professional football ranked as the most popular sport, and college football ranked third behind only professional football and baseball; 46% of participants ranked some form of the game as their favorite sport. Professional football has ranked as the most popular sport in the poll since 1985, when it surpassed baseball for the first time.[179] Professional football is most popular among those who live in the eastern United States and rural areas, while college football is most popular in the southern United States and among people with graduate and post-graduate degrees.[180] Football is also the most-played sport by high school and college athletes in the United States. As of 2022[update], the National Football Foundation reports nearly 1.04 million high-school athletes play the sport, with another 81,000 college athletes across both the NCAA and the NAIA;[3] in comparison, the second-most played sport, basketball, had around 920,000 participants in high school and 63,000 in college.[181]\n The Super Bowl is the most popular single-day sporting event in the United States,[35] and is among the biggest club sporting events in the world in terms of TV viewership.[140] The NFL made approximately $12 billion in revenue in 2022.[182] Super Bowl games account for eight of the top ten most-watched broadcasts in American history; Super Bowl LVII, played on February 12, 2023, was watched by a record 115.1 million Americans,[183] and is second only to the Apollo 11 moon landing (125 million viewers).[184]\n American football also plays a significant role in American culture. The day on which the Super Bowl is held is considered a de facto national holiday,[185] and in parts of the country like Texas, the sport has been compared to a religion.[186][187] Football is also linked to other holidays; New Year's Day is traditionally the date for several college football bowl games, including the Rose Bowl. However, if New Year's Day is on a Sunday, the bowl games are moved to another date so as not to conflict with the typical NFL Sunday schedule.[188] Thanksgiving football is another American tradition,[189] hosting many high school, college, and professional games.[190] Implicit rules such as playing through pain and sacrificing for the better of the team are promoted in football culture.[191]\n In Canada, the game has a significant following. According to a 2013 poll, 21% of respondents said they followed the NFL \"very closely\" or \"fairly closely\", making it the third-most followed league behind the National Hockey League (NHL) and Canadian Football League (CFL).[192] American football also has a long history in Mexico, which was introduced to the sport in 1896. It was the second-most popular sport in Mexico in the 1950s, with the game being particularly popular in colleges.[193] The Los Angeles Times notes the NFL claims over 16 million fans in Mexico, which places the country third behind the U.S. and Canada.[194] American football is played in Mexico both professionally and as part of the college sports system.[195] A professional league, the Liga de Fútbol Americano Profesional (LFA), was founded in 2016.[196]\n Japan was introduced to the sport in 1934 by Paul Rusch, a teacher and Christian missionary who helped to establish football teams at three universities in Tokyo.[197] Play was halted during World War II by order of Emperor Hirohito, but the sport began growing in popularity again after the war.[198] As of 2010[update], there are more than 400 high school football teams in Japan, with over 15,000 participants, and over 100 teams play in the Kantoh Collegiate Football Association (KCFA).[197] The X-League is the largest American football league in Japan, and the largest American football league in the world to use a promotion-relegation system. Some teams in the X-League, like the Panasonic Impulse, are sponsored by corporations, and all Japanese players on these teams are employed by the corporation. The league operates in separate spring and fall seasons, with each team playing five games. The top eight teams make the playoffs, which are played in Tokyo Dome; the champion is determined by the Rice Bowl.[198]\n Europe is a major target for the expansion of the game by football organizers. In the United Kingdom in the 1980s, the sport was popular, with the 1986 Super Bowl being watched by over four million people (about 1 out of every 14 Britons). Its popularity faded during the 1990s, coinciding with the establishment of the Premier League—top level of the English football league system. According to BBC America, there is a \"social stigma\" surrounding American football in the UK, with many Brits feeling the sport has no right to call itself \"football\" due to the lack of emphasis on kicking.[199] Nonetheless, the sport has retained a following in the United Kingdom; the NFL operates a media network in the country, and since 2007 has hosted the NFL International Series in London. Super Bowl viewership has also rebounded, with over 4.4 million Britons watching Super Bowl XLVI.[200] The sport is played in European countries like Switzerland, which has American football clubs in every major city,[201] and Germany, where the sport has around 45,000 registered amateur players.[195]\n In Brazil, football is a growing sport. It was generally unknown there until the 1980s when a small group of players began playing on Copacabana Beach in Rio de Janeiro. The sport grew gradually with 700 amateur players registering within 20 years. Games were played on the beach with modified rules and without the traditional football equipment due to its lack of availability in Brazil. Eventually, a tournament, the Carioca championship, was founded, with the championship Carioca Bowl played to determine a league champion. The country saw its first full-pad game of football in October 2008.[202] According to The Rio Times, the sport is one of the fastest-growing sports in Brazil and is almost as commonly played as soccer on the beaches of Copacabana and Botafogo.[203]\n Football in Brazil is governed by the Confederação Brasileira de Futebol Americano (CBFA), which had over 5,000 registered players as of November 2013. The sport's increase in popularity has been attributed to games aired on ESPN, which began airing in Brazil in 1992 with Portuguese commentary.[204] The popularity and \"easy accessibility\" of non-contact versions of the sport in Brazil has led to a rise in participation by female players.[203] According to ESPN, the American football audience in Brazil increased 800% between 2013 and 2016. The network, along with Esporte Interativo, airs games there on cable television. Football is often associated in Brazil as being the sport of supermodel Gisele Bündchen's ex-husband Tom Brady. The NFL has expressed interest in having games in the country, and the Super Bowl has become a widely watched event in Brazil at bars and movie theaters.[205]\n Further countries have also expressed interest in football to lesser degrees. The Arab world has expressed growing interest in American football, with many countries in the region being members of IFAF Asia. Jordan and the United Arab Emirates, which are not members of IFAF, have established their own domestic leagues. Egypt established two leagues for the sport, namely the Egyptian League of American Football and the Egyptian Federation of American Football, and Saudi Arabia hosts two teams based out of Jeddah and Yanbu respectively.[206][207] China has additionally been a target for the expansion of the sport, with the Mainland being the home of the Chinese National Football League as well as a growing audience of Super Bowl watchers. Three franchises are also based out of Hong Kong, which prior to the COVID-19 pandemic regularly played mainland teams. NFL games average 900,000 viewers in China, though the league has cited logistical challenges which would prevent teams from playing games akin to abroad games in European countries.[208]\n Canadian football, the predominant form of football in Canada, is closely related to American football—both sports developed from rugby and are considered to be the chief variants of gridiron football.[209] Although both games share a similar set of rules, there are several key rule differences: for example, in Canadian football the field measures 150 by 65 yards (137 by 59 m), including two 20-yard end zones (for a distance between goal lines of 110 yards),[210] teams have three downs instead of four, there are twelve players on each side instead of eleven,[211] fair catches are not allowed, and a rouge, worth a single point is scored if the offensive team kicks the ball out of the defense's end zone.[212] The Canadian Football League (CFL) is the major Canadian league and is the second-most popular sporting league in Canada, behind the National Hockey League.[212] The NFL and CFL had a formal working relationship from 1997 to 2006.[213] The CFL has a strategic partnership with two American football leagues, the German Football League (GFL) and the Liga de Futbol Americano Profesional (LFA).[214] The Canadian rules were developed separately from the American game.\n Indoor football leagues constitute what The New York Times writer Mike Tanier described as the \"most minor of minor leagues.\" Leagues are unstable, with franchises regularly moving from one league to another or merging with other teams, and teams or entire leagues dissolving completely; games are only attended by a small number of fans, and most players are semi-professional athletes. The Indoor Football League is an example of a prominent indoor league.[215] The Arena Football League, which was founded in 1987 and ceased operations in 2019, was one of the longest-lived indoor football leagues.[216] In 2004, the league was called \"America's fifth major sport\" by ESPN The Magazine.[217]\n There are several non-contact variants of football, such as flag football.[218] In flag football the ballcarrier is not tackled; instead, defenders aim to pull a flag tied around the ballcarrier's waist.[219] Another variant, touch football, simply requires the ballcarrier to be touched to be considered downed. Depending on the rules used, a game of touch football may require the ballcarrier be touched with either one or two hands to be considered downed.[220]\n"}
{"key":"Football","link":"https:\/\/en.wikipedia.org\/wiki\/Gridiron_football","headline":"Gridiron football - Wikipedia","content":"Gridiron football (\/ˈɡrɪdaɪərn\/ GRID-iren),[1] also known as North American football,[2] or in North America as simply football, is a family of football team sports primarily played in the United States and Canada. American football, which uses 11 players, is the form played in the United States and the best known form of gridiron football worldwide, while Canadian football, which uses 12 players, predominates in Canada. Other derivative varieties include arena football, flag football and amateur games such as touch and street football. Football is played at professional, collegiate, high school, semi-professional, and amateur levels.\n These sports originated in the 19th century out of older games related to modern rugby football, more specifically rugby union football. Early on, American and Canadian football developed alongside (but independently from) each other; the root of the game known as \"football\" today originates with an 1874 game between Harvard and McGill Universities, following which the American school adopted the Canadian school's more rugby-like rules.[3]\n Over time, Canadian teams adopted features of the American variant of the game and vice versa. Both varieties are distinguished from other football sports by their use of hard plastic helmets and shoulder pads, the forward pass, the system of downs, a number of unique rules and positions, measurement in customary units of yards (even in Canada, which largely metricated in the 1970s), and a distinctive brown leather ball in the shape of a prolate spheroid with pointed ends.\n The international governing body for the sport is the International Federation of American Football (IFAF); although the organization plays all of its international competitions under American rules, it uses a definition of the game that is broad enough that it includes Canadian football under its umbrella, and Football Canada (the governing body for Canadian football) is an IFAF member.\n The sport is typically known as simply \"football\" in the countries where it originated, regardless of the specific variety.[4] Various sources use the term \"North American football\" when discussing the American and Canadian games together, but this term is quite rare.[5][6][7][8]\n The two sports are also sometimes known as \"gridiron football\".[1] The name originated with the sport's once-characteristic playing field: the original American football and Canadian football fields were marked by a series of parallel lines along both the width and length of the field, which produced a checkerboard pattern resembling a cross-hatched cooking gridiron.[9][10][11] The ball would be snapped in the grid in which it was downed on the previous play. By 1920, the grid system was abandoned in favor of the system of yard lines and hash marks used today. \n The International Federation of American Football (IFAF), uses \"American football\" inclusive of Canadian football and other varieties.[12]\n In Australia, American football is often referred to as \"gridiron\" or (in more formal contexts) \"American football\", as \"football\" usually refers to Australian rules football, rugby league or rugby union, similar to how association football is usually called \"soccer\" in Australian English. The governing body for American football in Australia is Gridiron Australia.\n Similarly, in the UK American football is known as American football, as \"football\" is used to refer to soccer.\n The sport developed from informal games played in North America during the 19th century. Early games had a variety of local rules and were generally similar to modern rugby union and soccer. The earliest recorded instance of gridiron football occurred at University of Toronto's University College in November 1861.[3]\n Later in the 1860s, teams from universities were playing each other, leading to more standardized rules and the creation of college football. While several American schools adopted rules based on the soccer rules of the English Football Association, Harvard University held to its traditional \"carrying game\". Meanwhile, McGill University in Montreal used rules based on rugby union. In 1874, Harvard and McGill organized two games using each other's rules. Harvard took a liking to McGill's rugby-style rules and adopted them.[3][13] In turn, they were used when Harvard and Yale University played their first intercollegiate sports game in 1875, after which the rugby-style Canadian game was adopted by Yale players and spectators from Yale and Princeton University.[13] This version of the game was subsequently played with several other U.S. colleges over the next several years.[3]\n American football teams and organizations subsequently adopted new rules which distinguished the game from rugby.[14] Many of these early innovations were the work of Walter Camp, including the sport's line of scrimmage and the system of downs.[15] Another consequential change was the adoption of the forward pass in 1906, which allowed the quarterback to throw the ball forward over the line of scrimmage to a receiver.[16] Canadian football remained akin to rugby for decades, though a progressive faction of players, chiefly based in the western provinces, demanded changes to the game based on the innovations in American football. Over the years, the sport adopted more Americanized rules, though it retained some of its historical features, including a 110-yard (100 m) field, 12-player teams, and three downs instead of four.[17]  Around the same time Camp devised the rules for American football, the Canadian game would develop in the same way (but separately) from the American game; the Burnside rules were instrumental in establishing many of the rules for the modern game.[18]\n The best NFL players are among the highest paid athletes in the world.[19][20]\n This is a minimal description of the game in general, with elements common to all or almost all variants of the game. For more specific rules, see each code's individual articles.\n Prior to the start of a game, a coin toss determines which team will decide if they want to kick off the ball to their opponent, or receive the ball from their opponent. Each team lines up on opposite halves of the field, with a minimum ten yards of space between them for the kickoff. The team receiving the ball can make a fair catch (which stops the play immediately), catch the ball and run it back until the ball carrier is tackled, or, if the ball is kicked out of bounds, let the ball go dead on its own (the last case usually happens when the ball is kicked all the way into or through the opponent's end zone, resulting in a touchback and the ball being brought several yards out of the end zone to begin play). A kicking team can, under special circumstances, attempt to recover its own kick, but the rules of the game make it very difficult to do so reliably, and so this tactic is usually only used as a surprise or desperation maneuver.\n At this point, play from scrimmage begins.  The team in possession of the ball is on offense and the opponent is on defense. The offense is given a set amount of time (up to forty seconds, depending on the governing body), during which the teams can set up a play in a huddle and freely substitute players to set into a formation, in which the offense must remain perfectly still for at least one second (the formation requirement does not apply to Canadian football). At least half of the players (seven in standard American and Canadian football, four in standard indoor ball) on the offense must line up on the line of scrimmage in this formation, including the snapper, who handles the ball before play commences; the rest can (and almost always do) line up behind the line. Neither the offense nor the defense can cross the line of scrimmage before the play commences. Once the formation is set, the snapper snaps the ball to one of the players behind him. (A snapper must snap the ball within 20 to 25 seconds of the official setting the ball back into position after the previous play, and a play clock is kept to enforce the measure.) Once the ball is snapped, the play has commenced, and the offense's goal is to continue advancing the ball toward their opponent's end zone. This can be done either by running with the ball or by a rule unique to football known as the forward pass. In a forward pass, a player from behind the line of scrimmage throws the ball to an eligible receiver (another back or one player on each end of the line), who must catch the ball before it touches the ground. The play stops when a player with the ball touches any part of their body other than hand or foot to the ground, runs out of the boundaries of the field, is obstructed from making further forward progress, or a forward pass hits the ground without being caught (in the last case, the ball returns to the spot it was snapped). To stop play, players on defense are allowed to tackle the ball carrier at any time the ball is in play, provided they do not grab the face mask of the helmet or make helmet-to-helmet contact when doing so. At any time, the player with the ball can attempt a backward, or lateral, pass to any other player in order to keep the ball in play; this is generally rare. Any player on defense can, at any time, attempt to intercept a forward pass in flight, at which point the team gains possession; they can also gain possession by recovering a fumble or stripping the ball away from the ball carrier (a \"forced fumble\"). A typical play can last between five and twenty seconds.\n If any illegal action happens during the play, then the results of the previous play are erased and a penalty is assessed, forcing the offending team to surrender between five and fifteen yards of field to the opponent. Whether this yardage is measured from the original spot of the ball before the play, the spot of the illegal action, or the end of the play depends on the individual foul. The most common penalties include false start (when an offensive player jumps to begin the play before the ball is snapped, a five-yard penalty), holding (the grabbing of a player other than the ball carrier to obstruct their progress; a ten-yard penalty against offensive players and a five-yard penalty against defensive ones), and pass interference (when either a receiver or the defending player pushes or blocks the other to prevent them from catching the pass). A team on offense cannot score points as the direct result of a penalty; a defensive foul committed in the team's own end zone, if the penalty is assessed from the spot of the foul, places the ball at the one-yard line. In contrast, a defensive team can score points as a direct result of a penalty; if the offense commits a foul under the same scenario, the defensive team receives two points and a free kick. In all other circumstances (except for the open-ended and extremely rare unfair act clause), a penalty cannot exceed more than half the distance to the end zone. If the penalty would be less advantageous than the result of the actual play, then the team not committing the penalty can decline it.\n In order to keep play moving, the offense must make a certain amount of progress (10 yards in most leagues) within a certain number of plays (3 in Canada, 4 in the United States), called downs. If the offense does indeed make this progress, a first down is achieved, and the team gets 3 or 4 more plays to achieve another 10 yards. If not, the offense loses possession to their opponent at the spot where the ball is. More commonly, however, the team on offense will, if they have a minimal chance of gaining a first down and have only one play left to do it (fourth down in the U.S., third down in Canada), attempt a scrimmage kick. There are two types of scrimmage kick: a punt is when the ball is released from the punter's hand and kicked downfield as close to the opponent's end zone as possible without entering it; the kicking team loses possession of the ball after the kick and the receiving team can attempt to advance the ball or call a fair catch. The other scrimmage kick is a field goal attempt. This must be attempted by place kick or (more rarely) drop kick, and if the kicked ball passes through the goal set at the edge of the opponent's end zone, the team scores three points. (Four-point field goals have been offered in a few variations of the game under special rules, but the NFL, college and high school football only offer three-point field goals.) In Canada, any kick that goes into the end zone and is not returned, whether it be a punt or a missed field goal, is awarded one single point.\n If the team in possession of the ball, at any time, advances (either by carrying or catching) the ball into the opponent's end zone, it is a touchdown, and the team scores six points and a free play known as a try. In a try, a team attempts to score one or two points (rules vary by each league, but under standard rules, a field goal on a try is worth one point while another touchdown is worth two). At the college and professional levels, the defense can also score on a try, but only on the same scale (thus a botched try the defense returns for a touchdown scores only two points and not six). Kickoffs occur after every touchdown and field goal.\n If a team is in its own end zone and commits a foul, is tackled with the ball, or bats, fumbles, kicks or throws the ball backward out of the field of play through the same end zone, the defense scores a safety, worth two points.\n After a try, safety or field goal, the team that had possession of the ball goes back to the middle of the field and kicks the ball off to their opponent, and play continues as it did in the beginning of the game.\n Play continues until halftime. (Each team switches their side of the field with the other halfway through each half, at the end of a quarter.) After the halftime break, a new kickoff occurs. Whichever team has more points at the end of the game is declared the winner; in the event of a tie, each league has its own rules for overtime to break the tie. Because of the nature of the game, pure sudden-death overtimes have been abolished at all levels of the game as of 2012.\n At all adult levels of the game, a game is 60 timed minutes in length, split into four 15-minute quarters. (High school football uses 12-minute quarters, and the general rule is that the younger the players, the shorter the quarters typically are.) Because of the halftime, quarter breaks, time-outs, the minute warnings (two minutes before the end of a half in the NFL, three minutes in Canadian football), and frequent stoppages of the game clock (the clock stops, for example, after every incomplete pass and any time a ball goes out of bounds), the actual time it takes for a football game to be completed is typically over three hours in the NFL[23] and slightly under three hours in the CFL.[24]\n According to 2017 study on brains of deceased gridiron football players, 99% of tested brains of NFL players, 88% of CFL players, 64% of semi-professional players, 91% of college football players, and 21% of high school football players had various stages of CTE.[25]\n Other common injuries include injuries of legs, arms and lower back.[26][27][28][29]\n"}
{"key":"Football","link":"https:\/\/en.wikipedia.org\/wiki\/Association_football","headline":"Association football - Wikipedia","content":"\n Association football, more commonly known as football or soccer,[a] is a team sport played between two teams of 11 players each, who primarily use their feet to propel a ball around a rectangular field called a pitch. The objective of the game is to score more goals than the opposing team by moving the ball beyond the goal line into a rectangular-framed goal defended by the opposing team. Traditionally, the game has been played over two 45-minute halves, for a total match time of 90 minutes. With an estimated 250 million players active in over 200 countries and territories, it is the world's most popular sport.\n The game of association football is played in accordance with the Laws of the Game, a set of rules that has been in effect since 1863 and maintained by the IFAB since 1886. The game is played with a football that is 68–70 cm (27–28 in) in circumference. The two teams compete to get the ball into the other team's goal (between the posts, under the bar, and across the goal line), thereby scoring a goal. When the ball is in play, the players mainly use their feet, but may use any other part of their body, except for their hands or arms, to control, strike, or pass the ball. Only the goalkeepers may use their hands and arms, and only then within the penalty area. The team that has scored more goals at the end of the game is the winner. There are situations where a goal can be disallowed, such as an offside call or a foul in the build-up to the goal. Depending on the format of the competition, an equal number of goals scored may result in a draw being declared, or the game goes into extra time or a penalty shoot-out.[5]\n Internationally, association football is governed by FIFA. Under FIFA, there are six continental confederations: AFC, CAF, CONCACAF, CONMEBOL, OFC, and UEFA. Of these confederations, CONMEBOL is the oldest one, being founded in 1916. National associations (e.g. The FA or JFA) are responsible for managing the game in their own countries both professionally and at an amateur level, and coordinating competitions in accordance with the Laws of the Game. The most senior and prestigious international competitions are the FIFA World Cup and the FIFA Women's World Cup. The men's World Cup is the most-viewed sporting event in the world, surpassing the Olympic Games.[6] The two most prestigious competitions in European club football are the UEFA Champions League and the UEFA Women's Champions League, which attract an extensive television audience throughout the world. Since 2009, the final of the men's tournament has been the most-watched annual sporting event in the world.[7]\n \n Association football is one of a family of football codes that emerged from various ball games played worldwide since antiquity. Within the English-speaking world, the sport is now usually called \"football\" in Great Britain and most of Ulster in the north of Ireland, whereas people usually call it \"soccer\" in regions and countries where other codes of football are prevalent, such as Australia,[8] Canada, South Africa, most of Ireland (excluding Ulster),[9] and the United States. A notable exception is New Zealand, where in the first two decades of the 21st century, under the influence of international television, \"football\" has been gaining prevalence, despite the dominance of other codes of football, namely rugby union and rugby league.[10]\n The term soccer comes from Oxford \"-er\" slang, which was prevalent at the University of Oxford in England from about 1875, and is thought to have been borrowed from the slang of Rugby School. Initially spelt assoccer (a shortening of \"association\"), it was later reduced to the modern spelling.[11][12] This form of slang also gave rise to rugger for rugby football, fiver and tenner for five pound and ten pound notes, and the now-archaic footer that was also a name for association football.[13] The word soccer arrived at its current form in 1895 and was first recorded in 1889 in the earlier form of socca.[14]\n Kicking ball games arose independently multiple times across multiple cultures.[b] The Chinese competitive game cuju (蹴鞠, literally \"kick ball\"; also known as tsu chu) resembles modern association football.[16] This is the earliest form of the game for which there is scientific evidence, a military manual from the Han dynasty.[17] Cuju players could use any part of the body apart from hands and the intent was to kick a ball through an opening into a net. During the Han dynasty (206 BCE – 220 CE), cuju games were standardised and rules were established.[18] The Silk Road facilitated the transmission of cuju, especially the game popular in the Tang dynasty, the period when the inflatable ball was invented and replaced the stuffed ball.[19] Other East Asian games included kemari in Japan and chuk-guk in Korea, both influenced by cuju.[20][21] Kemari originated after the year 600 during the Asuka period. It was a ceremonial rather than a competitive game, and involved the kicking of a mari, a ball made of animal skin.[22] In North America, pasuckuakohowog was a ball game played by the Algonquians; it was described as \"almost identical to the kind of folk football being played in Europe at the same time, in which the ball was kicked through goals\".[23]\n Phaininda and episkyros were Greek ball games.[17][24] An image of an episkyros player depicted in low relief on a stele of c. 375–400 BCE in the National Archaeological Museum of Athens[15] appears on the UEFA European Championship trophy.[25] Athenaeus, writing in 228 CE, mentions the Roman ball game harpastum. Phaininda, episkyros and harpastum were played involving hands and violence. They all appear to have resembled rugby football, wrestling, and volleyball more than what is recognisable as modern football.[18][26][27][28][29][30] As with pre-codified mob football, the antecedent of all modern football codes, these three games involved more handling the ball than kicking it.[31][32]\n Association football in itself does not have a classical history.[25] Notwithstanding any similarities to other ball games played around the world, FIFA has described that no historical connection exists with any game played in antiquity outside Europe.[33] The history of football in England dates back to at least the eighth century.[34] The modern rules of association football are based on the mid-19th century efforts to standardise the widely varying forms of football played in the public schools of England.\n The Cambridge rules, first drawn up at the University of Cambridge in 1848, were particularly influential in the development of subsequent codes, including association football. The Cambridge rules were written at Trinity College, Cambridge, at a meeting attended by representatives from Eton, Harrow, Rugby, Winchester and Shrewsbury schools. They were not universally adopted. During the 1850s, many clubs unconnected to schools or universities were formed throughout the English-speaking world to play various forms of football. Some came up with their own distinct codes of rules, most notably the Sheffield Football Club, formed by former public school pupils in 1857,[35] which led to the formation of a Sheffield FA in 1867. In 1862, John Charles Thring of Uppingham School also devised an influential set of rules.[36]\n These ongoing efforts contributed to the formation of The Football Association (The FA) in 1863, which first met on the morning of 26 October 1863 at the Freemasons' Tavern in Great Queen Street, London.[37] The only school to be represented on this occasion was Charterhouse. The Freemasons' Tavern was the setting for five more meetings of The FA between October and December 1863; the English FA eventually issued the first comprehensive set of rules named Laws of the Game, forming modern football.[38] The laws included bans on running with the ball in hand and hacking (kicking an opponent in the shins), tripping and holding.[39] Eleven clubs, under the charge of FA secretary Ebenezer Cobb Morley, ratified the original thirteen laws of the game.[37] The sticking point was hacking, which a twelfth club at the meeting, Blackheath FC, had wanted to keep, resulting in them withdrawing from the FA.[37] Other English rugby clubs followed this lead and did not join the FA, and instead in 1871, along with Blackheath, formed the Rugby Football Union. The FA rules included handling of the ball by \"marks\" and the lack of a crossbar, rules which made it remarkably similar to Victorian rules football being developed at that time in Australia. The Sheffield FA played by its own rules until the 1870s, with the FA absorbing some of its rules until there was little difference between the games.[40]\n The world's oldest football competition is the FA Cup, which was founded by the footballer and cricketer Charles W. Alcock, and has been contested by English teams since 1872. The first official international football match also took place in 1872, between Scotland and England in Glasgow, again at the instigation of Alcock. England is also home to the world's first football league, which was founded in Birmingham in 1888 by Aston Villa director William McGregor.[41] The original format contained 12 clubs from the Midlands and Northern England.[42]\n Laws of the Game are determined by the International Football Association Board (IFAB).[43] The board was formed in 1886[44] after a meeting in Manchester of the Football Association, the Scottish Football Association, the Football Association of Wales, and the Irish Football Association. FIFA, the international football body, was formed in Paris in 1904 and declared that they would adhere to the Laws of the Game of the Football Association.[45] The growing popularity of the international game led to the admittance of FIFA representatives to the IFAB in 1913. The board consists of four representatives from FIFA and one representative from each of the four British associations.[46]\n For most of the 20th century, Europe and South America were the dominant regions in association football. The FIFA World Cup, inaugurated in 1930, became the main stage for players of both continents to show their worth and the strength of their national teams.[47] In the second half of the century, the European Cup and the Copa Libertadores were created, and the champions of these two club competitions would contest the Intercontinental Cup to prove which team was the best in the world.[48]\n In the 21st century, South America has continued to produce some of the best footballers in the world,[49] but its clubs have fallen behind the still dominant European clubs, which often sign the best players from Latin America and elsewhere.[47][49] Meanwhile, football has improved in Africa, Asia and North America,[49] and nowadays, these regions are at least on equal grounds with South America in club football,[50] although countries in the Caribbean and Oceania regions (except Australia) have yet to make a mark in international football.[51][52] When it comes to national teams, however, Europeans and South Americans continue to dominate the FIFA World Cup, as no team from any other region has managed to even reach the final.[47][49]\n Football is played at a professional level all over the world. Millions of people regularly go to football stadiums to follow their favourite teams,[53] while billions more watch the game on television or on the internet.[54][55] A very large number of people also play football at an amateur level. According to a survey conducted by FIFA published in 2001, over 240 million people from more than 200 countries regularly play football.[56] Football has the highest global television audience in sport.[57]\n In many parts of the world, football evokes great passions and plays an important role in the life of individual fans, local communities, and even nations. Ryszard Kapuściński says that Europeans who are polite, modest, or humble fall easily into rage when playing or watching football games.[58] The Ivory Coast national football team helped secure a truce to the nation's civil war in 2006[59] and it helped further reduce tensions between government and rebel forces in 2007 by playing a match in the rebel capital of Bouaké, an occasion that brought both armies together peacefully for the first time.[60] By contrast, football is widely considered to have been the final proximate cause for the Football War in June 1969 between El Salvador and Honduras.[61] The sport also exacerbated tensions at the beginning of the Croatian War of Independence of the 1990s, when a match between Dinamo Zagreb and Red Star Belgrade degenerated into rioting in May 1990.[62]\n Women's association football has historically seen opposition, with national associations severely curbing its development and several outlawing it completely. Women may have been playing football for as long as the game has existed. Evidence shows that a similar ancient game (cuju, or tsu chu) was played by women during the Han dynasty (25–220 CE), as female figures are depicted in frescoes of the period playing tsu chu.[63][64] There are also reports of annual football matches played by women in Midlothian, Scotland, during the 1790s.[65][66]\n Association football, the modern game, has documented early involvement of women.[66] In 1863, football governing bodies introduced standardised rules to prohibit violence on the pitch, making it more socially acceptable for women to play.[67] The first match recorded by the Scottish Football Association took place in 1892 in Glasgow.[65] In England, the first recorded game of football between women took place in 1895.[67] Women's football has traditionally been associated with charity games and physical exercise, particularly in the United Kingdom.[68]\n Association football continued to be played by women since the time of the first recorded women's games in the late 19th century.[68][69] The best-documented early European team was founded by activist Nettie Honeyball in England in 1894. It was named the British Ladies' Football Club. Honeyball is quoted as, \"I founded the association late last year [1894], with the fixed resolve of proving to the world that women are not the 'ornamental and useless' creatures men have pictured. I must confess, my convictions on all matters where the sexes are so widely divided are all on the side of emancipation, and I look forward to the time when ladies may sit in Parliament and have a voice in the direction of affairs, especially those which concern them most.\"[70] Honeyball and those like her paved the way for women's football. However, the women's game was frowned upon by the British football associations and continued without their support. It has been suggested that this was motivated by a perceived threat to the \"masculinity\" of the game.[71]\n Women's football became popular on a large scale at the time of the First World War, when female employment in heavy industry spurred the growth of the game, much as it had done for men 50 years earlier. The most successful team of the era was Dick, Kerr Ladies F.C. of Preston, England. The team played in one of the first women's international matches against a French XI team in 1920,[72][73] and also made up most of the England team against a Scottish Ladies XI in the same year, winning 22–0.[65]\n Despite being more popular than some men's football events, with one match seeing a 53,000 strong crowd in 1920,[74][75] women's football in England suffered a blow in 1921 when The Football Association outlawed the playing of the game on association members' pitches,[76] stating that \"the game of football is quite unsuitable for females and should not be encouraged.\"[77] Players and football writers have argued that this ban was, in fact, due to envy of the large crowds that women's matches attracted,[75] and because the FA had no control over the money made from the women's game.[77] The FA ban led to the formation of the short-lived English Ladies Football Association and play moved to rugby grounds.[78] Women's football also faced bans in several other countries, notably in Brazil from 1941 to 1979,[79] in France from 1941 to 1970,[80] and in Germany from 1955 to 1970.[81]\n Restrictions began to be reduced in the 1960s and 1970s. The Italian women's football league was established in 1968.[82] In December 1969, the Women's Football Association was formed in England,[68][83] with the sport eventually becoming the most prominent team sport for women in the United Kingdom.[68] Two unofficial women's World Cups were organised by the FIEFF in 1970 and in 1971.  Also in 1971, Union of European Football Associations (UEFA) members voted to officially recognise women's football,[68] while The Football Association rescinded the ban that prohibited women from playing on association members' pitches in England.[83]\n Women's football still faces many struggles, but its worldwide growth[84] has seen major competitions being launched at both the national and international levels, mirroring the men's competitions. The FIFA Women's World Cup was inaugurated in 1991: the first tournament was held in China, featuring 12 teams from the respective six confederations.  The World Cup has been held every four years since;[85] by the 2019 FIFA Women's World Cup in France, it had expanded to 24 national teams, and 1.12 billion viewers watched the competition.[86] Women's football has been an Olympic event since 1996.[87]\n North America is the dominant region in women's football, with the United States winning most FIFA Women's World Cups and Olympic tournaments. Europe and Asia come second and third in terms of international success,[88][89] and the women's game has been improving in South America.[90]\n Association football is played in accordance with a set of rules known as the Laws of the Game. The game is played using a spherical ball of 68–70 cm (27–28 in) circumference,[91] known as the football (or soccer ball). Two teams of eleven players each compete to get the ball into the other team's goal (between the posts and under the bar), thereby scoring a goal. The team that has scored more goals at the end of the game is the winner; if both teams have scored an equal number of goals then the game is a draw. Each team is led by a captain who has only one official responsibility as mandated by the Laws of the Game: to represent their team in the coin toss before kick-off or penalty kicks.[5]\n The primary law is that players other than goalkeepers may not deliberately handle the ball with their hands or arms during play, though they must use both their hands during a throw-in restart. Although players usually use their feet to move the ball around, they may use any part of their body (notably, \"heading\" with the forehead)[92] other than their hands or arms.[93] Within normal play, all players are free to play the ball in any direction and move throughout the pitch, though players may not pass to teammates who are in an offside position.[94]\n During gameplay, players attempt to create goal-scoring opportunities through individual control of the ball, such as by dribbling, passing the ball to a teammate, and by taking shots at the goal, which is guarded by the opposing goalkeeper. Opposing players may try to regain control of the ball by intercepting a pass or through tackling the opponent in possession of the ball; however, physical contact between opponents is restricted. Football is generally a free-flowing game, with play stopping only when the ball has left the field of play or when play is stopped by the referee for an infringement of the rules. After a stoppage, play recommences with a specified restart.[95]\n At a professional level, most matches produce only a few goals. For example, the 2022–23 season of the English Premier League produced an average of 2.85 goals per match.[96] The Laws of the Game do not specify any player positions other than goalkeeper,[97] but a number of specialised roles have evolved.[98] Broadly, these include three main categories: strikers, or forwards, whose main task is to score goals; defenders, who specialise in preventing their opponents from scoring; and midfielders, who dispossess the opposition and keep possession of the ball to pass it to the forwards on their team. Players in these positions are referred to as outfield players, to distinguish them from the goalkeeper.\n These positions are further subdivided according to the area of the field in which the player spends the most time. For example, there are central defenders and left and right midfielders. The ten outfield players may be arranged in any combination. The number of players in each position determines the style of the team's play; more forwards and fewer defenders creates a more aggressive and offensive-minded game, while the reverse creates a slower, more defensive style of play. While players typically spend most of the game in a specific position, there are few restrictions on player movement, and players can switch positions at any time.[99] The layout of a team's players is known as a formation. Defining the team's formation and tactics is usually the prerogative of the team's manager.[100]\n There are 17 laws in the official Laws of the Game, each containing a collection of stipulations and guidelines. The same laws are designed to apply to all levels of football for both sexes, although certain modifications for groups such as juniors, seniors and people with physical disabilities are permitted.[c] The laws are often framed in broad terms, which allow flexibility in their application depending on the nature of the game. The Laws of the Game are published by FIFA, but are maintained by the IFAB.[101] In addition to the seventeen laws, numerous IFAB decisions and other directives contribute to the regulation of association football.[102][103] Within the United States, Major League Soccer used a distinct ruleset during the 1990s[104] and the National Federation of State High School Associations and National Collegiate Athletic Association still use rulesets that are comparable to, but different from, the IFAB Laws.\n Each team consists of a maximum of eleven players (excluding substitutes), one of whom must be the goalkeeper. Competition rules may state a minimum number of players required to constitute a team, which is usually seven. Goalkeepers are the only players allowed to play the ball with their hands or arms, provided they do so within the penalty area in front of their own goal. Though there are a variety of positions in which the outfield (non-goalkeeper) players are strategically placed by a coach, these positions are not defined or required by the Laws.[97]\n The basic equipment or kit players are required to wear includes a shirt, shorts, socks, footwear and adequate shin guards. An athletic supporter and protective cup is highly recommended for male players by medical experts and professionals.[105][106] Headgear is not a required piece of basic equipment, but players today may choose to wear it to protect themselves from head injury.[107] Players are forbidden to wear or use anything that is dangerous to themselves or another player, such as jewellery or watches. The goalkeeper must wear clothing that is easily distinguishable from that worn by the other players and the match officials.[108]\n A number of players may be replaced by substitutes during the course of the game. The maximum number of substitutions permitted in most competitive international and domestic league games is five in 90 minutes,[109] with each team being allowed one more if the game should go into extra-time; the permitted number may vary in other competitions or in friendly matches. Common reasons for a substitution include injury, tiredness, ineffectiveness, a tactical switch, or timewasting at the end of a finely poised game. In standard adult matches, a player who has been substituted may not take further part in a match.[110] IFAB recommends \"that a match should not continue if there are fewer than seven players in either team\". Any decision regarding points awarded for abandoned games is left to the individual football associations.[111]\n \nA game is officiated by a referee, who has \"full authority to enforce the Laws of the Game in connection with the match to which he has been appointed\" (Law 5), and whose decisions are final. The referee is assisted by two assistant referees. In many high-level games there is also a fourth official who assists the referee and may replace another official should the need arise.[112]\n Goal line technology is used to measure if the whole ball has crossed the goal-line thereby determining whether a goal has been scored or not; this was brought in to prevent controversy. Video assistant referees (VAR) have also been increasingly introduced in high-level matches to assist officials through video replays to correct clear and obvious mistakes. There are four types of calls that can be reviewed: mistaken identity in awarding a red or yellow card, goals and whether there was a violation during the buildup, direct red card decisions, and penalty decisions.[113]\n The ball is spherical with a circumference of between 68 and 70 cm (27 and 28 in), a weight in the range of 410 to 450 g (14 to 16 oz), and a pressure between 0.6 and 1.1 standard atmospheres (8.5 and 15.6 pounds per square inch) at sea level. In the past the ball was made up of leather panels sewn together, with a latex bladder for pressurisation, but modern balls at all levels of the game are now synthetic.[114][115]\n As the Laws were formulated in England, and were initially administered solely by the four British football associations within IFAB, the standard dimensions of a football pitch were originally expressed in imperial units. The Laws now express dimensions with approximate metric equivalents (followed by traditional units in brackets), though use of imperial units remains popular in English-speaking countries with a relatively recent history of metrication (or only partial metrication), such as Britain.[116]\n The length of the pitch, or field, for international adult matches is in the range of 100–110 m (110–120 yd) and the width is in the range of 64–75 m (70–80 yd). Fields for non-international matches may be 90–120 m (100–130 yd) in length and 45–90 m (50–100 yd) in width, provided the pitch does not become square. In 2008, the IFAB initially approved a fixed size of 105 m (115 yd) long and 68 m (74 yd) wide as a standard pitch dimension for international matches;[117] however, this decision was later put on hold and was never actually implemented.[118]\n The longer boundary lines are touchlines, while the shorter boundaries (on which the goals are placed) are goal lines. A rectangular goal is positioned on each goal line, midway between the two touchlines.[119] The inner edges of the vertical goal posts must be 7.32 m (24 ft) apart, and the lower edge of the horizontal crossbar supported by the goal posts must be 2.44 m (8 ft) above the ground. Nets are usually placed behind the goal, but are not required by the Laws.[120]\n In front of the goal is the penalty area. This area is marked by the goal line, two lines starting on the goal line 16.5 m (18 yd) from the goalposts and extending 16.5 m (18 yd) into the pitch perpendicular to the goal line, and a line joining them. This area has a number of functions, the most prominent being to mark where the goalkeeper may handle the ball and where a penalty foul by a member of the defending team becomes punishable by a penalty kick. Other markings define the position of the ball or players at kick-offs, goal kicks, penalty kicks and corner kicks.[121]\n A standard adult football match consists of two halves of 45 minutes each. Each half runs continuously, meaning that the clock is not stopped when the ball is out of play. There is usually a 15-minute half-time break between halves. The end of the match is known as full-time.[122] The referee is the official timekeeper for the match, and may make an allowance for time lost through substitutions, injured players requiring attention, or other stoppages. This added time is called \"additional time\" in FIFA documents,[123][124] but is most commonly referred to as stoppage time or injury time, while lost time can also be used as a synonym. The duration of stoppage time is at the sole discretion of the referee. Stoppage time does not fully compensate for the time in which the ball is out of play, and a 90-minute game typically involves about an hour of \"effective playing time\".[125][126] The referee alone signals the end of the match. In matches where a fourth official is appointed, towards the end of the half, the referee signals how many minutes of stoppage time they intend to add. The fourth official then informs the players and spectators by holding up a board showing this number. The signalled stoppage time may be further extended by the referee.[122] Added time was introduced because of an incident which happened in 1891 during a match between Stoke and Aston Villa. Trailing 1–0 with two minutes remaining, Stoke were awarded a penalty kick. Villa's goalkeeper deliberately kicked the ball out of play; by the time it was recovered, the clock had run out and the game was over, leaving Stoke unable to attempt the penalty.[127] The same law also states that the duration of either half is extended until a penalty kick to be taken or retaken is completed; thus, no game can end with an uncompleted penalty.[128]\n In league competitions, games may end in a draw. In knockout competitions where a winner is required, various methods may be employed to break such a deadlock; some competitions may invoke replays.[129] A game tied at the end of regulation time may go into extra time, which consists of two further 15-minute periods. If the score is still tied after extra time, some competitions allow the use of penalty shoot-outs (known officially in the Laws of the Game as \"kicks from the penalty mark\") to determine which team will progress to the next stage of the tournament or be the champion. Goals scored during extra time periods count towards the final score of the game, but kicks from the penalty mark are only used to decide the team that progresses to the next part of the tournament, with goals scored in a penalty shoot-out not making up part of the final score.[5]\n In competitions using two-legged matches, each team competes at home once, with an aggregate score from the two matches deciding which team progresses. Where aggregates are equal, the away goals rule may be used to determine the winners, in which case the winner is the team that scored the most goals in the leg they played away from home. If the result is still equal, extra time and potentially a penalty shoot-out are required.[5]\n Under the Laws, the two basic states of play during a game are ball in play and ball out of play. From the beginning of each playing period with a kick-off until the end of the playing period, the ball is in play at all times, except when either the ball leaves the field of play, or play is stopped by the referee. When the ball becomes out of play, play is restarted by one of eight restart methods depending on how it went out of play:\n A foul occurs when a player commits an offence listed in the Laws of the Game while the ball is in play. The offences that constitute a foul are listed in Law 12. Handling the ball deliberately, tripping an opponent, or pushing an opponent, are examples of \"penal fouls\", punishable by a direct free kick or penalty kick depending on where the offence occurred. Other fouls are punishable by an indirect free kick.[93]\n The referee may punish a player's or substitute's misconduct by a caution (yellow card) or dismissal (red card). A second yellow card in the same game leads to a red card, which results in a dismissal. A player given a yellow card is said to have been \"booked\", the referee writing the player's name in their official notebook. If a player has been dismissed, no substitute can be brought on in their place and the player may not participate in further play. Misconduct may occur at any time, and while the offences that constitute misconduct are listed, the definitions are broad. In particular, the offence of \"unsporting behaviour\" may be used to deal with most events that violate the spirit of the game, even if they are not listed as specific offences. A referee can show a yellow or red card to a player, substitute, substituted player, and to non-players such as managers and support staff.[93][135]\n Rather than stopping play, the referee may allow play to continue if doing so will benefit the team against which an offence has been committed. This is known as \"playing an advantage\".[136] The referee may \"call back\" play and penalise the original offence if the anticipated advantage does not ensue within \"a few seconds\". Even if an offence is not penalised due to advantage being played, the offender may still be sanctioned for misconduct at the next stoppage of play.[137]\n The referee's decision in all on-pitch matters is considered final.[138] The score of a match cannot be altered after the game, even if later evidence shows that decisions (including awards\/non-awards of goals) were incorrect.\n Along with the general administration of the sport, football associations and competition organisers also enforce good conduct in wider aspects of the game, dealing with issues such as comments to the press, clubs' financial management, doping, age fraud and match fixing. Most competitions enforce mandatory suspensions for players who are sent off in a game.[139] Some on-field incidents, if considered very serious (such as allegations of racial abuse), may result in competitions deciding to impose heavier sanctions than those normally associated with a red card.[d] Some associations allow for appeals against player suspensions incurred on-field if clubs feel a referee was incorrect or unduly harsh.[139]\n Sanctions for such infractions may be levied on individuals or on clubs as a whole. Penalties may include fines, point deductions (in league competitions) or even expulsion from competitions. For example, the English Football League deduct 12 points from any team that enters financial administration.[140] Among other administrative sanctions are penalties against game forfeiture. Teams that had forfeited a game or had been forfeited against would be awarded a technical loss or win.\n The recognised international governing body of football (and associated games, such as futsal and beach soccer)[c] is FIFA. The FIFA headquarters are located in Zürich, Switzerland. Six regional confederations are associated with FIFA; these are:[141]\n National associations (or national federations) oversee football within individual countries. These are generally synonymous with sovereign states (for example, the Cameroonian Football Federation in Cameroon), but also include a smaller number of associations responsible for sub-national entities or autonomous regions (for example, the Scottish Football Association in Scotland). 211 national associations are affiliated both with FIFA and with their respective continental confederations.[141] Other national associations may be members of continental confederations but otherwise not participate in FIFA competitions.[142]\n While FIFA is responsible for arranging competitions and most rules related to international competition, the actual Laws of the Game are set by the IFAB, where each of the UK Associations has one vote, while FIFA collectively has four votes.[46]\n \n International competitions in association football principally consist of two varieties: competitions involving representative national teams or those involving clubs based in multiple nations and national leagues. International football, without qualification, most often refers to the former. In the case of international club competition, it is the country of origin of the clubs involved, not the nationalities of their players, that renders the competition international in nature.\n The major international competition in football is the World Cup, organised by FIFA. This competition has taken place every four years since 1930, with the exception of the 1942 and 1946 tournaments, which were cancelled because of World War II. As of 2022, over 200 national teams compete in qualifying tournaments within the scope of continental confederations for a place in the finals.[143] The finals tournament, held every four years, involved 32 national teams (expanding to 48 teams for the 2026 tournament) competing over a four-week period.[144][e] The World Cup is the most prestigious association football tournament as well as the most widely viewed and followed sporting event in the world, exceeding even the Olympic Games; the cumulative audience of all matches of the 2006 FIFA World Cup was estimated to be 26.29 billion with an estimated 715.1 million people watching the final match, one-ninth of the entire population of the planet.[145][146][147][148] The 1958 World Cup saw the emergence of Pelé as a global sporting star, a period that coincided with \"the explosive spread of television, which massively amplified his presence everywhere\".[149] The current champions are Argentina, who won their third title at the 2022 tournament in Qatar.[150] The FIFA Women's World Cup has been held every four years since 1991. Under the tournament's current format that was expanded in 2023, national teams vie for 31 slots in a three-year qualification phase, while the host nation's team enters automatically as the 32nd slot.[151] The current champions are Spain, after winning their first title in the 2023 tournament.[152]\n There has been a football tournament at every Summer Olympic Games since 1900, except at the 1932 games in Los Angeles when FIFA and the IOC had disagreed over the status of amateur players.[153][154] Before the inception of the World Cup, the Olympics (especially during the 1920s) were the most prestigious international event. Originally, the tournament was for amateurs only.[45] As professionalism spread around the world, the gap in quality between the World Cup and the Olympics widened. The countries that benefited most were the Soviet Bloc countries of Eastern Europe, where top athletes were state-sponsored while retaining their status as amateurs. Between 1948 and 1980, 23 out of 27 Olympic medals were won by Eastern Europe, with only Sweden (gold in 1948 and bronze in 1952), Denmark (bronze in 1948 and silver in 1960) and Japan (bronze in 1968) breaking their dominance. For the 1984 Los Angeles Games, the IOC allowed professional players to compete. Since 1992, male competitors must be under 23 years old, although since 1996, three players over the age of 23 have been allowed per squad.[155] A women's tournament was added in 1996; in contrast to the men's event, full international sides without age restrictions play the women's Olympic tournament.[156]\n After the World Cup, the most important international football competitions are the continental championships, which are organised by each continental confederation and contested between national teams. These are the European Championship (UEFA), the Copa América (CONMEBOL), the African Cup of Nations (CAF), the Asian Cup (AFC), the CONCACAF Gold Cup (CONCACAF) and the OFC Nations Cup (OFC).[157] These competitions are not strictly limited to members of the continental confederations, with guest teams from other continents sometimes invited to compete.[158] The FIFA Confederations Cup was contested by the winners of all six continental championships, the current FIFA World Cup champions, and the country which was hosting the next World Cup. This was generally regarded as a warm-up tournament for the upcoming FIFA World Cup and did not carry the same prestige as the World Cup itself.[157] The tournament was discontinued following the 2017 edition with its calendar slot replaced by an expanded FIFA Club World Cup.[159] The UEFA Nations League and the CONCACAF Nations League were introduced in the late 2010s to replace international friendlies during the two-year cycle between major tournaments.[160]\n The most prestigious competitions in club football are the respective continental championships, which are generally contested between national champions, for example, the UEFA Champions League in Europe and the Copa Libertadores in South America. The winners of each continental competition contest the FIFA Club World Cup.[161]\n The governing bodies in each country operate league systems in a domestic season, normally comprising several divisions, in which the teams gain points throughout the season depending on results. Teams are placed into tables, placing them in order according to points accrued. Most commonly, each team plays every other team in its league at home and away in each season, in a round-robin tournament. At the end of a season, the top team is declared the champion. The top few teams may be promoted to a higher division, and one or more of the teams finishing at the bottom are relegated to a lower division.[163]\n The teams finishing at the top of a country's league may also be eligible to play in international club competitions in the following season. The main exceptions to this system occur in some Latin American leagues, which divide football championships into two sections named Apertura and Clausura (Spanish for Opening and Closing), awarding a champion for each.[164] Most countries supplement the league system with one or more \"cup\" competitions organised on a knock-out basis. These include the domestic cup, which may be open to all eligible teams in a country's league system—both professional and amateur—and is organised by the national federation.[165]\n Some countries' top divisions feature highly-paid star players; in smaller countries, lower divisions, and many women's clubs, players may be part-timers with a second job, or amateurs. The five top European leagues – Premier League (England),[166] Bundesliga (Germany), La Liga (Spain), Serie A (Italy), and Ligue 1 (France) – attract most of the world's best players and, during the 2006–07 season, each of these leagues had a total wage cost in excess of €600 million.[167][needs update] These leagues also generated a combined €17.2 billion in revenue in the 2021–22 season from television contracts, matchday tickets, sponsorships, and other sources.[168]\n"}
